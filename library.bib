Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Layman2006g,
author = {Layman, Lucas and Williams, Laurie and Damian, Daniela and Bures, Hynek},
journal = {Information and Software Technology},
keywords = {mypubs},
mendeley-tags = {mypubs},
number = {9},
pages = {781--794},
title = {{Essential Communication Practices for Extreme Programming in a Global Software Development Team}},
volume = {48},
year = {2006}
}
@inproceedings{Edwards2003b,
abstract = {Despite our best efforts and intentions as educators, student programmers continue to struggle in acquiring comprehension and analysis skills. Students believe that once a program runs on sample data, it is correct; most programming errors are reported by the compiler; when a program misbehaves, shuffling statements and tweaking expressions to see what happens is the best debugging approach. This paper presents a new vision for computer science education centered around the use of test-driven development in all programming assignments, from the beginning of CS1. A key element to the strategy is comprehensive, automated evaluation of student work, in terms of correctness, the thoroughness and validity of the student's tests, and an automatic coding style assessment performed using industrial-strength tools. By systematically applying the strategy across the curriculum as part of a student's regular programming activities, and by providing rapid, concrete, useful feedback that students find valuable, it is possible to induce a cultural shift in how students behave.},
annote = {Rethinking computer science education from a test-first perspective},
author = {Edwards, S H},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {148--155},
publisher = {ACM Press New York, NY, USA},
title = {{Rethinking computer science education from a test-first perspective}},
url = {citeulike-article-id:3934594 {\#}},
year = {2003}
}
@article{Buck2000,
abstract = {We have recognized that the natural tendency to teach according to the structure of one's own understanding runs contrary to established models of cognitive development. Bloom's Taxonomy has provided a basis for establishing a more efficacious pedagogy. Emphasizing a hierarchical progression of skill sets and gradual learning through example, our approach advocates teaching software development from the inside/out rather than beginning with either console apps or monolithic designs.},
address = {Austin, TX, USA},
author = {Buck, Duane and Stucki, David J.},
doi = {10.1145/331795.331817},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buck, Stucki - 2000 - Design early considered harmful.pdf:pdf},
isbn = {1581132131},
issn = {00978418},
journal = {ACM SIGCSE Bulletin},
keywords = {Bloom's taxonomy,CS1,CS2,control structure diagrams,formal specifications,inside/out pedagogy},
mendeley-tags = {CS2},
number = {1},
pages = {75--79},
publisher = {ACM Press},
title = {{Design early considered harmful}},
url = {http://portal.acm.org/citation.cfm?doid=330908.331817},
volume = {32},
year = {2000}
}
@book{Jancowicz2004,
address = {West Sussex, England},
author = {Jankowicz, Devi},
isbn = {0-470-85404-9},
publisher = {John Wiley {\&} Sons Ltd.},
title = {{The Easy Guide to Repertory Grids}},
year = {2004}
}
@inproceedings{Linstead2009,
address = {Vancouver, BC},
author = {Linstead, Erik and Baldi, Pierre},
booktitle = {2009 6th IEEE International Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2009.5069486},
isbn = {978-1-4244-3493-0},
keywords = {Bayesian methods,Coherence,Computer science,Databases,GNOME Bugzilla database,GNOME bug report coherence mining,Information theory,Linear discriminant analysis,Software measurement,Text mining,Vocabulary,XML,data mining,debugging process,information-theoretic measure,latent Dirichlet allocation,program debugging,software project,statistical analysis,statistical text mining algorithm,statistical topic model,text analysis},
language = {English},
month = {may},
pages = {99--102},
publisher = {IEEE},
title = {{Mining the coherence of GNOME bug reports with statistical topic models}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5069486},
year = {2009}
}
@inproceedings{5654982,
abstract = {A number of homeland security occupations require vigilance to potentially subtle events in the environment, with high stakes for missing infrequent but consequential items. Sustained vigilance can be required for long periods of time or when sleep-deprived or physically inactive, compounding the difficulty of this task. Research on sustained vigilance has largely focused on tasks such as driving, air traffic control, medical screening, and military specialties, but the findings closely apply also to other homeland security-related occupations. A research area that has received relatively little attention, but is of critical importance to homeland security, involves the role of individual differences in vigilance. Prior research suggests that certain individuals are better than others at searching for rarely present targets over long time periods, yet what is driving this effect remains unclear. Further, it is not known whether or not sustained vigilance can be improved through training. This research team is studying two research questions: Are there individual differences in the inherent ability to sustain vigilance? and What are the most effective approaches for training and improving sustained vigilance for rare items or events?. The intent is to employ tasks (primarily visual identification and gross motor tests) that readily translate to the relevant homeland security occupations requiring sustained vigilance.},
author = {Hubal, R and Mitroff, S R and Cain, M S and Scott, B and DeWitt, R},
booktitle = {Technologies for Homeland Security (HST), 2010 IEEE International Conference on},
doi = {10.1109/THS.2010.5654982},
keywords = {baggage security assessment,gross motor test,homel},
pages = {543--548},
title = {{Simulating a vigilance task: Extensible technology for baggage security assessment and training}},
year = {2010}
}
@inproceedings{Enbody2009,
abstract = {How suitable is a Python-based CS1 course as preparation for a C++-based CS2 course? After fifteen years of using C++ for both CS1 and CS2, the Computer Science Department at Michigan State University changed the CS1 course to Python. This paper examines ...},
address = {Chattanooga, TN},
annote = {look at the performance of the Python and not-Python groups in the final exam and the programming projects (home- work). There was no significant difference between the two groups in either outcome. The t-values for each of the anal- yses were: final exam (t = -.51, p = .61), and project grade (t = -.85, p = .40) as shown in Table 5 and Table 6.},
author = {Enbody, Richard J. and Punch, William F. and McCullen, Mark},
booktitle = {SIGCSE'09 - Proceedings of the 40th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1508865.1508907},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Enbody, Punch, McCullen - 2009 - Python CS1 as preparation for C CS2.pdf:pdf},
isbn = {9781605585048},
keywords = {CS1,CS2,Curriculum,Intro. to programming,Python},
mendeley-tags = {CS2},
pages = {116--120},
publisher = {ACM Press},
title = {{Python CS1 as preparation for C++ CS2}},
url = {http://portal.acm.org/citation.cfm?doid=1508865.1508907},
year = {2009}
}
@inproceedings{Jajodia2007a,
address = {New York, New York, USA},
author = {Jajodia, Sushil},
booktitle = {Proceedings of the 2nd ACM symposium on Information, computer and communications security - ASIACCS '07},
doi = {10.1145/1229285.1229288},
isbn = {1595935746},
month = {mar},
pages = {1},
publisher = {ACM Press},
title = {{Topological analysis of network attack vulnerability}},
url = {http://dl.acm.org/citation.cfm?id=1229285.1229288 http://dl.acm.org/citation.cfm?id=1501434.1501437},
year = {2007}
}
@article{Stringfellow2002,
author = {Stringfellow, C. and Andrews, A. Amschler},
journal = {Empirical Software Engineering},
pages = {319 -- 343},
title = {{An Empirical Method for Selecting Software Reliability Growth Models}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.10.466},
volume = {7},
year = {2002}
}
@techreport{NIST2013,
address = {FIPS 186-4},
author = {NIST},
institution = {NIST},
title = {{Digital Signature Standard (DSS)}},
year = {2013}
}
@book{Kan2003,
address = {Boston, MA},
annote = {Chapter 3 has good notes on the differences between nominal, ordinal, interval, and ratio scales.  "Six sigma" is discussed.},
author = {Kan, S},
edition = {Second},
keywords = {measurement,metrics},
publisher = {Addison Wesley},
title = {{Metrics and Models in Software Quality Engineering}},
year = {2003}
}
@article{Kit2006,
author = {Kit, Lo Kwun and Man, Chan Kwun and Baniassad, Elisa},
doi = {10.1145/1167515.1167506},
isbn = {1-59593-348-4},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {aspect-oriented requirements analysis,early aspects,latent semantic analysis,theme/doc,visualisation},
month = {oct},
number = {10},
pages = {383},
title = {{Isolating and relating concerns in requirements using latent semantic analysis}},
url = {http://dl.acm.org/citation.cfm?id=1167515.1167506},
volume = {41},
year = {2006}
}
@inproceedings{Oberheide2006b,
address = {New York, New York, USA},
author = {Oberheide, Jon and Karir, Manish and Blazakis, Dionysus},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179592},
isbn = {1595935495},
keywords = {BGP,routing,security visualization},
month = {nov},
pages = {71},
publisher = {ACM Press},
title = {{VAST}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179592},
year = {2006}
}
@inproceedings{Bailey2001,
address = {Tokyo, Japan},
author = {Bailey, Brian P and Konstan, Joseph A and Carlis, John V},
booktitle = {Proceedings of Human-Computer Interaction 2001 (INTERACT '01)},
keywords = {hci,interruption},
pages = {593--601},
title = {{The Effects of Interruptions on Task Performance, Annoyance, and Anxiety in the User Interface}},
year = {2001}
}
@inproceedings{Kohn2020,
abstract = {Development environments play a crucial role for novice programmers. Not only do they act as interface to type in and execute programs, but a programming environment is also responsible for reporting errors, managing in- and output when the program is running, or offering the programmer access to the underlying notional machine. In recent years several new educational programming environments for Python have been presented. However, the important issue of reporting errors has rarely been addressed and evaluations often hint that students main issue is the poor quality of Python's error messages. We have therefore written an educational Python environment with enhanced error messages. This paper presents the design and rationale of its three primary features: modifications to Python, enhanced error messages, and the visual debugger.},
address = {Portland, OR, USA},
author = {Kohn, Tobias and Manaris, Bill},
booktitle = {SIGCSE '20: Proceedings of the 51st ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3328778.3366920},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohn, Manaris - 2020 - Tell Me What's Wrong A Python IDE with Error Messages.pdf:pdf},
isbn = {9781450367936},
issn = {1942647X},
pages = {1054--1060},
publisher = {ACM},
title = {{Tell Me What's Wrong: A Python IDE with Error Messages}},
url = {https://doi.org/10.1145/3328778.3366920},
year = {2020}
}
@inproceedings{Gupta2008,
abstract = {This paper reports on an industrial case study in a large Norwegian Oil and Gas company (StatoilHydro ASA) involving a reusable Java-class framework and an application that uses that framework. We analyzed software changes from three releases of the framework and the application. On the basis of our analysis of the data, we found that perfective and corrective changes account for the majority of changes in both the reusable framework and the non-reusable application. Although adaptive changes are more frequent and has longer active time in the reusable framework, it went through less refactoring compared to the non-reusable application. For the non-reusable application we saw preventive changes as more frequent and with longer active time. We also found that designing for reuse seems to lead to fewer changes, as well as we saw a positive effect on doing refactoring.},
annote = {Best Paper Award},
author = {Gupta, A and Shull, Forrest and Cruzes, Daniela and Ackermann, Chris and Ronneberg, H and Landre, E},
booktitle = {Product Focused Software Process Improvement Conference (PROFES)},
keywords = {Report,Software Process Improvement,software process},
month = {jun},
pages = {158--173},
publisher = {Springer},
title = {{Experience Report on the Effect of Software Development Characteristics on Change Distribution}},
url = {http://www.springerlink.com/content/544v50567p80h302/},
year = {2008}
}
@article{Bertolino2005,
author = {Bertolino, a. and Marchetti, E. and Muccini, H.},
doi = {10.1016/j.entcs.2004.02.084},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {model-based testing,models integration,testing accuracy,testing effort},
month = {jan},
pages = {85--97},
title = {{Introducing a Reasonably Complete and Coherent Approach for Model-based Testing}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S157106610405279X},
volume = {116},
year = {2005}
}
@incollection{Berenson2007,
address = {Wellesley, MA},
author = {Berenson, Sarah B and Williams, Laurie and Slaten, Kelli M and Burger, C and Creamer, E and Meszaros, P},
pages = {65--76},
publisher = {A. K. Peter, Ltd.},
title = {{Using Pair Programming and Agile Development Methods in a University Software Engineering Course to Develop a Model of Social Interactions}},
year = {2007}
}
@article{Khandpur,
author = {Khandpur, RP and Ji, T and Jan, S and Wang, G and arXiv preprint arXiv {\ldots}, CT Lu - and 2017, Undefined},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khandpur et al. - Unknown - Crowdsourcing Cybersecurity Cyber Attack Detection using Social Media.pdf:pdf},
journal = {arxiv.org},
keywords = {twitter},
mendeley-tags = {twitter},
title = {{Crowdsourcing Cybersecurity: Cyber Attack Detection using Social Media}},
url = {https://arxiv.org/abs/1702.07745}
}
@article{Zhang2004,
abstract = {A new software development process based on model-driven development (MDD) is now available. Called test-driven modeling, the process involves automatic testing through simulation and using executable models as living software system architecture documents. Developers can effectively apply TDM to large projects with high productivity and quality in terms of the number of code defects.},
annote = {Test-driven modeling for model-driven development},
author = {Zhang, Y},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {5},
pages = {80--86},
title = {{Test-driven modeling for model-driven development}},
url = {citeulike-article-id:3934852 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-4644363382{\&}{\#}38 partnerID=40},
volume = {21},
year = {2004}
}
@inproceedings{Kazanzides99,
address = {Tokyo, Japan},
author = {Kazanzides, P},
booktitle = {Intl. Symp. on Robotics (ISR)},
month = {nov},
pages = {281--286},
title = {{Robot Assisted Surgery: The ROBODOC Experience}},
volume = {30},
year = {1999}
}
@inproceedings{Katipally2010b,
address = {New York, New York, USA},
author = {Katipally, Rajeshwar and Gasior, Wade and Cui, Xiaohui and Yang, Li},
booktitle = {Proceedings of the Sixth Annual Workshop on Cyber Security and Information Intelligence Research - CSIIRW '10},
doi = {10.1145/1852666.1852722},
isbn = {9781450300179},
keywords = {intrusions,multistage attacks,visualization},
month = {apr},
pages = {1},
publisher = {ACM Press},
title = {{Multistage attack detection system for network administrators using data mining}},
url = {http://dl.acm.org/citation.cfm?id=1852666.1852722},
year = {2010}
}
@inproceedings{Enbody2010,
abstract = {If you change the CS1 language to Python, what is the impact on the rest of the curriculum? In earlier work we examined the impact of changing CS1 from C++ to Python while leaving CS2 in C++. We found that Python-prepared CS1 students fared no differently in CS2 than students whose CS1 course was in C++, even though CS2 was taught in C++ and covered the same topics as in previous years. Was that an anomaly? What happens in the next tier of courses? When our CS1 was first changed to Python there were many students who had taken CS1 in C++ still in the system. The result is that there is a cadre of students with either CS1 in Python or CS1 in C++ moving together through our curriculum. This one-time occurrence is an opportunity to study the students with many variables fixed. Our next tier of courses is a C-based computer organization course, a C++ based object-oriented software design course, and a data structures course. We found that the students who started with Python fared as well as the CS1 C++ students. As before, the best predictor of performance was their college GPA. Python versus C++ CS1 preparation was not a predictor of performance in any course. We conclude again that in our C++ based curriculum changing CS1 to Python had no negative impact on student performance and did not require any significant change in those subsequent courses. Copyright 2010 ACM.},
address = {Milwaukee, WI, USA},
author = {Enbody, Richard J. and Punch, William F.},
booktitle = {SIGCSE'10 - Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1734263.1734437},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Enbody, Punch - 2010 - Performance of python CS1 students in mid-level non-python CS courses.pdf:pdf},
isbn = {9781605588858},
keywords = {CS1,Curriculum,Introduction to programming,Python},
pages = {520--523},
publisher = {ACM Press},
title = {{Performance of Python CS1 students in mid-level non-Python CS courses}},
url = {http://portal.acm.org/citation.cfm?doid=1734263.1734437},
year = {2010}
}
@inproceedings{Succi2001,
address = {Toronto, ON},
author = {Succi, Giancarlo and Stefanovic, Milorad and Pedrycz, Witold},
booktitle = {Proceedigns of the 2001 Canadian Conference on Electrical and Computer Engineering},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Succi, Stefanovic, Pedrycz - 2001 - Quantitative assessment of extreme programming practices.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
month = {may},
number = {3479},
title = {{Quantitative assessment of extreme programming practices}},
volume = {135},
year = {2001}
}
@article{Carver2008,
author = {Carver, J and Juristo, Natalie and Shull, Forrest and Vegas, S},
journal = {Empirical Software Engineering: An International Journal},
keywords = {Empirical Software Engineering,Software Engineering},
month = {apr},
number = {2},
pages = {211--218},
title = {{The Role of Replications in Empirical Software Engineering}},
url = {http://dx.doi.org/10.1007/s10664-008-9060-1},
volume = {13},
year = {2008}
}
@inproceedings{626257,
abstract = {The continuing requirement for corporate right sizing demand's businesses ensure their security structure minimizes costs while maximizing efficiency and effectiveness. In large campus organizations with a security protective force and multiple security systems, the advance of security systems technologies and telecommunication technologies enable massive reduction in the manpower required to monitor the command and control of the systems through the consolidation of resources, the automation and integration of the security systems, and process re-engineering. This paper presents a business case and user experience in the development and implementation of a centralized Communication Center (ComCenter) within a large geographically dispersed multiple building multi-campus organization. The centralized ComCenter replaces three large geographically located Communication Centers and several small monitoring locations throughout the metropolitan area. Through process reengineering, the protective force required to monitor the process is reduced in half, a decrease of more than forty-nine (49) man years of effort. The consolidation replaced multiple independent intrusion detection systems with a single system. The ComCenter has three identical integrated operator consoles with access to the Intrusion Detection System, Closed Circuit Television (CCTV) system, Emergency Services (911), Radio Communications, Access Control, and National Crime Information Center information. The command and control of each of the security systems is provided within a computing platform using graphical user interfaces. The ComCenter process monitors greater than five (5) million square feet of corporate space and facilities using greater than 4,000 individual intrusion detection devices and greater than 300 CCTV cameras. The CCTV camera monitoring and control is provided on the operator console screen and allows forwarding via a worldwide video teleconferencing system. The effects of the large scale re-engineering and consolidation effort upon the vendors, the security force, and the organization's security infrastructure shall be presented },
author = {Carback, R T},
booktitle = {Security Technology, 1997. Proceedings. The Institute of Electrical and Electronics Engineers 31st Annual 1997 International Carnahan Conference on},
doi = {10.1109/CCST.1997.626257},
keywords = {Closed Circuit Television,Intrusion Detection Syst},
month = {oct},
pages = {140--152},
title = {{Large campus communication center and security system integration, consolidation, and re-engineering}},
year = {1997}
}
@inproceedings{Dyba2012,
address = {Lund, Sweden},
author = {Dyb{\aa}, Tore and Sj{\o}berg, Dag I.K. and Cruzes, Daniela S.},
booktitle = {Proc. of the ACM-IEEE Int'l Symp. on Empirical Software Engineering and Measurement (ESEM '12)},
doi = {10.1145/2372251.2372256},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dyb{\aa}, Sj{\o}berg, Cruzes - 2012 - What works for whom, where, when, and why.pdf:pdf},
isbn = {9781450310567},
keywords = {empirical methods,evidence-based software engineering,generalization,sociotechnical system,theory},
month = {sep},
pages = {19--28},
title = {{What works for whom, where, when, and why?}},
url = {http://dl.acm.org/citation.cfm?id=2372251.2372256},
year = {2012}
}
@article{Robillard2005,
author = {Robillard, Martin P.},
doi = {10.1145/1095430.1081711},
isbn = {1595930140},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {feature location,static analysis,structural program dependencies},
month = {sep},
number = {5},
pages = {11},
title = {{Automatic generation of suggestions for program investigation}},
url = {http://portal.acm.org/citation.cfm?doid=1095430.1081711},
volume = {30},
year = {2005}
}
@misc{Part2007,
abstract = {In this paper, we suggest guidelines related to the design limit or range by body dimensions based on 'SizeKorea 2004'. This paper describes three sequential tests. First, body dimensions' percentile curves were analyzed in order to find out their trends. Second, their appropriateness with respect to normality assumptions by gender and age was tested. Finally, the steepest slopes at both extremes of female and male percentile curves were checked and analyzed. By performing these sequential tests, five patterns of body dimensions were found. Findings from this research were two-fold. First of all, adult percentile curves, by and large, did not follow a normal distribution. The other finding was that the design limit for 33{\%} of the male body dimensions must be from 5th to 97.5th percentiles and the limit for 85{\%} of the female body dimensions must be from 2.5th to 97.5th percentiles, which shows their steepest slope at the extremes of the percentile curves. From this study, eight specific design guidelines for extreme design by patterns of body dimensions were found. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Design guidelines to the application of extreme design with Korean anthropometry},
author = {Part and Cho, Y and Jung, E S and Park, S and Jeong, S W and Park, W},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {30--39},
title = {{Design guidelines to the application of extreme design with Korean anthropometry}},
url = {citeulike-article-id:3934750 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38149100937{\&}{\#}38 partnerID=40},
volume = {4560 LNCS},
year = {2007}
}
@inproceedings{Raemaekers2013,
address = {San Francisco, CA},
author = {Raemaekers, Steven and Nane, Gabriela F. and van Deursen, Arie and Visser, Joost},
booktitle = {2013 10th Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2013.6624037},
isbn = {978-1-4673-2936-1},
month = {may},
pages = {257--266},
title = {{Testing principles, current practices, and effects of change localization}},
url = {http://ieeexplore.ieee.org/document/6624037/},
year = {2013}
}
@article{Gilmore1991,
author = {Gilmore, David J},
doi = {10.1016/0001-6918(91)90009-O},
issn = {0001-6918},
journal = {Acta Psychologica},
number = {1–3},
pages = {151--172},
title = {{Models of debugging}},
volume = {78},
year = {1991}
}
@article{schmeichel2007,
author = {Schmeichel, Brandon J.},
journal = {Journal of Experimental Psychology: General},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {241},
publisher = {American Psychological Association},
title = {{Attention control, memory updating, and emotion regulation temporarily reduce the capacity for executive control.}},
volume = {136},
year = {2007}
}
@article{Basili1984,
abstract = {An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.},
author = {Basili, Victor R. and Weiss, David M.},
doi = {10.1109/TSE.1984.5010301},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {nov},
number = {6},
pages = {728--738},
shorttitle = {Software Engineering, IEEE Transactions on},
title = {{A Methodology for Collecting Valid Software Engineering Data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5010301},
volume = {10},
year = {1984}
}
@inproceedings{Battestini2010,
author = {Battestini, Agathe and Setlur, Vidya and Sohn, Timothy},
booktitle = {Proceedings of the 12th international conference on Human computer interaction with mobile devices and services - MobileHCI '10},
doi = {10.1145/1851600.1851638},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Battestini, Setlur, Sohn - 2010 - A large scale study of text-messaging use.pdf:pdf},
isbn = {9781605588353},
keywords = {agile,large-scale study,mobile device,nsf,short message service,sms,text messaging,texting},
mendeley-tags = {agile,nsf},
month = {sep},
pages = {229--238},
publisher = {ACM Press},
title = {{A large scale study of text-messaging use}},
url = {http://dl.acm.org/citation.cfm?id=1851600.1851638},
year = {2010}
}
@inproceedings{Ekanayake2009,
address = {Vancouver, BC},
author = {Ekanayake, Jayalath and Tappolet, Jonas and Gall, Harald C. and Bernstein, Abraham},
booktitle = {2009 6th IEEE International Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2009.5069480},
isbn = {978-1-4244-3493-0},
month = {may},
pages = {51--60},
title = {{Tracking concept drift of software projects using defect prediction quality}},
url = {http://ieeexplore.ieee.org/document/5069480/},
year = {2009}
}
@inproceedings{Teiniker2005,
abstract = {Component-Based Development (CBD) distinguishes the process of component development from that of component-based system development. While reuse is the foundation of CBD, conventional development methods are not suitable for that kind of software development. In this paper we present a hybrid development process for component-based software systems, that capitalizes on benefits from model-driven and test-driven process models, and give an experience report from a pilot project in which this novel development methodology has been applied. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {A hybrid component-based system development process},
author = {Teiniker, E and Schmoelzer, G and Faschingbauer, J and Kreiner, C and Weiss, R},
booktitle = {Software Engineering and Advanced Applications, 2005. 31st EUROMICRO Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {152--159},
title = {{A hybrid component-based system development process}},
url = {citeulike-article-id:3934814 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33747430470{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@book{Pfleeger1998,
address = {Upper Saddle River, NJ},
author = {Pfleeger, Shari Lawrence},
pages = {1--44},
publisher = {Prentice Hall},
title = {{Software Engineering:  Theory and Practice}},
year = {1998}
}
@inproceedings{1565273,
abstract = {The most common computer authentication method is to use alphanumerical usernames and passwords. This method has been shown to have significant drawbacks. For example, users tend to pick passwords that can be easily guessed. On the other hand, if a password is hard to guess, then it is often hard to remember. To address this problem, some researchers have developed authentication methods that use pictures as passwords. In this paper, we conduct a comprehensive survey of the existing graphical password techniques. We classify these techniques into two categories: recognition-based and recall-based approaches. We discuss the strengths and limitations of each method and point out the future research directions in this area. We also try to answer two important questions: "Are graphical passwords as secure as text-based passwords?"; "What are the major design and implementation issues for graphical passwords?" This survey will be useful for information security researchers and practitioners who are interested in finding an alternative to text-based authentication methods},
author = {Suo, Xiaoyuan and Zhu, Ying and Owen, G S},
booktitle = {Computer Security Applications Conference, 21st Annual},
doi = {10.1109/CSAC.2005.27},
issn = {1063-9527},
keywords = {computer authentication,graphical passwords,inform},
pages = {10 pp. --472},
title = {{Graphical passwords: a survey}},
year = {2005}
}
@inproceedings{Madeyski2005,
abstract = {Abstract. Test-driven development (TDD) and pair programming (PP) are the key practices of eXtreme Programming methodology that have caught the attention of software engineers and researchers worldwide. One of the aims of the large experiment performed at Wroclaw University of Technology was to investigate the difference between test-driven development and the traditional, test-last development as well as pair programming and solo programming with respect to the external code quality. It appeared that the external code quality was lower when testdriven development was used instead of the classic, test-last software development approach in case of solo programmers (p = 0.028) and pairs (p = 0.013). There was no difference in the external code quality when pair programming was used instead of solo programming. 1},
annote = {Preliminary Analysis of the Effects of Pair Programming},
author = {Madeyski, Lech},
booktitle = {and Test-Driven Development on the External Code Quality,{\^{a}}€ Software Engineering: Evolution and Emerging Technologies},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {113--123},
title = {{Preliminary Analysis of the Effects of Pair Programming}},
url = {citeulike-article-id:3934698 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.59.6724},
year = {2005}
}
@inproceedings{Nagappan2007,
address = {Madrid, Spain},
author = {Nagappan, Nachiappan and Ball, Thomas},
booktitle = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
doi = {10.1109/ESEM.2007.13},
isbn = {978-0-7695-2886-1},
keywords = {churn},
mendeley-tags = {churn},
month = {sep},
pages = {364--373},
title = {{Using Software Dependencies and Churn Metrics to Predict Field Failures: An Empirical Case Study}},
url = {http://ieeexplore.ieee.org/document/4343764/},
year = {2007}
}
@techreport{NationalAeronauticsandSpaceAdministration2007,
address = {NPR 7120.5D},
author = {{National Aeronautics and Space Administration}},
institution = {National Aeronautics and Space Administration},
title = {{NASA Space Flight Program and Project Management Requirements}},
year = {2007}
}
@inproceedings{6095925,
abstract = {This paper presents the novel demosaicing approach that is based upon region of interest analysis. The question of if only the reconstruction quality of areas where human gazes during identification process of security objects are crucial for quality of identification is solved. Two fixation density maps were constructed on the basis of the ground through visual attention data which was acquired during an eye tracking experiment. Two security scenes containing traffic signs were used as testing images. SSIM and VIF with subjective testing were used for evaluation of reconstruction quality. The results tackle the questions - if quality of identification mainly depends on the quality of the areas where human eye is attracted, areas of interest, and whether the quality of the remaining part of image perceived by peripheral vision is crucial for quality of identification. Furthermore, the results show that embedding the information relating to these visual fixation patterns into demosaicing procedure can preserve the perceived image quality and quality of identification, while the computational complexity of proposed approach can be low.},
author = {Dostal, P and Klima, M},
booktitle = {Security Technology (ICCST), 2011 IEEE International Carnahan Conference on},
doi = {10.1109/CCST.2011.6095925},
issn = {1071-6572},
keywords = {color filter array,computational complexity,eye tr},
pages = {1--10},
title = {{Locally adaptive demosaicing technique for security images based upon region-of-interest analysis}},
year = {2011}
}
@article{Schneidewind1997,
abstract = {Software reliability predictions can increase trust in the reliability of safety critical software such as the NASA Space Shuttle Primary Avionics Software System (Shuttle flight software). This objective was achieved using a novel approach to integrate software-safety criteria, risk analysis, reliability prediction, and stopping rules for testing. This approach applies to other safety-critical software. The authors cover only the safety of the software in a safety-critical system. The hardware and human-operator components of such systems are not explicitly modeled nor are the hardware and operator-induced software failures. The concern is with reducing the risk of all failures attributed to software. Thus, safety refers to software-safety and not to system-safety. By improving the software reliability, where the reliability measurements and predictions are directly related to mission and crew safety, they contribute to system safety. Software reliability models provide one of several tools that software managers of the Shuttle flight software are using to assure that the software meets required safety goals. Other tools are inspections, software reviews, testing, change control boards, and perhaps most important-experience and judgement},
author = {Schneidewind, N.F.},
doi = {10.1109/24.589933},
issn = {00189529},
journal = {IEEE Transactions on Reliability},
month = {mar},
number = {1},
pages = {88--98},
title = {{Reliability modeling for safety-critical software}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=589933},
volume = {46},
year = {1997}
}
@book{Jensen2001,
address = {New York, NY},
author = {Jensen, Finn V},
publisher = {Springer},
title = {{Bayesian Networks and Decision Graphs}},
year = {2001}
}
@techreport{Layman2013c,
address = {College Park, MD},
author = {Layman, Lucas},
doi = {TR 13-101},
institution = {Fraunhofer Center for Experimental Software Engineering},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {TR 13--301},
publisher = {Fraunhofer Center for Experimental Software Engineering},
title = {{Information and Processes Used When Investigating Web Server Log Files for Malicious Activity}},
year = {2013}
}
@techreport{Siniaalto2006a,
abstract = {This document contains the results of the multiple case study on the impact of Testdriven development (TDD) on design quality. The empirical content of this document is based on the evidence obtained from three case projects conducted within VTT Technical Research Centre of Finland. This document is structured as follows: First, Testdriven development technique is introduced followed by a chapter presenting the design of the research along with the used metrics. Next, the results are presented and, finally, conclusions are drawn.},
annote = {The Impact of Test-Driven Devleopment on Design Quality},
author = {Siniaalto, Maria},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {15},
publisher = {VTT Agile},
title = {{The Impact of Test-Driven Devleopment on Design Quality}},
url = {citeulike-article-id:3934794 {\#}},
year = {2006}
}
@article{Boehm1988,
abstract = {A short description is given of software process models and the issues they address. An outline is given of the process steps involved in the spiral model, an evolving risk-driven approach that provides a framework for guiding the software process, and its application to a software project is shown. A summary is given of the primary advantages and implications involved in using the spiral model and the primary difficulties in using it at its current incomplete level of elaboration.},
author = {Boehm, B. W.},
doi = {10.1109/2.59},
issn = {0018-9162},
journal = {Computer},
month = {may},
number = {5},
pages = {61--72},
title = {{A spiral model of software development and enhancement}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=59},
volume = {21},
year = {1988}
}
@inproceedings{6107844,
abstract = {Digital television (DTV) provides a critical broadband broadcast resource for interoperable public safety and homeland security applications. Beyond the simplistic view of DTV as a resource for repurposed spectrum to public safety, existing DTV stations are providing megabits per second of encrypted data, including Internet Protocol (IP) video, geospatial visualization, data files, text messages, and any digital media. These datacasts, not available to the public, can be targeted to one or any number of selected receivers or groups of receivers in the broadcast area with no congestion effects, unlike cellular systems; that is, DTV ensures all subscribers receive the full bandwidth available. This paper is an introduction to DTV for both day-to-day applications as well as resilient emergency and post-disaster datacast for wide-area situational awareness and command coordination. The paper also emphasizes the development of a datacast concept of operations for public safety and homeland security.},
author = {Desourdis, Robert I. and Vest, Kevin F. and O'Brien, Mark and Mulholland, David J.},
booktitle = {2011 IEEE International Conference on Technologies for Homeland Security (HST)},
doi = {10.1109/THS.2011.6107844},
isbn = {978-1-4577-1376-7},
keywords = {DTV,IP video,Internet protocol video,broadband bro},
month = {nov},
pages = {33--42},
publisher = {Ieee},
title = {{Digital televison for homeland security: Broadband datacast for situational awareness and command coordination}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6107844},
year = {2011}
}
@inproceedings{Joseph,
address = {Reno, NV},
author = {Joseph, Anthony and Payne, Mabel},
pages = {368--371},
publisher = {ACM},
title = {{Group Dynamics and Collaborative Group Performance}},
volume = {35}
}
@article{Tenenberg2005,
abstract = {This paper describes an empirical study that investigated the knowledge that Computer Science students have about the extent of their own previous learning. The study compared self-generated estimates of performance with actual performance on a data structures quiz taken by undergraduate students in courses requiring data structures as a prerequisite. The study was contextualized and grounded within a research paradigm in Psychology called calibration of knowledge that suggests that self-knowledge across a range of disciplines is highly unreliable. Such self-knowledge is important because of its role in meta-cognition, particularly in cognitive self-regulation and monitoring, as well as in the credence that instructors give to student self-reports. Our results indicated that Computer Science student self-estimates are highly correlated with performance, more so for estimates provided after the performance than before. This high level of calibration, however, was likely the result of a number of conditions that do not always hold: that the students already had domain expertise, that the quiz had unambiguous and verifiable answers, and that students expected their estimates to be validated. When these conditions are not met, it becomes more important for students to have direct feedback about their performance so as to uncover those areas where their intuitions might mislead them. Students also had weak knowledge about their standing relative to their peers, particularly those in the lower performance quartiles, exhibiting the well known better-than-average heuristic. There was, additionally, no correlation between calibration ability and degree of liking or difficulty with the data structures material, suggesting that instructors and researchers should not treat liking or difficulty as reliable indicators of the learning that has occurred. {\textcopyright} 2005 Taylor {\&} Francis Ltd.},
author = {Tenenberg, Josh and Murphy, Laurie},
doi = {10.1080/08993400500307677},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tenenberg, Murphy - 2005 - Knowing what I know An investigation of undergraduate knowledge and self-knowledge of data structures.pdf:pdf},
issn = {17445175},
journal = {Computer Science Education},
number = {4},
pages = {297--315},
publisher = {Routledge},
title = {{Knowing what I know: An investigation of undergraduate knowledge and self-knowledge of data structures}},
url = {https://www.tandfonline.com/doi/abs/10.1080/08993400500307677},
volume = {15},
year = {2005}
}
@techreport{Abrahamsson2002,
address = {Oulu, Finland},
author = {Abrahamsson, Pekka and Salo, Outi and Ronkainen, Jussi and Warsta, Juhani},
booktitle = {Relat{\'{o}}rio T{\'{e}}cnico,},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abrahamsson et al. - 2002 - Agile software development methods Review and analysis.pdf:pdf},
institution = {VTT Electronics},
keywords = {agile},
mendeley-tags = {agile},
month = {sep},
number = {3},
title = {{Agile software development methods: Review and analysis}},
volume = {12},
year = {2002}
}
@inproceedings{Fokum2019,
abstract = {As computing enrollments have grown in the last decade there have been anecdotal claims from faculty that the discipline has attracted weaker students. To the best of our knowledge the only paper that has studied, and debunked, this claim has been based on data from an exclusive North American university. In this paper we examine data from six courses drawn taught over a 3 year period at a public university in a developing country. We find that in the case of the CS1 course, the distribution of student grades has fluctuated over this period. In addition, courses in object-oriented programming, networking, algorithms and discrete mathematics have also shown variation in student grades. We provide discussion as to why the grades in the courses show variation. We also explain why our findings differ from previous claims of stability in student grades.},
address = {Minneapolis, MN, USA},
annote = {grades are varying significantly but not predictably. contradicts Stanford study

However, using data from an eight year span at Stanford University, Sahami and Piech [7] showed that the distribution of student grades remained the same, and that there was no support for the claim that the CS discipline was attracting weaker student.

[5] E. Patitsas, J. Berlin, M. Craig, and S. Easterbrook. 2016. Evidence That Computer Science Grades Are Not Bimodal. In Proceedings of the 2016 ACM Conference on International Computing Education Research - ICER '16. ACM Press, New York, New York, USA, 113–121. https://doi.org/10.1145/2960310.2960312

[7] M. Sahami and C. Piech. 2016. As CS Enrollments Grow, Are We Attracting Weaker Students?. In Proc. 47th ACM Technical Symposium on Computing Science Education (SIGCSE '16). ACM, New York, NY, USA, 54–59. https://doi.org/10. 1145/2839509.2844621

Our results do not show the level of stability that was documented in [7]. There are several reasons for this: For one, thing our institution tends to attract more average students. A review of the average grade in the regional examination offered by our local examination body shows that students in our COMP1126 and COMP1127 classes had an entry grade of 3 (1 is the highest and best pass, while 5 is the lowest pass). Secondly, one of the authors of [7] stated that he had used the same set of assignments over a long period. On the other hand students at our institution are not bound by an honor code, and so instructors come up with new assignments every year. These new assignments, in spite of the instructors best efforts, can introduce some variation in the coursework grades. Another threat to the validity of our results comes from any changes to teaching methods or styles. For example, in 2015, the COMP2190 course moved away from using quizzes administered on a Learning Management System (LMS), to having short quizzes done in class every week. The quizzes still contributed the same proportion to the final grade, however, the students were being assessed differently. Finally, another threat to the results comes from differences in the tutors that were assigned to the courses from year to year. While the number of tutors assigned to courses has remained relatively constant in spite of enrollment growth, different tutors are assigned to the courses from year to year, and they would introduce minor differences in teaching methods},
author = {Fokum, Daniel T. and Coore, Daniel N. and Ferguson, Eyton and Mansingh, Gunjan and Beckford, Carl},
booktitle = {SIGCSE 2019 - Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3287324.3287354},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fokum et al. - 2019 - Student performance in computing courses in the face of growing enrollments.pdf:pdf},
isbn = {9781450358903},
keywords = {Algorithms,CS2,Enrollment growth,Introductory programming,Mixture modeling,Student performance},
mendeley-tags = {CS2},
month = {feb},
pages = {43--48},
publisher = {Association for Computing Machinery, Inc},
title = {{Student performance in computing courses in the face of growing enrollments}},
year = {2019}
}
@inproceedings{1594840,
abstract = { Nowadays, highly available (HA) systems are a must, for almost any business process. More recently, the need for HA systems have increased as electronic commerce and other internet-based applications have become widely used with the growing Web usage. Security is a major concern for these systems. Companies want to make sure that their security systems are working flawlessly and efficiently. Making sure that these systems are available to allow the right people access to the right areas of the company is imperative. Traditionally, HA systems consist of proprietary hardware and software components. However, the price/performance advantages of commercial-off-the-shelf (COTS) based clusters have had a compelling affect on HA vendors and their marketplace. The emergence of computational grids makes it feasible to develop cost-effective, large-scale geographically distributed HA systems. Making sure that critical applications on this new generation of HA systems are secured is a challenging proposition. In this article, we have identified a list of challenges for the next generation of grid-based HA systems. We have explored the visualization of security services with their pluggable implementation to address the security needs of these grid-based HA systems. The main advantages of this solution include independence with respect to the underlying security mechanisms; best trade-off between security guarantees and processing capabilities; configurability of security architecture; better portability across heterogeneous platforms; and a smaller application development cycle for the HA functionality in the system.},
author = {Naqvi, S and Riguidel, M},
booktitle = {Security Technology, 2005. CCST '05. 39th Annual 2005 International Carnahan Conference on},
doi = {10.1109/CCST.2005.1594840},
keywords = {application development cycle,grid-based HA syste},
pages = {47--50},
title = {{Security challenges for the next generation of highly available systems}},
year = {2005}
}
@inproceedings{1532074,
abstract = { This paper describes our work to develop an environment and novel visualization techniques for the visual representation, exploration, and analysis of network traffic data to ease the identification and analysis of sophisticated attacks above and beyond the ability for traditional network firewalls to detect and block. The visualization techniques are geared towards aiding analysts in filtering unwanted or unneeded data in favor of data deemed more critical and more representative of the sophisticated attacks the analysts must focus their attention on. The environment provides the needed capabilities for analyzing traditional network traffic data without additional filtering, i.e., the environment itself provides the needed capabilities.},
author = {Erbacher, R F and Christensen, K and Sundberg, A},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532074},
keywords = {graphical user interfaces,intrusion detection sys},
pages = {121--127},
title = {{Designing visualization capabilities for IDS challenges}},
year = {2005}
}
@article{Muraven1998,
abstract = {If self-regulation conforms to an energy or strength model, then self-control should be impaired by prior exertion. In Study 1, trying to regulate one's emotional response to an upsetting movie was followed by a decrease in physical stamina. In Study 2, suppressing forbidden thoughts led to a subsequent tendency to give up quickly on unsolvable anagrams. In Study 3, suppressing thoughts impaired subsequent efforts to control the expression of amusement and enjoyment. In Study 4, autobiographical accounts of successful versus failed emotional control linked prior regulatory demands and fatigue to self-regulatory failure. A strength model of self-regulation fits the data better than activation, priming, skill, or constant capacity models of self-regulation.},
author = {Muraven, Mark and Tice, Dianne M. and Baumeister, Roy F.},
journal = {Journal of Personality and Social Psychology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {3},
pages = {773--789},
title = {{Self-control as a limited resource: Regulatory depletion patterns.}},
volume = {74},
year = {1998}
}
@misc{Facebook,
annote = {Accessed: 2016-06-14},
author = {Paul, Ryan},
howpublished = {$\backslash$url{\{}http://arstechnica.com/business/2012/04/exclusive-a-behind-the-scenes-look-at-facebook-release-engineering/1/{\}}},
publisher = {Ars Technica},
title = {{Exclusive: a behind-the-scenes look at Facebook release engineering}}
}
@inproceedings{4777549,
abstract = {In this paper we attempt to review the current technology of 3D accelerators for animating graphics used in games and visual simulation systems, together with associated techniques and software architectures. Topics covered include OpenGL, DirectX, anti-aliasing, motion blur and depth of field. A summary of work in progress is given in the conclusions section.},
author = {Xu, Huang and CuiPing, Wang},
booktitle = {Intelligent Information Technology and Security Informatics, 2009. IITSI '09. Second International Symposium on},
doi = {10.1109/IITSI.2009.20},
keywords = {3-D accelerator technology,games,graphics animatio},
pages = {59--63},
title = {{A Review and Development of 3-D Accelerator Technology for Games}},
year = {2009}
}
@inproceedings{Sahoo2004,
author = {Sahoo, Ramendra K and Sivasubramaniam, Anand and Squillante, Mark S and Zhang, Yanyong},
booktitle = {Proc.$\backslash$ 2004 International Conference on Dependable Systems and Networks},
isbn = {0-7695-2052-9},
pages = {772--781},
title = {{Failure Data Analysis of a Large-Scale Heterogeneous Server Environment}},
year = {2004}
}
@inproceedings{4721558,
abstract = {Graphical analysis of network traffic flows helps security analysts detect patterns or behaviors that would not be obvious in a text-based environment. The growing volume of network data generated and captured makes it increasingly difficult to detect increasingly sophisticated reconnaissance and stealthy network attacks. We propose a network flow filtering mechanism that leverages the exposure maps technique of Whyte et al. (2007), reducing the traffic for the visualization process according to the network services being offered. This allows focus to be limited to selected subsets of the network traffic, for example what might be categorized (correctly or otherwise) as the unexpected or potentially malicious portion. In particular, we use this technique to filter out traffic from sources that have not gained knowledge from the network in question. We evaluate the benefits of our technique on different visualizations of network flows. Our analysis shows a significant decrease in the volume of network traffic that is to be visualized, resulting in visible patterns and insights not previously apparent.},
author = {Alsaleh, M and Barrera, D and van Oorschot, P C},
booktitle = {Computer Security Applications Conference, 2008. ACSAC 2008. Annual},
doi = {10.1109/ACSAC.2008.16},
issn = {1063-9527},
keywords = {exposure map filtering,graphical analysis,network},
pages = {205--214},
title = {{Improving Security Visualization with Exposure Map Filtering}},
year = {2008}
}
@inproceedings{Kelley2013,
address = {New York, New York, USA},
author = {Kelley, Patrick Gage and Cranor, Lorrie Faith and Sadeh, Norman},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2466466},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelley, Cranor, Sadeh - 2013 - Privacy as part of the app decision-making process.pdf:pdf},
isbn = {9781450318990},
keywords = {agile,android,decision-making,interface,mobile,nsf,privacy},
mendeley-tags = {agile,nsf},
month = {apr},
pages = {3393},
publisher = {ACM Press},
title = {{Privacy as part of the app decision-making process}},
url = {http://dl.acm.org/citation.cfm?id=2470654.2466466},
year = {2013}
}
@misc{Koopman2006,
abstract = {This paper tackles a problem often overlooked in functional programming community: that of testing. Fully automatic test tools like Quickcheck and GVST can test first order functions successfully. Higher order functions, HOFs, are an essential and distinguishing part of functional languages. Testing HOFs automatically is still troublesome since it requires the generation of functions as test argument for the HOF to be tested. Also the functions that are the result of the higher order function needs to be identified. If a counter example is found, the generated and resulting functions should be printed, but that is impossible in most functional programming languages. Yet, bugs in HOFs do occur and are usually more subtle due to the high abstraction level. In this paper we present an effective and efficient technique to test higher order functions by using intermediate data types. Such a data type mimics and controls the structure of the function to be generated. A simple additional function transforms this data structure to the function needed. We use a continuation based parser library as main example of the tests. Our automatic testing method for HOFs reveals errors in the library that was used for a couple of years without problems. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {Automatic testing of higher order functions},
author = {Koopman, P and Plasmeijer, R},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {148--164},
title = {{Automatic testing of higher order functions}},
url = {citeulike-article-id:3934681 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33845951704{\&}{\#}38 partnerID=40},
volume = {4279 LNCS},
year = {2006}
}
@article{Lindvall2005,
author = {Lindvall, Mikael and Rus, Ioana and Shull, Forrest and Zelkowitz, Marvin V and Donzelli, Paolo and Memon, A and Basili, Victor R and Costa, Patricia and Tvedt, Roseanne Tesoriero and Hochstein, Lorin and Asgari, Sima and Ackermann, Chris and Pech, Dan},
journal = {Innovations in Systems and Software Engineering a NASA Journal},
keywords = {Software Technology Evaluation},
month = {jan},
number = {1},
pages = {3--11},
title = {{An Evolutionary Testbed for Software Technology Evaluation}},
url = {http://www.cs.umd.edu/{~}mvz/pub/hdcp-nasa-j.pdf},
volume = {1},
year = {2005}
}
@misc{Cubranic2003,
abstract = {A newcomer to a software project must typically come up-to-speed on a large, varied amount of information about the project before becoming productive. Assimilating this information in the open-source context is difficult because a newcomer cannot rely on the mentoring approach that is commonly used in traditional software developments. To help a newcomer to an open-source project become productive faster, we propose Hipikat, a tool that forms an implicit group memory from the information stored in a project's archives, and that recommends artifacts from the archives that are relevant to a task that a newcomer is trying to perform. To investigate this approach, we have instantiated the Hipikat tool for the Eclipse open-source project. In this paper we describe the Hipikat tool, we report on a qualitative study conducted with a Hipikat mock-up on a medium-sized in-house project, and we report on a case study in which Hipikat recommendations were evaluated for a task on Eclipse.},
address = {Edinburgh, Scotland},
author = {{\v{C}}ubrani{\'{c}}, Davor and Murphy, Gail C},
pages = {408--418},
title = {{Hipikat: Recommending Pertinent Software Development Artifacts}},
year = {2003}
}
@inproceedings{5629223,
abstract = {Nowadays, graphical password has not been widely used in practical. Most of the graphical password authentication schemes are only discussed in laboratory. In this paper, some typical graphical password authentication schemes are introduced, and the security of them are analyzed according to its estimate criterions. One conclusion is drawn that the memorability and security of graphical password are better than that of text-based password. In addition, it's shown that the graphical password scheme has better resistance to major password attacks than others.},
author = {Hu, Wei and Wu, Xiaoping and Wei, Guoheng},
booktitle = {Communications and Intelligence Information Security (ICCIIS), 2010 International Conference on},
doi = {10.1109/ICCIIS.2010.35},
keywords = {graphical passwo,graphical password authentication},
pages = {200--203},
title = {{The Security Analysis of Graphical Passwords}},
year = {2010}
}
@inproceedings{5380369,
abstract = {Non-photorealistic rendering (NPR) for 3D mesh models has made considerable progress in recent years and it has become a hot topic in computer graphics and other related fields. This paper presents an overview of the key techniques in non-photorealistic rendering for 3D mesh models, such as line drawings based on geometric and visual features, the surface brightness and stylized shadow setting. We introduce popular methods dealing with these problems and point out the characteristics of these methods and their deficiencies. Discussion is given and future work is addressed at the end of the paper.},
author = {Lin, Si-da},
booktitle = {Computer and Communications Security, 2009. ICCCS '09. International Conference on},
doi = {10.1109/ICCCS.2009.42},
keywords = {3D mesh models,computer graphics,geometric feature},
pages = {17--20},
title = {{Non-photorealistic Rendering for 3D Mesh Models}},
year = {2009}
}
@inproceedings{Davies2011,
abstract = {We present results from a nationwide survey of undergraduate computer science departments regarding languages and techniques taught in CS0, CS1, and CS2. This snapshot of 371 schools provides an intriguing look into the state of computing education today in the U.S., quantifying which practices are actually in common use. Among other things, the study reveals the great variety in CS0 approaches, the relative uniformity of CS1 and CS2 approaches, the dominance of Java as a language for the introductory major sequence, and the tendency for departments to teach CS1 and CS2 in a consistent manner, rather than exposing students to different ideas in each.},
address = {Dallas, Texas, USA},
author = {Davies, Stephen and Polack-Wahl, Jennifer A. and Anewalt, Karen},
booktitle = {SIGCSE'11 - Proceedings of the 42nd ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1953163.1953339},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Davies, Polack-Wahl, Anewalt - 2011 - A snapshot of current practices in teaching the introductory programming sequence.pdf:pdf},
isbn = {9781450305006},
keywords = {CS0,CS1,CS2,Programming languages,Survey},
mendeley-tags = {CS2},
pages = {625--630},
publisher = {ACM Press},
title = {{A snapshot of current practices in teaching the introductory programming sequence}},
url = {http://portal.acm.org/citation.cfm?doid=1953163.1953339},
year = {2011}
}
@techreport{Tassey2002,
address = {Gaithersburg, MD},
author = {Tassey, Gregory},
booktitle = {National Institute of Standards and Technology},
institution = {National Institute of Standards and Technology},
pages = {309},
title = {{The economic impacts of inadequate infrastructure for software testing}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.122.3316{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Nuseibeh1994,
author = {Nuseibeh, B and Kramer, J and Finkelstein, A},
keywords = {requirements},
number = {10},
pages = {760--773},
title = {{A Framework for Expressing the Relationships between Multiple Views in Requirements Specification}},
volume = {20},
year = {1994}
}
@inproceedings{Schwarz2005,
abstract = {In this paper we describe AutAT, an open source Eclipse plugin to better enable test driven development of web applications. AutAT lets non-technical people write acceptance tests (or functional tests) using a user-friendly graphical editor, and convert this visual representation of the tests into executable tests.},
address = {New York, NY, USA},
annote = {AutAT: an eclipse plugin for automatic acceptance testing of web applications},
author = {Schwarz, Christian and Skytteren, Stein and {\O}vstetun, Trond},
booktitle = {OOPSLA '05: Companion to the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {182--183},
publisher = {ACM},
title = {{AutAT: an eclipse plugin for automatic acceptance testing of web applications}},
url = {citeulike-article-id:3934786 http://dx.doi.org/10.1145/1094855.1094925},
year = {2005}
}
@inproceedings{5375538,
abstract = {Visualization is used by security analysts to help detect patterns and trends in large volumes of network traffic data. With IPv6 slowly being deployed around the world, network intruders are beginning to adapt their tools and techniques to work over IPv6 (vs. IPv4). Many tools for visualizing network activity, while useful for detecting large scale attacks and network behavior anomalies still only support IPv4. In this paper, we explore the current state of IPv6 support in some popular security visualization tools and identify the roadblocks preventing those tools from supporting the new protocol. We propose a filtering technique that helps reduce the occlusion of IPv6 sources on graphs. We also suggest using treemaps for visually representing the vast space of remote addresses in IPv6.},
author = {Barrera, D and van Oorschot, P C},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375538},
keywords = {IPv6 addresses,filtering technique,network behavio},
pages = {21--26},
title = {{Security visualization tools and IPv6 addresses}},
year = {2009}
}
@article{Yang,
author = {Yang, Rui and Chen, Zhenyu and Xu, Baowen and Zhang, Zhiyi},
number = {4},
title = {{A New Approach to Evaluate Path Feasibility and Coverage Ratio of EFSM Based on Multi-objective Optimization *}}
}
@inproceedings{6189634,
abstract = {Multi-bit minimum error replacement (MER) is a method that can embed multi-bit logo/secret data into k least-significant bits (LSBs) of cover data only introduces minimum embedding error (MEE). However, k-LSBs MER suffers from weak anti-forensics. Moreover, it is unfortunate because other previous steganography works have seldom considered both large embedding capacity and high image quality. Therefore, this work proposes an anti-forensic steganography system using multi-bit adaptive embedding algorithm with flexible bit location to overcome the problem of forensics and to achieve high performance includes both large embedding capacity and high image quality. The proposed embedding algorithm embeds and hides multi-bit (k-bit, k {\#}x2265; =1) logo/secret data into any adjoining (starting with the ith location) k-bit bibi-1 {\#}x22EF;Kbi-k+1 of cover data only introduces MEE in the range of 0 to - {\#}x00B1;(2k-1)2i-k, moreover, its flexible bit location enhances the embedded security as security increases as embedding location increases. As the proposed embedding algorithm can embed multi-bit logo/secret data into any adjoining bits of cover data and has large embedding capacity and high embedding quality, this method was applied to develop image steganography systems. Finally, anti-forensics of the proposed steganography systems are demonstrated using the visual attack and the statistical attack of Chi-square analysis.},
author = {Chen, Rong-Jian and Horng, Shi-Jinn},
booktitle = {2012 International Symposium on Biometrics and Security Technologies},
doi = {10.1109/ISBAST.2012.29},
isbn = {978-1-4673-0917-2},
keywords = {Chi-square analysis,anti-forensic steganography,em},
month = {mar},
pages = {82--89},
publisher = {Ieee},
title = {{Multi-bit Adaptive Embedding Algorithm for Anti-forensic Steganography}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6189634},
year = {2012}
}
@book{Jr.1999,
author = {Jr., Edmund M. Clarke and Grumberg, Orna and Peled, Doron A.},
keywords = {Model-checking,safety},
mendeley-tags = {Model-checking,safety},
pages = {314},
publisher = {The MIT Press},
title = {{Model Checking}},
url = {http://www.amazon.com/Model-Checking-Edmund-Clarke-Jr/dp/0262032708},
year = {1999}
}
@article{Anvik2011,
author = {Anvik, John and Murphy, Gail C.},
doi = {10.1145/2000791.2000794},
issn = {1049331X},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {Bug report triage,configuration assistance,machine learning,recommendation,task assignment},
month = {aug},
number = {3},
pages = {1--35},
publisher = {ACM},
title = {{Reducing the effort of bug report triage}},
url = {http://dl.acm.org/citation.cfm?id=2000791.2000794},
volume = {20},
year = {2011}
}
@article{Menzies2016,
abstract = {{\textcopyright} 2016, Springer Science+Business Media New York. Many practitioners and academics believe in a delayed issue effect (DIE); i.e. the longer an issue lingers in the system, the more effort it requires to resolve. This belief is often used to justify major investments in new development processes that promise to retire more issues sooner. This paper tests for the delayed issue effect in 171 software projects conducted around the world in the period from 2006–2014. To the best of our knowledge, this is the largest study yet published on this effect. We found no evidence for the delayed issue effect; i.e. the effort to resolve issues in a later phase was not consistently or substantially greater than when issues were resolved soon after their introduction. This paper documents the above study and explores reasons for this mismatch between this common rule of thumb and empirical data. In summary, DIE is not some constant across all projects. Rather, DIE might be an historical relic that occurs intermittently only in certain kinds of projects. This is a significant result since it predicts that new development processes that promise to faster retire more issues will not have a guaranteed return on investment (depending on the context where applied), and that a long-held truth in software engineering should not be considered a global truism.},
author = {Menzies, Tim and Nichols, William and Shull, Forrest and Layman, Lucas},
doi = {10.1007/s10664-016-9469-x},
file = {:C$\backslash$:/Users/laymanl/Desktop/layman pubs/Menzies2016.pdf:pdf},
issn = {15737616},
journal = {Empirical Software Engineering: An International Journal},
keywords = {Cost to fix,Phase delay,Software economics},
number = {4},
pages = {1903--1935},
title = {{Are delayed issues harder to resolve? Revisiting cost-to-fix of defects throughout the lifecycle}},
url = {http://dx.doi.org/10.1007/s10664-016-9469-x},
volume = {22},
year = {2016}
}
@inproceedings{ghotra2015icse,
author = {Ghotra, Baljinder and McIntosh, Shane and Hassan, Ahmed E},
booktitle = {Proc. of the International Conference on Software Engineering (ICSE)},
pages = {789--800},
title = {{Revisiting the Impact of Classification Techniques on the Performance of Defect Prediction Models}},
year = {2015}
}
@article{Beck2001,
abstract = {The author argues that test-first coding is not testing. Test-first coding is not new. It is nearly as old as programming. It is an analysis technique. We decide what we are programming and what we are not programming, and we decide what answers we expect. Test-first is also a design technique},
annote = {Aim, fire [test-first coding]},
author = {Beck, K},
isbn = {0740-7459},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {5},
pages = {87--89},
title = {{Aim, fire [test-first coding]}},
url = {citeulike-article-id:3934553 {\#}},
volume = {18},
year = {2001}
}
@inproceedings{1532073,
abstract = { Intrusion detection systems have been popular tools in the battle against adversaries who, for whatever reason, desire to break into networks, compromise hosts, and steal valuable information. One problem with current implementations, however, is the sheer number of alerts they can generate, many of which tend to be false alarms. This drawback makes effective use of such systems a challenging task. In this paper we explore three-dimensional approaches to visualizing network intrusion detection system alerts and aggregated network statistics in order to provide the system administrator with a better picture of the events occurring on his or her network. While some research has been done using two-dimensional concepts, 3D approaches have not received much attention with regard to detecting network intrusions. Evaluation of our visualizations using the 1999 DARPA intrusion detection evaluation data set demonstrates the potential benefit of utilizing the third dimension. We show how a number of attack types in the data set generate visual evidence of abnormal activity that a security administrator might use as motivation for further investigation.},
author = {Oline, A and Reiners, D},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532073},
keywords = {1999 DARPA intrusion detection evaluation data set},
pages = {113--120},
title = {{Exploring three-dimensional visualization for intrusion detection}},
year = {2005}
}
@article{Blackburn2004,
author = {Blackburn, Mark and Busser, Robert and Nauman, Aaron},
keywords = {automation,experience,interface-driven model-based test,pilot projects,requirement-based testing,test automation technology and,test driver generation},
title = {{Why Model-Based Test Automation is Different and What You Should Know to Get Started}},
year = {2004}
}
@article{Torr2005,
author = {Torr, P.},
doi = {10.1109/MSP.2005.119},
issn = {1540-7993},
journal = {IEEE Security and Privacy Magazine},
keywords = {Authentication,Data security,Design automation,HTML,Information security,Libraries,Microsoft,Web and internet services,Web server,Web services,XML,development life cycle,formal specification,hostile online environment,object-oriented programming,product development,security,security of data,software design,software development,software maintenance,software reusability,threat environment,threat modeling,threat modeling process},
language = {English},
month = {sep},
number = {5},
pages = {66--70},
title = {{Demystifying the Threat-Modeling Process}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1514406' escapeXml='false'/{\%}3E},
volume = {3},
year = {2005}
}
@phdthesis{Tew2010,
author = {Tew, Allison Elliot},
school = {Georgia Insitute of Technology},
title = {{Assessing Fundamental Introductory Computing Concept Knowledge in a Language Independent Manner}},
year = {2010}
}
@article{Shepperd2006,
author = {Song, Qinbao and Shepperd, M. and Cartwright, M. and Mair, C.},
doi = {10.1109/TSE.2006.1599417},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Accuracy,Association rules,Data mining,Inspection,Job shop scheduling,Project management,Resource management,SEL defect data,Software defect prediction,Software development management,Software quality,Software systems,association rule mining,data mining,defect association,defect correction effort prediction,defect correction effort.,defect isolation effort,program testing,software defect association prediction,software process improvement,software quality},
language = {English},
month = {feb},
number = {2},
pages = {69--82},
publisher = {IEEE},
title = {{Software defect association mining and defect correction effort prediction}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1599417},
volume = {32},
year = {2006}
}
@misc{Ko2007,
address = {Minneapolis, MN},
author = {Ko, Andrew J and DeLine, Robert and Venolia, Gina},
pages = {344--353},
title = {{Information Needs in Collocated Software Development Teams}},
year = {2007}
}
@article{Ferguson2012,
abstract = {Learning analytics is a significant area of technology-enhanced learning that has emerged during the last decade. This review of the field begins with an examination of the technological, educational and political factors that have driven the development of analytics in educational settings. It goes on to chart the emergence of learning analytics, including their origins in the 20th century, the development of data-driven analytics, the rise of learningfocused perspectives and the influence of national economic concerns. It next focuses on the relationships between learning analytics, educational data mining and academic analytics. Finally, it examines developing areas of learning analytics research, and identifies a series of future challenges.},
author = {Ferguson, Rebecca},
doi = {https://doi.org/10.1504/IJTEL.2012.051816},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferguson - 2012 - Learning analytics drivers, developments and challenges.pdf:pdf},
journal = {International Journal of Technology Enhanced Learning},
number = {5/6},
pages = {304--317},
title = {{Learning analytics: drivers, developments and challenges}},
url = {http://oro.open.ac.uk/36374/1/IJTEL40501{\_}Ferguson Jan 2013.pdf},
volume = {4},
year = {2012}
}
@article{sampson2012,
author = {Sampson, Robert J.},
journal = {Science},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {6101},
pages = {1464--1465},
publisher = {American Association for the Advancement of Science},
title = {{Moving and the neighborhood glass ceiling}},
volume = {337},
year = {2012}
}
@article{Brooks1983a,
abstract = {psychology;program comprehension},
author = {Brooks, Ruven},
journal = {International Journal of Man-Machine Studies},
pages = {543--554},
title = {{Towards a theory of the comprehension of computer programs}},
volume = {18},
year = {1983}
}
@article{Clune2011,
author = {Clune, Thomas L and Goddard, Nasa and Flight, Space},
journal = {IEEE Software},
number = {6},
pages = {49--55},
title = {{Software Testing and Verification in Climate Model Development}},
volume = {28},
year = {2011}
}
@inproceedings{Parker2016,
address = {Melbourne, Australia},
author = {Parker, Miranda C and Guzdial, Mark and Engleman, Shelly},
booktitle = {Proceedings of the 2016 ACM Conference on International Computing Education Research},
doi = {10.1145/2960310.2960316},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parker, Guzdial, Engleman - 2016 - Replication, Validation, and Use of a Language Independent CS1 Knowledge Assessment.pdf:pdf},
isbn = {978-1-4503-4449-4},
keywords = {assessment,cs1,replication,validity},
pages = {93--101},
publisher = {ACM},
series = {ICER '16},
title = {{Replication, Validation, and Use of a Language Independent CS1 Knowledge Assessment}},
url = {http://doi.acm.org/10.1145/2960310.2960316},
year = {2016}
}
@book{Pink2015,
address = {Washington, DC},
author = {Pink, Sarah and Horst, Heather and Postill, John and Jhorth, Larissa and Lewis, Tania and Tacchi, Jo},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Sage Publications},
title = {{Digital Ethnography: Principles and Practice}},
year = {2015}
}
@inproceedings{Jaferian2011,
address = {New York, New York, USA},
author = {Jaferian, Pooya and Hawkey, Kirstie and Sotirakopoulos, Andreas and Beznosov, Konstantin},
booktitle = {Proceedings of the 2011 annual conference extended abstracts on Human factors in computing systems - CHI EA '11},
doi = {10.1145/1979742.1979820},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaferian et al. - 2011 - Heuristics for evaluating IT security management tools.pdf:pdf},
isbn = {9781450302685},
keywords = {computer supported cooperative work,heuristic evaluation,it security management},
month = {may},
pages = {1633},
publisher = {ACM Press},
title = {{Heuristics for evaluating IT security management tools}},
url = {http://dl.acm.org/citation.cfm?id=1979742.1979820},
year = {2011}
}
@inproceedings{Vanhanen2007,
abstract = {The interest in pair programming (PP) has increased recently, e.g. by the popularization of agile software development. However, many practicalities of PP are poorly understood. We present experiences of using PP extensively in an industrial project. The fact that the team had a limited number of high-end work-stations forced it in a positive way to quick deployment and rigorous use of PP. The developers liked PP and learned it easily. Initially, the pairs were not rotated frequently but adopting daily, random rotation improved the situation. Frequent rotation seemed to improve knowledge transfer. The driver/navigator roles were switched seldom, but still the partners communicated actively. The navigator rarely spotted defects during coding, but the released code contained almost no defects. Test-driven development and design in pairs possibly decreased defects. The developers considered that PP improved quality and knowledge transfer, and was better suited for complex tasks than for easy tasks. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Experiences of using pair programming in an agile project},
author = {Vanhanen, J and Korpi, H},
booktitle = {Proceedings of the Annual Hawaii International Conference on System Sciences},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Experiences of using pair programming in an agile project}},
url = {citeulike-article-id:3934827 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-39749173930{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{VanDeursen2002,
abstract = {Testing and refactoring are core activities in extreme programming (XP). In principle, they are separate activities where the tests are used to verify that refactorings do not change behavior of the system. In practice however, they can become intertwined when refactorings invalidate tests. This paper explores the precise relationship between the two. First, we identify which of the published refactorings affect the test code. Second, we observe that if test-first design is a way to arrive at well-designed code, {\^{a}}€{\oe}test-first refactoring{\^{a}}€ is a way to arrive at a better design for existing code. Third, some refactorings improve testability, and should therefore be followed by improvements of the test code. To emphasize this, we propose the notion of {\^{a}}€{\oe}refactoring session{\^{a}}€ which includes changes to the code followed by changes to the tests. To guide the developer in the steps to take, we propose to extend the description of the mechanics of individual refactorings with consequences for the corresponding test code.},
annote = {The video store revisited{\^{a}}€“thoughts on refactoring and testing},
author = {van Deursen, A and Moonen, L},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {71{\^{a}}€“76},
title = {{The video store revisited{\^{a}}€“thoughts on refactoring and testing}},
url = {citeulike-article-id:3934824 {\#}},
year = {2002}
}
@book{Halstead1977,
address = {New York},
author = {Halstead, Maurice Howard},
publisher = {Elsevier},
title = {{Elements of Software Science}},
year = {1977}
}
@inproceedings{Yurcik2006d,
address = {New York, New York, USA},
author = {Yurcik, William and Abdullah, Kulsoom and Copeland, John A. and Yurcik, William and Abdullah, Kulsoom and Copeland, John A. and Yurcik, William and Abdullah, Kulsoom and Copeland, John A. and Yurcik, William},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179588},
isbn = {1595935495},
keywords = {IDS alarm visualization,IP address space visualization,anomaly detection,difference view,filtering network data,intrusion detection,netflows,network security information visualization,parallel coordinates,security visualization,sparklines,traffic visualization},
month = {nov},
pages = {63},
publisher = {ACM Press},
title = {{Tool update}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179588 http://dl.acm.org/citation.cfm?id=1179576.1179589 http://dl.acm.org/citation.cfm?id=1179576.1179587},
year = {2006}
}
@inproceedings{5375536,
abstract = {Many network visualizations make the assumption that an administrator has previously determined the subset of data that should be visualized. Yet the problem remains that if the visualization provides no insight into the network events that warrant further consideration, then the administrator must go back to the data to determine what should be visualized next. This is a critical issue given the amount of network data under consideration, only a small portion of which can be examined at any one time. In this paper we present a visualization that provides context for network visualizations by providing a high-level view of network events. Our visualization not only provides a starting point for network visualization, but also reduces the cognitive burden of the analyst by providing a visual paradigm for both the filtering of network data and the selection of network data to drill into and visualize with alternative representations. We demonstrate, through the use of a case study, that our visualization can provide motivation for further investigation into anomalous network activity.},
author = {Glanfield, J and Brooks, S and Taylor, T and Paterson, D and Smith, C and Gates, C and McHugh, J},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375536},
keywords = {OverFlow,anomalous network activity,network analys},
pages = {11--19},
title = {{Over flow: An overview visualization for network analysis}},
year = {2009}
}
@inproceedings{Korel2009,
address = {Denver, CO},
author = {Korel, Bogdan and Koutsogiannakis, George},
booktitle = {2009 International Conference on Software Testing, Verification, and Validation Workshops},
doi = {10.1109/ICSTW.2009.45},
isbn = {978-0-7695-3671-2},
pages = {77--84},
title = {{Experimental Comparison of Code-Based and Model-Based Test Prioritization}},
url = {http://ieeexplore.ieee.org/document/4976373/},
year = {2009}
}
@inproceedings{6128269,
abstract = {Visual cryptography is a cryptographic technique to achieve visual secret sharing. Shares are distributed to every participant and overlapping a number of shares can recover the original secret. The participants' cheating is discussed more in previous literatures. However, the issue of the distribution center's dishonesty exists similarly in visual cryptography. In this paper, a verifiable visual cryptography scheme is proposed to verify whether the share is authorized, in which we introduce the Third Trusted Party (TTP) whose action is guaranteed. The scheme solves the participants distrusting in the center so as to improve the security of visual cryptography schemes.},
author = {Yanyan, Han and Dong, Yao and Xiaoni, Cheng and Wencai, He},
booktitle = {Computational Intelligence and Security (CIS), 2011 Seventh International Conference on},
doi = {10.1109/CIS.2011.218},
keywords = {TTP,VVCS,distribution center,third trusted party,v},
pages = {974--977},
title = {{VVCS: Verifiable Visual Cryptography Scheme}},
year = {2011}
}
@inproceedings{Grant2010,
address = {Timosoara, Romania},
author = {Grant, Scott and Cordy, James R.},
booktitle = {2010 10th IEEE Working Conference on Source Code Analysis and Manipulation},
doi = {10.1109/SCAM.2010.22},
isbn = {978-1-4244-8655-7},
keywords = {Biological system modeling,Cloning,Data models,Information retrieval,Measurement,Natural languages,Semantics,concept location,latent Dirichlet allocation models,latent concepts,latent dirichlet allocation,latent substructure,latent topic model,latent topics,program diagnostics,software corpus,source code analysis,source code blocks,source code corpus,statistical analysis},
language = {English},
month = {sep},
pages = {65--74},
publisher = {IEEE},
title = {{Estimating the Optimal Number of Latent Concepts in Source Code Analysis}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5601828},
year = {2010}
}
@inproceedings{4804443,
abstract = {NetFlow data is routinely captured at the border of many enterprise networks. Although not as rich as full packet-capture data, NetFlow provides a compact record of the interactions between host pairs on either side of the monitored border. Analysis of this data presents a challenge to the security analyst due to its volume. We report preliminary results on the development of a suite of visualization tools that are intended to complement command line tools, such as those from the SiLK Tools, that are currently used by analysts to perform forensic analysis of NetFlow data. The current version of the tool set draws on three visual paradigms: activity diagrams that display various aspects of multiple individual host behaviors as color coded time series, connection bundles that show the interactions among hosts and groups of hosts, and the NetBytes viewer that allows detailed examination of the port and volume behaviors of an individual host over a period of time. The system supports drill down for additional detail and pivoting that allows the analyst to examine the relationships among the displays. SiLK data is preprocessed into a relational database to drive the display modes, and the tools can interact with the SiLK system to extract additional data as necessary.},
author = {Taylor, T and Paterson, D and Glanfield, J and Gates, C and Brooks, S and McHugh, J},
booktitle = {Conference For Homeland Security, 2009. CATCH '09. Cybersecurity Applications Technology},
doi = {10.1109/CATCH.2009.18},
keywords = {NetBytes,NetFlow data,SiLK Tools,coded time series},
month = {mar},
pages = {186--198},
title = {{FloVis: Flow Visualization System}},
year = {2009}
}
@inproceedings{Muller2003a,
abstract = {Test-driven development is one of the central tech- niques of Extreme Programming. However, the im- pact of test-driven development on the business value of a project has not been studied so far. We present an economic model for the return on investment when using test-driven development instead of the conven- tional development process. Two factors contribute to the return on investment of test-driven development: the productivity di{\^{A}}{\textregistered}erence between test-driven develop- ment, and the conventional process and the ability of test-driven development to deliver higher quality code. Furthermore, we can identify when TDD breaks even with conventional development.},
annote = {About the return on investment of test-driven development},
author = {M{\"{u}}ller, M M and Padberg, F},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {8},
title = {{About the return on investment of test-driven development}},
url = {citeulike-article-id:3934728 {\#}},
volume = {5},
year = {2003}
}
@article{harter2012does,
author = {Harter, Donald E and Kemerer, Chris F and Slaughter, Sandra A},
journal = {Software Engineering, IEEE Transactions on},
number = {4},
pages = {810--827},
publisher = {IEEE},
title = {{Does software process improvement reduce the severity of defects? A longitudinal field study}},
volume = {38},
year = {2012}
}
@inproceedings{5604077,
abstract = {Recently several terrorist acts have created disruptions in the airline industries, tourism as well as the financial markets. It is believed just like the World War II had accelerated the development of nuclear energy, the longer-term impact of the current war on terrorism could lead to the allocation of major resources (manpower, funding etc.) towards dealing with this. In this paper, we present a social network concept to visualize a terrorist network. Visualization is very important part for analyzing a network since it can quickly provide good insight into the network structure, major members, and their properties. We used a matrix factorization methods called Semi Discrete Decomposition, which is highly suitable for dealing with huge networks. Empirical results using the 9-11 network data illustrate the efficiency of the proposed approach.},
author = {Snasel, Vaclav and Horak, Zdenek and Abraham, Ajith},
booktitle = {2010 Sixth International Conference on Information Assurance and Security},
doi = {10.1109/ISIAS.2010.5604077},
isbn = {978-1-4244-7407-3},
keywords = {airline industry,financial market,link suggestion},
month = {aug},
pages = {335--337},
publisher = {Ieee},
title = {{Link suggestions in terrorists networks using Semi Discrete Decomposition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5604077},
year = {2010}
}
@inproceedings{Panichella2013,
address = {San Francisco, CA},
author = {Panichella, Annibale and Dit, Bogdan and Oliveto, Rocco and {Di Penta}, Massimiliano and Poshyvanyk, Denys and {De Lucia}, Andrea},
booktitle = {Proceedings of the 2013 International Conference on Software Engineering (ICSE '13)},
isbn = {978-1-4673-3076-3},
month = {may},
pages = {522--531},
publisher = {IEEE Press},
title = {{How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms}},
url = {http://dl.acm.org/citation.cfm?id=2486788.2486857},
year = {2013}
}
@article{Abacus2005,
abstract = {Test Driven Development: A Practical Guide by David AstelsAgile {\&} Iterative Development: A Manager's Guide by Craig LarmanSoftware by Numbers: Low-Risk, High-Return Development by Mark Denne and Jane Cleland-Huang},
annote = {Using Test-Driven Software Development Tools},
author = {Abacus, A and Barker, M and Freedman, P},
journal = {Software, IEEE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {88--91},
title = {{Using Test-Driven Software Development Tools}},
url = {citeulike-article-id:3934529 http://dx.doi.org/10.1109/MS.2005.51},
volume = {22},
year = {2005}
}
@article{Runeson2008,
author = {Runeson, Per and H{\"{o}}st, Martin},
doi = {10.1007/s10664-008-9102-8},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Runeson, H{\"{o}}st - 2008 - Guidelines for conducting and reporting case study research in software engineering.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {dec},
number = {2},
pages = {131--164},
title = {{Guidelines for conducting and reporting case study research in software engineering}},
url = {http://link.springer.com/10.1007/s10664-008-9102-8},
volume = {14},
year = {2008}
}
@article{Nuseibeh1997,
author = {Nuseibeh, Bashar},
doi = {10.1109/MS.1997.589224},
issn = {0740-7459},
journal = {IEEE Software},
month = {may},
number = {3},
pages = {15--16},
title = {{Ariane 5: Who Dunnit?}},
url = {https://www.computer.org/csdl/mags/so/1997/03/s3015.pdf http://dl.acm.org/citation.cfm?id=624619.625708},
volume = {14},
year = {1997}
}
@techreport{Atwood1978,
address = {Alexandria, VA},
author = {Atwood, M E and Ramsey, H R},
institution = {.S. Army Research Institute for the Behavioral and Social Sciences},
pages = {TR--78--A21},
title = {{Cognitive structures in the comprehension and memory of computer programs: An investigation of computer debugging}},
year = {1978}
}
@inproceedings{Murgia2014,
address = {Hyderabad, India},
author = {Murgia, Alessandro and Tourani, Parastou and Adams, Bram and Ortu, Marco},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories - MSR 2014},
doi = {10.1145/2597073.2597086},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murgia et al. - 2014 - Do developers feel emotions an exploratory analysis of emotions in software artifacts.pdf:pdf},
isbn = {9781450328630},
keywords = {sentiment},
mendeley-tags = {sentiment},
pages = {262--271},
publisher = {ACM Press},
title = {{Do developers feel emotions? an exploratory analysis of emotions in software artifacts}},
url = {http://dl.acm.org/citation.cfm?doid=2597073.2597086},
year = {2014}
}
@article{Gruber1995,
author = {Gruber, T R},
keywords = {ontologies,ontology},
pages = {907--928},
title = {{Towards Principles for the Design of Ontologies Used for Knowledge Sharing}},
volume = {43},
year = {1995}
}
@inproceedings{5438118,
abstract = {An attack graph increasingly plays an important role in network security. It shows possible paths of actions consisting of the network vulnerability exploits that can lead to security breaches. Because most attack graphs are very large and complex, much research has focused on how these graphs can be automatically and efficiently generated. However, little has been done on attack graph analysis, namely how we can use attack graphs to better protect the network. This paper addresses the latter issue. We present a suit of systematic approaches to statically analyzing attack graphs by means of reasoning mechanisms based on logical expressions and conditional preference networks. The proposed approaches are general and theoretically grounded. The paper describes the approaches in details. We show how the resulting analysis can help derive many useful decisions. For example, it can assist a security administrator in selecting most cost-effective countermeasures, based on his preference criteria, to improve the security flaws found in the attack graph. For understandability, we illustrate our approach by presenting a study of a simple and small but realistic case scenario.},
author = {Kijsanayothin, P and Hewett, R},
booktitle = {Availability, Reliability, and Security, 2010. ARES '10 International Conference on},
doi = {10.1109/ARES.2010.21},
keywords = {analytical approach;attack graph analysis;conditio},
pages = {25--32},
title = {{Analytical Approach to Attack Graph Analysis for Network Security}},
year = {2010}
}
@book{PearsonHartley,
author = {Pearson, E S and Hartley, H O},
publisher = {Cambridge University Press},
title = {{Biometrika Tables for Statisticians, Vol. II}},
year = {1972}
}
@article{McLaughlin2016,
abstract = {ABSTRACTPrevious literature has suggested that examining Twitter messages can be productive for studying how the public shares and spreads health information on social media. Preexposure prophylaxis (PrEP) is a promising approach to HIV prevention, yet there are many issues that may influence its effective implementation. This study examined social representations of PrEP on Twitter. One thousand four hundred and thirty-five Tweets were collected and 774 English Tweets were content-analyzed to explore propagation of various issues around daily oral PrEP, as well as characteristics of the sources of those Tweets. We also examined how Twitter message content influenced information propagation. Our findings revealed that PrEP-related information on Twitter covered a wide range of issues, and individual users constituted the majority of the Tweet creators among all the sources, including news media, nonprofit and academic groups, and commercial entities. Using Poisson regression, we also found that a Tweet's ...},
author = {McLaughlin, Margaret L. and Hou, Jinghui and Meng, Jingbo and Hu, Chih-Wei and An, Zheng and Park, Mina and Nam, Yujung},
doi = {10.1080/10410236.2015.1027033},
issn = {1041-0236},
journal = {Health Communication},
keywords = {twitter},
mendeley-tags = {twitter},
month = {aug},
number = {8},
pages = {998--1007},
publisher = {Routledge},
title = {{Propagation of Information About Preexposure Prophylaxis (PrEP) for HIV Prevention Through Twitter}},
url = {http://www.tandfonline.com/doi/full/10.1080/10410236.2015.1027033},
volume = {31},
year = {2016}
}
@inproceedings{4290951,
abstract = {Constructive type theory (CTT) is both a formal logic and a programming language which contains inherent benefits both in terms of formality and program correctness and in the potential for efficient concurrent execution. In contrast, the mammalian visual cortex represents a naturally occurring visual processing system capable of the rapid concurrent evaluation of complex data domains. The efficient exploitation of a merger between these two systems would represent major advantages in such diverse fields as machine reading, automated guidance, navigation and, significantly, biometrically based security identification systems. The current paper explores the possibilities of achieving such a merger and the technological challenges and opportunities it would represent in constructing a practical remote biometric based identification system.},
author = {Howells, G and McDonald-Maier, K D and Binzegger, T and Young, M P},
booktitle = {Bio-inspired, Learning, and Intelligent Systems for Security, 2007. BLISS 2007. ECSIS Symposium on},
doi = {10.1109/BLISS.2007.13},
keywords = {MAVIS,concurrent execution,constructive type theor},
pages = {115--118},
title = {{MAVIS: A Secure Formal Computational Paradigm based on the Mammalian Visual System}},
year = {2007}
}
@article{Katz1987,
author = {Katz, Irvin R and Anderson, John R},
issn = {0737-0024},
journal = {Hum.-Comput. Interact.},
month = {dec},
number = {4},
pages = {351--399},
title = {{Debugging: an analysis of bug-location strategies}},
volume = {3},
year = {1987}
}
@inproceedings{569669,
abstract = {The Interactive Link forms part of a suite of products being developed as part of the Starlight research program. This research program is investigating methods for achieving military-relevant information security capabilities which are genuinely cost-effective. The Interactive Link is a retrofittable device which fits to commercial off-the-shelf workstations and PCs which are connected to classified networks. The Link allows interactive access to low or unclassified networks, such as the Internet, in a manner which is accreditable via the authorising government agencies. The advantage of the Interactive Link over other methods is that it allows untrusted graphical windowing applications, such as X Windows applications or Microsoft Windows applications, to run in a manner permitting users to have windows at differing security levels},
author = {Anderson, M. and North, C. and Griffin, J. and Milner, R. and Yesberg, J. and Yiu, K.},
booktitle = {Proceedings 12th Annual Computer Security Applications Conference},
doi = {10.1109/CSAC.1996.569669},
isbn = {0-8186-7606-X},
keywords = {Interactive Link,Internet,Microsoft Windows applic},
month = {dec},
pages = {55--63},
publisher = {IEEE Comput. Soc. Press},
title = {{Starlight: Interactive Link}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=569669},
year = {1996}
}
@inproceedings{Ricca2008,
abstract = {Test-driven software development tackles the problem of operationally defining the features to be implemented by means of test cases. This approach was recently ported to the early development phase, when requirements are gathered and clarified. Among the existing proposals, Fit (Framework for Integrated Testing) supports the precise specification of requirements by means of so called Fit tables, which express relevant usage scenarios in a tabular format, easily understood also by the customer. Fit tables can be turned into executable test cases through the creation of pieces of glue code, called fixtures. In this paper, we test the claimed benefits of Fit through a series of three controlled experiments in which Fit tables and related fixtures are used to clarify a set of change requirements, in a software evolution scenario. Results indicate improved correctness achieved with no significant impact on time, however benefits of Fit vary in a substantial way depending on the developers' experience. Preliminary results on the usage of Fit in combination with pair programming revealed another relevant source of variation. Copyright 2008 ACM.},
annote = {Are fit tables really talking? a series of experiments to understand whether fit tables are useful during evolution tasks},
author = {Ricca, F and {Di Penta}, M and Torchiano, M and Tonella, P and Ceccato, M and Visaggio, C A},
booktitle = {Proceedings - International Conference on Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {361--370},
title = {{Are fit tables really talking? a series of experiments to understand whether fit tables are useful during evolution tasks}},
url = {citeulike-article-id:3934767 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-56749160500{\&}{\#}38 partnerID=40},
year = {2008}
}
@book{Lahman2011,
author = {Lahman, H.S.},
keywords = {Model based development,Model-checking,Safety},
mendeley-tags = {Model based development,Model-checking,Safety},
pages = {560},
publisher = {Addison-Wesley Professional},
title = {{Model-Based Development: Applications}},
url = {http://www.amazon.com/Model-Based-Development-Applications-H-S-Lahman/dp/0321774078},
year = {2011}
}
@inproceedings{Wilking2005,
abstract = {The main goal to be answered by this Ph.D. thesis is whether there is a potential for a successful and powerful application of agile methods and related techniques to embedded systems development or not (cf. [2]). Regarding the special context of embedded system, there are some aspects to be mentioned as stated in [3]. These include the function oriented development which lead to early testing of the system, the use of target-processor simulation and the problem of hardware software codesign. The first problem being addressed is the evaluation of well known sub-techniques like refactoring, TDD, fast development cycles, short design horizon, or similar methods in the context of embedded systems. A complementary approach consists of the elaboration of underlying root causes which make agile methods appear as a sound alternative to classic techniques. For example assumptions like source code degrading over time, non-costumer oriented development, overly complex systems, and wrong development focus could be checked. A possible subdivision of the causes can be done by distinguishing effects that are generated by agile methods and effects that typically exist in embedded system engineering. This can be regarded as an alternative upside down procedure which will more likely yield a justification for agile methods in embedded system development. Finding a causation with an appropriate prioritization appears more challenging and thus will be used only to verify techniques which have a strong effect. The first step toward an assessment of agile methods has been started by executing a study during a lab course which is guided by the ideas described in [1]. Here, the students are divided into a planning group and an agile group, each developing a pre-crash system based on ultrasonic sensors. The two data collection mechanisms are a biweekly survey and a time recording log. The underlying aim is to show the influence of the planning horizon on embedded system development. This approach already sketches the main validation technique, which will be quantitative and composed of case studies and experiments. In addition, a case study with a high degree of variable control as proposed by [4] will be executed in order to guide the evaluation process to the most promising aspects of agile methods for embedded system development. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {Agile methods for embedded systems},
author = {Wilking, D},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {319--320},
title = {{Agile methods for embedded systems}},
url = {citeulike-article-id:3934837 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444506316{\&}{\#}38 partnerID=40},
volume = {3556},
year = {2005}
}
@article{Murthy2006,
address = {New York, New York, USA},
author = {Murthy, P. V.R. and Anitha, P. C. and Mahesh, M. and Subramanyan, Rajesh},
doi = {10.1145/1138953.1138968},
isbn = {1595933948},
journal = {Proceedings of the 2006 international workshop on Scenarios and state machines: models, algorithms, and tools - SCESM '06},
keywords = {context-free grammar,model based testing,uml statecharts},
pages = {75},
publisher = {ACM Press},
title = {{Test ready UML statechart models}},
url = {http://portal.acm.org/citation.cfm?doid=1138953.1138968},
year = {2006}
}
@inproceedings{Rivers2016,
abstract = {The recent surge in interest in using educational data mining on student written programs has led to discoveries about which compiler errors students encounter while they are learning how to program. However, less attention has been paid to the actual code that students produce. In this paper, we investigate programming data by using learning curve analysis to determine which programming elements students struggle with the most when learning in Python. Our analysis extends the traditional use of learning curve analysis to include less structured data, and also reveals new possibilities for when to teach students new programming concepts. One particular discovery is that while we find evidence of student learning in some cases (for example, in function definitions and comparisons), there are other programming elements which do not demonstrate typical learning. In those cases, we discuss how further changes to the model could affect both demonstrated learning and our understanding of the different concepts that students learn.},
address = {Melbourne, Australia},
author = {Rivers, Kelly and Harpstead, Erik and Koedinger, Ken},
booktitle = {ICER 2016 - Proceedings of the 2016 ACM Conference on International Computing Education Research},
doi = {10.1145/2960310.2960333},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rivers, Harpstead, Koedinger - 2016 - Learning curve analysis for programming Which concepts do students struggle with.pdf:pdf},
isbn = {9781450344494},
keywords = {Educational data mining,Knowledge components,Learning curve analysis,Programming syntax},
month = {aug},
pages = {143--151},
publisher = {Association for Computing Machinery, Inc},
title = {{Learning curve analysis for programming: Which concepts do students struggle with?}},
year = {2016}
}
@inproceedings{Briand2004,
abstract = {A number of testing strategies have been proposed using state machines and statecharts as test models in order to derive test sequences and validate classes or class clusters. Though such criteria have the advantage of being systematic, little is known on how cost effective they are and how they compare to each other. This article presents a precise simulation and analysis procedure to analyze the cost-effectiveness of statechart-based testing techniques. We then investigate, using this procedure, the cost and fault detection effectiveness of adequate test sets for the most referenced coverage criteria for statecharts on three different representative case studies. Through the analysis of common results and differences across studies, we attempt to draw more general conclusions regarding the costs and benefits of using the criteria under investigation.},
address = {Edinburgh, Scotland},
author = {Briand, L.C. and Labiche, Y and Wang, Y},
booktitle = {Proceedings. 26th International Conference on Software Engineering},
doi = {10.1109/ICSE.2004.1317431},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Briand, Labiche, Wang - 2004 - Using simulation to empirically investigate test coverage criteria based on statechart.pdf:pdf},
isbn = {0-7695-2163-0},
issn = {0270-5257},
keywords = {Analytical models,Computational modeling,Costs,Fault detection,Laboratories,Performance analysis,Software quality,Software testing,System testing,Systems engineering and theory,cost detection,fault detection,finite state machines,program testing,simulation,state machines,statechart-based testing,test coverage,test coverage criteria},
mendeley-tags = {test coverage},
pages = {86--95},
publisher = {Association for Computing Machinery},
title = {{Using simulation to empirically investigate test coverage criteria based on statechart}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1317431},
year = {2004}
}
@inproceedings{Goetz2002,
address = {Essen, Germany},
annote = {How XP can help, but also one or two flaws. Not very thorough.},
author = {Goetz, R},
keywords = {XP,requirements},
title = {{How Agile Processes Can Help in Time-Constrained Requirements Engineering}},
year = {2002}
}
@inproceedings{5593242,
abstract = {Digital control systems are essential to the safe and efficient operation of a variety of industrial processes in sectors such as electric power, oil and gas, water treatment, and manufacturing. Modern control systems are increasingly connected to other control systems as well as to corporate systems. They are also increasingly adopting networking technology and system and application software from conventional enterprise systems. These trends can make control systems vulnerable to cyber attack, which in the case of control systems may impact physical processes causing environmental harm or injury. We present some results of the DATES (Detection and Analysis of Threats to the Energy Sector) project, wherein we adapted and developed several intrusion detection technologies for control systems. The suite of detection technologies was integrated and connected to a commercial security event correlation framework from ArcSight. We demonstrated the efficacy of our detection and correlation solution on two coupled testbed environments. We particularly focused on detection, correlation, and visualization of a network traversal attack, where an attacker penetrates successive network layers to compromise critical assets that directly control the underlying process. Such an attack is of particular concern in the layered architectures typical of control system implementations.},
author = {Briesemeister, Linda and Cheung, Steven and Lindqvist, Ulf and Valdes, Alfonso},
booktitle = {2010 Eighth International Conference on Privacy, Security and Trust},
doi = {10.1109/PST.2010.5593242},
isbn = {978-1-4244-7551-3},
keywords = {DATES,commercial security event correlation framew},
month = {aug},
pages = {15--22},
publisher = {Ieee},
title = {{Detection, correlation, and visualization of attacks against critical infrastructure systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5593242},
year = {2010}
}
@inproceedings{Goldwasser2008,
abstract = {There is an ongoing debate regarding the role of object orien-tation in the introductory programming sequence. While the pendulum swings to and fro between the " objects first " and " back to basics " extremes, there is general agreement that object-oriented programming is central to modern software development and therefore integral to a computer science curriculum. Developing effective approaches to teach these principles raises challenges that have been exacerbated by the use of Java or C++ as the first instructional language. In this paper, we recommend Python as an excellent choice for teaching an object-oriented CS1. Although often viewed as a " scripting " language, Python is a fully object-oriented language with a consistent object model and a rich set of built-in classes. Based upon our experiences, we describe aspects of the language that help support a balanced intro-duction to object orientation in CS1. We also discuss the downstream effects on our students' transition to Java and C++ in subsequent courses.},
address = {Madrid, Spain},
author = {Goldwasser, Michael H. and Letscher, David},
booktitle = {Proceedings of the Conference on Integrating Technology into Computer Science Education, ITiCSE},
doi = {10.1145/1384271.1384285},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goldwasser, Letscher - 2008 - Teaching an object-oriented CS1 - With python.pdf:pdf},
isbn = {9781605580784},
keywords = {CS1,CS2,Object orientation,Python},
mendeley-tags = {CS2},
pages = {42--46},
publisher = {ACM Press},
title = {{Teaching an object-oriented CS1 - With python}},
url = {http://portal.acm.org/citation.cfm?doid=1384271.1384285},
year = {2008}
}
@misc{Ou2003,
abstract = {Test-Driven Development (TDD) is one of the core programming practices of XP. However, developing database access code test-driven is often difficult, if not impossible. This paper presents a practical solution to this problem, making use of local development databases for testing and Open Source tools for schema migration and test data management. The examples are outlined in Java, but the basic ideas and principles are widely applicable to different languages and platforms. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
annote = {Test-driven database development: A practical guide},
author = {Ou, R},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {82--90},
title = {{Test-driven database development: A practical guide}},
url = {citeulike-article-id:3934744 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35248848426{\&}{\#}38 partnerID=40},
volume = {2753},
year = {2003}
}
@article{Corritore1999,
author = {Corritore, CL},
journal = {International Journal of Human-Computer},
keywords = {program comprehension},
mendeley-tags = {program comprehension},
pages = {61--83},
title = {{Mental representations of expert procedural and object-oriented programmers in a software maintenance task}},
url = {http://www.sciencedirect.com/science/article/pii/S1071581998902361},
volume = {50},
year = {1999}
}
@techreport{Williams2008,
address = {Starkville, MS},
author = {Williams, Byron and Carver, J},
institution = {Mississippi State University},
pages = {MSU--081216},
title = {{Characterizing software architecture changes: A systematic review}},
year = {2008}
}
@inproceedings{1625379,
abstract = { Most e-government applications have to find a solution for simple, reliable, secure and authentic signing of official documents. Citizens need a simple way to verify the authenticity and integrity of an official document. Currently XML documents allow representing such documents. However, the XML format does not guarantee a definite visual presentation of the document (presentation problem). In this paper we describe a solution approach -so-called authentic PDF - using PDF technology that fulfills the following key requirements: 1) A visual presentation that resembles the traditional style of an official document; 2) A visual representation of the signature value that does not change the document authenticity; 3) The option for the holder of an official document to restore the electronic version of the authentic official document from the visual representation of the document (e.g. printout); 4) The filtering of dynamic content. We implemented and evaluated different approaches in a feasibility study using a typical e-government document set. The results of the study indicate that PDF is suitable to meet specific legal requirements on a signature solution in combination with a smartcard; the method has proven to be reliable and support a sufficient level of security.},
author = {Neubauer, T. and Weippl, E. and Biffl, S.},
booktitle = {First International Conference on Availability, Reliability and Security (ARES'06)},
doi = {10.1109/ARES.2006.54},
isbn = {0-7695-2567-9},
keywords = {XML documents,authentic PDF,digital signatures},
month = {apr},
pages = {8 pp.--731},
publisher = {Ieee},
title = {{Digital signatures with familiar appearance for e-government documents: authentic PDF}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1625379},
year = {2006}
}
@article{6036174,
abstract = {We propose a weak security notion for visual secret sharing (VSS) schemes. Under such a weak security notion, VSS schemes are designed to be secure against attackers' eyesight, but are not unconditionally secure, in general. In this paper, we theoretically discuss the relation between unconditionally secure (US) and weakly secure (WS) VSS schemes and present two constructions of WS-VSS schemes for color images. We show that WS-VSS schemes can achieve clearer color reproduced images with a smaller pixel expansion compared to those using US-VSS schemes, while we clarify that the basis matrices in both types of VSS schemes for black-white binary images are the same. These results suggest that the proposed VSS schemes can be regarded as ramp (or nonperfect) VSS schemes for color secret images.},
author = {Iwamoto, M},
doi = {10.1109/TIFS.2011.2170975},
issn = {1556-6013},
journal = {Information Forensics and Security, IEEE Transactions on},
keywords = {color images,unconditionally secure VSS schemes,vi},
month = {apr},
number = {2},
pages = {372--382},
title = {{A Weak Security Notion for Visual Secret Sharing Schemes}},
volume = {7},
year = {2012}
}
@inproceedings{5678692,
abstract = {Correctional Service Canada (CSC) operates 57 Institutions equipped with a range of mission critical Security, Communications and Access Management systems. These include CCTV assessment, intrusion detection, radio communications and door control systems. User Interfaces range from colour graphic touch screens to knobs, push buttons etc. Operational and technical challenges include the following issues: 1) sub-system technology is vendor specific, 2) reliability is critical in a challenging environment, 3) system maintenance is expensive, 4) inconsistent "Look and Feel" resulting in usability issues, 5) costly to add new systems, multiple, legacy and proprietary protocols amp; connectivity approaches, legacy installations are cable intensive, 6) minimal operator or maintenance training crossover, 7) procurement requirements drive "silo" system implementation. The objective is to develop a "next generation" systems architecture that will: "abstract" the subsystem user management interfaces, ensuring all sub-systems can be managed using a simple, consistent graphical user interface, accessible using a web browser; model the data and behaviour of the edge devices, doors; cameras, sensors, etc. and normalizes it; allowing them to be managed by and provide notifications to standard software based applications; support inter-domain interoperability; use an industry standard communications protocol, likely with extensions, such as BACNet or SNMP, for edge device to application server connectivity; use TCP/IP over fibre as the preferred transport, network and physical technologies; integrate legacy sub-systems into the application server using mediation software; define the look and feel of the graphical icons that represent the edge devices and the physical environment in which they are placed, so that each domain is presented in a consistent manner. Developing and adopting this architecture will allow Correctional Service Canada to address the challenges of its unique e- - nvironment and support the definition, procurement and deployment of subsystems more effectively.},
author = {Morton, E},
booktitle = {Security Technology (ICCST), 2010 IEEE International Carnahan Conference on},
doi = {10.1109/CCST.2010.5678692},
issn = {1071-6572},
keywords = {CCTV assessment,Communications and Access Manageme},
pages = {7--16},
title = {{Correctional Service Canada's {\#}x201C;next generation {\#}x201D; command and control systems architecture}},
year = {2010}
}
@article{PeterBishop,
author = {{Peter Bishop}, Robin Bloomfield},
keywords = {Assurance Case},
mendeley-tags = {Assurance Case},
title = {{A Methodology for Safety Case Development}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.219.6546}
}
@article{Holt2013,
abstract = {Computers and the Internet have become a vital part of modern life across the world, affecting communications, finance, and governance. At the same time, technology has created unparalleled opportunities for crime and deviance on- and off-line. Criminological research has expanded its focus over the last two decades to address the various forms of technology-enabled crime and the applicability of traditional theories to account for offending. There is, however, a need for careful consideration of the state of the field in order to identify issues requiring further study and analysis. This study examines the current literature on virtually all forms of cybercrime and the theoretical frameworks used to address these issues. In turn, we hope to give direction to refine our understanding of criminological theory and social policies to combat these offenses.},
author = {Holt, Thomas J. and Bossler, Adam M.},
doi = {10.1080/01639625.2013.822209},
issn = {0163-9625},
journal = {Deviant Behavior},
keywords = {agile,nsf},
language = {en},
mendeley-tags = {agile,nsf},
month = {sep},
number = {1},
pages = {20--40},
publisher = {Taylor {\&} Francis Group},
title = {{An Assessment of the Current State of Cybercrime Scholarship}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01639625.2013.822209},
volume = {35},
year = {2013}
}
@inproceedings{Hernan-Losada2008,
abstract = {The test-first approach has been repeatedly advocated to learn both programming and software engineering. Besides, automatic graders are broadly used, both in programming contests and in everyday teaching. The sort of problems stated in both situations is always placed at the application and higher levels of Bloom's taxonomy, and it demands some degree of mastery on students. In this paper we address the combined use of automatic grading and the test-driven approach from a pedagogical view. In particular, we address assessment at the levels of comprehension, application, analysis and synthesis in Bloom's taxonomy. We make a proposal, based on sorts of exercises and suggested examples. We also gather a set of features that exercises should include as metadata to facilitate its use from pedagogical criteria. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Testing-based automatic grading: A proposal from Bloom{\&}{\#}039;s taxonomy},
author = {Hern{\'{a}}n-Losada, I and Pareja-Flores, C and Vel{\'{a}}zquez-Iturbide},
booktitle = {Proceedings - The 8th IEEE International Conference on Advanced Learning Technologies, ICALT 2008},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {847--849},
title = {{Testing-based automatic grading: A proposal from Bloom's taxonomy}},
url = {citeulike-article-id:3934639 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-51849145088{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Wellington2007,
abstract = {We are interested in how to expose our students to Test Driven Development (TDD) and have experimented with a variety of ways of leveraging testing technology to help our students learn to program in our first programming course. Initially, we developed a framework that allows the students to run tests that are developed by the faculty member. That experience led us to developing a JUnit plug-in that allowed the students to specify the tests without having to write the test code. As a result of these experiences, we have re-structured this class into these roughly sequential phases: learning to read code, learning to write code, and learning to program. Throughout this course, the students are using TDD, writing their own JUnit tests, and refactoring as they develop their code iteratively. This change has been made without dropping any of the required course content. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Experiences using automated tests and test driven development in computer science i},
author = {Wellington, C A and Briggs, T H and Dudley},
booktitle = {Proceedings - AGILE 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {106--112},
title = {{Experiences using automated tests and test driven development in computer science i}},
url = {citeulike-article-id:3934835 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-46449102747{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Paul2013a,
abstract = {We present the result of assessing first-year students' mis- conceptions related to algorithms and data structures. Our study confirms findings fromprevious small-scale studies but additionally broadens the scope of the topics and methods investigated. The evaluation of our experiments sheds light on dependencies between active and passive knowledge as well as on the instruments used; in particular, we conclude that there is no“one size fits all” instrument but that instru- ments should be selected depending on the topic at hand.},
address = {Denver, CO, USA},
author = {Paul, Wolfgang and Vahrenhold, Jan},
booktitle = {Proceeding of the 44th ACM Technical Symposium on Computer Science Education - SIGCSE '13},
doi = {10.1145/2445196.2445212},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paul, Vahrenhold - 2013 - Hunting high and low instruments to detect misconceptions related to algorithms and data structures.pdf:pdf},
isbn = {9781450318686},
keywords = {CS1/2,instruments,misconceptions},
pages = {29--34},
publisher = {ACM Press},
title = {{Hunting high and low: instruments to detect misconceptions related to algorithms and data structures}},
url = {http://dl.acm.org/citation.cfm?doid=2445196.2445212},
year = {2013}
}
@inproceedings{6102491,
abstract = {For the VAST 2011 Network Security Mini-Challenge, we adopted geovisual analytic methods and applied them in the field of network security. We used the GeoViz Toolkit [1] to represent cyber security events, by fabricating a simple {\#}x201C;geography {\#}x201D; of several sets of blocks (one for the workstations, one for the servers, and one for the Internet) using ArcGIS 10 (by ESRI - Environmental System Research Institute). Security data was tabulated using Perl scripts to parse the logs in order to create representations of event frequency and where they occurred on the network. The tabulated security data was then added as attributes of the geography. Exploration of the data and subsequent analysis of the meaning and impact of the cyber security events was made possible using the GeoViz Toolkit.},
author = {Giacobe, N A and Xu, Sen},
booktitle = {Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on},
doi = {10.1109/VAST.2011.6102491},
keywords = {ArcGIS 10,GeoViz Toolkit,Perl scripts,VAST 2011 Ne},
pages = {315--316},
title = {{Geovisual analytics for cyber security: Adopting the GeoViz Toolkit}},
year = {2011}
}
@article{Safavi2013,
abstract = {The popularity of mobile devices in the market is impressive, but this influx of different products has made it difficult for users to secure their infrastructures from potential data breaches. As the number of exposures and attacks increase, there has been a corresponding rise in security solutions offered by researchers. This article reviews the literature to prevent the cybercrime affecting portable devices especially smartphones running Android OS. Extant researches are analyzed and opportunities for future research are identified. Four research questions have been developed and out of 493 articles retrieved, 33 articles have been selected to be analyzed. From the analysis, we have found that Data leakage resulting from device loss or theft, unintentional disclosure of data and phishing attacks are most common between attackers. With no doubt, security investigators have stressed the grandness of protecting classified personal data residing in Android portable devices. They have suggested to use the permission-based security model and behavior-based detection method for protecting classified information. In result we found that Android OS can handle and apply the integrated protection model but still there are opportunities for us to improve the security of personal data.},
author = {Safavi, Seyedmostafa and Shukur, Zarina and Razali, Rozilawati},
doi = {10.1016/j.protcy.2013.12.241},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Safavi, Shukur, Razali - 2013 - Reviews on Cybercrime Affecting Portable Devices.pdf:pdf},
issn = {22120173},
journal = {Procedia Technology},
keywords = {Android,Mobile security,Portable device,Prevent cybercrime,agile,nsf},
mendeley-tags = {agile,nsf},
pages = {650--657},
title = {{Reviews on Cybercrime Affecting Portable Devices}},
url = {http://www.sciencedirect.com/science/article/pii/S2212017313003952},
volume = {11},
year = {2013}
}
@inproceedings{Robbes2007,
address = {Minneapolis, MN},
author = {Robbes, Romain and Romain},
booktitle = {Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007)},
doi = {10.1109/MSR.2007.18},
isbn = {0-7695-2950-X},
month = {may},
pages = {Article No. 15},
title = {{Mining a Change-Based Software Repository}},
url = {http://ieeexplore.ieee.org/document/4228652/},
year = {2007}
}
@inproceedings{Williams2007c,
address = {Minneapolis, MN},
author = {Williams, Laurie and Layman, Lucas and Slaten, Kelli M and Seaman, Carolyn and Berenson, Sarah B},
pages = {677--687},
title = {{On the Impact of a Collaborative Pedagogy on African-American Millennial Students in Software Engineering}},
year = {2007}
}
@misc{KaperskyLab2009,
author = {{Kapersky Lab}},
keywords = {agile,nsf,security},
mendeley-tags = {agile,nsf,security},
title = {{First SMS Trojan detected for smartphones running android}},
url = {http://www.kaspersky.com/about/news/virus/2010/First{\_}SMS{\_}Trojan{\_}detected{\_}for{\_}smartphones{\_}running{\_}Android},
year = {2009}
}
@article{Chen2013,
author = {Chen, Ying and Xu, Heng and Zhou, Yilu and Zhu, Sencun},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2013 - Is this app safe for children a comparison study of maturity ratings on Android and iOS applications.pdf:pdf},
isbn = {978-1-4503-2035-1},
keywords = {agile,android apps,application permissions,children safety,ios apps,maturity rating,mobile app,nsf,privacy},
mendeley-tags = {agile,nsf},
month = {may},
pages = {201--212},
publisher = {International World Wide Web Conferences Steering Committee},
title = {{Is this app safe for children?: a comparison study of maturity ratings on Android and iOS applications}},
url = {http://dl.acm.org/citation.cfm?id=2488388.2488407},
year = {2013}
}
@article{Zhang2006,
abstract = {In order to popularize the Test Driven Development (TDD) practice in Chinese offshore companies, an experimental research was firstly conducted to compare TDD with the traditional waterfall development in a small-scale project. Although the project scale was small and all the subjects were students, this experiment was designed very strictly to guarantee the reliable evaluation of the efficacy of TDD. Furthermore, it is also the first time to evaluate the maintainability and the flexibility of TDD by experiment. SE  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
annote = {Source: Scopus Export Date: 8 January 2009},
author = {Zhang, L and Akifuji, S and Kawai, K and Morioka, T},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Comparison between test driven development and waterfall development in a small-scale project}},
url = {citeulike-article-id:3934850 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746199979{\&}{\#}38 partnerID=40},
year = {2006}
}
@article{Lindvall2007,
abstract = {The evolution of a new technology depends upon a good theoretical basis for developing the technology, as well as upon its experimental validation. In order to provide for this experimentation, we have investigated the creation of a software testbed and the feasibility of using the same testbed for experimenting with a broad set of technologies. The testbed is a set of programs, data, and supporting documentation that allows researchers to test their new technology on a standard software platform. An important component of this testbed is the Unified Model of Dependability (UMD), which was used to elicit dependability requirements for the testbed software. With a collection of seeded faults and known issues of the target system, we are able to determine if a new technology is adept at uncovering defects or providing other aids proposed by its developers. In this paper, we present the Tactical Separation Assisted Flight Environment (TSAFE) testbed environment for which we modeled and evaluated dependability requirements and defined faults to be seeded for experimentation. We describe two completed experiments that we conducted on the testbed. The first experiment studies a technology that identifies architectural violations and evaluates its ability to detect the violations. The second experiment studies model checking as part of design for verification. We conclude by describing ongoing experimental work studying testing, using the same testbed. Our conclusion is that even though these three experiments are very different in terms of the studied technology, using and re-using the same testbed is beneficial and cost effective.},
author = {Lindvall, Mikael and Rus, Ioana and Donzelli, Paolo and Memon, A and Zelkowitz, Marvin V and Betin-Can, Aysu and Bultan, Tevfik and Ackermann, Chris and Anders, Bettina and Asgari, Sima and Basili, Victor R and Fellman, Joerg and Hirschbach, Daniel and Hochstein, Lorin and Shull, Forrest and Tvedt, Roseanne Tesoriero and Pech, Dan},
journal = {Empirical Software Engineering: An International Journal},
keywords = {Empirical Software Engineering,Software Engineering},
month = {aug},
number = {4},
pages = {417--444},
title = {{Experimenting with Software Testbeds for Evaluating New Technologies}},
url = {http://portal.acm.org/citation.cfm?id=1285505},
volume = {12},
year = {2007}
}
@misc{Beck2001c,
author = {Beck, Kent and Beedle, Mike and van Bennekum, A and Cockburn, Alistair and Cunningham, Ward and Fowler, Martin and Grenning, James and Highsmith, Jim and Hunt, Andrew and Jeffries, Ron and Kern, Jon and Marick, Brian and Martin, Robert C and Mallor, S and Schwaber, Ken and Sutherland, Jeff and Thomas, Dave and van Bennekum, Arie and Cockburn, Alistair and Cunningham, Ward and Fowler, Martin and Grenning, James and Highsmith, Jim and Hunt, Andrew and Jeffries, Ron and Kern, Jon and Marick, Brian and Martin, Robert C and Mellor, Steve and Schwaber, Ken and Sutherland, Jeff and Thomas, Dave},
isbn = {http://www.agileAlliance.org},
title = {{The Agile Manifesto}},
url = {http://www.agilemanifesto.org},
year = {2001}
}
@article{Dadeau2008,
author = {Dadeau, F and Kermadec, Adrien De and Tissot, R},
journal = {Abstract State Machines, B and Z},
keywords = {b machine,mation,model-based testing,posix challenge,scenarios,symbolic ani-},
pages = {153--166},
title = {{Combining scenario-and model-based testing to ensure posix compliance}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-87603-8{\_}13},
year = {2008}
}
@article{Hovemeyer2004,
author = {Hovemeyer, David and Pugh, William},
journal = {ACM SIGPLAN Notices},
number = {12},
pages = {91--106},
publisher = {ACM},
title = {{Finding bugs is easy}},
url = {http://portal.acm.org/citation.cfm?id=1052895},
volume = {39},
year = {2004}
}
@book{deming1986out,
author = {Deming, W Edwards},
booktitle = {Center for advanced engineering study, Cambridge, MA},
publisher = {MIT Press},
title = {{Out of the crisis}},
year = {1986}
}
@inproceedings{LeSceller2017,
address = {Reggio Calabria, Italy},
author = {{Le Sceller}, Quentin and Karbab, ElMouatez Billah and Debbabi, Mourad and Iqbal, Farkhund},
booktitle = {Proceedings of the 12th International Conference on Availability, Reliability and Security - ARES '17},
doi = {10.1145/3098954.3098992},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Le Sceller et al. - 2017 - SONAR.pdf:pdf},
isbn = {9781450352574},
keywords = {twitter},
mendeley-tags = {twitter},
pages = {Article 23},
publisher = {ACM Press},
title = {{SONAR}},
url = {http://dl.acm.org/citation.cfm?doid=3098954.3098992},
year = {2017}
}
@inproceedings{1377216,
abstract = { Anomaly-based intrusion detection (AID) techniques are useful for detecting novel intrusions without known signatures. However, AID techniques suffer from higher false alarm rate compared to signature-based intrusion detection techniques. In this paper, the concept of intrusion context identification is introduced to address the problem. The identification of the intrusion context can help to significantly enhance the detection rate and lower the false alarm rate of AID techniques. To evaluate the effectiveness of the concept, a simple but representative scheme for intrusion context identification is proposed, in which the anomalies in the intrusive datasets are visualized first, and then the intrusion contexts are identified from the visualized anomalies. The experimental results show that using the scheme, the intrusion contexts can be visualized and extracted from the audit trails correctly. In addition, as an application of the visualized anomalies, an implicit design drawback in t-stide is found after careful analysis. Finally, based on the identified intrusion context and the efficiency comparison, several findings are made which can offer useful insights and benefit future research on AID techniques.},
author = {Li, Z and Das, A},
booktitle = {Computer Security Applications Conference, 2004. 20th Annual},
doi = {10.1109/CSAC.2004.48},
issn = {1063-9527},
keywords = {anomaly-based intrusion detection techniques,fals},
pages = {61--70},
title = {{Visualizing and identifying intrusion context from system calls trace}},
year = {2004}
}
@inproceedings{Li2008,
abstract = {The key factor of component based software development is component composition technology. Although many researches have focused on this subject, the quality of system that is composed of components has not been guaranteed indubitably. Test-driven development (TDD) is a software development methodology for achieving high reliability. The combination of these two technologies will help to locate the component mismatch early and reduce the cost of system testing. In this paper, according to the components interaction graph we analyze the processes of testing based components composition (TBCC) and introduce the method as well as propose an algorithm. By applying testing ideas to component composition, it helps to control the composition process and lay foundation for high quality component software. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {An approach to testing based component composition},
author = {Li, L and Wang, Z and Zhang, X},
booktitle = {Proceedings - ISECS International Colloquium on Computing, Communication, Control, and Management, CCCM 2008},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {735--739},
title = {{An approach to testing based component composition}},
url = {citeulike-article-id:3934694 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-54149107109{\&}{\#}38 partnerID=40},
volume = {1},
year = {2008}
}
@misc{Williams2000a,
address = {Salt Lake City, UT},
author = {Williams, Laurie A},
publisher = {University of Utah},
title = {{The Collaborative Software Process}},
year = {2000}
}
@inproceedings{Jung2012a,
address = {New York, New York, USA},
author = {Jung, Jaeyeon and Han, Seungyeop and Wetherall, David},
booktitle = {Proceedings of the second ACM workshop on Security and privacy in smartphones and mobile devices - SPSM '12},
doi = {10.1145/2381934.2381944},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jung, Han, Wetherall - 2012 - Enhancing Mobile Application Permissions with Runtime Feedback and Constraints.pdf:pdf},
isbn = {9781450316668},
keywords = {agile,nsf,permission architecture,smartphone privacy,user study},
mendeley-tags = {agile,nsf},
month = {oct},
pages = {45},
publisher = {ACM Press},
title = {{Enhancing Mobile Application Permissions with Runtime Feedback and Constraints}},
url = {http://dl.acm.org/citation.cfm?id=2381934.2381944},
year = {2012}
}
@inproceedings{Hulten2001,
address = {New York, New York, USA},
author = {Hulten, Geoff and Spencer, Laurie and Domingos, Pedro},
booktitle = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '01},
doi = {10.1145/502512.502529},
isbn = {158113391X},
pages = {97--106},
publisher = {ACM Press},
title = {{Mining time-changing data streams}},
url = {http://portal.acm.org/citation.cfm?doid=502512.502529},
year = {2001}
}
@article{Ricca2009,
abstract = {One of the main reasons for the failure of many software projects is the late discovery of a mismatch between the customers' expectations and the pieces of functionality implemented in the delivered system. At the root of such a mismatch is often a set of poorly defined, incomplete, under-specified, and inconsistent requirements. Test driven development has recently been proposed as a way to clarify requirements during the initial elicitation phase, by means of acceptance tests that specify the desired behavior of the system. The goal of the work reported in this paper is to empirically characterize the contribution of acceptance tests to the clarification of the requirements coming from the customer. We focused on Fit tables, a way to express acceptance tests, which can be automatically translated into executable test cases. We ran two experiments with students from University of Trento and Politecnico of Torino, to assess the impact of Fit tables on the clarity of requirements. We considered whether Fit tables actually improve requirement understanding and whether this requires any additional comprehension effort. Experimental results show that Fit helps in the understanding of requirements without requiring a significant additional effort. {\^{A}}{\textcopyright} 2008 Elsevier B.V. All rights reserved.},
annote = {Using acceptance tests as a support for clarifying requirements: A series of experiments},
author = {Ricca, F and Torchiano, M and {Di Penta}, M and Ceccato, M and Tonella, P},
journal = {Information and Software Technology},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {270--283},
title = {{Using acceptance tests as a support for clarifying requirements: A series of experiments}},
url = {citeulike-article-id:3934768 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-56349142435{\&}{\#}38 partnerID=40},
volume = {51},
year = {2009}
}
@inproceedings{Marinescu,
author = {Marinescu, R.},
booktitle = {20th IEEE International Conference on Software Maintenance, 2004. Proceedings.},
doi = {10.1109/ICSM.2004.1357820},
isbn = {0-7695-2213-0},
issn = {1063-6773},
keywords = {Computer science,Design engineering,Diseases,Intelligent systems,Large-scale systems,Maintenance engineering,Object oriented modeling,Software design,Software maintenance,Software systems,abnormal metric value,design flaw detection,design heuristics,detection strategy,isolation metrics,metrics-based rule,object-oriented design,object-oriented programming,object-oriented software system,program debugging,quality assurance,software maintenance,software metrics},
language = {English},
pages = {350--359},
publisher = {IEEE},
title = {{Detection strategies: metrics-based rules for detecting design flaws}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1357820}
}
@article{Newkirk2002,
abstract = {Test-Driven Development is often described in the context of writing new software or adding functionality to existing pieces of software. This paper examines the role that Test-Driven Development can play when working with 3rd party packages. The paper documents using test-driven development to explore understand and verify functionality of an existing class library},
annote = {A light in a dark place: test-driven development with 3rd party packages},
author = {Newkirk, J},
isbn = {3540440240},
journal = {Extreme Programming and Agile Methods XP/Agile Universe 2002 Second XP Universe and First Agile Universe Conference Proceedings Lecture Notes in Computer Science Vol 2418},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {144--152},
title = {{A light in a dark place: test-driven development with 3rd party packages}},
url = {citeulike-article-id:3934739 {\#}},
volume = {2418},
year = {2002}
}
@inproceedings{Miller2007,
abstract = {Learning embedded programming is a highly demanding exercise. The beginner is bombarded with complexity from the start: embedded production based around a myriad of C++ constructs with low-level elements integrated onto ever more complicated processor architectures. The picture is further compounded by tool support having unfamiliar roles and appearances from previous student experiences. This demanding situation often has the student bewildered; seeking for "a crutch" or the simplest way forward regardless of the overall consequences. To control this potentially chaotic picture, the instructor needs to introduce devices to combat this complexity. We argue that test driven development (TDD) should become the instructor's principal weapon in this fight. Reasons for this belief combined with our, and the students', experiences with this novel approach are discussed. Copyright 2007 ACM.},
annote = {A TDD approach to introducing students to embedded programming},
author = {Miller, J and Smith, M},
booktitle = {ITiCSE 2007: 12th Annual Conference on Innovation and Technology in Computer Science Education - Inclusive Education in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {33--37},
title = {{A TDD approach to introducing students to embedded programming}},
url = {citeulike-article-id:3934719 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34548069316{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Chillarege1992,
abstract = {Orthogonal defect classification (ODC), a concept that enables in-process feedback to software developers by extracting signatures on the development process from defects, is described. The ideas are evolved from an earlier finding that demonstrates the use of semantic information from defects to extract cause-effect relationships in the development process. This finding is leveraged to develop a systematic framework for building measurement and analysis methods. The authors define ODC and discuss the necessary and sufficient conditions required to provide feedback to a developer; illustrate the use of the defect type distribution to measure the progress of a product through a process; illustrate the use of the defect trigger distribution to evaluate the effectiveness and eventually the completeness of verification processes such as inspection or testing; provides sample results from pilot projects using ODC; and open the doors to a wide variety of analysis techniques for providing effective and fast feedback based on the concepts of ODC},
author = {Chillarege, R. and Bhandari, I.S. and Chaar, J.K. and Halliday, M.J. and Moebus, D.S. and Ray, B.K. and Wong, M.-Y.},
doi = {10.1109/32.177364},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
number = {11},
pages = {943--956},
title = {{Orthogonal defect classification-a concept for in-process measurements}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=177364},
volume = {18},
year = {1992}
}
@inproceedings{Xu2006,
abstract = {This paper investigates the effects of some extreme programming practices in game development by conducting a case study with 12 students who were assigned to implement a simple game application either as pairs or as individuals. The pairs used some XP practices, such as pair programming, test-driven and refactoring, while the individuals applied the traditional waterfall-like approach. The results of the case study showed that paired students completed their tasks faster and with higher quality than individuals. The programs written by pairs pass more test cases than those developed by individuals. Paired programmers also wrote cleaner code with higher cohesion by creating more reasonable number of methods. Therefore, some XP practices, such as pair programming, test-driven and refactoring could be used in game development. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Empirical validation of test-driven pair programming in game development},
author = {Xu, S and Rajlich, V},
booktitle = {Proceedings - 5th IEEE/ACIS Int. Conf. on Comput. and Info. Sci., ICIS 2006. In conjunction with 1st IEEE/ACIS, Int. Workshop Component-Based Software Eng., Softw. Archi. and Reuse, COMSAR 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {500--505},
title = {{Empirical validation of test-driven pair programming in game development}},
url = {citeulike-article-id:3934845 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33947676366{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@book{Keirsey1998,
address = {Del Mar, CA},
author = {Keirsey, David},
publisher = {Prometheus Nemesis Book Company},
title = {{Please Understand Me II}},
year = {1998}
}
@techreport{Symantec2013,
author = {Symantec},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Symantec - 2013 - 2013 Internet Security Threat Report.pdf:pdf},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {April},
title = {{2013 Internet Security Threat Report}},
volume = {18},
year = {2013}
}
@misc{Mendeley2009,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
annote = {From Duplicate 5 (Getting Started with Mendeley - Mendeley)

Double click on the entry on the left to view the PDF.},
author = {Mendeley and {The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@article{Geras2004b,
abstract = {In this thesis, two related streams of research are presented: the empirical evaluation stream and the organization readiness stream. In the empirical evaluation stream, we describe the results of an experiment comparing traditional test-last development to test-driven development. Test-driven devwelopment is an approach for designing software that involves using tests to guide the design effort. In the organizational readiness stream of the research, we report on the results of a regional survey of software testing practices and present the contents of a brief analysis of risks facing those organizations that are in the process of adopting test-driven development. the common thread between the two streams is software testing, specifically characterizing the testing-related skills that both developers and organizations require in order to continue to compete in the global software market. We conclude that there is a need for increased testing knowledge and avility; especially at the system and acceptance test levels. This increased knowledge would also decrease the risk the organization faces when migrating f rom a traditional test-last devleopment process to a test-driven development process.},
annote = {The effectiveness of test driven development},
author = {Geras, A M},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{The effectiveness of test driven development}},
url = {citeulike-article-id:3934622 http://www.collectionscanada.gc.ca/obj/s4/f2/dsk4/etd/MQ97560.PDF},
year = {2004}
}
@inproceedings{5066599,
abstract = {This paper describes how to make use of human visual capability to improve information security. Here in this paper, two pilot studies are shown; a content protection scheme with image tainting, and a user authentication scheme with unclear images.},
author = {Nishigaki, M and Yamamoto, T},
booktitle = {Availability, Reliability and Security, 2009. ARES '09. International Conference on},
doi = {10.1109/ARES.2009.157},
keywords = {content protection scheme,human visual capability},
month = {mar},
pages = {990--994},
title = {{Making Use of Human Visual Capability to Improve Information Security}},
year = {2009}
}
@inproceedings{Kittur2008,
address = {New York, New York, USA},
author = {Kittur, Aniket and Chi, Ed H. and Suh, Bongwon},
booktitle = {Proceeding of the twenty-sixth annual CHI conference on Human factors in computing systems - CHI '08},
doi = {10.1145/1357054.1357127},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kittur, Chi, Suh - 2008 - Crowdsourcing user studies with Mechanical Turk.pdf:pdf},
isbn = {9781605580111},
keywords = {Mechanical Turk,Wikipedia,micro task,remote user study},
month = {apr},
pages = {453},
publisher = {ACM Press},
title = {{Crowdsourcing user studies with Mechanical Turk}},
url = {http://dl.acm.org/citation.cfm?id=1357054.1357127},
year = {2008}
}
@article{Ling2002,
abstract = {A discussion on cost reduction in application specific integrated circuits (ASIC) by LSI logic was presented. The RapidChip, by LSI logic has been designed from the prospective of market's requirements as well as from a technological viewpoint. The growing market oppurtunity in ASSP, FPGA and ASICs was also discussed.},
annote = {Test driven by you},
author = {Ling, P},
journal = {New Electronics},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {20},
pages = {35--36},
title = {{Test driven by you}},
url = {citeulike-article-id:3934696 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0037180670{\&}{\#}38 partnerID=40},
volume = {35},
year = {2002}
}
@article{Chen2008,
abstract = {CR (capture and replay) has been a widely accepted methodology for GUI testing. However, a deficiency of a CR-based approach is that test scripts can not be produced before an application under test (AUT) is correctly implemented, which excludes the possibility of doing test-driven development (TDD). An alternative is the specification-based approach, which defines GUI behaviors by using a GUI specification language. A specification-based approach is suitable for doing TDD. However, after the AUT is partially or fully implemented, the specification-based approach becomes less convenient than the CR-based approach, since capturing can be very useful in maintaining test scripts. In this paper, we propose the integration of the specification-based and CR-based approaches so as to incorporate both of their advantages. We define an event model which servers as the core of both the specification language and the capture/replay mechanism. Based on this event model, we implement a GUI testing tool, called GTT, for Java applications. We show how to apply GTT in a TDD style for GUI testing and quantitatively report the benefits of the integration.},
annote = {Integration of specification-based and CR-based approaches for GUI testing},
author = {Chen, W K and Shen, Z W and Tsai, T H},
journal = {Journal of Information Science and Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {5},
pages = {1293--1307},
title = {{Integration of specification-based and CR-based approaches for GUI testing}},
url = {citeulike-article-id:3934570 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-54149119364{\&}{\#}38 partnerID=40},
volume = {24},
year = {2008}
}
@inproceedings{Davidsson2004,
abstract = {Software testing is an integral part of the software development process. Some software developers, particularly those who use the Extreme Programming test-driven development practice, continuously write automated tests to verify their code. We present a tool to complement the feedback loops created by continuous testing. The tool combines static source code metrics with dynamic test coverage for use throughout the development phase to predict a reliability estimate based on a linear combination of these values. Implemented as an open source plug-in to the Eclipse IDE, the tool facilitates the rapid transition between unit test case completions and testing feedback. The color-coded results highlight inadequate testing efforts as well as weaknesses in overall program structure. To illustrate the tool's efficacy, we share the results of its use on university software engineering course projects. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {GERT: An empirical reliability estimation and testing feedback tool},
author = {Davidsson, M and Zheng, J and Nagappan, N and Williams, L and Vouk, M},
booktitle = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {269--280},
title = {{GERT: An empirical reliability estimation and testing feedback tool}},
url = {citeulike-article-id:3934584 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-16244416220{\&}{\#}38 partnerID=40},
year = {2004}
}
@inproceedings{4035755,
abstract = {Realizing operational analytics solutions where large and complex data must be analyzed in a time-critical fashion entails integrating many different types of technology. This paper focuses on an interdisciplinary combination of scientific data management and visualization/analysis technologies targeted at reducing the time required for data filtering, querying, hypothesis testing and knowledge discovery in the domain of network connection data analysis. We show that use of compressed bitmap indexing can quickly answer queries in an interactive visual data analysis application, and compare its performance with two alternatives for serial and parallel filtering/querying on 2.5 billion records' worth of network connection data collected over a period of 42 weeks. Our approach to visual network connection data exploration centers on two primary factors: interactive ad-hoc and multiresolution query formulation and execution over n dimensions and visual display of the n-dimensional histogram results. This combination is applied in a case study to detect a distributed network scan and to then identify the set of remote hosts participating in the attack. Our approach is sufficiently general to be applied to a diverse set of data understanding problems as well as used in conjunction with a diverse set of analysis and visualization tools},
author = {Bethel, E W and Campbell, S and Dart, E and Stockinger, K and Wu, Kesheng},
booktitle = {Visual Analytics Science And Technology, 2006 IEEE Symposium On},
doi = {10.1109/VAST.2006.261437},
keywords = {compressed bitmap indexing,data filtering,data min},
pages = {115--122},
title = {{Accelerating Network Traffic Analytics Using Query-Driven Visualization}},
year = {2006}
}
@article{Boehm1991,
abstract = {The emerging discipline of software risk management is described. It is defined as an attempt to formalize the risk-oriented correlates of success into a readily applicable set of principles and practices. Its objectives are to identify, address, and eliminate risk items before they become either threats to successful software operation or major sources of software rework. The basic concepts are set forth, and the major steps and techniques involved in software risk management are explained. Suggestions for implementing risk management are provided.},
author = {Boehm, B.W.},
doi = {10.1109/52.62930},
issn = {0740-7459},
journal = {IEEE Software},
month = {jan},
number = {1},
pages = {32--41},
title = {{Software risk management: principles and practices}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=62930},
volume = {8},
year = {1991}
}
@misc{Marchesi2001,
address = {Boston},
author = {Marchesi, M and Succi, G and Beck, Kent},
publisher = {Addison Wesley},
title = {{Extreme Programming Examined}},
year = {2001}
}
@inproceedings{Ho2006,
abstract = {Underspecified performance requirements can cause performance issues in a software system. However, a complete, upfront analysis of a software system is difficult, and usually not desirable. We propose an evolutionary model for performance requirements specifications and corresponding validation testing. The principles of the model can be integrated into agile development methods. Using this approach, the performance requirements and test cases can be specified incrementally, without big upfront analysis. We also provide a post hoc examination of a development effort at IBM that had a high focus on performance requirements. The examination indicates that our evolutionary model can be used to specify performance requirements such that the level of detail is commensurate with the nature of the project. Additionally, the IBM experience indicates that test driven development-type validation testing corresponding to the model can be used to determine if performance objectives have been met. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {On Agile performance requirements specification and testing},
author = {Ho, C W and Johnson, M J and Williams, L and Maximilien, E M},
booktitle = {Proceedings - AGILE Conference, 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {47--52},
title = {{On Agile performance requirements specification and testing}},
url = {citeulike-article-id:3934642 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34247574873{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@article{Betin-Can2007,
abstract = {The increasing level of automation in critical infrastructures requires development of effective ways for finding faults in safety critical software components. Synchronization in concurrent components is especially prone to errors and, due to difficulty of exploring all thread interleavings, it is difficult to find synchronization faults. In this paper we present an experimental study demonstrating the effectiveness of model checking techniques in finding synchronization faults in safety critical software when they are combined with a design for verification approach. We based our experiments on an automated air traffic control software component called the Tactical Separation Assisted Flight Environment (TSAFE). We first reengineered TSAFE using the concurrency controller design pattern. The concurrency controller design pattern enables a modular verification strategy by decoupling the behaviors of the concurrency controllers from the behaviors of the threads that use them using interfaces specified as finite state machines. The behavior of a concurrency controller is verified with respect to arbitrary numbers of threads using the infinite state model checking techniques implemented in the Action Language Verifier (ALV). The threads which use the controller classes are checked for interface violations using the finite state model checking techniques implemented in the Java Path Finder (JPF). We present techniques for thread isolation which enables us to analyze each thread in the program separately during interface verification. We conducted two sets of experiments using these verification techniques. First, we created 40 faulty versions of TSAFE using manual fault seeding. During this exercise we also developed a classification of faults that can be found using the presented design for verification approach. Next, we generated another 100 faulty versions of TSAFE using randomly seeded faults that were created automatically based on this fault classification. We used both infinite and finite state verification techniques for finding the seeded faults. The results of our experiments demonstrate the effectiveness of the presented design for verification approach in eliminating synchronization faults.},
author = {Betin-Can, Aysu and Bultan, Tevfik and Lindvall, Mikael and Lux, Benjamin and Topp, Stefan},
journal = {Automated Software Engineering},
keywords = {Journal},
number = {2},
pages = {129--178},
title = {{Eliminating synchronization faults in air traffic control software via design for verification with concurrency controllers}},
url = {http://www.springerlink.com/content/r2x7575g12l57836/},
volume = {14},
year = {2007}
}
@inproceedings{Herzig2014,
author = {Herzig, Kim},
booktitle = {2014 IEEE 25th International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.2014.21},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herzig - 2014 - Using Pre-Release Test Failures to Build Early Post-Release Defect Prediction Models.pdf:pdf},
isbn = {978-1-4799-6033-0},
issn = {1071-9458},
keywords = {Context,Correlation,DP industry,Logic gates,Measurement,Predictive models,Software,Testing,Windows 8 development,defect prediction,demand forecasting,development process,market demands,market opportunities,measurement,post-release defect prediction models,prerelease test failures,problematic code,product quality,program testing,software developing companies,software quality,software testing,stability,test executions,test metrics},
language = {English},
month = {nov},
pages = {300--311},
publisher = {IEEE},
title = {{Using Pre-Release Test Failures to Build Early Post-Release Defect Prediction Models}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6982636},
year = {2014}
}
@inproceedings{Ackermann2007,
abstract = {Inspections are widely used and studies have found them to be effective in uncovering defects. However, there is less data available regarding the impact of inspections on different defect types and almost no data quantifying the link between inspections and desired end product qualities. This paper addresses this issue by investigating whether design inspection checklists can be tailored so as to effectively target certain defect types without impairing the overall defect detection rate. The results show that the design inspection approach used here does uncover useful design quality issues and that the checklists can be effectively tailored for some types of defects.},
address = {Madrid, Spain},
author = {Ackermann, Chris and Shull, Forrest and Carbon, R and Denger, C and Lindvall, Mikael},
booktitle = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
keywords = {Empirical Software Engineering,Measurement,Software Engineering},
month = {sep},
series = {International Symposium on Empirical Software Engineering and Measurement (ESEM) (Short Papers Track)},
title = {{Assessing the Quality Impact of Design Inspections}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/ESEM.2007.63},
year = {2007}
}
@article{George2004,
abstract = {Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. We ran a set of structured experiments with 24 professional pair programmers. One group developed a small Java program using TDD while the other (control group), used a waterfall-like approach. Experimental results, subject to external validity concerns, tend to indicate that TDD programmers produce higher quality code because they passed 18{\%} more functional black-box test cases. However, the TDD programmers took 16{\%} more time. Statistical analysis of the results showed that a moderate statistical correlation existed between time spent and the resulting quality. Lastly, the programmers in the control group often did not write the required automated test cases after completing their code. Hence it could be perceived that waterfall-like approaches do not encourage adequate testing. This intuitive observation supports the perception that TDD has the potential for increasing the level of unit testing in the software industry. {\^{A}}{\textcopyright} 2003 Elsevier B.V. All rights reserved.},
annote = {A structured experiment of test-driven development},
author = {George, B and Williams, L},
journal = {Information and Software Technology},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {5 SPEC. ISS.},
pages = {337--342},
title = {{A structured experiment of test-driven development}},
url = {citeulike-article-id:3934620 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-1142299882{\&}{\#}38 partnerID=40},
volume = {46},
year = {2004}
}
@book{Rosenthal1991,
address = {New York},
author = {Rosenthal, Robert and Rosnow, Ralph L.},
publisher = {McGraw-Hill},
title = {{Essentials of behavioral research: methods and data analysis}},
year = {1991}
}
@article{Layman2013b,
abstract = {Determining whether systems achieve desired emergent properties, such as safety or reliability, requires an analysis of the system as a whole, often in later development stages when changes are difficult and costly to implement. In this article we propose the Process Risk Indicator (PRI) methodology for analyzing and evaluating emergent properties early in the development cycle. A fundamental assumption of system engineering is that risk mitigation processes reduce system risks, yet these processes may also be a source of risk: (1) processes may not be appropriate for achieving the desired emergent property; or (2) processes may not be followed appropriately. PRI analyzes development process artifacts (e.g., designs pertaining to reliability or safety analysis reports) to quantify process risks that may lead to higher system risk.We applied PRI to the hazard analysis processes of a network-centric, Department of Defense system-of-systems and two NASA spaceflight projects to assess the risk of not achieving one such emergent property, software safety, during the early stages of the development lifecycle. The PRI methodology was used to create measurement baselines for process indicators of software safety risk, to identify risks in the hazard analysis process, and to provide feedback to projects for reducing these risks. {\textcopyright} 2014 ACM.},
author = {Layman, Lucas and Basili, Victor R and Zelkowitz, Marvin V},
doi = {10.1145/2560048},
file = {:C$\backslash$:/Users/laymanl/Desktop/layman pubs/Layman2013b.pdf:pdf},
issn = {15577392},
journal = {Transactions on Software Engineering Methodology},
keywords = {Process risk,Risk measurement,Software safety,mypubs},
mendeley-tags = {mypubs},
number = {3},
pages = {Article 22},
title = {{A Methodology for Exposing Risk in Achieving Emergent System Properties}},
volume = {22},
year = {2014}
}
@inproceedings{DAmbros2010,
address = {Cape Town, South Africa},
author = {D'Ambros, Marco and Lanza, Michele and Robbes, Romain},
booktitle = {2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
doi = {10.1109/MSR.2010.5463279},
isbn = {978-1-4244-6802-7},
month = {may},
pages = {31--41},
title = {{An extensive comparison of bug prediction approaches}},
url = {http://ieeexplore.ieee.org/document/5463279/},
year = {2010}
}
@article{Paradkar,
author = {Paradkar, Amit},
keywords = {2,a structural criteria on,and,based test selection,efsm-,fsm,generate test,model,model-based test generation,mutation-based test selection,representation from the provided,the extracted fsm to,use},
title = {{Plannable Test Selection Criteria for FSMs Extracted From Operational}}
}
@article{Menzies2013a,
author = {Menzies, Tim},
journal = {IEEE Software},
number = {3},
pages = {90--92},
title = {{Beyond Data Mining}},
volume = {30},
year = {2013}
}
@inproceedings{Layman2015,
abstract = {{\textcopyright} 2015 IEEE. Maturity in software projects is often equated with data-driven predictability. However, data collection is expensive and measuring all variables that may correlate with project outcome is neither practical nor feasible. In contrast, a project engineer can identify a handful of factors that he or she believes influence the success of a project. The challenge is to quantify engineers' insights in a way that is useful for data analysis. In this exploratory study, we investigate the repertory grid technique for this purpose. The repertory grid technique is an interview-based procedure for eliciting 'constructs' (e.g., Adhering to coding standards) that individuals believe influence a worldly phenomenon (e.g., What makes a high-quality software project) by comparing example elements from their past (e.g., Projects they have worked on). We investigate the relationship between objective metrics of project performance and repertory grid constructs elicited from eight software engineers. Our results show correlations between the engineers' subjective constructs and the objective project outcome measures. This suggests that repertory grids may be of benefit in developing models of project outcomes, particularly when project data is limited.},
address = {Florence, Italy},
author = {Layman, Lucas and Seaman, Carolyn and Falessi, Davide and Diep, Madeline},
booktitle = {8th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE 2015)},
doi = {10.1109/CHASE.2015.25},
isbn = {9781479919345},
keywords = {agile,mypubs,nsf,practitioners,repertory grids,software data analytics},
mendeley-tags = {agile,mypubs,nsf},
pages = {81--84},
title = {{Ask the Engineers: Exploring Repertory Grids and Personal Constructs for Software Data Analysis}},
year = {2015}
}
@misc{Gagliardi2007,
abstract = {In this paper we outline a methodological similarity between test driven software development and scientific theories evolution. We argue that falsificationism and its modus tollens are foundational concepts for both software engineering and scientific method. In this perspective we propose an epistemological justification of test driven development using theoretical reasons and empirical evidences. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Epistemological justification of test driven development in agile processes},
author = {Gagliardi, F},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {253--256},
title = {{Epistemological justification of test driven development in agile processes}},
url = {citeulike-article-id:3934614 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38149100407{\&}{\#}38 partnerID=40},
volume = {4536 LNCS},
year = {2007}
}
@inproceedings{Nagappan2006,
abstract = {The benefits that a software organization obtains from estimates of product quality are dependent upon how early in the product cycle that these estimates are available. Early estimation of software quality can help organizations make informed decisions about corrective actions. To provide such early estimates we present an empirical case study of two large scale commercial operating systems, Windows XP and Windows Server 2003. In particular, we leverage various historical in-process and product metrics from Windows XP binaries to create statistical predictors to estimate the post-release failures/failure-proneness of Windows Server 2003 binaries. These models estimate the failures and failure-proneness of Windows Server 2003 binaries at statistically significant levels. Our study is unique in showing that historical predictors for a software product line can be useful, even at the very large scale of the Windows operating system},
author = {Nagappan, Nachiappan and Ball, Thomas and Murphy, Brendan},
booktitle = {2006 17th International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.2006.50},
isbn = {0-7695-2684-5},
issn = {1071-9458},
month = {nov},
pages = {62--74},
publisher = {IEEE},
title = {{Using Historical In-Process and Product Metrics for Early Estimation of Software Failures}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=4021972{\&}contentType=Conference+Publications{\&}searchField=Search{\_}All{\&}queryText=nagappan+ball},
year = {2006}
}
@article{Breitman1999,
author = {Breitman, K and Leite, J and Finkelstein, A},
number = {1},
pages = {13--37},
title = {{The World's a Stage: A Survey on Requirements Engineering Using a Real-Life Case Study}},
volume = {6},
year = {1999}
}
@inproceedings{4227775,
abstract = {The safety and security of transportation infrastructure has become the focus of increased attention in the post-9/11 era. Concerns in this area include the possibility of terrorist activity directed at travelers and/or property. As the leader in intelligent video, ObjectVideo is developing intelligent capabilities specifically tailored to meet the needs of the transportation industry that work effectively in diverse environments, for example, tunnels of a subway, highway rail grade crossings, and air and seaports. ObjectVideo's intelligent system employs a network of visual sensors to monitor sensitive areas. The technology accurately detects, identifies, classifies, and tracks objects and people; automatically detects activities that violate user specified security rules and notifies security personnel in real-time about rule violations and suspicious activities. The system automatically monitors user specified areas for unauthorized people and vehicles loitering on a platform, for suspicious objects (e.g., backpacks or bags) left on or near sensitive locations, or for objects that are large enough to cause serious accidents. Because the system is capable of distinguishing non-threatening objects and controlling false alerts, the software maximizes the effectiveness of security personnel, allowing them to focus their efforts on potential threats, enhancing protection for all passengers, freight and facilities.},
author = {Haering, N and Shafique, K},
booktitle = {Technologies for Homeland Security, 2007 IEEE Conference on},
doi = {10.1109/THS.2007.370012},
keywords = {ObjectVideo,automatic visual analysis,intelligent},
month = {may},
pages = {13--18},
title = {{Automatic Visual Analysis for Transportation Security}},
year = {2007}
}
@inproceedings{Madeyski2007a,
abstract = {Code coverage and mutation score measure how thoroughly tests exercise programs and how effective they are, respectively. The objective is to provide empirical evidence on the impact of pair programming on both, thoroughness and effectiveness of test suites, as pair programming is considered one of the practices that can make testing more rigorous, thorough and effective. A large experiment with MSc students working solo and in pairs was conducted. The subjects were asked to write unit tests using JUnit, and to follow test-driven development approach, as suggested by eXtreme Programming methodology. It appeared that branch coverage, as well as mutation score indicator (the lower bound on mutation score), was not significantly affected by using pair programming, instead of solo programming. However, slight but insignificant positive impact of pair programming on mutations score indicator was noticeable. The results do not support the positive impact of pair programming on testing to make it more effective and thorough. The generalization of the results is limited due to the fact that MSc students participated in the study. It is possible that the benefits of pair programming will exceed the results obtained in this experiment for larger, more complex and longer projects. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {On the effects of pair programming on thoroughness and fault-finding effectiveness of unit tests},
author = {Madeyski, L},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {207--221},
title = {{On the effects of pair programming on thoroughness and fault-finding effectiveness of unit tests}},
url = {citeulike-article-id:3934700 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35148833554{\&}{\#}38 partnerID=40},
volume = {4589 LNCS},
year = {2007}
}
@inproceedings{4777552,
abstract = {The simulation of natural scenery is a very important branch of Computer Graphics and has a broad application fields. The traditional Euclidian theory is used in describing the regular-shaped objects. However, the fractal theory is adapted to describe the irregular-shaped objects. This paper mainly introduces how to simulate various kinds of natural scenery to the life in the nature by combining the free surface modeling technology of the traditional Euclidian theory with the Iteration Function System (IFS) of the fractal theory. Firstly, it introduced how to use the free surface (B-spline surface) modeling technology to generate three-dimensional terrains. Secondly, it introduced the algorithms and the implement processes of three-dimensional fractal trees using the Iteration Function System. Finally, it realized a generally practical application of three-dimensional terrain with some three-dimensional trees. This paper mainly used the programming interface language-ObjectARX of AutoCAD and Visual C++ to implement the simulation of natural scenery. The simulative effect is quite reality to the natural scenery, and it has certain validity and practicability to use ObjectARX to simulate the natural scenery.},
author = {Yun-ping, Han and Pei-sheng, Liu},
booktitle = {Intelligent Information Technology and Security Informatics, 2009. IITSI '09. Second International Symposium on},
doi = {10.1109/IITSI.2009.23},
keywords = {3D fractal trees,AutoCAD,Euclidian theory,ObjectAR},
pages = {74--76},
title = {{The Simulation of Natural Scenery Based on ObjectARX}},
year = {2009}
}
@article{Gross2005,
author = {Gross, Hans-Gerhard and Schieferdecker, Ina and Din, George},
doi = {10.1016/j.entcs.2004.12.001},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {001,10,1016,12,1571-0661,2004,2004 elsevier b,all rights reserved,component-based testing,doi,entcs,j,model-based testing,see front matter,test automation,test control notation,test modeling,testing and,uml modeling,v},
month = {jan},
pages = {161--182},
title = {{Model-Based Built-In Tests}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1571066104052375},
volume = {111},
year = {2005}
}
@misc{PaulkB.CurtisandM.B.Chrisis1993,
author = {{Paulk  B. Curtis, and M. B. Chrisis}, M C},
isbn = {CMU/SEI-93-TR},
publisher = {Software Engineering Institute},
title = {{Capability Maturity Model for Software Version 1.1}},
year = {1993}
}
@inproceedings{Bird2009,
address = {Vancouver, BC},
author = {Bird, Christian and Rigby, Peter C. and Barr, Earl T. and Hamilton, David J. and German, Daniel M. and Devanbu, Prem},
booktitle = {2009 6th IEEE International Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2009.5069475},
isbn = {978-1-4244-3493-0},
month = {may},
pages = {1--10},
title = {{The promises and perils of mining git}},
url = {http://ieeexplore.ieee.org/document/5069475/},
year = {2009}
}
@inproceedings{He2009,
address = {Lake Buena Vista, FL},
author = {He, L and Carver, J},
booktitle = {International Symposium on Empirical Software Engineering},
pages = {430--434},
title = {{Modifiability measurement from a task complexity perspective: A feasibility study}},
year = {2009}
}
@inproceedings{Layman2005b,
abstract = {This paper presents the results of an initial quantitative investigation to assess a variety of factors that potentially affect the collaborative software development experience. This research was conducted with 119 students in two undergraduate software engineering classes at North Carolina State University. A survey was administered where students could reflect on their collaborative experiences. We analyzed these factors for interrelationships as well as for correlations with performance in the course, grade point average, and SAT scores. Our findings support the components of the proposed Social Interaction Model of Pair Programming. The substantiation of the Social Interaction Model of Pair Programming values suggests that they should be considered in course planning. We also find that work ethic and self-perceived programming ability positively correlate with GPA},
address = {Indianapolis, Indiana},
author = {Layman, Lucas and Williams, Laurie and Osborne, Jason and Berenson, Sarah and Slaten, Kelli and Vouk, Mladen and {L. Williams}, L and Osborne, Jason and Berenson, Sarah and Slaten, Kelli and Vouk, Mladen},
booktitle = {Proceedings Frontiers in Education 35th Annual Conference},
doi = {10.1109/FIE.2005.1611964},
isbn = {0-7803-9077-6},
issn = {0190-5848},
keywords = {Collaborative development,Collaborative software,Collaborative work,Computer science,Computer science education,Education research,Educational programs,Educational technology,Engineering profession,LS,MBTI,Mathematics,North Carolina State University,Programming profession,Software engineering,collaborative software development,educational courses,educational institutions,ethical aspects,learning,mypubs,pair programming,quantitative investigation,self-perceived programming ability,social interaction model,software engineering classes,software engineering course,survey,work ethic},
mendeley-tags = {mypubs},
pages = {T4C 9--14},
publisher = {IEEE},
shorttitle = {Frontiers in Education, 2005. FIE '05. Proceedings},
title = {{How and Why Collaborative Software Development Impacts the Software Engineering Course}},
year = {2005}
}
@book{Cockburn2001,
address = {Reading, Massachusetts},
author = {Cockburn, Alistair and Highsmith, Jim},
isbn = {0201699699},
publisher = {Addison Wesley Longman},
title = {{Agile Software Development}},
year = {2001}
}
@article{larman2003iterative,
author = {Larman, Craig and Basili, Victor R},
journal = {Computer},
number = {6},
pages = {47--56},
publisher = {IEEE},
title = {{Iterative and incremental development: A brief history}},
volume = {36},
year = {2003}
}
@article{Jeffries2007,
abstract = {Test-driven development is a discipline of design and programming where every line of new code is written in response to a test the programmer writes just before coding. This special issue of IEEE Software includes seven feature articles on various aspects of TDD and a Point/ Counterpoint debate on the use of mock objects in applying it. The articles demonstrate the ways TDD is being used in nontrivial situations (database development, embedded software development, GUI development, performance tuning), signifying an adoption level for the practice beyond the visionary phase and into the early mainstream. In this introduction to the special issue on TDD, the guest editors also summarize selected TDD empirical studies from industry and academia. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Guest editor{\&}{\#}039;s introduction: TDD - The art of fearless programming},
author = {Jeffries, R and Melnik, G},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {24--30},
title = {{Guest editor's introduction: TDD - The art of fearless programming}},
url = {citeulike-article-id:3934661 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248356409{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@article{1607921,
abstract = { The VisAlert visual correlation tool facilitates situational awareness in complex network environments by providing a holistic view of network security to help detect malicious activities. Information visualization techniques and methods in many applications have effectively increased operators' situational awareness, letting them more effectively detect, diagnose, and treat anomalous conditions. Visualization elevates information comprehension by fostering rapid correlation and perceived associations. Our visualization technique integrates the information in log and alert files into an intuitive, flexible, extensible, and scalable visualization tool - VisAlert - that presents critical information concerning network activity in an integrated manner, increasing the user's situational awareness.},
author = {Foresti, S and Agutter, J and Livnat, Y and Moon, S and Erbacher, R},
doi = {10.1109/MCG.2006.49},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {Computer-Assisted,Software,User-Computer Interfac,VisAlert visual correlation tool,alert files,com},
number = {2},
pages = {48--59},
title = {{Visual correlation of network alerts}},
volume = {26},
year = {2006}
}
@article{Necaise2008,
abstract = {Python is becoming more popular as the language of choice for introductory computer science courses. The computer science department at Washington and Lee University adopted Python in the Fall of 2006 for both the CS1 and CS2 courses. We opted to transition both courses to Python at the same time instead of waiting for students to progress through the course sequence. In this paper, we present our approach used to manage this transition along with the pros and cons of our decisions. We offer our experiences in hopes of helping others who may be contemplating a transition to Python. INTRODUCTION Python is a cross-platform scripting language which offers a simple yet powerful syntax that can be used to create a wide range of programs from basic two or three liners to very complex object-oriented solutions [14]. Its ease of use allows for more focus on problem solving and less on the syntax to perform specific operations. The simple syntax resembles pseudocode allowing for easier specification and implementation of algorithms.},
author = {Necaise, Rance},
doi = {10.5555/1409823.1409847},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Necaise - 2008 - TRANSITIONING FROM JAVA TO PYTHON IN CS2.pdf:pdf},
issn = {1937-4771},
journal = {Journal of Computing Sciences in Colleges},
number = {2},
pages = {92--97},
title = {{Transitioning from Java to Python in CS2}},
volume = {24},
year = {2008}
}
@article{Eisenstadt1997,
author = {Eisenstadt, Marc},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eisenstadt - 1997 - My hairiest bug war stories.pdf:pdf},
journal = {Communications of the ACM},
keywords = {collaboration},
mendeley-tags = {collaboration},
number = {4},
pages = {30--37},
publisher = {ACM},
title = {{My hairiest bug war stories}},
url = {http://portal.acm.org/citation.cfm?id=248448.248456},
volume = {40},
year = {1997}
}
@article{Wood2003a,
author = {Wood, William A. and Kleb, William L.},
doi = {10.1109/MS.2003.1196317},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood, Kleb - 2003 - Exploring xp for scientific research.pdf:pdf},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {agile},
mendeley-tags = {agile},
month = {may},
number = {3},
pages = {30--36},
publisher = {Published by the IEEE Computer Society},
title = {{Exploring xp for scientific research}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1196317},
volume = {20},
year = {2003}
}
@inproceedings{Abrahamsson2003a,
address = {Portland, OR},
author = {Abrahamsson, Pekka and Warsta, Juhani and Siponen, Mikko T. and Ronkainen, Jussi},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
doi = {10.1109/ICSE.2003.1201204},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abrahamsson et al. - 2003 - New directions on agile methods a comparative analysis.pdf:pdf},
isbn = {0-7695-1877-X},
keywords = {agile},
mendeley-tags = {agile},
pages = {244--254},
publisher = {IEEE Computer Society},
title = {{New directions on agile methods: a comparative analysis}},
url = {http://portal.acm.org/citation.cfm?id=776816.776846{\&}type=series},
volume = {6},
year = {2003}
}
@inproceedings{Pinheiro2007,
author = {Pinheiro, Eduardo and Weber, Wolf-Dietrich and Barroso, Luiz Andr{\'{e}}},
booktitle = {Proc.$\backslash$ 5th USENIX Conference on File and Storage Technologies},
pages = {17--28},
title = {{Failure trends in a large disk drive population}},
year = {2007}
}
@article{Ebel1954,
author = {Ebel, Robert L.},
doi = {10.1177/001316445401400215},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ebel - 1954 - Procedures for the analysis of classroom tests.pdf:pdf},
issn = {15523888},
journal = {Educational and Psychological Measurement},
month = {jul},
number = {2},
pages = {352--364},
title = {{Procedures for the analysis of classroom tests}},
url = {http://journals.sagepub.com/doi/10.1177/001316445401400215},
volume = {14},
year = {1954}
}
@article{Debarati2009,
author = {Debarati, Halder and Karuppannan, Jaishankar},
journal = {Journal of Victimization},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {3},
pages = {5--26},
title = {{Cyber Socializing and Victimization of Women}},
volume = {12},
year = {2009}
}
@inproceedings{Thomas2003,
address = {Reno, NV},
author = {Thomas, L and Ratcliffe, M and Robertson, A},
pages = {363--367},
title = {{Code Warriors and Code-a-Phobes: A study in attitude and pair programming}},
year = {2003}
}
@misc{Robillard2008a,
address = {Antwerp, Belgiu,},
author = {Robillard, Martin P and Dagenais, Barthelemy},
pages = {17--26},
title = {{Retrieving Task-Related Clusters from Change History}},
year = {2008}
}
@article{Derks2012,
author = {Derks, Daantje and Bakker, Arnold},
issn = {0269-994X},
journal = {Applied Psychology: An International Review},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jan},
pages = {1--30},
title = {{Smartphone use, work-home interference, and burnout: A diary study on the role of recovery}},
url = {http://repub.eur.nl/pub/38866/},
volume = {2012},
year = {2012}
}
@misc{Read2003,
abstract = {Agile software development processes are best applied to small teams on small to medium sized projects. Scaling agile methodologies is desired in order to bring the benefits of agile to larger, more complex projects. One way to scale agile methods is via an architecture-centric approach, in which a project is divided into smaller modules on which sub teams can use agile effectively. However, a problem with architecture-centric modifications to agile methods is the introduction of non-agile elements, for instance up-front design and integration difficulties. These issues are discussed and a tool-based solution is presented facilitating the adoption of the architecture-centric agile approach. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
annote = {Issues in scaling agile using an architecture-centric approach: A tool-based solution},
author = {Read, K and Maurer, F},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {142--150},
title = {{Issues in scaling agile using an architecture-centric approach: A tool-based solution}},
url = {citeulike-article-id:3934762 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35248861253{\&}{\#}38 partnerID=40},
volume = {2753},
year = {2003}
}
@inproceedings{Sutherland2008,
abstract = {Projects combining agile methods with CMMI1 are more successful in producing higher quality software that more effectively meets customer needs at a faster pace. Systematic Software Engineering works at CMMI level 5 and uses Lean Software Development as a driver for optimizing software processes. Early pilot projects showed productivity on Scrum teams almost twice that of traditional teams. Other projects using a story-based test-driven approach to software development reduced defects in final test by 40{\%}. We assert that Scrum and CMMI together bring a more powerful combination of adaptability and predictability than either one alone and suggest how other companies can combine them. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Scrum and CMMI level 5: The magic potion for code warriors},
author = {Sutherland, J and Jakobsen, C R and Johnson, K},
booktitle = {Proceedings of the Annual Hawaii International Conference on System Sciences},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Scrum and CMMI level 5: The magic potion for code warriors}},
url = {citeulike-article-id:3934811 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-51449090769{\&}{\#}38 partnerID=40},
year = {2008}
}
@article{McCrickard2003a,
author = {McCrickard, D. Scott and Chewar, C. M.},
doi = {10.1145/636772.636800},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {mar},
number = {3},
pages = {67},
title = {{Attuning notification design to user goals and attention costs}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=636800{\&}type=html},
volume = {46},
year = {2003}
}
@inproceedings{5593249,
abstract = {Click-based graphical passwords are a new method of authentication where passwords are created and entered by clicking in particular places on an image. This paper presents a study that investigated eye tracking as a potential threat to the security of such passwords. If the gaze data from people looking at an image resembles the click-points of other people's passwords, then covert eye tracking might be used to create dictionaries to effectively guess passwords. The study used an eye tracker to record the participants' gaze as they looked at images that had been used as the basis for passwords in an earlier study. We then compared the eye tracker data with the actual password click-points gathered during the earlier study, and conducted several forms of analysis to determine the likely success of guessing passwords. The eye tracker data did somewhat resemble the password click-points, and might offer attackers an advantage over guessing at random. The effectiveness shown for this approach was limited, however, although might allow improvement that would result in greater danger, especially if gaze data could be gathered without explicit interaction.},
author = {LeBlanc, Daniel and Forget, Alain and Biddle, Robert},
booktitle = {2010 Eighth International Conference on Privacy, Security and Trust},
doi = {10.1109/PST.2010.5593249},
isbn = {978-1-4244-7551-3},
keywords = {authentication,click-based graphical passwords,eye},
month = {aug},
pages = {197--204},
publisher = {Ieee},
title = {{Guessing click-based graphical passwords by eye tracking}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5593249},
year = {2010}
}
@book{Sommerville2001,
address = {Harlow, England},
author = {Sommerville, Ian and McGettrick, A D},
edition = {Sixth},
publisher = {Addison-Wesley},
title = {{Software Engineering}},
year = {2001}
}
@inproceedings{Felt2011,
address = {New York, New York, USA},
author = {Felt, Adrienne Porter and Finifter, Matthew and Chin, Erika and Hanna, Steve and Wagner, David},
booktitle = {Proceedings of the 1st ACM workshop on Security and Privacy in Smartphones and Mobile Devices (SPSM '11)},
doi = {10.1145/2046614.2046618},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Felt et al. - 2011 - A survey of mobile malware in the wild.pdf:pdf},
isbn = {9781450310000},
keywords = {agile,cybercrime,malware,mobile devices,nsf,security,smartphones},
mendeley-tags = {agile,cybercrime,malware,nsf,security},
month = {oct},
pages = {3--14},
publisher = {ACM Press},
title = {{A survey of mobile malware in the wild}},
url = {http://dl.acm.org/citation.cfm?id=2046614.2046618},
year = {2011}
}
@article{Schreck2006,
author = {Schreck, Christopher J. and Stewart, Erick A. and Fisher, Bonnie S.},
journal = {Journal of Quantitative Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {4},
pages = {319--340},
title = {{Self-control, Vicitmization, and their Influence on Risky Lifestyles: A Longitudinal Analysis Using Panel Data}},
volume = {22},
year = {2006}
}
@article{Evans1997,
author = {Evans, David T. and Cullen, Francis T. and Burton, Velmer S. and Dunaway, R. Gregory and Benson, Michael L.},
doi = {10.1111/j.1745-9125.1997.tb01226.x},
issn = {0011-1384},
journal = {Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {aug},
number = {3},
pages = {475--504},
title = {{The Social Consequences of Self-control: Testing the General Theory of Crime}},
url = {http://doi.wiley.com/10.1111/j.1745-9125.1997.tb01226.x},
volume = {35},
year = {1997}
}
@article{Basili1984b,
author = {Basili, Victor R. and Perricone, Barry T.},
doi = {10.1145/69605.2085},
journal = {Communications of the ACM},
month = {jan},
number = {1},
pages = {42--52},
publisher = {ACM},
title = {{Software errors and complexity: an empirical investigation}},
url = {http://portal.acm.org/citation.cfm?doid=69605.2085},
volume = {27},
year = {1984}
}
@inproceedings{Francia2008b,
address = {New York, New York, USA},
author = {Francia, Guillermo A},
booktitle = {Proceedings of the 5th annual conference on Information security curriculum development - InfoSecCD '08},
doi = {10.1145/1456625.1456637},
isbn = {9781605583334},
keywords = {cybersecurity,cybertrust,gadgets,monitoring,regulatory compliance,security,visualization},
month = {sep},
pages = {40},
publisher = {ACM Press},
title = {{Visual security monitoring gadgets}},
url = {http://dl.acm.org/citation.cfm?id=1456625.1456637},
year = {2008}
}
@techreport{Pentti,
address = {Helsinki, Finland},
author = {Pentti, Haapanen and Atte, Helminen},
booktitle = {Management Research News},
doi = {10.1108/eb027839},
institution = {Radiation and Nuclear Safety Authority (STUK)},
number = {1},
title = {{Failure Mode and Effects Analysis of Software-Based Automation Systems}},
volume = {7}
}
@inproceedings{Layman2006a,
address = {Houston, TX},
author = {Layman, Lucas and Cornwell, Travis and Williams, Laurie},
booktitle = {Proceedings of the 37th SIGCSE Technical Symposium on Computer Science Education},
keywords = {learning styles,mypubs,personality,personality types},
mendeley-tags = {mypubs},
pages = {428--432},
title = {{Personality Types, Learning Styles, and an Agile Approach to Software Engineering Education}},
year = {2006}
}
@inproceedings{Mittal2016,
author = {Mittal, Sudip and Das, Prajit Kumar and Mulwad, Varish and Joshi, Anupam and Finin, Tim},
booktitle = {2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)},
doi = {10.1109/ASONAM.2016.7752338},
file = {:C$\backslash$:/Users/laymanl/Desktop/816.pdf:pdf},
isbn = {978-1-5090-2846-7},
keywords = {twitter},
mendeley-tags = {twitter},
month = {aug},
pages = {860--867},
publisher = {IEEE},
title = {{CyberTwitter: Using Twitter to generate alerts for cybersecurity threats and vulnerabilities}},
url = {http://ieeexplore.ieee.org/document/7752338/},
year = {2016}
}
@article{1607920,
abstract = {A technique for visualizing intrusion-detection system log files using hierarchical data based on IP addresses represents the number of incidents for thousands of computers in one display space. Our technique applies a hierarchical data visualization technique that represents leaf nodes as black square icons and branch nodes as rectangular borders enclosing the icons. This representation style visualizes thousands of hierarchical data leaf nodes equally in one display space. We applied the technique to bioactive chemical visualization and job distribution in parallel-computing environments.},
author = {Itoh, T and Takakura, H and Sawada, A and Koyamada, K},
doi = {10.1109/MCG.2006.34},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {Computer-Assisted,IP addresses,Software,User-Computer Interfac,bioactive chemical visualization,blac},
number = {2},
pages = {40--47},
title = {{Hierarchical visualization of network intrusion detection data}},
volume = {26},
year = {2006}
}
@incollection{Norman1986,
address = {Hillsdale, NJ},
author = {Norman, Donald A and Draper, Stephen W},
keywords = {cognitive engineering,hci,user activities},
pages = {31--61},
publisher = {Lawrence Erlbaum Associates},
title = {{Cognitive Engineering}},
year = {1986}
}
@article{Sfetsos2006,
abstract = {In this paper we discuss our empirical study about the advantages and difficulties 15 Greek software companies experienced applying Extreme Programming (XP) as a holistic system in software development. Based on a generic XP system including feedback influences and using a cause-effect model including social-technical affecting factors, as our research tool, the study statistically evaluates the application of XP practices in the software companies being studied. Data were collected from 30 managers and developers, using the sample survey technique with questionnaires and interviews, in a time period of six months. Practices were analysed individually, using Descriptive Statistics (DS), and as a whole by building up different models using stepwise Discriminant Analysis (DA). The results have shown that companies, facing various problems with common code ownership, on-site customer, 40-hour week and metaphor, prefer to develop their own tailored XP method and way of working-practices that met their requirements. Pair programming and test-driven development were found to be the most significant success factors. Interactions and hidden dependencies for the majority of the practices as well as communication and synergy between skilled personnel were found to be other significant success factors. The contribution of this preliminary research work is to provide some evidence that may assist companies in evaluating whether the XP system as a holistic framework would suit their current situation. {\^{A}}{\textcopyright} Springer Science + Business Media, Inc. 2006.},
annote = {Investigating the extreme programming system - An empirical study},
author = {Sfetsos, P and Angelis, L and Stamelos, I},
journal = {Empirical Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {269--301},
title = {{Investigating the extreme programming system - An empirical study}},
url = {citeulike-article-id:3934789 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33644883897{\&}{\#}38 partnerID=40},
volume = {11},
year = {2006}
}
@inproceedings{5368737,
abstract = {Unauthorized network address translation (NAT) devices may be a significant security problem. They provide unrestricted access to any number of hosts connecting to them. Some attackers may use computers hidden behind NAT devices to conduct malicious activities such as denial of service. An algorithm is proposed in this work to detect hosts hidden behind NAT. Different from previous researches, the algorithm does not depend on any special field in any packet header. It is based on analyzing traffic features with directed acyclic graph support vector machine (DAGSVM). Firstly, traffic models of hosts are selected from training samples with DAGSVM. Then the models and classifier are used for predicting host number of unknown traces. What revealed by the experiment includes that the proposed algorithm is effective, even when there are more hosts in the test set than it is in the training set, and the accuracy will fall when there are more unknown hosts in the test traces.},
author = {Rui, Li and Hongliang, Zhu and Yang, Xin and Shoushan, Luo and Yixian, Yang and Cong, Wang},
booktitle = {Multimedia Information Networking and Security, 2009. MINES '09. International Conference on},
doi = {10.1109/MINES.2009.88},
keywords = {acyclic graph support vector machine;analyzing tra},
pages = {474--477},
title = {{Passive NATted Hosts Detect Algorithm Based on Directed Acyclic Graph Support Vector Machine}},
volume = {2},
year = {2009}
}
@inproceedings{Pingclasai2013,
address = {Bangkok, Thailand},
author = {Pingclasai, Natthakul and Hata, Hideaki and Matsumoto, Ken-ichi},
booktitle = {2013 20th Asia-Pacific Software Engineering Conference (APSEC)},
doi = {10.1109/APSEC.2013.105},
isbn = {978-1-4799-2144-7},
issn = {1530-1362},
keywords = {Bayes methods,Computer bugs,Data mining,Data models,F-measure score,HTTPClient,Jackrabbit,Logistics,Lucene project,Predictive models,Software,Vectors,automatic bug report classification techniques,bug classification,bug prediction,bug report preprocessing,bug reports,bug triaging,decision tree,decision trees,logistic regression,naive Bayes classifier,open-source software projects,pattern classification,program debugging,regression analysis,topic modeling},
language = {English},
month = {dec},
pages = {13--18},
publisher = {IEEE},
title = {{Classifying Bug Reports to Bugs and Other Requests Using Topic Modeling}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6754344},
year = {2013}
}
@inproceedings{Horvitz1999b,
address = {Pittsburgh, PA},
author = {Horvitz, Eric},
keywords = {hci,mixed intiitative},
pages = {159--166},
title = {{Principles of Mixed-Initiative User Interfaces}},
year = {1999}
}
@inproceedings{4258736,
abstract = {As the US-Mexico border areas recently emerge as a hub for cyber-criminal activities, the El Paso (Texas) division of FBI processes voluminous cases on Internet crimes. Using data from the Internet Crime Complaint Center, this research project aims to develop and validate scalable methods and techniques for profiling and visualizing cyber-criminal activities. A general framework and potential contributions are discussed.},
author = {Chung, Wingyan and Wang, G A},
booktitle = {Intelligence and Security Informatics, 2007 IEEE},
doi = {10.1109/ISI.2007.379512},
keywords = {FBI process,Internet crime complaint center,US-Mex,agile,nsf},
mendeley-tags = {agile,nsf},
month = {may},
pages = {376},
title = {{Profiling and Visualizing Cyber-criminal Activities: A General Framework}},
year = {2007}
}
@article{Schneiderman1977,
address = {London},
author = {Schneiderman, B},
journal = {International Journal of Man-Machine Studies},
keywords = {agile,comprehension,memory,nsf,programming,psychology},
mendeley-tags = {agile,nsf},
number = {4},
pages = {465--478},
publisher = {Academic Press},
title = {{Measuring Computer Programing Quality and Comprehension}},
volume = {9},
year = {1977}
}
@article{Durbin,
author = {Durbin, J},
journal = {Biometrika},
number = {1},
pages = {5--22},
title = {{{\{}K{\}}olmogorov-{\{}S{\}}mirnov tests when parameters are estimated with applications to tests of exponentiality and tests on spacings}},
volume = {62},
year = {1975}
}
@inproceedings{LeMalecot2006b,
address = {New York, New York, USA},
author = {{Le Mal{\'{e}}cot}, Erwan and Kohara, Masayoshi and Hori, Yoshiaki and Sakurai, Kouichi},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179600},
isbn = {1595935495},
keywords = {2D,3D,monitoring,network,security,visualization},
month = {nov},
pages = {123},
publisher = {ACM Press},
title = {{Interactively combining 2D and 3D visualization for network traffic monitoring}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179600},
year = {2006}
}
@inproceedings{Frens2007,
abstract = {Traditional approaches to semester-long projects in compiler courses force students to implement the early stages of a compiler in depth; since many students fall behind, they have little opportunity to implement the back end. Consequently, students have a deep knowledge of early material and no knowledge of latter material. We propose an approach based on incremental development and test-driven development; this approach solves the emphasis problem, provides experience with useful tools, and allows for such a course to be taught in a three or four weeks. Copyright 2006 ACM.},
annote = {Fifteen compilers in fifteen days},
author = {Frens, J D and Meneely, A},
booktitle = {Proceedings of the Thirty-Seventh SIGCSE Technical Symposium on Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {92--96},
title = {{Fifteen compilers in fifteen days}},
url = {citeulike-article-id:3934613 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-37849041097{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{4909112,
abstract = {Discusses DEViSE, a fully customizable system that provides a framework for developing a richer, fuller picture of network traffic. This not only helps locate past, present, and ongoing security attacks but also graphically identifies areas where organizations can implement stricter policies to lower the risk of data loss.},
author = {Read, H and Xynos, K and Blyth, A},
doi = {10.1109/MCG.2009.48},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {DEViSE,XML,data exchange,data sharing,information},
number = {3},
pages = {6--11},
title = {{Presenting DEViSE: Data Exchange for Visualizing Security Events}},
volume = {29},
year = {2009}
}
@inproceedings{Long1995,
author = {Long, D and Muir, A and Golding, R},
booktitle = {Proc.$\backslash$ 14th Symposium on Reliable Distributed Systems},
isbn = {0-8186-7153-X},
keywords = {Internet,Internet host reliability,fault tolerant computing,fault-tolerance,fault-tolerant replicated monitoring,host reliability,host system reliability,reliability,replication},
pages = {2--9},
title = {{A longitudinal survey of Internet host reliability}},
year = {1995}
}
@misc{Damm2007,
abstract = {Companies spend significant efforts on testing their products to achieve a sufficient quality level. This paper presents results from evaluating the quality impact of implementing a framework for component-level test automation and Test-Driven Development. The evaluation comprised six projects for two products at a software development department at Ericsson. The paper suggests how an existing measurement approach can be used for evaluating the quality impact of improvements in early phases, i.e. by classifying faults reported on released products after which phase they should have been caught in. Based on this measurement approach, the evaluation determined that the ratio of reported faults in the released products decreased significantly after implementing the framework. That is, the ratio of faults belonging to component-level testing decreased from between 60-70 percent to less than 20 percent in the two studied products. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Quality impact of introducing component-level test automation and test-driven development},
author = {Damm, L O and Lundberg, L},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {187--199},
title = {{Quality impact of introducing component-level test automation and test-driven development}},
url = {citeulike-article-id:3934582 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38349063395{\&}{\#}38 partnerID=40},
volume = {4764 LNCS},
year = {2007}
}
@book{Grady1992,
address = {Englewood Cliffs, NJ},
author = {Grady, R},
isbn = {0137203845},
keywords = {framework,measurement,metrics,strategies},
publisher = {Prentice Hall},
title = {{Practical Software Metrics for Project Managements and Process Improvement}},
year = {1992}
}
@misc{Katira2004a,
address = {Raleigh, NC},
author = {Katira, Neha},
publisher = {North Carolina State University},
title = {{Understanding the Compatibility of Pair Programmers MS Thesis}},
year = {2004}
}
@book{Glaser1971,
address = {Chicago, IL},
author = {Glaser, Daniel},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Markham},
title = {{Social Deviance}},
year = {1971}
}
@article{Dowty2004,
abstract = {Traditional software development methodologies mirror the development cycles used in other forms of engineering. Specications are gathered, software is designed, the design is `constructed` into a nished software product, then the nished product is tested. New methodologies such as Extreme Programming are improving on this by detecting problems early and exing under changing requirements. We focus on applying Extreme Programming's test driven development to embedded systems featuring custom hardware and software designs. An inexpensive and e ective method for testing embedded systems using existing software test frameworks and methodology is developed, applied, and evaluated.},
annote = {Test driven development of embedded systems using existing software test infrastructure},
author = {Dowty, M},
journal = {University of Colorado at Boulder, Tech. Rep., Mar},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Test driven development of embedded systems using existing software test infrastructure}},
url = {citeulike-article-id:3934592 http://svn.navi.cx/misc/trunk/docs/papers/embedded-test-driven-development.pdf},
year = {2004}
}
@inproceedings{5380527,
abstract = {Role based access control (RBAC) is a powerful security administration concept that can simplify permission assignment management. Migration to and maintenance of RBAC requires role engineering, the identification of a set of roles that offer administrative benefit. However, establishing that RBAC is desirable in a given enterprise is lacking in current role engineering processes. To help identify the practical need for RBAC, we propose RoleVAT, a Role engineering tool for the Visual Assessment of user and permission Tendencies. User and permission clusters can be visually identified as potential user groups or roles. The benefit and impact of this visual analysis in enterprise environments is discussed and demonstrated through testing on real life as well as synthetic datasets. Our experimental results show the effectiveness of RoleVAT as well as interesting user and role tendencies in real enterprise environments.},
author = {Zhang, Dana and Ramamohanarao, Kotagiri and Versteeg, Steven and Zhang, Rui},
booktitle = {2009 Annual Computer Security Applications Conference},
doi = {10.1109/ACSAC.2009.11},
isbn = {978-1-4244-5327-6},
issn = {1063-9527},
keywords = {RoleVAT,permission Tendencies,role based access co},
month = {dec},
pages = {13--22},
publisher = {Ieee},
title = {{RoleVAT: Visual Assessment of Practical Need for Role Based Access Control}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5380527},
year = {2009}
}
@inproceedings{Hawkey2008,
address = {New York, New York, USA},
author = {Hawkey, Kirstie and Botta, David and Werlinger, Rodrigo and Muldner, Kasia and Gagne, Andre and Beznosov, Konstantin},
booktitle = {Proceeding of the twenty-sixth annual CHI conference extended abstracts on Human factors in computing systems - CHI '08},
doi = {10.1145/1358628.1358905},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hawkey et al. - 2008 - Human, organizational, and technological factors of IT security.pdf:pdf},
isbn = {978160558012X},
keywords = {human factors,it security,organizational factors,qualitative research,technological factors},
month = {apr},
pages = {3639},
publisher = {ACM Press},
title = {{Human, organizational, and technological factors of IT security}},
url = {http://dl.acm.org/citation.cfm?id=1358628.1358905},
year = {2008}
}
@incollection{blei2009,
address = {New York, NY},
author = {Blei, David M. and Lafferty, John D.},
booktitle = {Text Mining: Classification, Clustering, and Applications},
editor = {Srivastava, Ashok N. and Sahami, Mehran},
isbn = {1420059459},
keywords = {lda},
mendeley-tags = {lda},
pages = {71--94},
publisher = {CRC Press},
title = {{Topic Models}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=BnvYaYhMl-MC{\&}pgis=1},
year = {2009}
}
@techreport{Briand1996,
address = {College Park, MD},
author = {Briand, Lionel and Kim, Yong-Mi and Melo, Walc{\'{e}}lio and Seaman, Carolyn and Basili, Victor},
booktitle = {University of Maryland at College Park College Park, MD, USA},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Briand et al. - 1996 - Qualitative analysis for maintenance process assessment.pdf:pdf},
institution = {University of Maryland at College Park},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {36},
publisher = {Citeseer},
title = {{Qualitative analysis for maintenance process assessment}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.1607{\&}rep=rep1{\&}type=pdf},
year = {1996}
}
@misc{ISO/IEC2008,
annote = {ISO standard on evaluation},
author = {ISO/IEC},
keywords = {evaluation,standard},
title = {{24765:2008 Systems and software engineering vocabulary}},
year = {2008}
}
@inproceedings{Novielli2015,
address = {Bergamo, Italy},
author = {Novielli, Nicole and Calefato, Fabio and Lanubile, Filippo},
booktitle = {Proceedings of the 7th International Workshop on Social Software Engineering - SSE 2015},
doi = {10.1145/2804381.2804387},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Novielli, Calefato, Lanubile - 2015 - The challenges of sentiment detection in the social programmer ecosystem.pdf:pdf},
isbn = {9781450338189},
keywords = {sentiment},
mendeley-tags = {sentiment},
pages = {33--40},
publisher = {ACM Press},
title = {{The challenges of sentiment detection in the social programmer ecosystem}},
url = {http://dl.acm.org/citation.cfm?doid=2804381.2804387},
year = {2015}
}
@inproceedings{4381276,
abstract = {3D mapping is increasingly important for mobile robotics in general and for safety, security and rescue robotics (SSRR) in particular as complex environments must but captured in this domain. But it is hard to visualize 3D data in a simple way, e.g. to print maps for first responders, or to use it in standard robotics algorithms, e.g., for path planning. This paper describes a new approach to extract standard planar maps from large scale 3D maps in a very fast manner. In doing so, the approach can detect multiple floors, e.g., in a multi-story building or in a pancake collapse, and segment the 3D map accordingly. To each floor or level, a planar map is assigned, which is augmented by semantic information, especially with respect to traversability. Experiments are presented that are based on 3D maps generated in the large scale environments of USARsim, a high fidelity robot simulator. It is shown that the approach is very fast. The total processing of a complete 3D map takes just a few hundred milliseconds, leading to a proper extraction of floor plans to each of which semantic maps are assigned.},
author = {Sakenas, Vytenis and Kosuchinas, Olegas and Pfingsthorn, Max and Birk, Andreas},
booktitle = {2007 IEEE International Workshop on Safety, Security and Rescue Robotics},
doi = {10.1109/SSRR.2007.4381276},
isbn = {978-1-4244-1568-7},
keywords = {3D data visualization,3D mapping,3D point cloud ma},
month = {sep},
pages = {1--6},
publisher = {Ieee},
title = {{Extraction of Semantic Floor Plans from 3D Point Cloud Maps}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4381276},
year = {2007}
}
@inproceedings{Sim2003,
address = {Portland, Oregon},
author = {Sim, S E and Easterbrook, S and Holt, R C},
keywords = {benchmark,benchmarking},
pages = {74--83},
title = {{Using Benchmarking to Advance Research: A challenge to Software Engineering}},
year = {2003}
}
@techreport{Kitchenham2007a,
address = {Keele, UK},
author = {Kitchenham, Barbara and Charters, Stuart},
institution = {Keele University},
isbn = {EBSE-2007-01},
publisher = {Software Engineering Group, Keele University and the Department of Computer Science, Univeristy of Durham},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
year = {2007}
}
@article{Tegarden1999,
author = {Tegarden, David P and Sheetz, Steven D},
number = {6},
pages = {779--798},
title = {{Cognitive Activities in OO Development}},
volume = {54},
year = {1999}
}
@inproceedings{Chotzen2019,
abstract = {This paper describes misconceptions students encounter as they design algorithms to implement linked list operations. The research was conducted in a qualitative manner [1], and aims to inform pedagogical practice in the data structures space. The design of our study involved conducting interviews with five undergraduate students, who were given a packet consisting of four questions involving arrays, linked lists, or binary search trees. Though participants showed a confident understanding of the way that linked lists work as static diagrams, when asked to reason about the reassignment of pointers during the design of insert and remove operations, the students seemed to struggle. The details of the representation of pointers as members of the linked list class appeared unclear to our participants, and we posit this gap of understanding was a primary source confusion. We posit increased attention to the object oriented implantation of linked lists in data structures education may result in improved learning outcomes.},
address = {Minneapolis, MN, USA},
author = {Chotzen, Harrison and Johnson, Alasdair J. and Desai, Parth M.},
booktitle = {SIGCSE '19 Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3287324.3293862},
keywords = {CS2},
mendeley-tags = {CS2},
pages = {1261--1261},
publisher = {Association for Computing Machinery (ACM)},
title = {{Exploring the Mental Models of Undergraduate Programmers in the Context of Linked Lists}},
year = {2019}
}
@inproceedings{Gousios2015,
author = {Gousios, Georgios and Zaidman, Andy and Storey, Margaret-Anne and van Deursen, Arie},
booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
doi = {10.1109/ICSE.2015.55},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gousios et al. - 2015 - Work Practices and Challenges in Pull-Based Development The Integrator's Perspective.pdf:pdf},
isbn = {978-1-4799-1934-5},
keywords = {Birds,Collaboration,Databases,Distributed Software Development,Electronic mail,Face,GitHub,Inspection,Pull Request,Pull-based Development,Software,decision making,decision making process,pull-based software development model,software quality},
month = {may},
pages = {358--368},
publisher = {IEEE},
title = {{Work Practices and Challenges in Pull-Based Development: The Integrator's Perspective}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7194588},
volume = {1},
year = {2015}
}
@inproceedings{Karppinen2007,
author = {Karppinen, Karina and Lindvall, Mikael},
booktitle = {Proc. World Congress in Computer Science, Computer Engineering, {\&} Applied Computing},
keywords = {Software Engineering,Software Security,Software architecture},
pages = {595--599},
series = {International Conference on Software Engineering Research and Practice, SERP},
title = {{Software architecture-driven detection of security vulnerabilities}},
year = {2007}
}
@inproceedings{5380503,
abstract = {Attack graphs play important roles in analyzing network security vulnerabilities, and previous works have provided meaningful conclusions on the generation and security measurement of attack graphs. However, it is still hard for us to understand attack graphs in a large network, and few suggestions have been proposed to prevent inside malicious attackers from attacking networks. To address these problems, we propose a novel approach to generate and describe attack graphs. Firstly, we construct a two-layer attack graph, where the upper layer is a hosts access graph and the lower layer is composed of some host-pair attack graphs. Compared with previous works, our attack graph has simpler structures, and reaches the best upper bound of computation cost in O(N2). Furthermore, we introduce the adjacency matrix to efficiently evaluate network security, with overall evaluation results presented by gray scale images vividly. Thirdly, by applying prospective damage and important weight factors on key hosts with crucial resources, we can create prioritized lists of potential threatening hosts and stepping stones, both of which can help network administrators to harden network security. Analysis on computation cost shows that the upper bound computation cost of our measurement methodology is O(N3), which could also be completed in real time. Finally, we give some examples to show how to put our methods in practice.},
author = {Xie, Anming and Cai, Zhuhua and Tang, Cong and Hu, Jianbin and Chen, Zhong},
booktitle = {Computer Security Applications Conference, 2009. ACSAC '09. Annual},
doi = {10.1109/ACSAC.2009.22},
issn = {1063-9527},
keywords = {host-pair attack graphs;hosts access graph;network},
pages = {127--136},
title = {{Evaluating Network Security With Two-Layer Attack Graphs}},
year = {2009}
}
@article{Wood1996,
author = {Wood, A.},
doi = {10.1109/2.544240},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wood - 1996 - Predicting software reliability.pdf:pdf},
issn = {00189162},
journal = {Computer},
keywords = {Application software,Business,Computer industry,DP industry,Feedback,Mathematical model,Predictive models,Software quality,Software reliability,Software testing,System testing,Tandem,business data processing,commercial applications,coordinated quality assurance effort,critical business applications,customer feedback,development process,major software release,software industry,software maintenance,software reliability,software reliability growth models,software reliability modeling,software reliability prediction,software vendors},
language = {English},
number = {11},
pages = {69--77},
publisher = {IEEE},
title = {{Predicting software reliability}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=544240},
volume = {29},
year = {1996}
}
@article{Haungs2001,
author = {Haungs, Jim},
doi = {10.1109/2.901173},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haungs - 2001 - Pair programming on the C3 project.pdf:pdf},
issn = {00189162},
journal = {Computer},
keywords = {agile},
mendeley-tags = {agile},
month = {mar},
number = {3},
pages = {118--119},
title = {{Pair programming on the C3 project}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=901173},
volume = {34},
year = {2001}
}
@book{Hutchison1973,
author = {Hutchison, David and Mitchell, John C},
isbn = {9783540258124},
title = {{Rapid Integration of Software Engineering Techniques}},
year = {1973}
}
@article{Horvitz2003,
author = {Horvitz, Eric and Kadie, Carl and Paek, Tim and Hovel, David},
doi = {10.1145/636772.636798},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Horvitz et al. - 2003 - Models of attention in computing and communication.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {agile,attention,hci,interruption,nsf},
mendeley-tags = {agile,attention,hci,interruption,nsf},
month = {mar},
number = {3},
pages = {52},
title = {{Models of attention in computing and communication}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=636798{\&}type=html},
volume = {46},
year = {2003}
}
@inproceedings{Song2020,
address = {Tampa, FL, USA},
author = {Song, Yang and Xiao, Yunkai and Stephens, Jonathan and Ruesch, Emma and Roginski, Sean and Layman, Lucas},
booktitle = {Proceedings of the 2020 ACM Southeast Conference (ACMSE 2020)},
doi = {10.1145/3374135.3385277},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song et al. - 2020 - Suitability of SCS1 as a Pre-CS2 Assessment Instrument A Comparison with Short Deliberate-practice Questions.pdf:pdf},
pages = {2},
title = {{Suitability of SCS1 as a Pre-CS2 Assessment Instrument : A Comparison with Short Deliberate-practice Questions}},
year = {2020}
}
@incollection{Sampson1994,
abstract = {summarize current knowledge on individual-, situational-, and community-level sources of criminal violence / identify key problems of causal interpretation in existing research / suggest new directions for future research and policy our goal . . . is to show how a multilevel perspective on both victimization and offending may substantially increase the understanding and control of violence},
address = {Washington, DC},
author = {Sampson, Robert J. and Lauritsen, Janet L.},
booktitle = {Understanding and Preventing Violence, Vol. 3: Social Influences},
editor = {{Reiss Jr.}, Albert J. and Roth, Jeffrey A.},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {1--114},
publisher = {National Academy Press},
title = {{Violent victimization and offending: Individual-, situational-, and community-level risk factors.}},
year = {1994}
}
@incollection{Fisher2009,
address = {San Francisco, CA},
author = {Fisher, Jeffrey D. and Fisher, William A. and Shiper, Paul A.},
booktitle = {Emerging Theories in Health Promotion Practice and Research},
edition = {2nd},
editor = {DiClemente, Ralph J. and Crosby, Richard A. and Kegler, Michelle C.},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {21--63},
publisher = {Jessey-Bass},
title = {{The Information-Motivation-Behavioral Skills Model of HIV Preventative Behavior}},
year = {2009}
}
@article{Desai2008,
abstract = {University professors traditionally struggle to incorporate software testing into their course curriculum. Worries include double-grading for correctness of both source and test code and finding time to teach testing as a topic. Test-driven development (TDD) has been suggested as a possible solution to improve student software testing skills and to realize the benefits of testing. According to most existing studies, TDD improves software quality and student productivity. This paper surveys the current state of TDD experiments conducted exclusively at universities. Similar surveys compare experiments in both the classroom and industry, but none have focused strictly on academia.},
address = {New York, NY, USA},
annote = {A survey of evidence for test-driven development in academia},
author = {Desai, Chetan and Janzen, David and Savage, Kyle},
isbn = {0097-8418},
journal = {SIGCSE Bull.},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {97--101},
title = {{A survey of evidence for test-driven development in academia}},
url = {citeulike-article-id:3934586 http://dx.doi.org/10.1145/1383602.1383644},
volume = {40},
year = {2008}
}
@book{Galloway2005,
address = {Berlin, Heidelberg},
author = {Zeng, Daniel and Chen, Hsinchun and Tseng, Chunju and Larson, Catherine and Chang, Wei and Eidson, Millicent and Gotham, Ivan and Lynch, Cecil and Ascher, Michael and Galloway, John and Simoff, Simeon J. and Zeng, Daniel and Chen, Hsinchun and Tseng, Chunju and Larson, Catherine and Chang, Wei and Eidson, Millicent and Gotham, Ivan and Lynch, Cecil and Ascher, Michael and Galloway, John and Simoff, Simeon J. and Zeng, Daniel and Chen, Hsinchun and Tseng, Chunju and Larson, Catherine and Chang, Wei and Eidson, Millicent and Gotham, Ivan and Lynch, Cecil and Ascher, Michael and Galloway, John and Simoff, Simeon J.},
doi = {10.1007/b136511},
editor = {Kantor, Paul and Muresan, Gheorghe and Roberts, Fred and Zeng, Daniel D. and Wang, Fei-Yue and Chen, Hsinchun and Merkle, Ralph C.},
isbn = {978-3-540-25999-2},
month = {may},
pages = {14--26},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Intelligence and Security Informatics}},
url = {http://dl.acm.org/citation.cfm?id=2154644.2154647 http://dl.acm.org/citation.cfm?id=2154644.2154732},
volume = {3495},
year = {2005}
}
@article{Androutsopoulos2013,
author = {Androutsopoulos, Kelly and Clark, David and Harman, Mark and Hierons, Robert M and Member, Senior and Li, Zheng and Tratt, Laurence},
number = {7},
pages = {892--909},
title = {{Amorphous Slicing of Extended Finite State Machines}},
volume = {39},
year = {2013}
}
@article{Agrawal1991,
abstract = {Spyder, a system for selective checkpointing of computational sequences, is presented. It lets users backtrack from checkpoints without the need to reexecute the program to reach recent prior states. In contrast to more comprehensive (and storage-intensive) checkpointing schemes, backtracking in this approach is constrained to limit storage requirements. The resulting debugger offers a structured view of dynamic events, similar to lexical scope rules' effect on static visibility. The debugger also speeds backtracking to statements before loops and provides what-if capabilities.},
author = {Agrawal, H. and {De Millo}, R.A. and Spafford, E.H.},
issn = {0740-7459},
journal = {IEEE Software},
month = {may},
number = {3},
pages = {21--26},
title = {{An execution-backtracking approach to debugging}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=88940},
volume = {8},
year = {1991}
}
@inproceedings{4258709,
abstract = {Semantic Event Tracker (SET) is a highly interactive visualization tool for tracking and associating activities (events) in a spatially and Multimedia Enriched Virtual Environment. SET provides integrated views of information spaces while providing overview and detail to improve perception and evaluation of complex scenarios. We model an event as an object that describes an action and its location, time, and relations to other objects. Real world event information is extracted from Internet sources, then stored and processed using Semantic Web technologies that enable us to discover semantic associations between events. We use RDF graphs to represent semantic metadata and ontologies. SET is capable of visualizing as well as navigating through the event data in all three aspects of space, time and theme.},
author = {Deligiannidis, L and Hakimpour, F and Sheth, A P},
booktitle = {Intelligence and Security Informatics, 2007 IEEE},
doi = {10.1109/ISI.2007.379483},
keywords = {Internet sources,RDF graphs,event visualization,in},
month = {may},
pages = {266--269},
title = {{Visualization of Events in a Spatially and Multimedia Enriched Virtual Environment}},
year = {2007}
}
@book{Boehm2000b,
address = {Upper Saddle River, NJ},
author = {Boehm, Barry W. and Abts, Chris and Brown, A. Winsor and Chulani, Sunita and Clark, Bradford K. and Horowitz, Ellis and Madachy, Ray and Reifer, Donald J. and Steece, Bart Bert},
publisher = {Prentice Hall},
title = {{Software Cost Estimation with COCOMO II}},
year = {2000}
}
@techreport{Madden2013,
address = {Washington, DC},
author = {Madden, Mary and Lenhart, Amanda and Cortesi, Sandra and Gasser, Urs},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Madden et al. - 2013 - Teens and Mobile Apps Privacy.pdf:pdf},
institution = {Pew Research Center},
title = {{Teens and Mobile Apps Privacy}},
year = {2013}
}
@inproceedings{Ryu2005,
abstract = {TDD is a software development approach which is based on test. TDD let us get improved code and refined design through lasting test with refactoring process. However, if network or database environment and other object were not developed, TDD could have a problem to make progress. If you will use the Mock Objects in this situation, TDD will be processed more effectively. To make Mock Objects needs a lot of cost and effort for network and database. Therefore this paper presents a Mock Objects frameworks for TDD which can save time and make safe Mock Objects. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {Mock objects framework for TDD in the network environment},
author = {Ryu, H Y and Sohn, B K and Park, J H},
booktitle = {Proceedings - Fourth Annual ACIS International Conference on Computer and Information Science, ICIS 2005},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {430--434},
title = {{Mock objects framework for TDD in the network environment}},
url = {citeulike-article-id:3934773 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746711040{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@article{Domino2007,
abstract = {The use of agile methods is growing in industrial practice due to the documented benefits of increased software quality, shared programmer expertise, and user satisfaction. These methods include pair programming (two programmers working side-by-side producing the code) and test-driven approaches (test cases written first to prepare for coding). In practice, software development organizations adapt agile methods to their environment. The purpose of this research is to understand better the impacts of adapting these methods. We perform a set of controlled experiments to investigate how adaptations, or variations, to the pair programming method impact programming performance and user satisfaction. We find that method variations do influence programming results. In particular, better performance and satisfaction outcomes are achieved when the pair programming is performed in face-to-face versus virtual settings, in combination with the test-driven approach, and with more experienced programmers. We also find that limiting the extent of collaboration can be effective, especially when programmers are more experienced. These experimental results provide a rigorous foundation for deciding how to adapt pair programming methods into specific project contexts. {\^{A}}{\textcopyright} Springer Science+Business Media, LLC 2007.},
annote = {Controlled experimentation on adaptations of pair programming},
author = {Domino, M A and Collins, R W and Hevner, A R},
journal = {Information Technology and Management},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {4},
pages = {297--312},
title = {{Controlled experimentation on adaptations of pair programming}},
url = {citeulike-article-id:3934590 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-36148974397{\&}{\#}38 partnerID=40},
volume = {8},
year = {2007}
}
@inproceedings{5375541,
abstract = {We describe the development of a set of tools for analyzing the textual contents of digital forensic evidence for the purpose of enhancing an investigator's ability to discover information quickly and efficiently. By examining the textual contents of files and unallocated space, relationships between sets of files and clusters can be formed based on the information that they contain. Using the information gathered from the evidence through the analysis tool, the visualization tool can be used to search through the evidence in an organized and efficient manner. The visualization depicts both the frequency of relevant terms and their location on disk. We also discuss a task analysis with forensics officers to motivate the design.},
author = {Jankun-Kelly, T.J. and Wilson, David and Stamps, Andrew S. and Franck, Josh and Carver, Jeffery and Swan, J. Edward},
booktitle = {2009 6th International Workshop on Visualization for Cyber Security},
doi = {10.1109/VIZSEC.2009.5375541},
isbn = {978-1-4244-5413-6},
keywords = {computer forensic,digital forensics evidence,task},
pages = {39--44},
publisher = {Ieee},
title = {{A visual analytic framework for exploring relationships in textual contents of digital forensics evidence}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5375541},
year = {2009}
}
@article{Eisenhardt1989,
author = {Eisenhardt, K M},
keywords = {case studies,cross-case analysis,qualitative},
pages = {532--550},
title = {{Building Theories from Case Study Research}},
volume = {14},
year = {1989}
}
@inproceedings{Saha2013,
address = {Palo Alto, CA},
author = {Saha, Ripon K. and Lease, Matthew and Khurshid, Sarfraz and Perry, Dewayne E.},
booktitle = {2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE)},
doi = {10.1109/ASE.2013.6693093},
isbn = {978-1-4799-0215-6},
keywords = {Accuracy,BLUiR,Bug localization,Computer bugs,Indexing,Information retrieval,Java,Mathematical model,Measurement,bug localization,bug reports,bug similarity data,code constructs,information retrieval,large-scale systems,natural language information retrieval,natural language processing,open source IR toolkit,program debugging,public domain software,search,source code,structured information retrieval},
language = {English},
month = {nov},
pages = {345--355},
publisher = {IEEE},
title = {{Improving bug localization using structured information retrieval}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6693093},
year = {2013}
}
@inproceedings{Hindle2011,
address = {Honolulu, HI},
author = {Hindle, Abram and Ernst, Neil A. and Godfrey, Michael W. and Mylopoulos, John},
booktitle = {Proceeding of the 8th working conference on Mining software repositories - MSR '11},
doi = {10.1145/1985441.1985466},
isbn = {9781450305747},
keywords = {lda,non-functional requirements,topic analysis},
month = {may},
pages = {163--172},
publisher = {ACM Press},
title = {{Automated topic naming to support cross-project analysis of software maintenance activities}},
url = {http://dl.acm.org/citation.cfm?id=1985441.1985466},
year = {2011}
}
@inproceedings{Murphy2008,
abstract = {As university-level distance learning programs become more and more popular, and software engineering courses incorporate extreme Programming (XP) into their curricula, certain challenges arise when teaching XP to students who are not physically co-located. In this paper, we present the results of a three-year study of such an online software engineering course targeted to graduate students, and describe some of the specific challenges faced, such as students' aversion to aspects of XP and difficulties in scheduling. We discuss our findings in terms of the course's educational objectives, and present suggestions to other educators who may face similar situations. Copyright 2008 ACM.},
annote = {A distance learning approach to teaching extreme programming},
author = {Murphy, C and Phung, D and Kaiser, G},
booktitle = {Proceedings of the Conference on Integrating Technology into Computer Science Education, ITiCSE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {199--203},
title = {{A distance learning approach to teaching extreme programming}},
url = {citeulike-article-id:3934734 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57349177042{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Nguyen2012,
address = {New York, New York, USA},
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Nguyen, Tien N. and Lo, David and Sun, Chengnian},
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering - ASE 2012},
doi = {10.1145/2351676.2351687},
isbn = {9781450312042},
keywords = {DBTM approach,Duplicate Bug Reports,IR approach,IR-based feature,Information Retrieval,Topic Model,duplicate bug report detection,information retrieval,program debugging,text analysis,text-based information retrieval,textual document,topic modeling,topic-based feature},
language = {English},
pages = {70--79},
publisher = {ACM Press},
title = {{Duplicate bug report detection with a combination of information retrieval and topic modeling}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6494907},
year = {2012}
}
@techreport{Gibson2006,
address = {CMU/SEI-2006-TR-004},
author = {Gibson, Diane L and Goldenson, Dennis R and Kost, Keith},
institution = {Carnegie-Mellon University Software Engineering Institute},
title = {{Performance Results of CMMI{\textregistered}-Based Process Improvement}},
url = {http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA454687},
year = {2006}
}
@inproceedings{Peters2015,
abstract = {{\textcopyright} 2015 IEEE. Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data. In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute "interesting" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multiparty approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).},
address = {Florence, Italy},
author = {Peters, Fayola and Menzies, Tim and Layman, Lucas},
booktitle = {37th International Conference on Software Engineering (ICSE '15)},
doi = {10.1109/ICSE.2015.92},
file = {:C$\backslash$:/Users/laymanl/Desktop/layman pubs/Peters2015.pdf:pdf},
isbn = {9781479919345},
issn = {02705257},
keywords = {agile,mypubs,nsf},
mendeley-tags = {agile,mypubs,nsf},
pages = {801--811},
title = {{LACE2: Better Privacy-Preserving Data Sharing for Cross Project Defect Prediction}},
volume = {1},
year = {2015}
}
@article{Kansomkeat2003,
author = {Kansomkeat, Supaporn},
pages = {296--300},
title = {{Automated-Generating Test Case Using UML Statechart Diagrams}},
year = {2003}
}
@article{Chandola2009a,
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
doi = {10.1145/1541880.1541882},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {Anomaly detection,outlier detection},
month = {jul},
number = {3},
pages = {1--58},
title = {{Anomaly detection}},
url = {http://dl.acm.org/citation.cfm?id=1541880.1541882},
volume = {41},
year = {2009}
}
@inproceedings{Bohmer2011a,
address = {New York, New York, USA},
author = {B{\"{o}}hmer, Matthias and Hecht, Brent and Sch{\"{o}}ning, Johannes and Kr{\"{u}}ger, Antonio and Bauer, Gernot},
booktitle = {Proceedings of the 13th International Conference on Human Computer Interaction with Mobile Devices and Services - MobileHCI '11},
doi = {10.1145/2037373.2037383},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/B{\"{o}}hmer et al. - 2011 - Falling asleep with Angry Birds, Facebook and Kindle.pdf:pdf},
isbn = {9781450305419},
keywords = {large-scale study,measuring,mobile apps,usage sensor},
month = {aug},
pages = {47},
publisher = {ACM Press},
title = {{Falling asleep with Angry Birds, Facebook and Kindle}},
url = {http://dl.acm.org/citation.cfm?id=2037373.2037383},
year = {2011}
}
@incollection{Blum2013,
address = {Berlin, Germany},
annote = {Accuracy of gps, gyroscope, compass

Read citation 3
},
author = {Blum, Jeffrey R. and Greencord, Daniel G. and Cooperstock, Jeremy R.},
booktitle = {Mobile and Ubiquitous Systems: Computing, Networking, and Services},
editor = {Zheng, Kan and Li, Mo and Jiang, Hongbo},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blum, Greencord, Cooperstock - 2013 - Smartphone Sensor Reliability for Augmented Reality Applications.pdf:pdf},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {127--138},
publisher = {Springer Berlin Heidelberg},
title = {{Smartphone Sensor Reliability for Augmented Reality Applications}},
year = {2013}
}
@article{Ambler2006a,
abstract = {The features and the issues regarding the agile software development in the IT and QA professionals, are discussed. Many traditional IT professionals are struggling to see how they fit into an agile software development project. Some people have the attitude that agilist have adopted techniques such as test driven development (TDD), acceptance testing, pair programming code refactoring, and database refactoring, all of which promote creation of high quality work products. The agile software development lifecycle (SDLC) looks very much like traditional SDLC, but the the former is highly collaborative, iterative, and incremental, and the roles that people take are more robust than on traditional projects. On an agile project, developers work closely with their stakeholders to understand their needs, to implement and test their solution, and the solution is shown to their stakeholders for quick feedback.},
annote = {Where did all the positions go?},
author = {Ambler, S W},
journal = {Dr. Dobb's Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {90--92},
title = {{Where did all the positions go?}},
url = {citeulike-article-id:3934536 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33744776929{\&}{\#}38 partnerID=40},
volume = {31},
year = {2006}
}
@article{Avritzer1997,
address = {Hingham, MA, USA},
author = {Avritzer, Alberto and Weyuker, Elaine J},
issn = {1382-3256},
journal = {Empirical Software Engineering},
number = {1},
pages = {59--77},
publisher = {Kluwer Academic Publishers},
title = {{Monitoring Smoothly Degrading Systems for Increased Dependability}},
volume = {2},
year = {1997}
}
@inproceedings{Hayes2003,
address = {Monterey, CA},
author = {Hayes, J.H. and Dekhtyar, A. and Osborne, J.},
booktitle = {Proceedings of the 11th IEEE International Requirements Engineering Conference},
doi = {10.1109/ICRE.2003.1232745},
isbn = {0-7695-1980-6},
issn = {1090-705X},
keywords = {Algorithm design and analysis,Capability maturity model,Computer aided software engineering,Computer science,Horses,Information retrieval,Performance analysis,Signal analysis,Software tools,Sparks,formal verification,information retrieval,information retrieval problem,irrelevant potential link,missed traceability link,requirements tracing,signal-to-noise levels,systems analysis},
language = {English},
pages = {138--147},
publisher = {IEEE Comput. Soc},
title = {{Improving requirements tracing via information retrieval}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1232745},
year = {2003}
}
@book{Boehm1981,
address = {Englewood Cliffs, NJ},
annote = {p. 212{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}In many economic treatments, "net value" is identified as the profit of a business firm, and a "theory of the firm" is developed, based on the assumption that the firm{\&}{\#}039;s overriding objective is to maximize its profit.  The use of dollar profit as the only criterion to be used in decisionmaking often leads to decisions with good short-term profit properties, but poor social outcomes for the people involved (and often, as a result, poor long-term profit prospects).  The net value approach used in this book assumes all of the relevant components of effectiveness -- employees{\&}{\#}039; need fulfillment, customers{\&}{\#}039; good will, users{\&}{\#}039; information privacy, operators{\&}{\#}039; ease of use . . . "},
author = {Boehm, B W},
publisher = {Prentice-Hall, Inc.},
title = {{Software Engineering Economics}},
year = {1981}
}
@inproceedings{5380684,
abstract = {High assurance MILS and MLS systems require strict limitation of the interactions between different security compartments based on a security policy. Virtualization can be used to provide a high degree of separation in such systems. Even with perfect isolation, however, the I/O devices are shared between different security compartments. Among the I/O controllers, the graphics subsystem is the largest and the most complex. This paper describes the design and implementation of TrustGraph, a trusted graphics subsystem for high assurance systems. First, we explain the threats and attacks possible against an unsecured graphics subsystem. We then describe the design of TrustGraph, the security principles it is built upon, and its implementation. Finally, we verify our implementation through different levels of verification which include functionality testing for simple operations, attack testing for security mechanisms, and formal verification for the critical components of the implementation. An analysis of the graphics API covert channel attack is presented, its channel capacity is measured, and the capacity is reduced using the idea of fuzzy time.},
author = {Okhravi, H and Nicol, D M},
booktitle = {Computer Security Applications Conference, 2009. ACSAC '09. Annual},
doi = {10.1109/ACSAC.2009.31},
issn = {1063-9527},
keywords = {TrustGraph,attack testing,channel capacity measure},
pages = {254--265},
title = {{TrustGraph: Trusted Graphics Subsystem for High Assurance Systems}},
year = {2009}
}
@article{Livshits2013,
author = {Livshits, Benjamin and Jung, Jaeyeon},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Livshits, Jung - 2013 - Automatic mediation of privacy-sensitive resource access in smartphone applications.pdf:pdf},
isbn = {978-1-931971-03-4},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {aug},
pages = {113--130},
publisher = {USENIX Association},
title = {{Automatic mediation of privacy-sensitive resource access in smartphone applications}},
url = {http://dl.acm.org/citation.cfm?id=2534766.2534777},
year = {2013}
}
@article{Herranz2003,
abstract = {This paper is an exploratory work where the authors study how the technology of formal methods (FM) can interact with agile process in general and with extreme programming (XP) in particular. Our thesis is that most of XP practices (pair programming , daily build , the simplest design or the metaphor ) are technology independent and therefore can be used in FM based developments. Additionally, other essential pieces like test first , incremental development and refactoring can be improved by using FM. In the paper we explore in a certain detail those pieces: when you write a formal specification you are saying what your code must do, when you write a test you are doing the same so the idea is to use formal specifications as tests. Incremental development is quite similar to the refinement process in FM: specifications evolve to code maintaining previous functionality. Finally FM can help to remove redundancy, eliminate unused functionality and transform obsolete designs into new ones, and this is refactoring},
annote = {Formal extreme (and extremely formal) programming},
author = {Herranz, A and Moreno-Navarro, J J},
isbn = {3540402152},
journal = {Extreme Programming and Agile Processes in Software Engineering 4th International Conference, XP 2003 Proceedings Lecture Notes in Computer Science Vol 2675},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {88--96},
title = {{Formal extreme (and extremely formal) programming}},
url = {citeulike-article-id:3934640 {\#}},
volume = {2675},
year = {2003}
}
@incollection{Abrahamsson2005,
abstract = {This Paper maintains that efficient business agility requires actions from all levels of the organization in order to strive for success in a turbulent business environment. Agility and agile software development solutions are suggested as yielding benefit in a volatile environment, which is characterized by continuously changing requirements and unstable development technologies. Test-driven development (TDD) is an agile practice where the tests are written before the actual program code. TDD is a technical enabler for increasing agility at the developer and product project levels. Existing empirical literature on TDD has demonstrated increased productivity and more robust code, among other important benefits. This paper reports results of a case study where a mobile application was developed for global markets, using the TDD approach. Our first results show that the adoption of TDD is difficult and the potential agility benefits may not be readily available. The lessons learned from the case study are presented.},
annote = {Improving Business Agility Through Technical Solutions: A Case Study on Test-Driven Development in Mobile Software Development},
author = {Abrahamsson, Pekka and Hanhineva, Antti and J{\"{a}}{\"{a}}linoja, Juho},
booktitle = {Business Agility and Information Technology Diffusion},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {227--243},
title = {{Improving Business Agility Through Technical Solutions: A Case Study on Test-Driven Development in Mobile Software Development}},
url = {citeulike-article-id:3934530 http://dx.doi.org/10.1007/0-387-25590-7{\_}14},
year = {2005}
}
@article{Basili1975b,
author = {Basili, Victor R and Turner, Albert J},
number = {4},
pages = {266--270},
title = {{Iterative Enhancement:  A Practical Technique for Software Development}},
volume = {1},
year = {1975}
}
@inproceedings{Nagappan2003a,
address = {New Orleans},
author = {Nagappan, Nachiappan and Williams, Laurie and Wiebe, Eric and Miller, Carol and Balik, Suzanne and Ferzli, Miriam and Petlick, Julie},
pages = {185--198},
publisher = {Springer-Verlag Lecture Notes in Computer Science},
title = {{Pair Learning:  With an Eye Toward Future Success}},
year = {2003}
}
@article{Martin2007,
abstract = {A professional software developer ships clean, flexible code that works-on time. Unfortunately, many software developers use high-stress heroics to ship late, buggy, messy, and bloated code. Test-driven development is a discipline that helps developers behave in a more professional manner. This article is part of a special issue on test-driven development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Professionalism and test-driven development},
author = {Martin, R C},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {32--36},
title = {{Professionalism and test-driven development}},
url = {citeulike-article-id:3934705 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248395605{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@inproceedings{Allowatt2005,
abstract = {Students need to learn testing skills, and using test-driven development on assignments is one way to help students learn. We use a flexible automated grading system called Web-CAT to assess student assignments, including the validity and completeness of their own test cases. By building on existing educational plug-ins for Eclipse, and adding our own plug-ins for electronic submission and for unit testing support in C++, we are able to use Eclipse as a portal to all the services our students will need, allowing them to accomplish all their tasks entirely within the IDE, from their project's inception to its submission and evaluation. Further, we are able to carry students through the transition from Java programming to C++ programming within this same environment.},
annote = {IDE Support for test-driven development and automated grading in both Java and C++},
author = {Allowatt, A and Edwards, S},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {100--104},
publisher = {ACM New York, NY, USA},
title = {{IDE Support for test-driven development and automated grading in both Java and C++}},
url = {citeulike-article-id:3934535 {\#}},
year = {2005}
}
@inproceedings{5688767,
abstract = {In this paper, a novel extended visual cryptography scheme is presented. Differ from any existing visual cryptography scheme, the proposed scheme encodes one secret image into one shared image with innocent-looking. The secret image is visually revealed by copying the share, shifting the copy and superimposing the shifted copy and the original share together. The proposed scheme has the advantages of resisting geometry distortion, easy recognition and management. Experimental results demonstrate the effectiveness and security of the proposed scheme.},
author = {Wu, Xiaotian and Sun, Wei},
booktitle = {2010 IEEE International Conference on Information Theory and Information Security},
doi = {10.1109/ICITIS.2010.5688767},
isbn = {978-1-4244-6942-0},
keywords = {geometry distortion resistance,im,image management},
month = {dec},
pages = {216--220},
publisher = {Ieee},
title = {{A novel extended visual cryptography scheme using one shared image}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5688767},
year = {2010}
}
@article{Ko2006,
author = {Ko, Andrew J and Myers, Brad A and Coblenz, Michael J and Aung, Htet Htet},
journal = {IEEE Transactions on Software Engineering},
number = {12},
pages = {971--987},
title = {{An exploratory study of how developers seek, relate, and collect relevant information during software maintenance masks}},
volume = {32},
year = {2006}
}
@inproceedings{Restivo2008,
abstract = {A large part of the software development effort is typically spent on maintenance and evolution, namely on adding new and unanticipated features. As aspect-oriented programming (AOP) can be easily used to compose software in non-planned ways, many researchers are investigating AOP as a technique that can play an important role in this particular field. However, unexpected interactions between aspects are still a major problem that compromise AOP's applicability, especially in large projects where many developers, often including new team members, are involved in the process. This paper addresses the issues of aspect conflicts and interactions and proposes a technique to help compose aspects in a disciplined way using a test-driven development approach. A simple example for a banking system helps on illustrating the technique.},
address = {New York, NY, USA},
annote = {Disciplined composition of aspects using tests},
author = {Restivo, Andr{\'{e}} and Aguiar, Ademar},
booktitle = {LATE '08: Proceedings of the 2008 AOSD workshop on Linking aspect technology and evolution},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1--5},
publisher = {ACM},
title = {{Disciplined composition of aspects using tests}},
url = {citeulike-article-id:3934766 http://dx.doi.org/10.1145/1404953.1404961},
year = {2008}
}
@inproceedings{1377215,
abstract = {We study the impact of selected parameters on the size of the password space for "Draw-A-Secret" (DAS) graphical passwords. We examine the role of and relationships between the number of composite strokes, grid dimensions, and password length in the DAS password space. We show that a very significant proportion of the DAS password space depends on the assumption that users will choose long passwords with many composite strokes. If users choose passwords having 4 or fewer strokes, with passwords of length 12 or less on a 5 times; 5 grid, instead of up to the maximum 12 possible strokes, the size of the DAS password space is reduced from 58 to 40 bits. Additionally, we found a similar reduction when users choose no strokes of length 1. To strengthen security, we propose a technique and describe a representative system that may gain up to 16 more bits of security with an expected negligible increase in input time. Our results can be directly applied to determine secure design choices, graphical password parameter guidelines, and in deciding which parameters deserve focus in graphical password user studies.},
author = {Thorpe, J. and van Oorschot, P.C.},
booktitle = {20th Annual Computer Security Applications Conference},
doi = {10.1109/CSAC.2004.44},
isbn = {0-7695-2252-1},
issn = {1063-9527},
keywords = {Draw-A-Secret graphical passwords,secure design c},
pages = {50--60},
publisher = {Ieee},
title = {{Towards Secure Design Choices for Implementing Graphical Passwords}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1377215},
year = {2004}
}
@inproceedings{Tew2011,
address = {Dallas, TX, USA},
author = {Tew, Allison Elliott and Guzdial, Mark},
booktitle = {Proceedings of the 42Nd ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1953163.1953200},
isbn = {978-1-4503-0500-6},
keywords = {CS1,assessment,programming,validity},
pages = {111--116},
publisher = {ACM},
series = {SIGCSE '11},
title = {{The FCS1: A Language Independent Assessment of CS1 Knowledge}},
url = {http://doi.acm.org/10.1145/1953163.1953200},
year = {2011}
}
@inproceedings{Turner2005,
address = {Noosa Heads, Australia},
author = {Shull, Forrest and Turner, Richard},
booktitle = {ACM/IEEE International Symposium on Empirical Software Engineering (ISESE)},
pages = {133--140},
title = {{An Empirical Approach to Best Practice Identification and Selection: The US Department of Defense Acquisition Best Practices Clearinghouse}},
year = {2005}
}
@inproceedings{Lemos2007a,
abstract = {We present CodeGenie, a tool that implements a test-driven approach to search and reuse of code available on largescale code repositories. With CodeGenie, developers design test cases for a desired feature first, similar to Test-driven Development (TDD). However, instead of implementing the feature from scratch, CodeGenie automatically searches for an existing implementation based on information available in the tests. To check the suitability of the candidate results in the local context, each result is automatically woven into the developer's project and tested using the original tests. The developer can then reuse the most suitable result. Later, reused code can also be unwoven from the project as wished. For the code searching and wrapping facilities, CodeGenie relies on Sourcerer, an Internet-scale source code infrastructure that we have developed.},
annote = {CodeGenie: A tool for test-driven source code search},
author = {Lemos, O A L and Bajracharya, S and Ossher, J},
booktitle = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {917--918},
title = {{CodeGenie: A tool for test-driven source code search}},
url = {citeulike-article-id:3934691 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42149118451{\&}{\#}38 partnerID=40},
year = {2007}
}
@misc{Freese2003a,
abstract = {Test-Driven Development is a technique where each change to the observable behavior of a program is motivated by a failing test. High design quality is maintained by continuous small design improvements called refactorings. While some integrated development environments support automated refactoring, they do not handle problems that occur if refactorings are used in development teams or on published interfaces. This paper outlines the idea of a specialized software configuration management tool which integrates into a development environment to record the steps of Test-Driven Development as operations. These operations are useful for summarizing local changes, merging parallel changes and for migrating code that uses published interfaces. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
annote = {Towards software configuration management for test-driven development},
author = {Freese, T},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {267--273},
title = {{Towards software configuration management for test-driven development}},
url = {citeulike-article-id:3934612 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35248844490{\&}{\#}38 partnerID=40},
volume = {2649},
year = {2003}
}
@inproceedings{Felt2012a,
address = {New York, New York, USA},
author = {Felt, Adrienne Porter and Ha, Elizabeth and Egelman, Serge and Haney, Ariel and Chin, Erika and Wagner, David},
booktitle = {Proceedings of the Eighth Symposium on Usable Privacy and Security (SOUPS '12)},
doi = {10.1145/2335356.2335360},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Felt et al. - 2012 - Android permissions User attention, comprehension, and behavior.pdf:pdf},
isbn = {9781450315326},
keywords = {Android,agile,mobile phones,nsf,security,smartphones,usable security},
mendeley-tags = {agile,nsf,security},
month = {jul},
pages = {3:1--3:14},
publisher = {ACM Press},
title = {{Android permissions: User attention, comprehension, and behavior}},
url = {http://dl.acm.org/citation.cfm?id=2335356.2335360},
year = {2012}
}
@article{Perry,
author = {Perry, Dewayne E. and Staudenmayer, Nancy A. and Votta, Lawrence G.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perry, Staudenmayer, Votta - Unknown - People, Organizations, and Process Improvement.pdf:pdf},
journal = {IEEE Software},
keywords = {collaboration},
mendeley-tags = {collaboration},
number = {4},
pages = {36--45},
publisher = {Published by the IEEE Computer Society},
title = {{People, Organizations, and Process Improvement}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/52.300082},
volume = {11},
year = {1994}
}
@article{Johnson2007,
abstract = {Performance design and performance testing are necessarily different from functional test case design. A rigorous test-driven design methodology isn't practical for all performance measurement. A test-first approach to performance provides some advantages in a TDD environment. Experience with applying early performance testing in a TDD framework for a device-driver development project provides insight into the test-first approach. The results show a trend of performance improvement throughout the development life cycle, and better performance compared to an earlier release. Lessons learned include the benefit of having a performance architect on the development team and of tracking performance measurements throughout the development life cycle.This article is part of a special issue on test-driven development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Incorporating performance testing in test-driven development},
author = {Johnson, M J and Maximilien, E M and Ho, C W and Williams, L},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {67--73},
title = {{Incorporating performance testing in test-driven development}},
url = {citeulike-article-id:3934662 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248338367{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@article{Javed2004,
author = {Javed, Talha and Maqsood, Manzil e and Durrani, Qaiser S.},
doi = {10.1145/986710.986727},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Javed, Maqsood, Durrani - 2004 - A study to investigate the impact of requirements instability on software defects.pdf:pdf},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {change request (CR's) 1,defects,high/medium/low change requests,pre/post release changes,requirements change,severity-1/severity-2 defects},
month = {may},
number = {3},
pages = {1},
publisher = {ACM},
title = {{A study to investigate the impact of requirements instability on software defects}},
url = {http://dl.acm.org/citation.cfm?id=986710.986727},
volume = {29},
year = {2004}
}
@inproceedings{Layman2012,
abstract = {In this fast abstract, we provide preliminary findings from an analysis of 14,500 spacecraft anomalies from unmanned NASA missions. We provide some baselines for the distributions of software vs. non-software anomalies in spaceflight systems, the risk ratings of software anomalies, and the corrective actions associated with software anomalies.},
address = {Dallas, Texas, USA},
author = {Layman, Lucas and Zelkowitz, Marvin and Basili, Victor and Nikora, Allen P},
booktitle = {2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops},
doi = {10.1109/ISSREW.2012.49},
isbn = {978-1-4673-5048-8},
keywords = {anomalies,baseline,metrics,mypubs,nasa},
mendeley-tags = {mypubs},
month = {nov},
pages = {13--14},
publisher = {IEEE},
title = {{Toward Baselining Software Anomalies in NASA Missions}},
year = {2012}
}
@article{Kahneman1979,
abstract = {This paper presents a critique of expected utility theory as a descriptive model of decision making under risk, and develops an alternative model, called prospect theory. Choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory. In particular, people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty. This tendency, called the certainty effect, contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses. In addition, people generally discard components that are shared by all prospects under consideration. This tendency, called the isolation effect, leads to inconsistent preferences when the same choice is presented in different forms. An alternative theory of choice is developed, in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights. The value function is normally concave for gains, commonly convex for losses, and is generally steeper for losses than for gains. Decision weights are generally lower than the corresponding probabilities, except in the range of low probabilities. Overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling.},
author = {Kahneman, Daniel and Tversky, Amos},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kahneman, Tversky - 1979 - Prospect Theory An Analysis of Decision under Risk.pdf:pdf},
issn = {00129682, 14680262},
journal = {Econometrica},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {263--291},
publisher = {The Econometric Society},
title = {{Prospect Theory: An Analysis of Decision under Risk}},
url = {http://www.jstor.org/stable/1914185},
volume = {47},
year = {1979}
}
@inproceedings{LloydM.B.RossonJ.D.Arthur2002,
address = {Essen, Germany},
annote = {This paper presents a new technique for eliciting safety-requirements during the analysis phase so that they may be incorporated into development. The new technique is based on an existing technique (HAZOP), and employs a combination of use cases and scenarios. The integration of these three elements allows a requirements team to specify various failure scenarios and relate them to system-level design elements. This method provides an easily-understood way to generate safety requirements early in the product lifecycle, but lacks the completeness of a more formal specification. More work needs to be done before this method can be applied to complex failure scenarios that may involve multiple use cases.},
author = {{Lloyd  M. B. Rosson, J. D. Arthur}, W J},
keywords = {case study,distributed,elicitation,requirements},
pages = {311--318},
title = {{Effectiveness of Elicitation Techniques in Distributed Requirements Engineering}},
year = {2002}
}
@inproceedings{Yurcik2004,
address = {New York, New York, USA},
author = {Yurcik, William and Meng, Xin and Kiyanclar, Nadir},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029230},
isbn = {1581139748},
keywords = {NVisionCC,cluster security,high performance security,security situational awareness,security visualization},
month = {oct},
pages = {133},
publisher = {ACM Press},
title = {{NVisionCC}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029230},
year = {2004}
}
@inproceedings{Patterson2005,
abstract = {In this paper we present results related to achieving finegrained activity recognition for context-aware computing applications. We examine the advantages and challenges of reasoning with globally unique object instances detected by an RFID glove. We present a sequence of increasingly powerful probabilistic graphical models for activity recognition. We show the advantages of adding additional complexity and conclude with a model that can reason tractably about aggregated object instances and gracefully generalizes from object instances to their classes by using abstraction smoothing. We apply these models to data collected from a morning household routine.},
author = {Patterson, D.J. and Fox, D. and Kautz, H. and Philipose, M.},
booktitle = {Ninth IEEE International Symposium on Wearable Computers (ISWC'05)},
doi = {10.1109/ISWC.2005.22},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Patterson et al. - 2005 - Fine-Grained Activity Recognition by Aggregating Abstract Object Usage.pdf:pdf},
isbn = {0-7695-2419-2},
keywords = {Character recognition,Inference algorithms,Machine vision,Multimodal sensors,Object detection,Radiofrequency identification,Robustness,Sensor phenomena and characterization,Wearable computers,Wearable sensors,abstract object usage,abstraction smoothing,agile,computer vision,context-aware computing RFID glove,fine-grained activity recognition,inference mechanisms,mobile computing,nsf,planning (artificial intelligence),probabilistic graphical model},
mendeley-tags = {agile,nsf},
pages = {44--51},
publisher = {IEEE},
shorttitle = {Wearable Computers, 2005. Proceedings. Ninth IEEE},
title = {{Fine-Grained Activity Recognition by Aggregating Abstract Object Usage}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1550785},
year = {2005}
}
@article{Zelkowitz1998d,
author = {Zelkowitz, M V and Wallace, D R},
number = {5},
pages = {23--31},
title = {{Experimental Models for Validating Technology}},
volume = {31},
year = {1998}
}
@inproceedings{Simon2010,
abstract = {Is there consensus on what students should learn in CS2? Should they learn to use data structures, understand their specific implementation details, or both? Finally, has the computing education community's answer to the second question changed over time? In this paper, we begin to explore these questions based on an analysis of a key artifact instructors use to assess their students' performance: their final exams. Specifically, we look at two CS2 concepts as covered in those exams: stacks and hashtables. Our dataset includes 76 exams from 14 institutions around the world spanning 1973-2009 that were gathered as part of the DCER project, which is investigating the feasibility of a repository for computing education research data; to our knowledge this is a novel dataset in computing education. We begin by giving a general feel for this extensive dataset by describing the formats and dificulty level of the stack and hashtable questions and the computing skill students must possess to answer them. Next, we look at the questions' assessment of implementation knowledge versus interface or application knowledge. Despite a number of calls for modern CS2 to focus more on application than implementation, we found no evidence of such a trend. We note, however, that there are institutional difierences in the data, and that there are alternative ways in which application may be assessed in a course. Copyright 2010 ACM.},
address = {Aarhus, Denmark},
author = {Simon, Beth and Clancy, Mike and McCartney, Robert and Morrison, Briana and Richards, Brad and Sanders, Kate},
booktitle = {ICER'10 - Proceedings of the International Computing Education Research Workshop},
doi = {10.1145/1839594.1839612},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simon et al. - 2010 - Making sense of data structures exams.pdf:pdf},
isbn = {9781450301466},
keywords = {CS2,Data structures,Exams},
mendeley-tags = {CS2},
pages = {97--105},
publisher = {ACM Press},
title = {{Making sense of data structures exams}},
url = {http://portal.acm.org/citation.cfm?doid=1839594.1839612},
year = {2010}
}
@book{Stephanidis2007b,
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-540-73279-2},
editor = {Stephanidis, Constantine},
isbn = {978-3-540-73278-5},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Universal Acess in Human Computer Interaction. Coping with Diversity}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-73279-2},
volume = {4554},
year = {2007}
}
@article{faul2007g,
author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
journal = {Behavior research methods},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {175--191},
publisher = {Springer},
title = {{G* Power 3: A flexible statistical power analysis program for the social, behavioral, and biomedical sciences}},
volume = {39},
year = {2007}
}
@book{Jacobson1992,
address = {Reading, MA},
author = {Jacobson, I and Christerson, M and Jonsson, P and Overgaard, G},
publisher = {Addison-Wesley},
title = {{Object-Oriented Software Engineering: A Use-Case Driven Approach}},
year = {1992}
}
@article{Menzies2007,
abstract = {The value of using static code attributes to learn defect predictors has been widely debated. Prior work has explored issues like the merits of "McCabes versus Halstead versus lines of code counts" for generating defect predictors. We show here that such debates are irrelevant since how the attributes are used to build predictors is much more important than which particular attributes are used. Also, contrary to prior pessimism, we show that such defect predictors are demonstrably useful and, on the data studied here, yield predictors with a mean probability of detection of 71 percent and mean false alarms rates of 25 percent. These predictors would be useful for prioritizing a resource-bound exploration of code that has yet to be inspected},
author = {Menzies, Tim and Greenwald, Jeremy and Frank, Art},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {data mining,defect prediction},
mendeley-tags = {data mining,defect prediction},
month = {jan},
number = {1},
pages = {2--13},
title = {{Data Mining Static Code Attributes to Learn Defect Predictors}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4027145},
volume = {33},
year = {2007}
}
@inproceedings{Mugridge2003a,
address = {Salt Lake City, UT},
author = {Mugridge, R},
keywords = {TDD},
mendeley-tags = {TDD},
pages = {47--52},
title = {{Test Driven Development and the Scientific Method}},
year = {2003}
}
@inproceedings{Newman2010,
address = {Gold Coast, Australia},
author = {Newman, David and Noh, Youn and Talley, Edmund and Karimi, Sarvnaz and Baldwin, Timothy},
booktitle = {Proceedings of the 10th Annual Joint Conference on Digital Libraries (JCDL '10)},
doi = {10.1145/1816123.1816156},
isbn = {9781450300858},
keywords = {evaluation,topic models,topic quality,user studies},
month = {jun},
pages = {215--224},
publisher = {ACM Press},
title = {{Evaluating topic models for digital libraries}},
url = {http://dl.acm.org/citation.cfm?id=1816123.1816156},
year = {2010}
}
@article{Yenduri2006,
abstract = {In this paper, we conduct an experimental study over two groups of students comprising of undergraduate students (seniors) who develop software using the conventional way of performing unit testing after development and also by extracting test cases before implementation as in agile programming. Both groups developed the same software using an incremental and iterative approach. The results showed that the software had less number of faults when developed using agile programming. Also, the quality of software was better and the productivity increased},
annote = {Impact of using test-driven development: a case study},
author = {Yenduri, S and Perkins, L A},
isbn = {1932415904},
journal = {Proceedings of the 2006 International Conference on Software Engineering Research and Practice and Conference on Programming Languages and Compilers SERP'06},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Impact of using test-driven development: a case study}},
url = {citeulike-article-id:3934846 {\#}},
volume = {1},
year = {2006}
}
@misc{KasperskyLabGlobalResearchandAnalysisTeam2013,
author = {{Kaspersky Lab Global Research and Analysis Team}},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaspersky Lab Global Research and Analysis Team - 2013 - Kaspersky Security Bulletin 2013.pdf:pdf},
institution = {Kaspersky Lab},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {C},
title = {{Kaspersky Security Bulletin 2013}},
url = {http://media.kaspersky.com/pdf/KSB{\_}2013{\_}EN.pdf},
year = {2013}
}
@inproceedings{Layman2020,
address = {Tampa, FL, USA},
author = {Layman, Lucas and Song, Yang and Guinn, Curry},
booktitle = {Proceedings of the 2020 ACM Southeast Conference (ACMSE 2020)},
doi = {10.1145/3374135.3385277},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Layman, Song, Guinn - 2020 - Toward Predicting Success and Failure in CS2 A Mixed-Method Analysis.pdf:pdf},
pages = {8},
publisher = {ACM},
title = {{Toward Predicting Success and Failure in CS2 : A Mixed-Method Analysis}},
url = {https://arxiv.org/abs/2002.11813},
year = {2020}
}
@article{Workman2008,
author = {Workman, Michael},
doi = {10.1002/asi.20779},
issn = {15322882},
journal = {Journal of the American Society for Information Science and Technology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {feb},
number = {4},
pages = {662--674},
title = {{Wisecrackers: A theory-grounded investigation of phishing and pretext social engineering threats to information security}},
url = {http://doi.wiley.com/10.1002/asi.20779},
volume = {59},
year = {2008}
}
@inproceedings{5689537,
abstract = {In advanced network information system sciences, many methods are applied to determine cyclic distributions of complex 0-1 sequences, such as time sequence and Poincare map method. Appropriate visualization models and methods are useful to describe cyclic distributions of complex 0-1 sequences. Time sequences using variant logic recursions and convert into measure sequences are discussed. It is convenient to apply 3D representation model to visualize cyclic distributions of complex 0-1 sequence, and display the results of typical measure sequences. The main procedure includes three stages: data acquisition, measuring determination and conditional visualization. Finally the related distributions are discussed and the exhibited characteristic of the method are summarized.},
author = {Zhou, Yao and Zheng, J Z},
booktitle = {Information Theory and Information Security (ICITIS), 2010 IEEE International Conference on},
doi = {10.1109/ICITIS.2010.5689537},
keywords = {3D representation model,3D visualization model,adv},
pages = {539--543},
title = {{3D visualization model using cyclic distributions of complex 0 {\#}x2013;1 sequences}},
year = {2010}
}
@inproceedings{Jongeling2015,
address = {Bremen, Germany},
author = {Jongeling, Robbert and Datta, Subhajit and Serebrenik, Alexander},
booktitle = {2015 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
doi = {10.1109/ICSM.2015.7332508},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jongeling, Datta, Serebrenik - 2015 - Choosing your weapons On sentiment analysis tools for software engineering research.pdf:pdf},
isbn = {978-1-4673-7532-0},
keywords = {sentiment},
mendeley-tags = {sentiment},
month = {sep},
pages = {531--535},
publisher = {IEEE},
title = {{Choosing your weapons: On sentiment analysis tools for software engineering research}},
url = {http://ieeexplore.ieee.org/document/7332508/},
year = {2015}
}
@article{Myers1978,
author = {Myers, Glenford J},
journal = {Communications of the ACM},
keywords = {and phrases,code,code inspections,debugging,personnel selection,program verification,software reliability,testing,walkthroughs},
number = {9},
pages = {760--768},
title = {{Experiment in Program Testing and Code Walkthroughs / Inspections}},
volume = {21},
year = {1978}
}
@inproceedings{5497999,
abstract = {The high computational power of graphics processing units (GPU) is used for several purposes nowadays. Factoring integers, computing discrete logarithms, and pattern matching in network intrusion detection systems (IDS) are popular tasks in the field of information security where GPUs are used for acceleration. GPUs are commodity components and are widely available in computer systems which would make them an ideal platform for a wide-spread IDS. We investigate the feasibility to use current GPUs for asynchronous host intrusion detection as proposed in a former work and come to the conclusion that several constraints of GPUs limit the use for concurrent and asynchronous off-CPU processing in host IDSs. GPUs have restrictions in terms of continuity, asynchronism, and unrestricted access to perform this task. We propose an observation mechanism and discuss current constraints on autonomous use of standard GPU components for intrusion detection. Finally, we come to the conclusion that several modifications to graphics cards are necessary to enable our approach.},
author = {Riedmu{\&}{\#}x0308 andller, R and Seeger, M M and Baier, H and Busch, C and Wolthusen, S D},
booktitle = {Security and Communication Networks (IWSCN), 2010 2nd International Workshop on},
doi = {10.1109/IWSCN.2010.5497999},
keywords = {asynchronous host intrusion detection,graphics car},
month = {may},
pages = {1--8},
title = {{Constraints on autonomous use of standard GPU components for asynchronous observations and intrusion detection}},
year = {2010}
}
@book{Jackson1991,
address = {New York},
author = {Jackson, J Edward},
publisher = {Wiley},
title = {{A User's Guide to Principal Components}},
year = {1991}
}
@article{thornbury1981tactile,
author = {Thornbury, Julia M and Mistretta, Charlotte M},
journal = {Journal of Gerontology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {1},
pages = {34--39},
publisher = {Oxford University Press},
title = {{Tactile sensitivity as a function of age}},
volume = {36},
year = {1981}
}
@inproceedings{Stolee2010,
address = {New York, New York, USA},
author = {Stolee, Kathryn T. and Elbaum, Sebastian},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement - ESEM '10},
doi = {10.1145/1852786.1852832},
isbn = {9781450300391},
keywords = {crowdsourcing,empirical studies},
month = {sep},
pages = {1},
publisher = {ACM Press},
title = {{Exploring the use of crowdsourcing to support empirical studies in software engineering}},
url = {http://dl.acm.org/citation.cfm?id=1852786.1852832},
year = {2010}
}
@inproceedings{Rendell2008,
abstract = {Test Driven Development has long been a key tool in the agile toolbox. Often it is suggested that the technique has moved into the mainstream and that not applying a test first approach is exceptional. Recent coverage in the community has even started to describe a post-TDD approach. Having worked with TDD for the last five years with varying degrees of rigor and success I have observed that far from being ubiquitous, effective application of TDD is uncommon. This paper takes a pragmatic approach in evaluating the implementation of, impediments against and measurable benefits of TDD on a large, commercially successful, project. Analysis of this experience will show how and why TDD is being used incorrectly and how this situation can be corrected. The analysis will show how project delivery improved when a more effective approach was applied. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Effective and pragmatic test driven development},
author = {Rendell, A},
booktitle = {Proceedings - Agile 2008 Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {298--303},
title = {{Effective and pragmatic test driven development}},
url = {citeulike-article-id:3934765 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-52949140263{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{1250415,
abstract = {The Internet pervades many aspects of our lives and is becoming indispensable to critical functions in areas such as commerce, government, production and general information dissemination. To maintain the stability and efficiency of the Internet, every effort must be made to protect it against various forms of attacks, malicious users, and errors. A key component in the Internet security effort is the routine examination of Internet routing data, which unfortunately can be too large and complicated to browse directly. We have developed an interactive visualization process which proves to be very effective for the analysis of Internet routing data. In this application paper, we show how each step in the visualization process helps direct the analysis and glean insights from the data. These insights include the discovery of patterns, detection of faults and abnormal events, understanding of event correlations, formation of causation hypotheses, and classification of anomalies. We also discuss lessons learned in our visual analysis study.},
author = {Teoh, Soon Tee and Ma, Kwan-Liu and Wu, S.F. F and Teoh, Soon Tee and Ma, Kwan-Liu and Wu, S.F. F},
booktitle = {IEEE Transactions on Ultrasonics, Ferroelectrics and Frequency Control},
doi = {10.1109/VISUAL.2003.1250415},
isbn = {0-7803-8120-3},
keywords = {I,Internet routing data analysis,Internet security,homeland security,information visualization,internet stability,network visualization,text visualization},
month = {oct},
pages = {523--530},
publisher = {IEEE},
title = {{A visual exploration process for the analysis of Internet routing data}},
url = {http://dl.acm.org/citation.cfm?id=1081432.1081513},
year = {2003}
}
@article{Mottu2012,
author = {Mottu, Jean-Marie and Sen, Sagar and Tisi, Massimo and Cabot, Jordi},
doi = {10.1109/ISSRE.2012.7},
isbn = {978-1-4673-4638-2},
journal = {2012 IEEE 23rd International Symposium on Software Reliability Engineering},
keywords = {-white box testing,alloy,automatic model com-,model transformation testing,model-driven engineering,mutation analysis,pletion},
month = {nov},
pages = {291--300},
publisher = {Ieee},
title = {{Static Analysis of Model Transformations for Effective Test Generation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6405377},
year = {2012}
}
@techreport{NIST,
address = {NIST Special Publication 800-22rev1a},
author = {NIST},
institution = {NIST},
keywords = {mypubs},
mendeley-tags = {mypubs},
title = {{A Statistical Test Suite for Random and Pseudorandom Number Generators for Cryptographic Applications}},
year = {2010}
}
@inproceedings{Layman2008f,
address = {Kaiserslautern, Germany},
author = {Layman, Lucas and Kudrjavets, Gunnar and Nagappan, Nachiappan},
booktitle = {Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement - ESEM '08},
doi = {10.1145/1414004.1414038},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Layman, Kudrjavets, Nagappan - 2008 - Iterative identification of fault-prone binaries using in-process metrics.pdf:pdf},
isbn = {9781595939715},
keywords = {churn,code churn,fault prediction,mypubs,regression,statistical models},
mendeley-tags = {churn,mypubs},
month = {oct},
pages = {206--212},
publisher = {ACM Press},
title = {{Iterative identification of fault-prone binaries using in-process metrics}},
url = {http://portal.acm.org/citation.cfm?doid=1414004.1414038},
year = {2008}
}
@article{Stewart2004,
abstract = {In 1999, Schreck extended Gottfredson and Hirschi's (1990) self-control theory to explain victimization and the victim-offender overlap. His analysis of college students revealed that low self-control was significantly associated with victimization. We build on Shreck's work by assessing whether low self-control contributes to victimization among a sample of female offenders while controlling for routine activities/lifestyle behaviors. We advanced two research questions: (1) Can self-control theory account for variations in victimization? (2) Do risky lifestyle behaviors mediate the effects of self-control? The results were consistent with the hypothesis that low self-control is a risk factor for victimization, even after lifestyle behaviors were controlled.
In 1999, Schreck extended Gottfredson and Hirschi's (1990) self-control theory to explain victimization and the victim-offender overlap. His analysis of college students revealed that low self-control was significantly associated with victimization. We build on Shreck's work by assessing whether low self-control contributes to victimization among a sample of female offenders while controlling for routine activities/lifestyle behaviors. We advanced two research questions: (1) Can self-control theory account for variations in victimization? (2) Do risky lifestyle behaviors mediate the effects of self-control? The results were consistent with the hypothesis that low self-control is a risk factor for victimization, even after lifestyle behaviors were controlled.},
author = {Stewart, Eric A. and Elifson, Kirk W. and Sterk, Claire E.},
doi = {10.1080/07418820400095771},
issn = {0741-8825},
journal = {Justice Quarterly},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {mar},
number = {1},
pages = {159--181},
publisher = {Routledge},
title = {{Integrating the general theory of crime into an explanation of violent victimization among female offenders}},
url = {http://dx.doi.org/10.1080/07418820400095771},
volume = {21},
year = {2004}
}
@inproceedings{Conti2004a,
address = {New York, New York, USA},
author = {Conti, Gregory and Abdullah, Kulsoom},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029216},
isbn = {1581139748},
keywords = {application fingerprinting,information visualization,network attack visualization,operating system fingerprinting,passive fingerprinting,visual fingerprinting},
month = {oct},
pages = {45},
publisher = {ACM Press},
title = {{Passive visual fingerprinting of network attack tools}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029216},
year = {2004}
}
@inproceedings{Manhart2004,
abstract = {A software engineering department in a Daimler-Chrysler business unit was highly professional at developing embedded software for busses and coaches. However, customer specific add-ons were a regular source of hassle. Simple as they are, those individual requirements have to be implemented in hours or days rather than weeks or months. Poor quality or late upload into the bus hardware would cause serious cost and overhead. Established software engineering methods were considered inadequate and needed to be cut short. Agile methods offer guidance when quality, flexibility and high speed need to be reconciled. However, we did not adopt any full agile method, but added single agile practices to our "process improvement toolbox". We suggested a number of classical process improvement activities (such as more systematic documentation and measurement) and combined them with agile elements (e.g. Test First Process). This combination seemed to foster acceptance of agile ideas and may help us to break the ice for a cautious extension of agile process improvement.},
annote = {Breaking the ice for agile development of embedded software: An industry experience report},
author = {Manhart, P and Schneider, K},
booktitle = {Proceedings - International Conference on Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {378--386},
title = {{Breaking the ice for agile development of embedded software: An industry experience report}},
url = {citeulike-article-id:3934703 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-4544343290{\&}{\#}38 partnerID=40},
volume = {26},
year = {2004}
}
@inproceedings{Layman2007b,
abstract = {The longer a fault remains in the code from the time it was injected, the more time it will take to fix the fault. Increasingly, automated fault detection (AFD) tools are providing developers with prompt feedback on recently-introduced faults to reduce fault fix time. If however, the frequency and content of this feedback does not match the developer's goals and/or workflow, the developer may ignore the information. We conducted a controlled study with 18 developers to explore what factors are used by developers to decide whether or not to address a fault when notified of the error. The findings of our study lead to several conjectures about the design of AFD tools to effectively notify developers of faults in the coding phase. The AFD tools should present fault information that is relevant to the primary programming task with accurate and precise descriptions. The fault severity and the specific timing of fault notification should be customizable. Finally, the AFD tool must be accurate and reliable to build trust with the developer.},
address = {Madrid, Spain},
author = {Layman, Lucas and Williams, Laurie and {St. Amant}, Robert},
booktitle = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
doi = {10.1109/ESEM.2007.11},
isbn = {978-0-7695-2886-1},
issn = {1938-6451},
keywords = {AFD tools,Automatic control,Computer science,Error correction,Fault detection,Feedback,Frequency,Software engineering,Software measurement,Time measurement,Timing,automated fault detection tools,developer behavior,fault diagnosis,fault fix time,fault information,fault notification,fault severity,mypubs,software tools},
mendeley-tags = {mypubs},
month = {sep},
pages = {176--185},
publisher = {IEEE},
shorttitle = {Empirical Software Engineering and Measurement, 20},
title = {{Toward Reducing Fault Fix Time: Understanding Developer Behavior for the Design of Automated Fault Detection Tools}},
year = {2007}
}
@misc{MarchesiG.SucciD.WellsandL.Williams2002,
address = {Boston},
author = {{Marchesi  G. Succi, D. Wells and L. Williams}, M and Beck, Kent},
publisher = {Addison Wesley},
title = {{Extreme Programming Perspectives}},
year = {2002}
}
@article{Schneider2000,
abstract = {A precise characterization is given for the class of security policies enforceable with mechanisms that work by monitoring system execution, and automata are introduced for specifying exactly that class of security policies. Techniques to enforce security policies specified by such automata are also discussed. {\textcopyright} 2000, ACM. All rights reserved.},
author = {Schneider, Fred B.},
doi = {10.1145/353323.353382},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/353323.353382.pdf:pdf},
issn = {15577406},
journal = {ACM Transactions on Information and System Security},
keywords = {EM security policies,SASI,Security,inlined reference monitors,proof carrying code,safety properties,security automata,security policies},
month = {feb},
number = {1},
pages = {30--50},
title = {{Enforceable Security Policies}},
url = {https://dl.acm.org/doi/10.1145/353323.353382},
volume = {3},
year = {2000}
}
@inproceedings{Martie2012,
address = {Zurich, Switzerland},
author = {Martie, Lee and Palepu, Vijay Krishna and Sajnani, Hitesh and Lopes, Cristina},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
isbn = {978-1-4673-1761-0},
keywords = {Android,bug logs,statistical trend analysis,topics},
month = {jun},
pages = {120--123},
publisher = {IEEE Press},
title = {{Trendy bugs: topic trends in the Android bug reports}},
url = {http://dl.acm.org/citation.cfm?id=2664446.2664464},
year = {2012}
}
@misc{Hummel2007a,
abstract = {Agile development and software reuse are both recognized as effective ways of improving time to market and quality in software engineering. However, they have traditionally been viewed as mutually exclusive technologies which are difficult if not impossible to use together. In this paper we show that, far from being incompatible, agile development and software reuse can be made to work together and, in fact, complement each other. The key is to tightly integrate reuse into the test-driven development cycles of agile methods and to use test cases - the agile measure of semantic acceptability - to influence the component search process. In this paper we discuss the issues involved in doing this in association with Extreme Programming, the most widely known agile development method, and Extreme Harvesting, a prototype technique for the test-driven harvesting of components from the Web. When combined in the appropriate way we believe they provide a good foundation for the fledgling concept of agile reuse. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Supporting agile reuse through extreme harvesting},
author = {Hummel, O and Atkinson, C},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {28--37},
title = {{Supporting agile reuse through extreme harvesting}},
url = {citeulike-article-id:3934648 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38149140937{\&}{\#}38 partnerID=40},
volume = {4536 LNCS},
year = {2007}
}
@inproceedings{DeSouza2004,
address = {New York, New York, USA},
author = {de Souza, Cleidson R. B. and Redmiles, David and Cheng, Li-Te and Millen, David and Patterson, John},
booktitle = {Proceedings of the 2004 ACM conference on Computer supported cooperative work - CSCW '04},
doi = {10.1145/1031607.1031620},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Souza et al. - 2004 - Sometimes You Need to See Through Walls — A Field Study of Application Programming Interfaces.pdf:pdf},
isbn = {1581138105},
keywords = {collaboration},
mendeley-tags = {collaboration},
number = {3},
pages = {63--71},
publisher = {ACM Press},
title = {{Sometimes You Need to See Through Walls — A Field Study of Application Programming Interfaces}},
url = {http://portal.acm.org/citation.cfm?doid=1031607.1031620},
volume = {6},
year = {2004}
}
@inproceedings{Sprenkle2005a,
address = {New York, New York, USA},
author = {Sprenkle, Sara and Gibson, Emily and Sampath, Sreedevi and Pollock, Lori},
booktitle = {Proceedings of the 20th IEEE/ACM international Conference on Automated software engineering - ASE '05},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sprenkle et al. - 2005 - Automated replay and failure detection for web applications.pdf:pdf},
keywords = {replay,software testing,test oracles,web applications},
month = {nov},
pages = {253},
publisher = {ACM Press},
title = {{Automated replay and failure detection for web applications}},
url = {http://dl.acm.org/citation.cfm?id=1101908.1101947},
year = {2005}
}
@article{Koulouri2014,
abstract = {2014 Copyright is held by the owner/author(s). Teaching programming to beginners is a complex task. In this article, the effects of three factors - choice of programming language, problem-solving training, and the use of formative assessment - on learning to program were investigated. The study adopted an iterative methodological approach carried out across 4 consecutive years. To evaluate the effects of each factor (implemented as a single change in each iteration) on students' learning performance, the study used quantitative, objective metrics. The findings revealed that using a syntactically simple language (Python) instead of a more complex one (Java) facilitated students' learning of programming concepts. Moreover, teaching problem solving before programming yielded significant improvements in student performance. These two factors were found to have variable effects on the acquisition of basic programming concepts. Finally, it was observed that effective formative feedback in the context of introductory programming depends on multiple parameters. The article discusses the implications of these findings, identifies avenues for further research, and argues for the importance of studies in computer science education anchored on sound research methodologies to produce generalizable results.},
author = {Koulouri, Theodora and Lauria, Stanislao and Macredie, Robert D.},
doi = {10.1145/2662412},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koulouri, Lauria, Macredie - 2014 - Teaching introductory programming A quantitative evaluation of different approaches.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {CS1,CS2,Curriculum,Empirical studies,Experimentation,Formative feedback,Human factors,K.3.2 [computers and education]: computer and info,Learning programming,Measurement,Novice programmers,Problem solving,Programming languages,Teaching strategies},
mendeley-tags = {CS2},
month = {dec},
number = {4},
pages = {Article 26},
publisher = {Association for Computing Machinery},
title = {{Teaching introductory programming: A quantitative evaluation of different approaches}},
volume = {14},
year = {2014}
}
@inproceedings{Mani2014,
address = {Hong Kong},
author = {Mani, Senthil and Sankaranarayanan, Karthik and Sinha, Vibha Singhal and Devanbu, Premkumar},
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering - FSE 2014},
doi = {10.1145/2635868.2635897},
isbn = {9781450330565},
keywords = {Mining Software Repositories,Requirements,Text Clustering},
month = {nov},
pages = {678--688},
publisher = {ACM Press},
title = {{Panning requirement nuggets in stream of software maintenance tickets}},
url = {http://dl.acm.org/citation.cfm?id=2635868.2635897},
year = {2014}
}
@article{Yarosh2008,
author = {Yarosh, Svetlana and Guzdial, Mark},
doi = {10.1145/1316450.1316456},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yarosh, Guzdial - 2008 - Narrating data structures.pdf:pdf},
issn = {15314278},
journal = {Journal on Educational Resources in Computing},
keywords = {CS1/2,CS2,Course design,media computation},
mendeley-tags = {CS2},
month = {jan},
number = {4},
pages = {1--20},
publisher = {ACM},
title = {{Narrating data structures}},
url = {http://portal.acm.org/citation.cfm?doid=1316450.1316456},
volume = {7},
year = {2008}
}
@inproceedings{Sooriamurthi2009,
abstract = {This paper discusses a learning exercise we use in our beginning programming classes to introduce students to the concepts of abstraction and decomposition. The assignment is to write a perpetual calendar generation program: given a month and a year ...},
author = {Sooriamurthi, Raja},
booktitle = {Proceedings of the Conference on Integrating Technology into Computer Science Education, ITiCSE},
doi = {10.1145/1562877.1562939},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sooriamurthi - 2009 - Introducing abstraction and decomposition to novice programmers.pdf:pdf},
isbn = {9781605583815},
issn = {00978418},
keywords = {CS2,Cs1/cs2,Programming case study},
mendeley-tags = {CS2},
month = {aug},
number = {3},
pages = {196--200},
publisher = {ACM},
title = {{Introducing abstraction and decomposition to novice programmers}},
url = {http://portal.acm.org/citation.cfm?doid=1595496.1562939},
volume = {41},
year = {2009}
}
@article{Park2013,
abstract = {This study examined the factors affecting the South Korean people's use of smartphones within the framework of the technology acceptance model (TAM). Using an in-person survey (N=852), the study confirmed the propositions of the TAM. The study also included individuals' psychological antecedents, such as motivations for social inclusion and instrumental use of smartphones, innovativeness, behavioral activation system (BAS), and locus of control. While the motivations and innovativeness verified previous studies' findings, BAS and locus of control demonstrated their unique contributions to explaining smartphone use. Smartphone dependency was also affected by the antecedents in the use of smartphones.},
author = {Park, Namkee and Kim, Yong-Chan and Shon, Hae Young and Shim, Hongjin},
doi = {10.1016/j.chb.2013.02.008},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Media dependency,Smartphones,South Korea,Technology acceptance model,Uses and gratifications,agile,nsf},
mendeley-tags = {agile,nsf},
month = {jul},
number = {4},
pages = {1763--1770},
title = {{Factors influencing smartphone use and dependency in South Korea}},
url = {http://www.sciencedirect.com/science/article/pii/S0747563213000745},
volume = {29},
year = {2013}
}
@article{DavidJohnson,
author = {David, F N and Johnson, N L},
journal = {Biometrika},
number = {1-2},
pages = {182--190},
title = {{The probability integral transformation when parameters are estimated from the sample}},
volume = {35},
year = {1948}
}
@techreport{CMU2005,
author = {Peret, S and Narasimham, P},
institution = {Carnegie Mellon University},
number = {CMU-PDL-05-109},
title = {{Causes of Failure in Web applications}},
year = {2005}
}
@incollection{Holzinger2007,
author = {Holzinger, Andreas and Searle, Gig and Nischelwitzer, Alexander},
booktitle = {Universal Access in Human Computer Interaction. Coping with Diversity},
doi = {10.1007/978-3-540-73279-2_103},
editor = {Stephanidis, Constantine},
isbn = {978-3-540-73278-5},
keywords = {Mobile Interfaces,Usability,User-Centered Design,agile,nsf},
mendeley-tags = {agile,nsf},
pages = {923--932},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{On Some Aspects of Improving Mobile Applications for the Elderly}},
url = {http://dx.doi.org/10.1007/978-3-540-73279-2{\_}103},
volume = {4554},
year = {2007}
}
@article{Wilson2015,
abstract = {ObjectivesTest whether the presence of a surveillance message on an attacked computer system influences system trespassers' active engagement with the compromised system (i.e., entering computer commands). The hypothesized restrictive deterrent effect is tested both in the context of a first system trespassing incident and in the progression of repeated trespassing incidents in an attacked computer system. MethodsWe designed a randomized controlled trial and deployed a series of virtual target computers with known vulnerabilities into the computer network of a large public university in the United States. The target computers were set to either display or not display a surveillance banner once system trespassers infiltrated them. ResultsWe find that the presence of a surveillance banner in the attacked computer systems reduced the probability of commands being typed in the system during longer first system trespassing incidents. Further, we find that the probability of commands being typed during subsequent system trespassing incidents (on the same target computer) is conditioned by the presence of a surveillance banner and by whether commands have been entered during previous trespassing incidents. ConclusionsThese findings offer modest support for the application of restrictive deterrence in the study of system trespassing.},
author = {Wilson, Theodore and Maimon, David and Sobesto, Bertrand and Cukier, Michel},
doi = {10.1177/0022427815587761},
issn = {0022-4278},
journal = {Journal of Research in Crime and Delinquency},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jun},
number = {6},
pages = {829--855},
title = {{The Effect of a Surveillance Banner in an Attacked Computer System: Additional Evidence for the Relevance of Restrictive Deterrence in Cyberspace}},
url = {http://jrc.sagepub.com/content/early/2015/06/12/0022427815587761.abstract},
volume = {52},
year = {2015}
}
@article{Grenning2001a,
author = {Grenning, James},
doi = {10.1109/52.965799},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grenning - 2001 - Launching extreme programming at a process-intensive company.pdf:pdf},
issn = {07407459},
journal = {IEEE Software},
keywords = {XP,agile},
mendeley-tags = {agile},
number = {6},
pages = {27--33},
title = {{Launching extreme programming at a process-intensive company}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=965799},
volume = {18},
year = {2001}
}
@article{1607924,
abstract = { The time-based network traffic visualizer combines low-level, textual detail with multiple visualizations of the larger context to help users construct a security event's big picture. TNV is a visualization tool grounded in an under standing of the work practices of security analysts. We designed it to support ID analysis by giving analysts a visual display that facilitates pattern and anomaly recognition, particularly overtime. It also offers more focused views on packet-level detail in the context of the surrounding network traffic.},
author = {Goodall, J R and Lutters, W G and Rheingans, P and Komlodi, A},
doi = {10.1109/MCG.2006.31},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {Computer-Assisted,Software,User-Computer Interfac,anomaly recognition,network traffic analysis,pat},
number = {2},
pages = {72--80},
title = {{Focusing on context in network traffic analysis}},
volume = {26},
year = {2006}
}
@article{Clarke2012,
abstract = {CONTEXT An optimal software development process is regarded as being dependent on the situational characteristics of individual software development settings. Such characteristics include the nature of the application(s) under development, team size, requirements volatility and personnel experience. However, no comprehensive reference framework of the situational factors affecting the software development process is presently available. OBJECTIVE The absence of such a comprehensive reference framework of the situational factors affecting the software development process is problematic not just because it inhibits our ability to optimise the software development process, but perhaps more importantly, because it potentially undermines our capacity to ascertain the key constraints and characteristics of a software development setting. METHOD To address this deficiency, we have consolidated a substantial body of related research into an initial reference framework of the situational factors affecting the software development process. To support the data consolidation, we have applied rigorous data coding techniques from Grounded Theory and we believe that the resulting framework represents an important contribution to the software engineering field of knowledge. RESULTS The resulting reference framework of situational factors consists of eight classifications and 44 factors that inform the software process. We believe that the situational factor reference framework presented herein represents a sound initial reference framework for the key situational elements affecting the software process definition. CONCLUSION In addition to providing a useful reference listing for the research community and for committees engaged in the development of standards, the reference framework also provides support for practitioners who are challenged with defining and maintaining software development processes. Furthermore, this framework can be used to develop a profile of the situational characteristics of a software development setting, which in turn provides a sound foundation for software development process definition and optimisation.},
author = {Clarke, Paul and O'Connor, Rory V.},
doi = {10.1016/j.infsof.2011.12.003},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clarke, O'Connor - 2012 - The situational factors that affect the software development process Towards a comprehensive reference framewo.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Process definition,Process implementation and change,Software engineering process},
month = {may},
number = {5},
pages = {433--447},
title = {{The situational factors that affect the software development process: Towards a comprehensive reference framework}},
url = {http://www.sciencedirect.com/science/article/pii/S0950584911002369},
volume = {54},
year = {2012}
}
@misc{Madeyski2007,
abstract = {Test-driven development (TDD) is entering the mainstream of software development. We examined the software development process for the purpose of evaluation of the TDD impact, with respect to software development productivity, in the context of a web based system development. The design of the study is based on Goal-Question-Metric approach, and may be easily replicated in different industrial contexts where the number of subjects involved in the study is limited. The study reveals that TDD may have positive impact on software development productivity. Moreover, TDD is characterized by the higher ratio of active development time (described as typing and producing code) in total development time than test-last development approach. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {The impact of test-driven development on software development productivity - An empirical study},
author = {Madeyski, L and Sza?a},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {200--211},
title = {{The impact of test-driven development on software development productivity - An empirical study}},
url = {citeulike-article-id:3934701 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38349001334{\&}{\#}38 partnerID=40},
volume = {4764 LNCS},
year = {2007}
}
@inproceedings{6120943,
abstract = {Visualizing computational processes of optimization passes helps to reason about, and to gain insight into, the inner workings of the optimization passes. In this paper, we visualize the computational processes of two procedural abstraction passes. We modified two procedural abstraction post pass optimizers to visualize for each the difference in machine code before and after optimization by drawing abstracted fragments in the original code. We then explain how the generated visualizations aid in better understanding the optimization passes and eventually improve them.},
author = {Schaeckeler, S and Jayadevaprakash, N},
booktitle = {Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on},
doi = {10.1109/TrustCom.2011.150},
keywords = {computational process visualization,machine code,p},
pages = {1099--1108},
title = {{Visualization of Computational Processes of Procedural Abstraction Optimization Passes}},
year = {2011}
}
@book{Augustine1991,
address = {Oxford},
author = {Augustine},
editor = {Chadwick, Henry},
isbn = {0192817744},
pages = {230},
publisher = {Oxford University Press},
title = {{Confessions: A new translation by Henry Chadwick}},
year = {1991}
}
@inproceedings{Reifer2002,
address = {Chicago, IL},
author = {Reifer, D J},
pages = {185--196},
title = {{How to Get the Most out of Extreme Programming/Agile Methods}},
year = {2002}
}
@inproceedings{Do2015,
address = {New York, New York, USA},
author = {Do, Trinh Minh Tri and Blom, Jan and Gatica-Perez, Daniel},
booktitle = {Proceedings of the 13th International Conference on Multimodal Interfaces - ICMI '11},
doi = {10.1145/2070481.2070550},
isbn = {9781450306416},
keywords = {agile,large-scale study,location context,nsf,phone usage,smartphone,social context},
mendeley-tags = {agile,nsf},
month = {nov},
pages = {353--360},
publisher = {ACM Press},
title = {{Smartphone usage in the wild}},
url = {http://dl.acm.org/citation.cfm?id=2070481.2070550},
year = {2011}
}
@inproceedings{Garg2006,
abstract = {Traffic volume and diversity can have a significant impact on the ability of network intrusion detection systems (NIDS) to report malicious activity accurately. Based on the observation that a great deal of traffic is, in fact, not important to accurate attack identification, we investigate connection filtering as a method for improving the performance of NIDS. We describe three different classes of connection filters that were developed to explore the design space and trade off's in load reduction versus alarm rates. We implement instances of each filter class on a network processor that can be used with any NIDS that runs on commodity hardware, and evaluate the impact of each filter in a series of laboratory-based tests. First, we establish an idealized maximum performance by using static connection filters for all benign traffic. Next, we show that volume sensitive random connection filters can improve performance significantly with respect to alarm rates under heavy traffic load. Finally, we show that dynamic connection filters that attempt to infer benign traffic can improve performance almost to the level of idealized static filters. These results underscore the potential for hardware-based connection filtering as an effective means for improving the performance of NIDS. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Improving NIDS performance through hardware-based connection filtering},
author = {Garg, V and Yegneswaran, V and Barford, P},
booktitle = {IEEE International Conference on Communications},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {2183--2188},
title = {{Improving NIDS performance through hardware-based connection filtering}},
url = {citeulike-article-id:3934615 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42549136505{\&}{\#}38 partnerID=40},
volume = {5},
year = {2006}
}
@article{Gnesi2004,
author = {Gnesi, Stefania and Latella, Diego and Massink, Mieke and Isti, C N R and Moruzzi, Via and Pisa, I},
title = {{Formal Test-case Generation for UML Statecharts}},
year = {2004}
}
@incollection{Schalliol2002,
address = {Indianapolis, IN},
author = {Schalliol, Gregory},
booktitle = {EXtreme Programming Perspectives, Addison-Wesley},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schalliol - 2002 - Challenges for Analysts on a Large XP Project.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
pages = {375--386},
publisher = {Pearson Education},
title = {{Challenges for Analysts on a Large XP Project}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.6596{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@article{Andrea2007,
abstract = {An explanation of test-driven development often begins by describing the red-green-refactor cycle. This slogan is so catchy and the description so simple that practitioners and tool developers tend to focus only on this localized cycle. Experience has shown that a successful functional test-driven development strategy must span the entire application life cycle and must be supported by effective tools. This article is a call to improve the state of the art of functional test-driven development by reflecting on the state of the art, describing processes and phases in the full application life cycle, and painting a vision for the next generation of functional testing tools.This article is part of a special issue on test-driven development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Envisioning next-generation functional testing tools},
author = {Andrea, J},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {58--66},
title = {{Envisioning next-generation functional testing tools}},
url = {citeulike-article-id:3934542 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248368554{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@inproceedings{Harrison2010,
address = {New York, New York, USA},
author = {Harrison, Lane and Hu, Xianlin and Ying, Xiaowei and Lu, Aidong and Wang, Weichao and Wu, Xintao},
booktitle = {Proceedings of the Seventh International Symposium on Visualization for Cyber Security - VizSec '10},
doi = {10.1145/1850795.1850806},
isbn = {9781450300131},
keywords = {Sybil attacks,coordinated multiple views,intrusion detection,security visualization,spectral analysis},
month = {sep},
pages = {91--101},
publisher = {ACM Press},
title = {{Interactive detection of network anomalies via coordinated multiple views}},
url = {http://dl.acm.org/citation.cfm?id=1850795.1850806},
year = {2010}
}
@inproceedings{Forbes2007,
abstract = {Our field continues to be blessed (and plagued) with continual curriculum change, from languages to techniques (objects first) to perspectives (sage on the stage vs. guide on the side). Particular emphasis has been spent crafting and re-crafting our introductory curricula [1]. This makes sense, since that not only defines the foundation upon which our upper-division courses arebased, but is exactly where we attract (or lose) our best students who had not considered majoring in computer science. With enrollments declining, retaining our fence-sitting prospective majors takes on that much more importance. When considering curriculum change at the introductory level, it often helps to look around at successful programs to see what they do. We surveyed the schools whose computer science Ph.D. programs were listed as the top 30 by the 2007 U.S. News World Report ranking [1].1 While other surveys have focused on departments, salaries and degree production 0, ours tried to capture the important aspects of each institution's lower-division curriculum. First, we looked at the material on each department's web page and course pages when accessible. We followed that initial sweep by asking representative faculty at each institution to report on the following questions, divided into seven major categories: Institution Is the institution on quarters or semesters? Are the classes taught every semester? In the summer? What are the introductory, lower-division courses, and how do they map (if at all) to the standard CS0, CS1, CS2, etc? Is a literacy course available for students not interested in programming? What is the flexibility in the lower-division sequence? Are the courses part of a common-first-year? Is ethics taught in the introductory sequence? Is there a survey course available? Staff What staff is required to teach each course (faculty, teaching assistants, readers, lab assistants)? Are the instructors research faculty, teaching faculty, or graduate students? Are the courses taught by the same person every semester? Who teaches discussion sections / recitations? Labs? Demographics What are recent enrollment numbers? How much have those numbers dropped (if at all) recently? What are the drop / withdraw / failure rates? What is the demographic of the student body? Are any non-majors required to take the courses? What is the typical grade histogram for the course? Content What versions of what languages are taught? What are the textbooks used? Is there a feeling that the course is fresh or stale? When was the last major course facelift? What works really well and what is broken? What overhauls (if any) are planned for the courses? Delivery How many contact hours are there and what is the breakdown into lecture / lab / discussion? How are the labs run? (instructor-led vs. problem-driven)? Style What programming paradigms are covered? Is the first introduction to OOP an objects-first approach? Is there any pair programming (or other XP techniques)? Meta Is there an institutional or departmental grading guideline, e.g., that the average GPA needs to fall in a certain range, or that there should be a given distribution of As, Bs, etc.? What are the innovative techniques being deployed, e.g., multimedia data as first-class objects, a collaborative content-delivery system, active learning, graphics, etc? What questions are missing on this survey? Was there anything not captured by this survey you would like to add about your institution, staff, demographics, content, delivery or style? We found that the schools used a large variety of approaches, but there were some common themes and clusters that emerged. For example, the first courses had almost as many different textbooks as schools. This model is distinct from some upper-division courses, (e.g., Artificial Intelligence, Algorithms, or Graphics) in which most schools used the same reference textbook. We will present the common and unusual cases, as well as celebrate the innovation that is taking place at various institutions.},
address = {Covington, KY, USA},
author = {Forbes, Jeffrey and Garcia, Daniel D.},
booktitle = {SIGCSE 2007: 38th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/1227310.1227396},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Forbes, Garcia - 2007 - ...But what do the top-rated schools do A survey of introductory computer science curricula.pdf:pdf},
isbn = {1595933611},
issn = {0097-8418},
keywords = {CS2,Comparison of curricula,Computer engineering curriculum,Computer science curriculum,Computing curricula,Information systems curriculum,Information technology curriculum,Software engineering curriculum},
mendeley-tags = {CS2},
number = {1},
pages = {245--246},
publisher = {ACM Press},
title = {{"...But what do the top-rated schools do?": A survey of introductory computer science curricula}},
url = {http://portal.acm.org/citation.cfm?doid=1227310.1227396},
volume = {39},
year = {2007}
}
@article{Malaiya2002,
author = {Malaiya, Y.K. and Li, M.N. and Bieman, J.M. and Karcich, R.},
doi = {10.1109/TR.2002.804489},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malaiya et al. - 2002 - Software reliability growth with test coverage.pdf:pdf},
issn = {0018-9529},
journal = {IEEE Transactions on Reliability},
keywords = {Automatic testing,Computer science,Computerized monitoring,Knee,NASA,Predictive models,Software reliability,Software testing,Space technology,Sun,automatic test generation,blocks,branches,computation-uses,defect-coverage,high reliability,logarithmic-exponential model,manual test generation,predicate-uses,probabilities,probability,program data-sets,program testing,software reliability,software reliability growth,software test-coverage measures,software testing,test coverage,testing effort,testing time,time to next failure prediction},
mendeley-tags = {test coverage},
month = {dec},
number = {4},
pages = {420--426},
publisher = {IEEE},
title = {{Software reliability growth with test coverage}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1044339},
volume = {51},
year = {2002}
}
@article{Diggins2004,
abstract = {Interfaces are noninstantiable types that contain only function declarations. Interface types can provide an alternative method of nonintrusive polymorphism, which, for certain scenarios, are more efficient and appropriate than traditional abstract base-class inheritance design patterns. Interfaces are a powerful concept that have applicability in a wide range of problem domains and exploration is being done as to their usefulness in domains; ranging from test-driven design and extreme programming to aspect-oriented programming and design by contract, among others},
annote = {C++ with interfaces},
author = {Diggins, C},
isbn = {1075-2838},
journal = {C/C++ Users Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {9},
pages = {36--39},
title = {{C++ with interfaces}},
url = {citeulike-article-id:3934588 {\#}},
volume = {22},
year = {2004}
}
@article{Shafique2010,
author = {Shafique, Muhammad and Labiche, Yvan},
keywords = {state-based testing,systematic review,transition-based testing},
number = {May},
pages = {1--21},
title = {{A Systematic Review of Model Based Testing Tool Support}},
year = {2010}
}
@misc{OGrady2020,
author = {O'Grady, Stephen},
booktitle = {Redmonk},
title = {{The RedMonk Programming Language Rankings: January 2020}},
url = {https://redmonk.com/sogrady/2020/02/28/language-rankings-1-20/},
urldate = {2020-08-21},
year = {2020}
}
@article{Chidamber,
author = {Chidamber, Shyam R and Kemerer, Chris F},
journal = {IEEE Transactions on Software Engineering},
keywords = {CK metrics,ck},
number = {6},
pages = {476--493},
title = {{A Metrics Suite for Object Oriented Design}},
volume = {20},
year = {1994}
}
@techreport{IEEE2010,
author = {IEEE},
doi = {10.1109/IEEESTD.2010.5446443},
institution = {IEEE},
publisher = {IEEE},
title = {{IEEE 1413-2010 - IEEE Standard Framework for Reliability Prediction of Hardware}},
year = {2010}
}
@article{Lutz2006,
author = {Lutz, Robyn and Patterson-Hine, Ann and Nelson, Stacy and Frost, Chad R. and Tal, Doron and Harris, Robert},
doi = {10.1007/s00766-006-0039-4},
issn = {0947-3602},
journal = {Requirements Engineering},
keywords = {anomaly handling,autonomy,contingency requirements,obstacle analysis,requirements evolution,safety-critical requirements},
month = {oct},
number = {1},
pages = {41--54},
title = {{Using obstacle analysis to identify contingency requirements on an unpiloted aerial vehicle}},
url = {http://www.springerlink.com/index/10.1007/s00766-006-0039-4},
volume = {12},
year = {2006}
}
@inproceedings{111386,
abstract = {A description is presented of MIDAS, the Mobile Intrusion Detection and Assessment System. MIDAS is a security system that can be quickly deployed to provide wide area coverage for a mobile asset. MIDAS uses two passive infrared imaging sensors, one for intruder detection and one for assessment. Detected targets are tracked while assessment cameras are directed to view the intruder location for operator observation and assessment. The dual sensor design allows simultaneous detection, assessment, and tracking. Control and status information is provided to an operator using a color graphics terminal, touch panel driven menus, and a joystick for control of the assessment sensor pan and tilt},
author = {Arlowe, H D and Coleman, D E},
booktitle = {Security Technology, 1990. Crime Countermeasures, Proceedings. IEEE 1990 International Carnahan Conference on},
doi = {10.1109/CCST.1990.111386},
keywords = {MIDAS,Mobile Intrusion Detection and Assessment Sy},
month = {oct},
pages = {54--61},
title = {{The mobile intrusion detection and assessment system (MIDAS)}},
year = {1990}
}
@inproceedings{Grottke2005a,
author = {Grottke, Michael and Trivedi, Kishor S},
booktitle = {Supplemental Proc.$\backslash$ Sixteenth International IEEE Symposium on Software Reliability Engineering},
pages = {4.19--4.20},
title = {{A classification of software faults}},
year = {2005}
}
@inproceedings{Layman2011a,
abstract = {In this case study, we examine software safety risk in three flight hardware systems in NASA's Constellation spaceflight program. We applied our Technical and Process Risk Measurement (TPRM) methodology to the Constellation hazard analysis process to quantify the technical and process risks involving software safety in the early design phase of these projects. We analyzed 154 hazard reports and collected metrics to measure the prevalence of software in hazards and the specificity of descriptions of software causes of hazardous conditions. We found that 49-70{\%} of 154 hazardous conditions could be caused by software or software was involved in the prevention of the hazardous condition. We also found that 12-17{\%} of the 2013 hazard causes involved software, and that 23-29{\%} of all causes had a software control. The application of the TRPM methodology identified process risks in the application of the hazard analysis process itself that may lead to software safety risk. {\textcopyright} 2011 ACM.},
address = {Honolulu, HI},
author = {Layman, Lucas and Basili, Victor R and Zelkowitz, Marvin V and Fisher, Karen L},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Software Engineering (ICSE '11)},
doi = {10.1145/1985793.1985881},
isbn = {9781450304450},
issn = {02705257},
keywords = {constellation program,empirical software engineering,hazard reports,measurement,mypubs,safety},
mendeley-tags = {mypubs},
pages = {623--632},
title = {{A Case Study of Measuring Process Risk for Early Insights into Software Safety}},
year = {2011}
}
@inproceedings{Noel2008,
abstract = {Extreme Programming (XP) suggests using Evolutionary design, always implementing the simplest solution that satisfies the current iteration's requirements, instead of Planned (or Traditional) up-front design. Some developers have questioned the usefulness of Evolutionary approach's enabling practices (e.g., refactoring, test-driven development) arguing for the naturalness of, and need for, Planned design. Two controlled experiments were conducted to compare both approaches regarding product quality and programmer productivity. Results from both studies show that (1) there is no significant difference in the product quality, independently of experience, but (2) novices are more productive using the Planned approach.},
address = {New York, NY, USA},
annote = {Adding planned design to xp might help novices{\&}{\#}039; productivity (or might not): two controlled experiments},
author = {No{\"{e}}l, Ren{\'{e}} and Valdes, Gonzalo and Visconti, Marcello and Astudillo, Hern{\'{a}}n},
booktitle = {ESEM '08: Proceedings of the Second ACM-IEEE international symposium on Empirical software engineering and measurement},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {285--287},
publisher = {ACM},
title = {{Adding planned design to xp might help novices' productivity (or might not): two controlled experiments}},
url = {citeulike-article-id:3934740 http://dx.doi.org/10.1145/1414004.1414053},
year = {2008}
}
@inproceedings{Ozarin2003,
address = {Los Angeles, CA},
author = {Ozarin, Nathaniel and Siracusa, Michael},
booktitle = {Proceedings of the Annual Reliability and Maintainability Symposium, 2003.},
doi = {10.1109/RAMS.2003.1182016},
isbn = {0-7803-7717-6},
keywords = {any single failure in,computer software can cause,fmea,means to determine whether,mission critical software,software failure,software fault tree,software fmea,software fmea is a,summary and conclusions},
pages = {365--370},
publisher = {IEEE},
title = {{A process for failure modes and effects analysis of computer software}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1182016},
year = {2003}
}
@article{4489841,
abstract = {Simson Garfinkel reviews Security Data Visualization: Graphical Techniques for Network Analysis by Greg Conti.},
author = {Garfinkel, Simson},
doi = {10.1109/MSP.2008.45},
issn = {1540-7993},
journal = {Security Privacy, IEEE},
number = {2},
pages = {5},
title = {{Sharp Figures, Fuzzy Purpose}},
volume = {6},
year = {2008}
}
@article{Carter2017,
abstract = {Analyzing the process data of students as they complete programming assignments has the potential to provide computing educators with insights into both their students and the processes by which they learn to program. In prior research, we explored the relationship between (a) students' programming behaviors and course outcomes, and (b) students' participation within an online social learning environment and course outcomes. In both studies, we developed statistical measures derived from our data that significantly correlate with students' course grades. Encouraged both by social theories of learning and a desire to improve the accuracy of our statistical models, we explore here the impact of incorporating our predictive measure derived from social behavior into three separate predictive measures derived from programming behaviors. We find that, in combining the measures, we are able to improve the overall predictive power of each measure. This finding affirms the importance of social interaction in the learning process, and provides evidence that predictive models derived from multiple sources of learning process data can provide significantly better predictive power by accounting for multiple factors responsible for student success.},
annote = {[16] A. S. Carter, C. D. Hundhausen, and O. Adesope. 2015. The normalized programming state model: Predicting student performance in computing courses based on programming behavior. In Proceedings of the 11th Annual International Conference on International Computing Education Research. ACM. 141–150.

Indeed, in Course 1, a correlational analysis between a student's level of participation and prior grade in CS 1 was found to be significant (r = 0.468, p {\textless} 0.001). However, in Course 2, this was not the case (r = 0.12, p = 0.26). Hence, we decided to retain prior CS 1 grade as a covariate in further analyses with Course 1 (by using a MANCOVA), whereas we used a MANOVA for Course 2, adding prior CS 1 grade as a separate independent variable for comparison purposes



blending social and programming meausre could account for variance in final cs2 grade early in the semester

good lit



earner's background [e.g., 10, 32], prior knowledge [e.g., 10, 39], cognitive abilities [e.g., 46], time- on-task [e.g., 48], and learning attitudes [e.g., 8]. In computing education, for example, Rosson et al. [44] found strong positive correlations between a number of attitudinal variables, including self- efficacy, and a learner's orientation toward the computing discipline},
author = {Carter, Adam S. and Hundhausen, Christopher D. and Adesope, Olusola},
doi = {10.1145/3120259},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carter, Hundhausen, Adesope - 2017 - Blending measures of programming and social behavior into predictive models of student achievement.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {CS2,Learning analytics,Learning interventions,Learning process data},
mendeley-tags = {CS2},
month = {aug},
number = {3},
pages = {Article 12},
publisher = {Association for Computing Machinery},
title = {{Blending measures of programming and social behavior into predictive models of student achievement in early computing courses}},
volume = {17},
year = {2017}
}
@misc{idc2015,
author = {{International Data Corporation}},
booktitle = {http://www.idc.com/prodserv/smartphone-os-market-share.jsp},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {http://www.idc.com/prodserv/smartphone--os--market--s},
title = {{Smartphone OS Market Share, 2015 Q2}},
url = {http://www.idc.com/prodserv/smartphone-os-market-share.jsp},
urldate = {2015-01-01},
year = {2015}
}
@inproceedings{Herzig2013,
address = {San Francisco, CA},
author = {Herzig, Kim and Zeller, Andreas},
booktitle = {2013 10th Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2013.6624018},
isbn = {978-1-4673-2936-1},
month = {may},
pages = {121--130},
title = {{The impact of tangled code changes}},
url = {http://ieeexplore.ieee.org/document/6624018/},
year = {2013}
}
@book{Creswell1998,
address = {Thousand Oaks, CA},
author = {Creswell, J W},
publisher = {Sage},
title = {{Qualitative inquiry and research design: Choosing among five traditions.}},
year = {1998}
}
@inproceedings{Papadopoulos2004b,
address = {New York, New York, USA},
author = {Papadopoulos, Christos and Kyriakakis, Chris and Sawchuk, Alexander and He, Xinming},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029223},
isbn = {1581139748},
keywords = {monitoring,network security,network visualization},
month = {oct},
pages = {90},
publisher = {ACM Press},
title = {{CyberSeer}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029223},
year = {2004}
}
@techreport{Lions1996,
address = {Paris, France},
author = {Lions, Jaques-Louis and Lubeck, Lennart and Fauquembergue, Jean-Luc and Kahn, Gilles and Kubbat, Wolfgang and Levedag, Stefan and Mazzini, Leonardo and Merle, Didier and O'Halloran, Collin},
title = {{Ariane 5 Flight 501 Failure Report by the Inquiry Board}},
url = {http://esamultimedia.esa.int/docs/esa-x-1819eng.pdf},
year = {1996}
}
@incollection{Turhan2010,
address = {Cambridge, MA},
author = {Turhan, Burak and Layman, Lucas and Diep, Madeline and Erdogmus, Hakan and Shull, Forrest},
booktitle = {Making Software: What Really Works, and Why We Believe It},
editor = {Oram, Andy and Wilson, Greg},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {207--219},
publisher = {O'Reilly},
title = {{How Effective is Test Driven Development?}},
year = {2010}
}
@book{Baumeister1994,
address = {San Diego, CA},
author = {Baumeister, Roy F. and Heatherton, Todd F. and Tice, Dianne M.},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Academic Press},
title = {{Losing Control: How and Why People Fail at Self-Regulation}},
year = {1994}
}
@inproceedings{Siirtola2013,
author = {Siirtola, Pekka and Roning, Juha},
booktitle = {2013 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)},
doi = {10.1109/CIDM.2013.6597218},
isbn = {978-1-4673-5895-8},
month = {apr},
pages = {59--64},
publisher = {IEEE},
title = {{Ready-to-use activity recognition for smartphones}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6597218},
year = {2013}
}
@inproceedings{1565237,
abstract = {Malware such as Trojan horses and spyware remain to be persistent security threats that exploit the overly complex graphical user interfaces of today's commodity operating systems. In this paper, we present the design and implementation of Nitpicker - an extremely minimized secure graphical user interface that addresses these problems while retaining compatibility to legacy operating systems. We describe our approach of kernelizing the window server and present the deployed security mechanisms and protocols. Our implementation comprises only 1,500 lines of code while supporting commodity software such as X11 applications alongside protected graphical security applications. We discuss key techniques such as client-side window handling, a new floating-labels mechanism, drag-and-drop, and denial-of-service-preventing resource management. Furthermore, we present an application scenario to evaluate the feasibility, performance, and usability of our approach},
author = {Feske, N. and Helmuth, C.},
booktitle = {21st Annual Computer Security Applications Conference (ACSAC'05)},
doi = {10.1109/CSAC.2005.7},
isbn = {0-7695-2461-3},
issn = {1063-9527},
keywords = {Nitpicker,Trojan horses,X11 application,client-sid},
pages = {85--94},
publisher = {Ieee},
title = {{A Nitpicker{\&}{\#}146;s guide to a minimal-complexity secure GUI}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1565237},
year = {2005}
}
@inproceedings{TrivediISSRE2011,
author = {Trivedi, Kishor S},
booktitle = {Proc.$\backslash$ IEEE 22nd International Symposium on Software Reliability Engineering},
title = {{Software Fault Tolerance via Environmental Diversity}},
year = {2011}
}
@article{Putnam1978,
author = {Putnam, L H},
number = {4},
pages = {345--361},
title = {{A General Empirical Solution to the Macro Software Sizing and Estimating Problem}},
volume = {SE-4},
year = {1978}
}
@inproceedings{Janzen2008a,
abstract = {Coercing new programmers to adopt disciplined development practices such as thorough unit testing is a challenging endeavor. Test-driven development (TDD) has been proposed as a solution to improve both software design and testing. Test-driven learning (TDL) has been proposed as a pedagogical approach for teaching TDD without imposing significant additional instruction time. This research evaluates the effects of students using a test-first (TDD) versus test-last approach in early programming courses, and considers the use of TDL on a limited basis in CS1 and CS2. Software testing, programmer productivity, programmer performance, and programmer opinions are compared between test-first and test-last programming groups. Results from this research indicate that a test-first approach can increase student testing and programmer performance, but that early programmers are very reluctant to adopt a test-first approach, even after having positive experiences using TDD. Further, this research demonstrates that TDL can be applied in CS1/2, but suggests that a more pervasive implementation of TDL may be necessary to motivate and establish disciplined testing practice among early programmers. Copyright 2008 ACM.},
annote = {Test-driven learning in early programming courses},
author = {Janzen, D S and Saiedian, H},
booktitle = {SIGCSE'08 - Proceedings of the 39th ACM Technical Symposium on Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {532--536},
title = {{Test-driven learning in early programming courses}},
url = {citeulike-article-id:3934659 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57449085524{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Santos2016,
address = {Austin, TX},
author = {Santos, Eddie Antonio and Hindle, Abram},
booktitle = {Proceedings of the 13th International Workshop on Mining Software Repositories - MSR '16},
doi = {10.1145/2901739.2903493},
isbn = {9781450341868},
pages = {504--507},
title = {{Judging a commit by its cover}},
url = {http://dl.acm.org/citation.cfm?doid=2901739.2903493},
year = {2016}
}
@inproceedings{Sakaki2010,
address = {Raleigh, NC},
author = {Sakaki, Takeshi and Okazaki, Makoto and Matsuo, Yutaka},
booktitle = {Proceedings of the 19th international conference on World wide web - WWW '10},
doi = {10.1145/1772690.1772777},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sakaki, Okazaki, Matsuo - 2010 - Earthquake shakes Twitter users.pdf:pdf},
isbn = {9781605587998},
keywords = {Twitter,earthquake,event detection,location estimation,social sensor,twitter},
mendeley-tags = {twitter},
pages = {851--860},
title = {{Earthquake shakes Twitter users}},
url = {http://portal.acm.org/citation.cfm?doid=1772690.1772777},
year = {2010}
}
@inproceedings{Czarlinska2007b,
address = {New York, New York, USA},
author = {Czarlinska, Alexandra and Kundur, Deepa},
booktitle = {Proceedings of the 9th workshop on Multimedia {\&} security - MM{\&}Sec '07},
doi = {10.1145/1288869.1288899},
isbn = {9781595938572},
keywords = {attack,error detection,event-driven,wireless visual sensor networks},
month = {sep},
pages = {215},
publisher = {ACM Press},
title = {{Attack vs. failure detection in event-driven wireless visual sensor networks}},
url = {http://dl.acm.org/citation.cfm?id=1288869.1288899},
year = {2007}
}
@phdthesis{Wasmus2006,
abstract = {This thesis is the result of a case study in Test Driven Development done at EPCOS, Inc. in Iselin, New Jersey for the TU Delft, The Netherlands. The case study is the implementation of a webservice and application replacing a Microsoft Access project. The duration of the case study is 10 months. After the case study the project itself will continue to be developed in the future. The purpose of the case study was to create a way to be able to register estimations of future market demand and to anticipate on these estimates. The thesis comes forth from a study which was the result of a literature research assignment on the reliability of software. The literature research was in strong relation with a project called TRADER, which is a collaboration of industrial and commercial partners. Among the partners in the TRADER project are; the Embedded Systems Institute (ESI), Phillips, Delft University of Technology, Twente University and the University of Leiden. A result from this study was that testing of software applications is one of the influences in the reliability of a software application. There is a movement in the software development world which uses Extreme Programming (XP) and Test Driven Development (TDD) as part of XP to increase the reliability of software. With the research carried out for this thesis a detailed analysis of the TDD approach is created, it will verify claims made about TDD and review if TDD is a good basis for creating reliable software.},
address = {Delft},
annote = {Evaluation of Test Driven Development},
author = {Wasmus, Hans},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {85},
publisher = {Delft University of Technology, EEMCS,},
title = {{Evaluation of Test Driven Development}},
url = {citeulike-article-id:3934831 {\#}},
year = {2006}
}
@inproceedings{Zou2015,
address = {Florence, Italy},
author = {Zou, Jie and Xu, Ling and Guo, Weikang and Yan, Meng and Yang, Dan and Zhang, Xiaohong},
booktitle = {2015 IEEE/ACM 12th Working Conference on Mining Software Repositories},
doi = {10.1109/MSR.2015.60},
isbn = {978-0-7695-5594-2},
keywords = {A Web sites programming,Data mining,LDA,Latent Dirichlet allocation (LDA),Manuals,Market research,NFR evolution visualization,Non-functional requirements (NFRs),Q{\&}amp,Reliability,Resource management,Stack Overflow,Topic model,Usability,Web sites,latent Dirichlet allocation,nonfunctional requirements,question and answer Web sites programming,software maintenance,software quality,software quality requirements,software reliability,stack overflow,systems analysis,textual content,topic analysis},
language = {English},
month = {may},
pages = {446--449},
publisher = {IEEE},
title = {{Which Non-functional Requirements Do Developers Focus On? An Empirical Study on Stack Overflow Using Topic Analysis}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7180114},
year = {2015}
}
@article{Taylor1999a,
abstract = {This paper describes an ongoing project to develop a computer-integrated system to assist surgeons in revision total hip replacement (RTHR) surgery. In RTHR surgery, a failing orthopedic hip implant, typically cemented, is replaced with a new one by removing the old implant, removing the cement and fitting a new implant into an enlarged canal broached in the femur. RTHR surgery is a difficult procedure fraught with technical challenges and a high incidence of complications. The goals of the computer-based system are the significant reduction of cement removal labor and time, the elimination of cortical wall penetration and femur fracture, the improved positioning and fit of the new implant resulting from precise, high-quality canal milling and the reduction of bone sacrificed to fit the new implant. Our starting points are the ROBODOC system for primary hip replacement surgery and the manual RTHR surgical protocol. We first discuss the main difficulties of computer-integrated RTHR surgery and identify key issues and possible solutions. We then describe possible system architectures and protocols for preoperative planning and intraoperative execution. We present a summary of methods and preliminary results in CT image metal artifact removal, interactive cement cut-volume definition and cement machining, anatomy-based registration using fluoroscopic X-ray images and clinical trials using an extended RTHR version of ROBODOC. We conclude with a summary of lessons learned and a discussion of current and future work.},
author = {Taylor, R H and Joskowicz, L and Williamson, B and Gu{\'{e}}ziec, A and Kalvin, A and Kazanzides, P and {Van Vorhis}, R and Yao, J and Kumar, R and Bzostek, A and Sahay, A and B{\"{o}}rner, M and Lahmer, A},
institution = {Computer Science Department, The Johns Hopkins University, Baltimore, MD, USA. rht@cs.jhu.edu},
journal = {Medical Image Analysis},
keywords = {algorithms,arthroplasty,artifacts,bone cements,calibration,computer assisted,fluoroscopy,fluoroscopy methods,hip,hip joint,hip joint radiography,hip joint surgery,hip methods,humans,intraoperative period,preoperative care,prosthesis failure,radiographic image enhancement,radiographic image enhancement methods,reoperation,replacement,reproducibility results,robotics,surface properties,therapy,tomography,x ray computed},
number = {3},
pages = {301--319},
pmid = {10710298},
title = {{Computer-integrated revision total hip replacement surgery: concept and preliminary results.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10710298},
volume = {3},
year = {1999}
}
@article{Tian1995b,
author = {Tian, Jeff and Lu, Peng and Palma, Joe},
isbn = {0098558919},
journal = {IEEE Transactions on Software Engineering},
number = {5},
pages = {405--414},
title = {{Test-execution-based reliability measurement and modeling for large commercial software}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=387470},
volume = {21},
year = {1995}
}
@article{Pretschner2005,
abstract = {Testing comprises activities that aim at showing that the intended and actual behaviors of a system differ, or at gaining confidence that they do not. The goal of testing is failure detection: observable differences between the behaviors of implementation and specification. Classical estimates relate one half of the overall development effort to testing. Model-based testing (MBT) relies on models (specifications) that encode the intended behavior of a system. Runs of the model are interpreted as test cases (in this paper, tests for short) for a system under test (SUT): input and expected output. Activities in MBT have attracted a major interest in the past years. In addition to the appeal of the concept, we see the major reasons (a) in a gain of momentum of model-based languages and technologies (UML, MDA) and their seemingly direct connection to testing activities, (b) in the increasing popularity of test-centered development processes such as TDD or XP, and (c) in the possibility of promoting research activities and results under the umbrella of "lightweight" formal method},
annote = {Model-based testing in practice},
author = {Pretschner, A},
isbn = {3540278826},
journal = {FM 2005: Formal Methods International Symposium of Formal Methods Europe Proceedings Lecture Notes in Computer Science Vol 3582},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {537--541},
title = {{Model-based testing in practice}},
url = {citeulike-article-id:3934758 {\#}},
volume = {3582},
year = {2005}
}
@article{Rostaher2002,
abstract = {The authors ran an experiment where a group of professional programmers working in pairs and a control group programming alone implemented a small system from predefined requirements. Most programmers spent between 50{\%} and 60{\%} of time on testing; only the most inexperienced spent less. Programmers reported more problems with refactoring than testing. The rhythm of switching the driver and navigator role is essential for test-first pair programming. The experiment showed that partners switched roles 21 times per day on average. The comparison of the control group of individuals and the group programming in pairs showed that both groups spent almost the same amount of time to complete the tasks. The result of this comparison is by applying a t-test not statistically significant. We believe that more detailed research apart of evaluating test-first programming is needed to compare solo vs. pair programming in the investigated group},
annote = {Tracking test first pair programming an experiment},
author = {Rostaher, M and Hericko, M},
isbn = {3540440240},
journal = {Extreme Programming and Agile Methods XP/Agile Universe 2002 Second XP Universe and First Agile Universe Conference Proceedings Lecture Notes in Computer Science Vol 2418},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {174--184},
title = {{Tracking test first pair programming an experiment}},
url = {citeulike-article-id:3934770 {\#}},
volume = {2418},
year = {2002}
}
@inproceedings{5210001,
abstract = {This paper proposes a novel differential energy watermarking based on the Watson visual model, which inserts robust watermark into video streaming according to the differential energy theory. This algorithm can control the watermark's embedding intensity of sub-low AC coefficients in the video streaming adaptively based on the Watson visual model. And it also can choose the region to embed watermark according to the relationship between the energy adjustable threshold and their differential energy. So watermark not only meets the non-visual perception, but also has the better robustness. Experiments show that this algorithm has strong robustness and security against the usual video attacks such as noise, filter and frame attack etc with low complexity of energy computation. This algorithm will be applied to the multimedia copyrights protection.},
author = {Sun, Tanfeng and Jiang, Xinghao and Shi, Shusen and Lin, Zhigao and Fu, Guanglei},
booktitle = {2009 Second International Symposium on Electronic Commerce and Security},
doi = {10.1109/ISECS.2009.223},
isbn = {978-0-7695-3643-9},
keywords = {Watson visual model,dif,differential energy theory},
month = {may},
pages = {179--183},
publisher = {Ieee},
title = {{A Novel Differential Energy Video Watermarking Based on Watson Visual Model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5210001},
volume = {1},
year = {2009}
}
@article{Lampson2009a,
author = {Lampson, Butler},
doi = {10.1145/1592761.1592773},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lampson - 2009 - Usable security How to get it.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {nov},
number = {11},
pages = {25--27},
title = {{Usable security: How to get it}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=1592773{\&}type=html},
volume = {52},
year = {2009}
}
@article{McCabe1976,
author = {McCabe, T J},
pages = {308--320},
title = {{A Complexity Measure}},
volume = {SE-2},
year = {1976}
}
@inproceedings{Roveta2011a,
address = {New York, New York, USA},
author = {Roveta, Francesco and Caviglia, Giorgio and {Di Mario}, Luca and Zanero, Stefano and Maggi, Federico and Ciuccarelli, Paolo},
booktitle = {Proceedings of the 8th International Symposium on Visualization for Cyber Security - VizSec '11},
doi = {10.1145/2016904.2016910},
isbn = {9781450306799},
month = {jul},
pages = {1--10},
publisher = {ACM Press},
title = {{BURN}},
url = {http://dl.acm.org/citation.cfm?id=2016904.2016910},
year = {2011}
}
@inproceedings{Podgurski2003,
abstract = {This paper proposes automated support for classifying reported software failures in order to facilitate prioritizing them and diagnosing their causes. A classification strategy is presented that involves the use of supervised and unsupervised pattern classification and multivariate visualization. These techniques are applied to profiles of failed executions in order to group together failures with the same or similar causes. The resulting classification is then used to assess the frequency and severity of failures caused by particular defects and to help diagnose those defects. The results of applying the proposed classification strategy to failures of three large subject programs are reported These results indicate that the strategy can be effective.},
author = {Podgurski, A. and Leon, D. and Francis, P. and Masri, W. and Minch, M.},
booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
doi = {10.1109/ICSE.2003.1201224},
isbn = {0-7695-1877-X},
issn = {0270-5257},
keywords = {Computer crashes,Estimation error,Frequency estimation,Humans,Instruments,Terminology,Visualization,multivariate visualization,pattern classification,program debugging,program diagnostics,program visualisation,software diagnosis,software failure,software fault tolerance,software maintenance,supervised pattern classification,unsupervised pattern classification},
pages = {465--475},
publisher = {IEEE},
shorttitle = {Software Engineering, 2003. Proceedings. 25th Inte},
title = {{Automated support for classifying software failure reports}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1201224},
year = {2003}
}
@inproceedings{5730280,
abstract = {In this paper, we are presenting a new software dedicated to maritime surveillance and security named AUTOMATIC SEA VISION {\#}x00AE; (ASV) (Waquet, 2007) ASV is the first entirely automatic optical software processing (video analytics) providing a smart solution to maritime safety and security issues, whether the intended use is for applications on board ships, or for protection of critical coastal areas and port facilities. The software processes in real time the output from multiple sensors (mainly IR camera, but also GPS, Inertial Navigation Unit...) in order to automatically detect any boat or object for a 24/7 surveillance. We rely on maritime environment and IR sensor data specificities to develop tailored algorithms to perform automatic object detection. These algorithms have been packaged into a complete solution which performs from data acquisition through a user friendly graphical user interface (GUI). Extensive tests have been carried out to validate our solution. We present a quantitative evaluation done on ground truth data. System performance has been calculated using Detection/False Alarm rates over multiple sequences acquired from numerous cameras. Moreover, qualitative evaluations have demonstrated the robustness of the system in numerous different weather conditions. The last section of this paper tackles algorithms enhancements as well as future evolutions of the system.},
author = {Samama, Arnaud},
booktitle = {2010 International WaterSide Security Conference},
doi = {10.1109/WSSC.2010.5730280},
isbn = {978-1-4244-8894-0},
keywords = {AUTOMATIC SEA VISION,GPS,IR camera,Waquet,automati},
month = {nov},
pages = {1--8},
publisher = {Ieee},
title = {{Innovative video analytics for maritime surveillance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5730280},
year = {2010}
}
@article{Keizer2008,
abstract = {Imagine that the neighborhood you are living in is covered with graffiti, litter, and unreturned shopping carts. Would this reality cause you to litter more, trespass, or even steal? A thesis known as the broken windows theory suggests that signs of disorderly and petty criminal behavior trigger more disorderly and petty criminal behavior, thus causing the behavior to spread. This may cause neighborhoods to decay and the quality of life of its inhabitants to deteriorate. For a city government, this may be a vital policy issue. But does disorder really spread in neighborhoods? So far there has not been strong empirical support, and it is not clear what constitutes disorder and what may make it spread. We generated hypotheses about the spread of disorder and tested them in six field experiments. We found that, when people observe that others violated a certain social norm or legitimate rule, they are more likely to violate other norms or rules, which causes disorder to spread.},
author = {Keizer, Kees and Lindenberg, Siegwart and Steg, Linda},
doi = {10.1126/science.1161405},
issn = {1095-9203},
journal = {Science},
keywords = {Cities,Crime,Environment,Female,Formal,Humans,Male,Netherlands,Refuse Disposal,Residence Characteristics,Social Behavior,Social Control,Social Environment,Social Problems,agile,nsf},
mendeley-tags = {agile,nsf},
month = {dec},
number = {5908},
pages = {1681--5},
pmid = {19023045},
title = {{The spreading of disorder.}},
url = {http://www.sciencemag.org/content/322/5908/1681.short},
volume = {322},
year = {2008}
}
@article{Ambler2007,
abstract = {Developers can use a test-driven development with database schema just as they use it with application code. Implementing test-driven database development (TDDD) involves three relatively simple steps: database refactoring, database regression testing, and continuous database integration. In database refactoring, developers make a simple change to a database to improve the design without changing its semantics. In database regression testing, they run a comprehensive test suite that validates the database regularly-ideally, whenever developers change the database schema or access the database in a different way. In continuous database integration, developers rebuild and retest the database schema whenever it changes. From a technical viewpoint, TDDD is straightforward. However, cultural challenges can make it difficult to adopt. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Test-driven development of relational databases},
author = {Ambler, S W},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {37--43},
title = {{Test-driven development of relational databases}},
url = {citeulike-article-id:3934538 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248398098{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@inproceedings{Kaufman1999,
address = {Charlotte, NC},
author = {Kaufman, Deborah B and Felder, Richard M and Fuller, Hugh},
title = {{Peer Ratings in Cooperative Learning Teams}},
year = {1999}
}
@article{Huhns1997,
author = {Huhns, M N and M P Singh},
keywords = {ontologies},
number = {6},
pages = {81--83},
title = {{Ontologies for Agents}},
volume = {1},
year = {1997}
}
@inproceedings{4529314,
abstract = {Telehealth applications can deliver medical services to patients at remote locations using telecommunications technologies, such as the Internet. At the same time, such applications also pose unique security challenges. First, the trust issue becomes more severe due to the lack of visual proofs in telehealth applications. The public key infrastructure (PKI) is insufficient for providing the same kind of trust a patient may attain during a face-to-face service. Second, telehealth services, such as tele-monitoring or tele-consultant, naturally demand a systematic organization of users, roles, resources, and flows of information. Existing access control mechanisms in an e-health system are usually incapable of dealing with such workflow-based services. This paper provides cost-efficient solutions to those issues in the context of a Web-based e-health portal system. First, we propose a PKI-like infrastructure for establishing trust between users using biometrics-based authentication and hierarchies of trust. Second, we develop an access control method for workflow-based telehealth services using a rule-based module already available in the portal system.},
author = {Liu, Qian and Lu, Shuo and Hong, Yuan and Wang, Lingyu and Dssouli, R},
booktitle = {Availability, Reliability and Security, 2008. ARES 08. Third International Conference on},
doi = {10.1109/ARES.2008.9},
keywords = {Web-based e-health portal system,access control me},
month = {mar},
pages = {3--9},
title = {{Securing Telehealth Applications in a Web-Based e-Health Portal}},
year = {2008}
}
@article{Levesonc,
author = {Leveson, Nancy},
journal = {The MIT Press. ISBN 978-0-262-01662-9},
title = {{Engineering a Safer World - Systems Thinking Applied To Safety}}
}
@inproceedings{Nair2005,
abstract = {When Primavera Systems decided to adopt agile methodologies for the development of their Project management suite, no one expected it to be easy. One part of the suite is a desktop solution with about 1.5 M lines of code in Delphi comprised of several applications. The other part is a set of web enabled applications using J2EE and Java. The development has tight release cycles and a high demand from marketing for new features. This paper describes from a software-architecture perspective, the transformation of a largely waterfall-based development strategy into an agile, test-driven practice. It looks at the obstacles and difficulties we faced during this change in the context of the desktop application and, the way we approached refactoring legacy code to make it testable. We describe how a Business Object Framework was created in an evolutionary manner as part of this process. The lessons learned from this year long process is summarized in the form of 5 axioms. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {Teaching a goliath to fly},
author = {Nair, S and Ramnath, P},
booktitle = {Proceedings - AGILE Confernce 2005},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {111--124},
title = {{Teaching a goliath to fly}},
url = {citeulike-article-id:3934737 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33847716353{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@inproceedings{Elssamadisy2002,
address = {Orlando, FL},
author = {Elssamadisy, Amr and Schalliol, Gregory},
booktitle = {Proceedings of the 24th International conference on Software Engineering},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elssamadisy, Schalliol - 2002 - Recognizing and responding to bad smells in extreme programming.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
pages = {617--622},
publisher = {ACM},
title = {{Recognizing and responding to bad smells in extreme programming}},
url = {http://portal.acm.org/citation.cfm?id=581339.581418},
year = {2002}
}
@article{Enquobahrie2007,
abstract = {This paper presents an overview of the image-guided surgery toolkit (IGSTK). IGSTK is an open source C++ software library that provides the basic components needed to develop image-guided surgery applications. It is intended for fast prototyping and development of image-guided surgery applications. The toolkit was developed through a collaboration between academic and industry partners. Because IGSTK was designed for safety-critical applications, the development team has adopted lightweight software processes that emphasizes safety and robustness while, at the same time, supporting geographically separated developers. A software process that is philosophically similar to agile software methods was adopted emphasizing iterative, incremental, and test-driven development principles. The guiding principle in the architecture design of IGSTK is patient safety. The IGSTK team implemented a component-based architecture and used state machine software design methodologies to improve the reliability and safety of the components. Every IGSTK component has a well-defined set of features that are governed by state machines. The state machine ensures that the component is always in a valid state and that all state transitions are valid and meaningful. Realizing that the continued success and viability of an open source toolkit depends on a strong user community, the IGSTK team is following several key strategies to build an active user community. These include maintaining a users and developers' mailing list, providing documentation (application programming interface reference document and book), presenting demonstration applications, and delivering tutorial sessions at relevant scientific conferences. {\^{A}}{\textcopyright} 2007 Society for Imaging Informatics in Medicine.},
annote = {The Image-Guided Surgery Toolkit IGSTK: An open source C++ software toolkit},
author = {Enquobahrie, A and Cheng, P and Gary, K and Ibanez, L and Gobbi, D and Lindseth, F and Yaniv, Z and Aylward, S and Jomier, J and Cleary, K},
journal = {Journal of Digital Imaging},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {SUPPL. 1},
pages = {21--33},
title = {{The Image-Guided Surgery Toolkit IGSTK: An open source C++ software toolkit}},
url = {citeulike-article-id:3934599 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35448985796{\&}{\#}38 partnerID=40},
volume = {20},
year = {2007}
}
@inproceedings{4606116,
abstract = {Trust and security are two important concepts in information security, but the difference between them is fuzzy. This paper proposes a new formal definition of trust for task-oriented information system. The new definition includes detailed information about trust itself. It defines trust of a component as the relationship of the expected behaviors and the trusted prerequisites, and puts forward a formal directed-graph model to express it. With the directed-graph model, it expands trust chain to trust tree and trust forest, use them to give a formal description of trust and security of information system, it also discusses trusted module, and brings forward a multi-layer trusted structure to design trusted module.},
author = {Wei-Peng, Liu and Ju, Hu},
booktitle = {Electronic Commerce and Security, 2008 International Symposium on},
doi = {10.1109/ISECS.2008.191},
keywords = {formal directed-graph model;multilayer trusted str},
pages = {502--506},
title = {{A Formal Model of Trust and Security for Task-Oriented Information System}},
year = {2008}
}
@book{Williams2003b,
address = {Reading, Massachusetts},
author = {Williams, Laurie and Kessler, Robert},
publisher = {Addison Wesley},
title = {{Pair Programming Illuminated}},
year = {2003}
}
@inproceedings{Parnas1994,
address = {Sorrento, Italy},
author = {Parnas, David Lorge},
booktitle = {Proceedings of the 16th International Conference on Software Engineering},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parnas - 1994 - Software aging.pdf:pdf},
isbn = {0-8186-5855-X},
month = {may},
pages = {279--287},
title = {{Software aging}},
url = {http://dl.acm.org/citation.cfm?id=257734.257788},
year = {1994}
}
@article{Janzen2008,
abstract = {Test-driven development (TDD) is first and foremost a design practice. The question is, "How good are the resulting designs?" Advocates claim that TDD produces code that is simpler, more cohesive, and less coupled than code developed in a more traditional test-last way. Sounds good, but is it true? We collected evidence to substantiate or question the claims regarding TDD's influence on software. We focused on internal software quality that is, design and code characteristics such as code complexity, size, coupling, and cohesion. We conducted three quasicontrolled experiments and one case study in a Fortune 500 company and another two quasicontrolled experiments with university students in undergraduate and graduate software engineering courses. We evaluated 24 student and professional programmers working on 21 software projects ranging in size from a few hundred to over 30,000 lines of code. The results indicate that TDD programmers tend to write software modules that are smaller, less complex, and more highly tested than modules produced by their test-last counterparts. However, the results didn't support claims for lower coupling and increased cohesion with TDD. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Does test-driven development really improve software design quality?},
author = {Janzen, D S and Saiedian, H},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {77--84},
title = {{Does test-driven development really improve software design quality?}},
url = {citeulike-article-id:3934658 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-40949159801{\&}{\#}38 partnerID=40},
volume = {25},
year = {2008}
}
@article{Matsubara2017,
author = {Matsubara, Yasuko and Sakurai, Yasushi and Prakash, B. Aditya and Li, Lei and Faloutsos, Christos},
doi = {10.1145/3057741},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matsubara et al. - 2017 - Nonlinear Dynamics of Information Diffusion in Social Networks.pdf:pdf},
journal = {ACM Transactions on the Web},
keywords = {twitter},
mendeley-tags = {twitter},
month = {apr},
number = {2},
pages = {Article No. 11},
publisher = {ACM},
title = {{Nonlinear Dynamics of Information Diffusion in Social Networks}},
url = {http://dl.acm.org/citation.cfm?doid=3079924.3057741},
volume = {11},
year = {2017}
}
@book{hindelang1978victims,
address = {Cambridge, MA},
author = {Hindelang, Michael J and Gottfredson, Michael R and Garofalo, James},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Ballinger},
title = {{Victims of Personal Crime: An Empirical Foundation for a Theory of Personal Victimization}},
year = {1978}
}
@article{Prowell2003a,
author = {Prowell, SJ},
isbn = {0769518745},
journal = {System Sciences, 2003. Proceedings of the 36th Annual Hawaii International Conference on System Sciences},
number = {C},
pages = {1--9},
title = {{JUMBL: A tool for model-based statistical testing}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1174916},
volume = {00},
year = {2003}
}
@inproceedings{4123784,
abstract = {The presentation will cover how visualization has evolved in a control room from Monitors to PCs and beyond. It will outline several areas of concern namely those below in control room visualization design ranging from specific monitor related issues to networked collaboration capabilities available to day. This discussion will show how these new visualization capabilities can contribute to help make decisions across control rooms, across jurisdictions and functions faster and better, ensuring that sensitive decisions are taken by the right level with the right information.},
author = {Wu, Robert},
booktitle = {Crime and Security, 2006. The Institution of Engineering and Technology Conference on},
pages = {353--354},
title = {{Visualisation in Security Control Rooms}},
year = {2006}
}
@inproceedings{Kremenek2004,
address = {Newport Beach, CA},
author = {Kremenek, T and Ashcraft, K and Yang, J and Enger, D},
keywords = {false positive,static analysis},
pages = {83--93},
title = {{Correlation Exploitation in Error Ranking}},
year = {2004}
}
@article{Grinter2001,
author = {Grinter, Rebecca E. and Eldridge, Margery A.},
isbn = {0-7923-7162-3},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {sep},
pages = {219--238},
publisher = {Kluwer Academic Publishers},
title = {y do tngrs luv 2 txt msg?},
url = {http://dl.acm.org/citation.cfm?id=1241867.1241879},
year = {2001}
}
@article{Paternoster2009,
author = {Paternoster, Ray and Pogarsky, Greg},
doi = {10.1007/s10940-009-9065-y},
issn = {0748-4518},
journal = {Journal of Quantitative Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {mar},
number = {2},
pages = {103--127},
title = {{Rational Choice, Agency and Thoughtfully Reflective Decision Making: The Short and Long-Term Consequences of Making Good Choices}},
url = {http://link.springer.com/10.1007/s10940-009-9065-y},
volume = {25},
year = {2009}
}
@inproceedings{Masticola2008,
abstract = {Mechatronics is the practice of creating systems that synergize electrical, mechanical, and software technology. With few exceptions, testing software that is embedded in mechatronics systems has historically been done only with the hardware in the loop (HIL). There are many disadvantages to HIL testing, including cost, schedule delays, and resource bottlenecks. Ironically, cost and schedule delays are also often seen by technical managers as impediments to simulating the mechatronics hardware.},
address = {New York, NY, USA},
annote = {Vision: testing of mechatronics software using agile simulation},
author = {Masticola, Stephen and Gall, Michael},
booktitle = {AST '08: Proceedings of the 3rd international workshop on Automation of software test},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {79--84},
publisher = {ACM},
title = {{Vision: testing of mechatronics software using agile simulation}},
url = {citeulike-article-id:3934706 http://dx.doi.org/10.1145/1370042.1370060},
year = {2008}
}
@inproceedings{Sanders2016,
abstract = {In this paper, we survey the work that has been done in threshold concepts in computing since they were first discussed in 2005: concepts that have been identified, method-ologies used, and issues discussed. Based on this survey, we then identify some promising unexplored areas for future work.},
address = {Koli, Finland},
author = {Sanders, Kate and McCartney, Robert},
booktitle = {Koli Calling '16 - Proceedings of the 16th Koli Calling International Conference on Computing Education Research},
doi = {10.1145/2999541.2999546},
isbn = {9781450347709},
keywords = {CS2,Threshold Concepts},
mendeley-tags = {CS2},
pages = {91--100},
publisher = {ACM Press},
title = {{Threshold concepts in computing: Past, present, and future}},
url = {http://dl.acm.org/citation.cfm?doid=2999541.2999546},
year = {2016}
}
@book{Bertin1977,
address = {Paris, France},
author = {Bertin, Jaques},
publisher = {Flammarion},
title = {{La graphique et le traitement graphique de l'information}},
year = {1977}
}
@article{Kemerer1992,
author = {Kemerer, C.F.},
doi = {10.1109/52.136161},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {Computer aided software engineering,Cost function,Investments,Manufacturing,Packaging,Productivity,Quality management,Software development management,Software quality,Software tools,agile,behavioural sciences,data collection,data validation,human factors,integrated CASE tools,learning curve,nsf,software engineering,software tools},
language = {English},
mendeley-tags = {agile,nsf},
month = {may},
number = {3},
pages = {23--28},
publisher = {IEEE},
title = {{How the learning curve affects CASE tool adoption}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=136161},
volume = {9},
year = {1992}
}
@inproceedings{Pinto2016,
author = {Pinto, Gustavo and Steinmacher, Igor and Gerosa, Marco Aur{\'{e}}lio},
booktitle = {2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)},
doi = {10.1109/SANER.2016.68},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pinto, Steinmacher, Gerosa - 2016 - More Common Than You Think An In-depth Study of Casual Contributors.pdf:pdf},
isbn = {978-1-5090-1855-0},
keywords = {Collaboration,Computer languages,Electronic mail,GitHub,Inspection,Manuals,OSS projects,Software,Software engineering,Web sites,bug fixing,grammar issues,open source software projects,program debugging,public domain software,software engineering,software repositories,source code (software),source code hosting Websites},
month = {mar},
pages = {112--123},
publisher = {IEEE},
title = {{More Common Than You Think: An In-depth Study of Casual Contributors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7476635},
volume = {1},
year = {2016}
}
@inproceedings{Xu2007,
abstract = {Test scenarios are frequently used in scenario-based software testing. However, generation of the test scenarios is usually a manual and labor-intensive task. It is often desired that test scenarios can be automatically generated. As a semi-formal modeling language, UML is widely used in both academia research and industry practice to describe analysis and design specifications. The UML activity diagrams (ADs) are mainly used in business logic processing at the early stage of software development life-cycle. Obviously, testing scenarios generated from ADs can contribute to test driven development. Nevertheless, it is difficult to automatically generate test scenarios from ADs which contains fork-join pairs mixed with loops and branches. In this paper, a systematic approach is proposed to automatically generate test scenarios from the UML activity diagrams which may contain complicated fork-join structure.},
annote = {A systematic approach to automatically generate test scenarios from uml activity diagrams},
author = {Xu, D and Li, H and Lam, C P},
booktitle = {Proceedings of the 3rd IASTED International Conference on Advances in Computer Science and Technology, ACST 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {134--139},
title = {{A systematic approach to automatically generate test scenarios from uml activity diagrams}},
url = {citeulike-article-id:3934843 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-56149118523{\&}{\#}38 partnerID=40},
year = {2007}
}
@incollection{Kazanzides2007a,
author = {Kazanzides, Peter},
booktitle = {Robotics in Surgery: History, Current and Future Applications},
pages = {61--94},
title = {{Robots for Orthopaedic Joint Reconstruction}},
year = {2007}
}
@article{Finstad2007,
abstract = {Printed circuits on flexible substrates can offer compact, low-mass packaging that can reduce weight and space significantly. Designer engineers should know how the various types of flexible circuits work, their design capabilities, applications, and limits, to best integrate flexible circuits in design. They should work with a flexible-circuit manufacturer for guidance on material properties and limitations. Before investing valuable efforts and money in creating a functional flexible-circuit prototype, a mechanical sample should be test first to ensure that the flex circuit has the right form and fit. A mechanical sample can help to avoid installation problems or latent mechanical issues that lead to failures.},
annote = {Staying flexible},
author = {Finstad, M},
journal = {Machine Design},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {13},
pages = {91--92},
title = {{Staying flexible}},
url = {citeulike-article-id:3934605 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34547802652{\&}{\#}38 partnerID=40},
volume = {79},
year = {2007}
}
@article{Alsaedi2017,
author = {Alsaedi, Nasser and Burnap, Pete and Rana, Omer},
doi = {10.1145/2996183},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alsaedi, Burnap, Rana - 2017 - Can We Predict a Riot Disruptive Event Detection Using Twitter.pdf:pdf},
journal = {ACM Transactions on Internet Technology},
keywords = {twitter},
mendeley-tags = {twitter},
month = {mar},
number = {2},
pages = {Article 18},
publisher = {ACM},
title = {{Can We Predict a Riot? Disruptive Event Detection Using Twitter}},
url = {http://dl.acm.org/citation.cfm?doid=3068849.2996183},
volume = {17},
year = {2017}
}
@article{Edwards2007,
abstract = {Including software testing practices in programming assignments has moved from a novel idea to accepted practice in recent years. Further, testing frameworks have spurred renewed interest in new approaches to automated grading, with some systems specifically aiming to give feedback on software testing skills. As more educators consider incorporating testing techniques in their own courses, lessons learned from using testing in the classroom as well as from using automated grading systems become more valuable. This paper summarizes experiences in using software testing in CS1- and CS2-level courses over the past three years. Among these experiences, this paper focuses on student perceptions of automated grading tools and how they might be addressed, approaches to designing project specifications, and strategies for providing meaningful feedback to students that can help improve their performance and reduce their frustration.},
annote = {Experiences using test-driven development with an automated grader},
author = {Edwards, S H and P{\'{e}}rez-Qui{\~{n}}ones, M A},
journal = {Journal of Computing Sciences in Colleges},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {44--50},
title = {{Experiences using test-driven development with an automated grader}},
url = {citeulike-article-id:3934598 http://portal.acm.org/citation.cfm?id=1181849.1181855},
volume = {22},
year = {2007}
}
@article{Zhang2008,
author = {Zhang, Hongyu},
doi = {10.1109/TSE.2007.70771},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2008 - On the Distribution of Software Faults.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Measurement applied to SQA and V{\&}{\#}x00026,Pareto analysis,Pareto principle,Product metrics,V,Weibull distribution,large software systems,software faults distribution},
language = {English},
month = {mar},
number = {2},
pages = {301--302},
publisher = {IEEE},
title = {{On the Distribution of Software Faults}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4407730},
volume = {34},
year = {2008}
}
@inproceedings{Layman2006d,
author = {Layman, Lucas},
booktitle = {International Doctoral Symposium on Empirical Software Engineering (IDoESE '06)},
keywords = {mypubs},
mendeley-tags = {mypubs},
title = {{Intelligent User Notifaction to Expedite Awareness of Fault Code}},
year = {2006}
}
@article{Sangwan2006,
abstract = {Test-driven development (TDD) is a key practice for agile developers because it involves writing test cases ahead of the code which can improve design. The TDD process works well for projects in which a collocated team develops a small to medium system, and the developers must take care of account for its focus on unit testing and its failure to address communication issues during system and integration testing. The developers must write codes and additional test cases with the TDD process. The TDD improves the code quality by identifying likely breaking points early, and can help with building a testing suite. TDD results in a high ratio of test to production code and therefore any system change is likely to affect the test code.},
annote = {Test-driven development in large projects},
author = {Sangwan, R S and Laplante, P A},
journal = {IT Professional},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {5},
pages = {25--29},
title = {{Test-driven development in large projects}},
url = {citeulike-article-id:3934777 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33750845106{\&}{\#}38 partnerID=40},
volume = {8},
year = {2006}
}
@incollection{Basili2005,
abstract = {Answering “macro-process” research issues – which require understanding how development processes fit or do not fit in different organizational systems and environments – requires families of related studies. While there are many sources of variation between development contexts, it is not clear a priori what specific variables influence the effectiveness of a process in a given context. These variables can only be discovered opportunistically, by comparing process effects from different environments and analyzing points of difference. In this paper, we illustrate this approach and the conclusions that can be drawn by presenting a family of studies on the subject of software defects and their behaviors – a key phenomenon for understanding macro-process issues. Specifically, we identify common “folklore,” i.e. widely accepted heuristics concerning how defects behave, and then build up a body of knowledge from empirical studies to refine the heuristics with information concerning the conditions under which they do and do not hold.},
address = {Berlin, Germany},
author = {Basili, Victor R and Shull, Forrest},
booktitle = {Lecture Notes in Computer Science},
keywords = {Journal},
month = {dec},
pages = {1--9},
publisher = {Springer},
title = {{Evolving Defect 'Folklore': A Cross-Study Analysis of Software Defect Behavior}},
url = {http://dx.doi.org/10.1007/11608035{\_}1},
volume = {3840},
year = {2005}
}
@article{Reza2008a,
author = {Reza, Hassan and Ogaard, Kirk and Malge, Amarnath},
doi = {10.1109/ITNG.2008.145},
isbn = {978-0-7695-3099-4},
journal = {Fifth International Conference on Information Technology: New Generations (itng 2008)},
month = {apr},
pages = {183--188},
publisher = {Ieee},
title = {{A Model Based Testing Technique to Test Web Applications Using Statecharts}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4492476},
year = {2008}
}
@inproceedings{Fernandez-Medina2013,
abstract = {The learning of programming presents many difficulties for students. Nowadays, a number of software tools are available that enable students in programming courses to develop and exercise their knowledge and skills. However, these tools do not examine their work or provide students with indications on their learning process. In this paper we introduce a learning approach for programming based on the analysis of students' mistakes during practical lessons in programming subjects. This approach makes use of compiler messages to analyse their quantity and semantic value, and report the individual and comparative learning progress. This approach is illustrated in practice by a case study conducted in a class of undergraduate students of computer science. This study makes it possible to provide an analytic representation of reflective learning practice, giving us a better understanding on programming learning processes. Copyright 2013 ACM.},
address = {Canterbury, England},
author = {Fernandez-Medina, Carlos and P{\'{e}}rez-P{\'{e}}rez, Juan Ram{\'{o}}n and {\'{A}}lvarez-Garc{\'{i}}a, V{\'{i}}ctor M. and {Del Puerto Paule-Ruiz}, M.},
booktitle = {Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
doi = {10.1145/2462476.2462496},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fernandez-Medina et al. - 2013 - Assistance in computer programming learning using educational data mining and learning analytics.pdf:pdf},
isbn = {9781450320788},
issn = {1942647X},
keywords = {E-learning,Eclipse plug-ins,Integrated Development Environment,Learning analytics,Programming errors,Programming language},
pages = {237--242},
publisher = {ACM Press},
title = {{Assistance in computer programming learning using educational data mining and learning analytics}},
url = {http://dl.acm.org/citation.cfm?doid=2462476.2462496},
year = {2013}
}
@article{Wilkinson2006,
abstract = {Time is an important aspect of all real world entities; temporal information is crucial in many computer-based applications. The Smalltalk community does not have a good model of the time domain. Smalltalk-80 and its commercial implementations provide only the classes Date and Time to model time domain entities. Squeak augmented the model with the abstractions Timespan, Year, Month and Week. These models fall short when complex situations of the time domain have to be programmed, forcing the programmers to create their own and repetitive solutions. In this paper, we present a model of the Gregorian Calendar based on a metaphor that maps time entities into points of lines, each line with its own resolution. The model addresses a great amount of functionality and reifies almost all the Gregorian Calendar entities. It allows programmers to design and program time related issues better than current time domain implementations, and in a more natural way. {\^{A}}{\textcopyright} 2005 Elsevier Ltd. All rights reserved.},
annote = {A point based model of the Gregorian Calendar},
author = {Wilkinson, H and Prieto, M and Romeo, L},
journal = {Computer Languages, Systems and Structures},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2-3},
pages = {157--183},
title = {{A point based model of the Gregorian Calendar}},
url = {citeulike-article-id:3934838 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-29244482233{\&}{\#}38 partnerID=40},
volume = {32},
year = {2006}
}
@article{Sargent2004,
author = {Sargent, John},
chapter = {1},
number = {3},
pages = {1--21},
title = {{An Overview of Past and Projected Employment Changes in the Professional IT Occupations}},
volume = {16},
year = {2004}
}
@inproceedings{Schulte2007,
abstract = {Pex takes test-driven development to the next level. Pex analyzes .NET applications. From a parameterized unit test, which serves as a specification, it automatically produces traditional unit tests cases with high code coverage. Moreover, when a generated test fails, Pex can often suggest a bug fix. To do so Pex performs a systematic program analysis (similar to path bounded model-checking). It records detailed execution traces of test cases. Pex learns the program behavior from the traces, and a constraint solver produces new test cases with different behavior. The result is a minimal test suite with maximal code coverage. For information about Pex, see http://research.microsoft.com/Pex/ . {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Pex - An intelligent assistant for rigorous developer testing},
author = {Schulte, W and Tillmann, N and {De Halleux}, P},
booktitle = {Proceedings of the IEEE International Conference on Engineering of Complex Computer Systems, ICECCS},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {161},
title = {{Pex - An intelligent assistant for rigorous developer testing}},
url = {citeulike-article-id:3934783 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-46749117978{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Nagappan2010,
address = {San Jose, CA},
author = {Nagappan, Nachiappan and Zeller, Andreas and Zimmermann, Thomas and Herzig, Kim and Murphy, Brendan},
booktitle = {2010 IEEE 21st International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.2010.25},
isbn = {978-1-4244-9056-1},
month = {nov},
pages = {309--318},
publisher = {IEEE},
title = {{Change Bursts as Defect Predictors}},
url = {http://ieeexplore.ieee.org/document/5635057/},
year = {2010}
}
@article{Baumeister1996,
abstract = {The major patterns of self-regulatory failure are reviewed. Underregulation occurs because of deficient standards, inadequate monitoring, or inadequate strength. Misregulation occurs because of false assumptions or misdirected efforts, especially an unwarranted emphasis on emotion. The evidence supports a strength (limited resource) model of self-regulation and suggests that people often acquiesce in losing control. Loss of control of attention, failure of transcendence, and various lapse-activated causes all contribute to regulatory failure.
The major patterns of self-regulatory failure are reviewed. Underregulation occurs because of deficient standards, inadequate monitoring, or inadequate strength. Misregulation occurs because of false assumptions or misdirected efforts, especially an unwarranted emphasis on emotion. The evidence supports a strength (limited resource) model of self-regulation and suggests that people often acquiesce in losing control. Loss of control of attention, failure of transcendence, and various lapse-activated causes all contribute to regulatory failure.},
author = {Baumeister, Roy F. and Heatherton, Todd F.},
doi = {10.1207/s15327965pli0701_1},
issn = {1047-840X},
journal = {Psychological Inquiry},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jan},
number = {1},
pages = {1--15},
publisher = {Routledge},
title = {{Self-Regulation Failure: An Overview}},
url = {http://dx.doi.org/10.1207/s15327965pli0701{\_}1},
volume = {7},
year = {1996}
}
@article{Sutherland2007,
abstract = {Projects combining agile methods with CMMI1 are more successful in producing higher quality software that more effectively meets customer needs at a faster pace. Systematic Software Engineering works at CMMI level 5 and uses Lean Software Development as a driver for optimizing software processes. Early pilot projects at Systematic showed productivity on Scrum teams almost twice that of traditional teams. Other projects demonstrated a story based test driven approach to software development reduced defects found during final test by 40{\%} We assert that Scrum and CMMI together bring a more powerful combination of adaptability and predictability than either one alone and suggest how other companies can combine them. {\^{A}}{\textcopyright} 2007 IEEE. SE  - Proceedings - AGILE 2007},
annote = {doi: 10.1109/AGILE.2007.52 Art. No.: 4293608 Source: Scopus Export Date: 8 January 2009 Cited By (since 1996): 1},
author = {Sutherland, J and Jakobsen, C R and Johnson, K},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Scrum and CMMI level 5: The magic potion for code warriors}},
url = {citeulike-article-id:3934810 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-46449126028{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Takagi1995,
address = {Toulouse, France},
author = {Takagi, Yasunari and Tanaka, Tashifumi and Niihara, Naoki and Sakamoto, Keishi and Kusumoto, Sinji and Kikuno, Tohru and {Takagi  T. Tanaka, N. Niihara, K. Sakamoto, S. Kusumoto, and T. Kikuno}, Y},
booktitle = {Proceedings of the Sixth International Symposium on Software Reliability Engineering},
keywords = {code review,design review,inspection},
pages = {34--39},
publisher = {IEEE Computer Society Press},
title = {{Analysis of review's effectiveness based on software metrics}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Analysis+of+Review's+Effectiveness+Based+on+Software+Metrics{\#}0},
year = {1995}
}
@article{Deng2004a,
author = {Deng, D. and Sheu, P.C.-Y. and Wang, T. and a.K. Onoma},
doi = {10.1109/MMSE.2004.51},
isbn = {0-7695-2217-3},
journal = {IEEE Sixth International Symposium on Multimedia Software Engineering},
pages = {278--285},
publisher = {Ieee},
title = {{Model-Based Testing and Maintenance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1376673},
year = {2004}
}
@book{Hutchison1973a,
author = {Hutchison, David and Mitchell, John C},
isbn = {3540253041},
title = {{Theoretical Aspects of Computing – ICTAC 2004}},
year = {1973}
}
@inproceedings{Dionne1996,
author = {Dionne, C and Feeley, M and Desbiens, J},
booktitle = {Int'l Conf. on Parallel and Distributed Processing Techniques and Applications},
pages = {203--214},
title = {{A taxonomy of distributed debuggers based on execution replay}},
year = {1996}
}
@inproceedings{LaToza2006,
address = {Shanghai, China},
author = {LaToza, Thomas D. and Venolia, Gina and DeLine, Robert},
booktitle = {Proceedings of the 28th international conference on Software engineering},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/LaToza, Venolia, DeLine - 2006 - Maintaining mental models a study of developer work habits.pdf:pdf},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {492--501},
publisher = {ACM New York, NY, USA},
title = {{Maintaining mental models: a study of developer work habits}},
url = {http://portal.acm.org/citation.cfm?id=1134355},
year = {2006}
}
@inproceedings{6060007,
abstract = {Recognition-based graphical password (RBGP) schemes are not easily compared in terms of security. Current research uses many different measures which results in confusion as to whether RBGP schemes are secure against guessing and capture attacks. If it were possible to measure all RBGP schemes in a common way it would provide an easy comparison between them, allowing selection of the most secure design. This paper presents a discussion of potential attacks against recognition-based graphical password (RBGP) authentication schemes. As a result of this examination a preliminary measure of the security of a recognition-based scheme is presented. The security measure is a 4-tuple based on distractor selection, shoulder surfing, intersection and replay attacks. It is aimed to be an initial proposal and is designed in a way which is extensible and adjustable as further research in the area develops. Finally, an example is provided by application to the PassFaces scheme.},
author = {English, Rosanne and Poet, Ron},
booktitle = {2011 5th International Conference on Network and System Security},
doi = {10.1109/ICNSS.2011.6060007},
isbn = {978-1-4577-0458-1},
keywords = {PassFaces scheme,authentication schemes,distractor},
month = {sep},
pages = {239--243},
publisher = {Ieee},
title = {{Towards a metric for recognition-based graphical password security}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6060007},
year = {2011}
}
@inproceedings{ROS2009,
author = {Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully B and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew Y},
booktitle = {ICRA Workshop on Open Source Software},
title = {{ROS: an open-source Robot Operating System}},
year = {2009}
}
@techreport{Briasco-Stewart2018,
address = {Cambridge, MA, USA},
author = {Briasco-Stewart, Samantha},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Briasco-Stewart - 2018 - Making Python Easier to Learn with Improved Syntax Error Reporting.pdf:pdf},
institution = {Massachusetts Institute of Techonology},
keywords = {Electrical Engineering and Computer Science.,Thesis},
publisher = {Massachusetts Institute of Technology},
title = {{Making Python Easier to Learn with Improved Syntax Error Reporting}},
url = {https://dspace.mit.edu/handle/1721.1/119778},
year = {2018}
}
@book{Bosch2010,
abstract = {This paper presents an analysis of the unit testing approach developed and used by the Core Flight Software (CFS) product line team at the NASA GSFC. The goal of the analysis is to understand, review, and recommend strategies for improving the existing unit testing infrastructure as well as to capture lessons learned and best practices that can be used by other product line teams for their unit testing. The CFS unit testing framework is designed and implemented as a set of variation points, and thus testing support is built into the product line architecture. The analysis found that the CFS unit testing approach has many practical and good solutions that are worth considering when deciding how to design the testing architecture for a product line, which are documented in this paper along with some suggested improvements.},
address = {Berlin, Heidelberg},
author = {Bosch, Jan and Lee, Jaejoon and Ganesan, Dharmalingam and Lindvall, Mikael and McComas, David and Bartholomew, Maureen and Slegel, Steve and Medina, Barbara},
doi = {10.1007/978-3-642-15579-6},
editor = {Bosch, Jan and Lee, Jaejoon},
isbn = {978-3-642-15578-9},
pages = {256--270--270},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Software Product Lines: Going Beyond}},
url = {http://www.springerlink.com/content/0t6184l846661p63/},
volume = {6287},
year = {2010}
}
@inproceedings{Horvitz1999,
address = {Stockholm, Sweden},
author = {Horvitz, Eric and Jacobs, Andy and Hovel, David},
keywords = {alert,alerts,attention-centric,attention-sensitive,hci,interruption},
title = {{Attention-sensitive Alerting}},
year = {1999}
}
@misc{Klischewski2008,
abstract = {In this paper we present a framework for identifying the test focus and test objectives based on the assumption that automatic information processing based on encoded meaning is the core of semantic e-government applications to be evaluated. Taking into account test strategies from software engineering and IT project management as well as different stakeholder perspectives, we identify possible test instruments. Several of these instruments have been applied in the Access-eGov project and we discuss the experience gathered in view of the newly developed framework in order to identify lessons learnt as well as to point to future research. The contribution of the paper is a portfolio of test strategies suggesting certain instruments to be applied from a systems view, agent view, and user view. We conclude that improving semantic e-government applications could be supported through applying a test-first approach, e.g. through providing an e-government test agent to be used in test labs or within the development process. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2008.},
annote = {Test strategies for evaluation of semantic eGovernment applications},
author = {Klischewski, R and Ukena, S},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {291--302},
title = {{Test strategies for evaluation of semantic eGovernment applications}},
url = {citeulike-article-id:3934677 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-52149108435{\&}{\#}38 partnerID=40},
volume = {5184 LNCS},
year = {2008}
}
@inproceedings{Cohoon1999,
abstract = {Although many computer professionals believe that inherent or deeply ingrained gender differences make women less suited to the study and practice of computer science, the results reported here demonstrate that female underrepresentation in computer science could be avoided. Women can and do succeed in computer science (CS) when conditions do not deter them. The variation that occurs in women's participation rates demonstrates that many women succeed as computer scientists in certain times and settings. Conditions affecting female retention in undergraduate computer science are identified in this article.},
address = {New Orleans, LA, USA},
annote = {attrition rates by gender vary across cs departments},
author = {Cohoon, J. Mc Grath},
booktitle = {SIGCSE 1999 - Proceedings of the 13th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/384266.299753},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohoon - 1999 - Departmental differences can point the way to improving female retention in computer science.pdf:pdf},
isbn = {9781581130850},
issn = {00978418},
keywords = {Attrition,CS2,Gender,Retention,Undergraduate education},
mendeley-tags = {CS2},
month = {mar},
pages = {198--202},
publisher = {ACM},
title = {{Departmental differences can point the way to improving female retention in computer science}},
url = {http://portal.acm.org/citation.cfm?doid=384266.299753},
year = {1999}
}
@techreport{PewResearchCenter2013,
address = {Washington, D. C.},
author = {Madden, Mary and Lenhart, Amanda and Duggan, Maeve and Cortesi, Sandra and Gasser, Urs},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Madden et al. - 2013 - Teens and Technology 2013.pdf:pdf},
institution = {Pew Research Center},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{Teens and Technology}},
url = {http://www.pewinternet.org/{~}/media//Files/Reports/2013/PIP{\_}TeensandTechnology2013.pdf},
year = {2013}
}
@inproceedings{Pancur2003,
abstract = {Test-Driven Development (TDD) is one of the core practices in increasingly popular agile software development methodologies (e.g. Extreme Programming - XP). In TDD, automated tests are the driving force in development and they are always written prior to the code they test. Since tests are so important, their execution must be automated and run as often as possible. In this paper, we discuss and compare different frameworks, framework extensions and tools for TDD on the rival platforms: Sun's Java and Microsoft's .NET. We look at the current Open Source and commercial integrated development environments (IDEs) for both platforms and evaluate their readiness for TDD style development. We provide recommendations and links to resources for tools and unit-testing frameworks that worked best for us, were stable and relatively bug free.},
annote = {Comparison of frameworks and tools for test-driven development},
author = {Pan{\v{c}}ur, M and Ciglari{\v{c}}, M and Trampu{\v{s}}, M and Vidmar, T},
booktitle = {IASTED International Multi-Conference on Applied Informatics},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {980--985},
title = {{Comparison of frameworks and tools for test-driven development}},
url = {citeulike-article-id:3934748 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-1442302382{\&}{\#}38 partnerID=40},
volume = {21},
year = {2003}
}
@inproceedings{Trendowicz2006,
address = {Shangha, China},
author = {Trendowicz, Adam and Heidrich, Jens and M{\"{u}}nch, J{\"{u}}rgen and Ishigai, Yasushi and Yokoyama, Kenji and Kikuchi, Nahomi},
pages = {331--340},
title = {{Development of a hybrid cost Estimation Model in an Iterative Manner}},
year = {2006}
}
@inproceedings{4725337,
abstract = {Network scans visualization provides very effective means for to detection large scale network scans. Many visualization methods have been developed to monitor network traffic, but all the techniques or tools still heavily rely on human detection. They seldom consider the importance of network event characteristics to the network data visualization, and cannot detect slow scans, hidden scans etc. In this paper a visual interactive network scans detection system called ScanViewer is designed to represent traffic activities that reside in network flows and their patterns. The ScanViewer combines the characteristics of network scan with novel visual structures, and utilizes a set of different visual concepts to map the collected datagram to the graphs that emphasize their patterns. Additionally, a new tool named localport is designed for to capture large-scale ports information. The experiments show that ScanViewer can not only detect network scans, port scans, distributed port scans, but also can detect the hidden scans etc.},
author = {Jiawan, Zhang and Liang, Li and Liangfu, Lu and Ning, Zhou},
booktitle = {Security Technology, 2008. SECTECH '08. International Conference on},
doi = {10.1109/SecTech.2008.47},
keywords = {monitor network traffic,network data visualization},
pages = {23--26},
title = {{A Novel Visualization Approach for Efficient Network Scans Detection}},
year = {2008}
}
@article{Hundhausen2017,
abstract = {In recent years, learning process data have become increasingly easy to collect through computer-based learning environments. This has led to increased interest in the field of learning analytics, which is concerned with leveraging learning process data in order to better understand, and ultimately to improve, teaching and learning. In computing education, the logical place to collect learning process data is through integrated development environments (IDEs), where computing students typically spend large amounts of time working on programming assignments. While the primary purpose of IDEs is to support computer programming, they might also be used as a mechanism for delivering learning interventions designed to enhance student learning. The possibility of using IDEs both to collect learning process data, and to strategically intervene in the learning process, suggests an exciting design space for computing education research: that of IDE-based learning analytics. In order to facilitate the systematic exploration of this design space, we present an IDE-based data analytics process model with four primary activities: (1) Collect data, (2) Analyze data, (3) Design intervention, and (4) Deliver intervention. For each activity, we identify key design dimensions and review relevant computing education literature. To provide guidance on designing effective interventions, we describe four relevant learning theories, and consider their implications for design. Based on our review, we present a call-to-action for future research into IDE-based learning analytics.},
author = {Hundhausen, C. D. and Olivares, D. M. and Carter, A. S.},
doi = {10.1145/3105759},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hundhausen, Olivares, Carter - 2017 - IDE-based learning analytics for computing education A process model, critical review, and researc.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {Learning analytics,Learning interventions,Learning process data},
month = {aug},
number = {3},
pages = {1--26},
publisher = {Association for Computing Machinery},
title = {{IDE-based learning analytics for computing education: A process model, critical review, and research agenda}},
url = {http://dl.acm.org/citation.cfm?doid=3135995.3105759},
volume = {17},
year = {2017}
}
@inproceedings{Layman2013a,
abstract = {Amazon's Mechanical Turk is a crowdsourcing technology that enables requesters to create tasks to be completed by human agents in exchange for compensation. Researchers in computer science have successfully used this service to quickly reach large numbers of subjects for a relatively low cost. However, the Mechanical Turk's model and policies introduce several experimental limitations and threats that must be controlled. In this short paper, we describe limitations imposed using Amazon's Mechanical Turk during an experiment on cyber-attack investigation techniques. While the experiment was successful, we were forced to change our experimental design and had to recover from some costly mistakes. The goal of this short paper is to identify these limitations and pitfalls and provide eight considerations for experimental design so that other researchers can maximize the benefits of using the Mechanical Turk as a research platform.},
address = {Baltimore, Maryland, USA},
author = {Layman, Lucas and Sigurdsson, Gunnar},
booktitle = {Proceedings of the 7th International Symposium on Empirical Software Engineering and Measurement (ESEM 2013)},
keywords = {amazon,experimentation,mechanical turk,mypubs,study design,user studies},
mendeley-tags = {amazon,experimentation,mechanical turk,mypubs,study design,user studies},
pages = {275--278},
title = {{Using Amazon's Mechanical Turk for User Studies: Eight Things You Need to Know}},
year = {2013}
}
@article{Han2007,
abstract = {Risk management and performance enhancement have always been the focus of software project management studies. The present paper shows the findings from an empirical study based on 115 software projects on analyzing the probability of occurrence and impact of the six dimensions comprising 27 software risks on project performance. The MANOVA analysis revealed that the probability of occurrence and composite impact have significant differences on six risk dimensions. Moreover, it indicated that no association between the probability of occurrence and composite impact among the six risk dimensions exists and hence, it is a crucial consideration for project managers when deciding the suitable risk management strategy. A pattern analysis of risks across high, medium, and low-performance software projects also showed that (1) the “requirement” risk dimension is the primary area among the six risk dimensions regardless of whether the project performance belongs to high, medium, or low; (2) for medium-performance software projects, project managers, aside from giving importance to “requirement risk”, must also continually monitor and control the “planning and control” and the “project complexity” risks so that the project performance can be improved; and, (3) improper management of the “team”, “requirement”, and “planning and control” risks are the primary factors contributing to a low-performance project.},
author = {Han, Wen-Ming and Huang, Sun-Jen},
doi = {10.1016/j.jss.2006.04.030},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {project performance,risk exposure,software project management,software risk management},
month = {jan},
number = {1},
pages = {42--50},
title = {{An empirical analysis of risk components and performance on software projects}},
url = {http://dx.doi.org/10.1016/j.jss.2006.04.030},
volume = {80},
year = {2007}
}
@inproceedings{4373483,
abstract = {On the road and during tactical maneuvers, the Blue Force must maintain situational awareness to effectively react and respond. The Tactical Network, or TacNet, is a mobile ad hoc communications network developed by Sandia National Laboratories to provide Blue Force personnel with secure access to critical data, such as real-time maps of resource positions. During development of the system, the Sandia team addressed a variety of issues, including the need for (a) mobile communications without fixed infrastructure and (b) security features, e.g., an access control list. The team considered commercial-off-the-shelf products, but determined that a semi-customized system would better suit its requirements. The final product, TacNet, a field-tested and proven mobile network, incorporates two major systems: (1) an In-Vehicle System, including a graphical user interface, and (2) a Dismounted Solution, also known as Tracker. TacNet employs a line-of-sight mesh radio network, which is self-forming, self-healing, and multi-hopping. Both the In- Vehicle and Tracker systems can be applied in combination or separately to a variety of purposes, including real-time training analysis, targeting capability, and friend or foe identification. This paper describes the development process for TacNet and its future potential, such as extending line-of-sight through unmanned aerial vehicles.},
author = {Riblett, Loren E. and Wiseman, James M.},
booktitle = {2007 41st Annual IEEE International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2007.4373483},
isbn = {978-1-4244-1129-0},
keywords = {Blue Force personnel,Sandia National Laboratories},
month = {oct},
pages = {156--162},
publisher = {Ieee},
title = {{TACNET: Mobile ad hoc Secure Communications Network}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4373483},
year = {2007}
}
@inproceedings{Allert2004,
address = {Joensuu, Finland},
author = {Allert, J},
keywords = {learning styles},
pages = {385--389},
title = {{Learning Style and Factors Contributing to Success in an Introductory Computer Science Course}},
year = {2004}
}
@inproceedings{Nagappan2003b,
address = {Reno},
author = {Nagappan, Nachiappan and Williams, Laurie and Ferzli, Miriam and Yang, Kai and Wiebe, Eric and Miller, Carol and Balik, Suzanne},
pages = {359--362},
title = {{Improving the CS1 Experience with Pair Programming}},
year = {2003}
}
@inproceedings{Yen2013,
address = {New York, New York, USA},
author = {Yen, Ting-Fang and Oprea, Alina and Onarlioglu, Kaan and Leetham, Todd and Robertson, William and Juels, Ari and Kirda, Engin},
booktitle = {Proceedings of the 29th Annual Computer Security Applications Conference on - ACSAC '13},
doi = {10.1145/2523649.2523670},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yen et al. - 2013 - Beehive.pdf:pdf},
isbn = {9781450320153},
month = {dec},
pages = {199--208},
publisher = {ACM Press},
title = {{Beehive}},
url = {http://dl.acm.org/citation.cfm?id=2523649.2523670},
year = {2013}
}
@article{Posner1966,
author = {Posner, Michael I and Konick, Andrew F},
keywords = {interruption,memory,psychology},
number = {2},
pages = {221--231},
title = {{On the Role of Interference in Short-term Retention}},
volume = {72},
year = {1966}
}
@inproceedings{Teiniker2003,
abstract = {Short product cycles and rapidly changing requirements are increasingly forcing developers to use agile development strategies like extreme programming and test-driven development. At the same time, hierarchical software design strategies such as component based software engineering are becoming necessary to overcome increasing software complexity. In this paper, we present a test-driven component development framework that encapsulates test code in a mirror component for automated testing during development, and as an executable semantic to simplify assessment and to increase trustworthiness of software systems.},
annote = {A Test-Driven Component Development Framework based on the CORBA Component Model},
author = {Teiniker, E and Mitterdorfer, S and Johnson, L M and Kreiner, C and Kov{\'{a}}cs, Z and Weiss, R},
booktitle = {Proceedings - IEEE Computer Society's International Computer Software and Applications Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {400--405},
title = {{A Test-Driven Component Development Framework based on the CORBA Component Model}},
url = {citeulike-article-id:3934813 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0345529067{\&}{\#}38 partnerID=40},
year = {2003}
}
@inproceedings{5375542,
abstract = {The goal of cyber security visualization is to help analysts increase the safety and soundness of our digital infrastructures by providing effective tools and workspaces. Visualization researchers must make visual tools more usable and compelling than the text-based tools that currently dominate cyber analysts' tool chests. A cyber analytics work environment should enable multiple, simultaneous investigations and information foraging, as well as provide a solution space for organizing data. We describe our study of cyber-security professionals and visualizations in a large, high-resolution display work environment and the analytic tasks this environment can support. We articulate a set of design principles for usable cyber analytic workspaces that our studies have brought to light. Finally, we present prototypes designed to meet our guidelines and a usability evaluation of the environment.},
author = {Fink, G A and North, C L and Endert, A and Rose, S},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375542},
keywords = {cyber analytics work environment,cyber security vi},
pages = {45--56},
title = {{Visualizing cyber security: Usable workspaces}},
year = {2009}
}
@inproceedings{Layman2010,
abstract = {Our current research is focused on identifying system engineering approaches that address four key development challenges in a tightly constrained, rapid reaction environment: 1) changing and emerging requirements; 2) conflicting stakeholder priorities; 3) concurrent sustainment and development activities; and 4) integration of independently evolving components. We are building a concept map of the key elements that form a strategic bridge between development challenges and the specific methods, processes, and tools that successfully address those challenges. In this paper, we present our methodology for constructing a robust mapping that incorporates interviews, surveys, and rigorous analysis methods. We summarize the results from interviews with sponsor personnel, the results of a best practices survey of 116 professionals, and qualitative analysis of the survey responses. {\textcopyright}2010 IEEE.},
address = {San Diego, CA},
author = {Layman, Lucas and Shull, Forrest and Componation, Paul and O'Brien, S. and Sabados, D. and Carrigy, Anne and Turner, Richard and Brien, Sue O and Carrigy, Anne and Turner, Richard},
booktitle = {Proceedings of the 4th Annual IEEE International Systems Conference},
doi = {http://dx.doi.org/10.1109/SYSTEMS.2010.5482336},
isbn = {9781424458837},
keywords = {Methodology,Qualitative analysis,Software engineering,Survey,Systems engineering,modeling,mypubs,systems engineering},
mendeley-tags = {mypubs},
pages = {294--299},
title = {{A Methodology for Mapping System Engineering Challenges to Recommended Approaches}},
year = {2010}
}
@inproceedings{4689119,
abstract = {Intrusion detection forms an indispensable component of cyber security. To keep pace with the growing trends of blackhat community, there is an urgent need to replace single layer detection technology with multi layer detection. Our practical experiences depicted the retrieval of attack evidences from system traces. This paper signifies the integration of host-based intrusion detection system (HIDS) with already existing network based detection on Gen 3 Honeynet architecture. The integration procedure involves the stealth mode operation of HIDS sensor, code organization to generate HIDS alerts in a standard format with requisite network parameters, enhancing the functionality of data fusion to pipeline HIDS sensor with other data sensors for real-time operation and correlation with established network sessions, and further visualization on graphical analysis console. The benefits of new Honeynet architecture have been established. The results in the form of statistical trend distribution and percentage reduction of Honeynet data have been presented.},
author = {Bhatia, J S and Sehgal, R and Bhushan, Bharat and Kaur, H},
booktitle = {New Technologies, Mobility and Security, 2008. NTMS '08.},
doi = {10.1109/NTMS.2008.ECP.65},
keywords = {Gen 3 Honeynet architecture,blackhat community,cod},
pages = {1--5},
title = {{Multi Layer Cyber Attack Detection through Honeynet}},
year = {2008}
}
@inproceedings{Bhattacharya2012,
address = {Orlando, FL},
author = {Bhattacharya, Devipsita and Ram, Sudha},
booktitle = {22nd Workshop on Information Technologies and Systems (WITS 2012)},
keywords = {twitter},
mendeley-tags = {twitter},
pages = {205--210},
title = {{News article propagation on twitter based on network measures - An exploratory analysis}},
url = {https://arizona.pure.elsevier.com/en/publications/news-article-propagation-on-twitter-based-on-network-measures-an-},
year = {2012}
}
@article{Shull2010,
author = {Shull, Forrest and Feldmann, Raimund L. and Seaman, Carolyn B. and Regardie, Myrna and Godfrey, Sally},
doi = {http://dx.doi.org/10.1007/s11334-010-0132-1},
journal = {Innovations in Systems and Software Engineering},
title = {{Fully Employing Software Inspections Data}},
url = {http://dx.doi.org/10.1007/s11334-010-0132-1},
year = {2010}
}
@techreport{NASA2008,
address = {NASA-HDBK-8739.18},
author = {NASA and {National Aeronautics and Space Administration}},
institution = {NASA},
number = {NASA Office of Safety and Missions Assurance},
title = {{Procedural Handbook for NASA Program and Project Management of Problems, Nonconformances, and Anomalies}},
url = {NASA-HDBK-8739.18 http://www.hq.nasa.gov/office/codeq/doctree/HDBK873918.pdf},
year = {2008}
}
@article{Yang2011a,
author = {Yang, Rui and Chen, Zhenyu and Xu, Baowen and Wong, W. Eric and Zhang, Jie},
doi = {10.1109/HASE.2011.12},
isbn = {978-1-4673-0107-7},
journal = {2011 IEEE 13th International Symposium on High-Assurance Systems Engineering},
keywords = {- test case generation,efsm model-based testing,executable model,path feasibility analysis,test oracle},
month = {nov},
pages = {17--24},
publisher = {Ieee},
title = {{Improve the Effectiveness of Test Case Generation on EFSM via Automatic Path Feasibility Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6113868},
year = {2011}
}
@article{Sokenou,
author = {Sokenou, Dehla},
title = {{Generating Test Sequences from UML Sequence Diagrams and State Diagrams}}
}
@inproceedings{Cubranic,
address = {Banff, Alberta, Canada},
author = {{\v{C}}ubrani{\'{c}}, Davor},
booktitle = {Proc. of the 16th Int'l Conf on Software Engineering {\&} Knowledge Engineering (SEKE '04)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/{\v{C}}ubrani{\'{c}} - 2004 - Automatic bug triage using text categorization.pdf:pdf},
pages = {92--97},
title = {{Automatic bug triage using text categorization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.6144},
year = {2004}
}
@inproceedings{Brown2018,
abstract = {The Blackbox project has been collecting programming activity data from users of BlueJ (a novice-targeted Java development environment) for nearly five years. The resulting dataset of more than two terabytes of data has been made available to interested researchers from the outset. In this paper, we assess the impact of the Blackbox project: we perform a mapping study to assess eighteen publications which have made use of the Blackbox data, and we report on the advantages and difficulties experienced by researchers working with this data, collected via a survey. We find that Blackbox has enabled pieces of research which otherwise would not have been possible, but there remain technical challenges in the analysis. Some of these - but not all - relate to the scale of the data. We provide suggestions for the future use of Blackbox, and reflections on the role of such data collection projects in programming research.},
address = {New York, NY, USA},
author = {Brown, Neil C.C. and Altadmri, Amjad and Sentance, Sue and K{\"{o}}lling, Michael},
booktitle = {ICER 2018 - Proceedings of the 2018 ACM Conference on International Computing Education Research},
doi = {10.1145/3230977.3230991},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bustamante - 2005 - A signal detection analysis of the effects of workload, task-critical and likelihood information on human alarm resp.pdf:pdf},
isbn = {9781450356282},
keywords = {Blackbox,Mapping Study,Shared Data},
month = {aug},
pages = {196--204},
publisher = {Association for Computing Machinery, Inc},
title = {{Blackbox, five years on: An evaluation of a large-scale programming data collection project}},
url = {https://dl.acm.org/doi/10.1145/3230977.3230991},
year = {2018}
}
@article{Peterson1994,
author = {Peterson, Robert A.},
journal = {Journal of Consumer Research},
number = {2},
pages = {381--391},
title = {{A Meta-Analysis of Cronbach's Coefficient Alpha}},
url = {http://www.journals.uchicago.edu/doi/abs/10.1086/209405},
volume = {21},
year = {1994}
}
@incollection{Pennington1990,
abstract = {Comptuer programming and other design tasks have often been characterized as a set on non-interacting subtasks. In principle, it may be possible to separate these ubtasks, but in practice there are substantial interations between them.  We argue that this is a fundamental feature or proframming deriving from the cognitive characteristics of the subtasks, teh high uncertainty in programmign environments, and the social nature of hte environments in which complex software development takes place.},
address = {New York, NY},
annote = {Date suggest that there is some amount of alternation between levels of planning as early decision have implications for later steps and later steps may call into question some aspects of earlier decompositions. (p.51)},
author = {Pennington, N and Grabowksi, B and Hoc, J.-M. and Green, T R G and Samurcay, R and Gilmore, D J and Gaines, B R and Monk, A},
keywords = {design,psychology,survey},
pages = {45--62},
publisher = {Harcourt Brace Jovanovich},
title = {{The Tasks of Programming}},
year = {1990}
}
@techreport{Burton1999,
abstract = {This report describes an automated method of unit test design based on requirements specified in a subset of the statechart notation. The behaviour under test is first extracted from the requirements and specified in the Z notation. Existing methods and tools are then applied to this specification to derive the tests. Using Z to model the requirements and specify the tests allows for a deductive approach to verifying test satisfiability, test result correctness and certain properties of the requirements. An examination of the specification coverage achieved by the tests is provided and the report concludes with an evaluation of the work to date and a set of directions for future work. SE  - REPORT-UNIVERSITY OF YORK DEPARTMENT OF COMPUTER SCIENCE YCS},
annote = {Towards Automated Unit Testing of Statechart Implementations},
author = {Burton, S},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Towards Automated Unit Testing of Statechart Implementations}},
url = {citeulike-article-id:3934562 {\#}},
year = {1999}
}
@book{Beck2005,
address = {New York},
annote = {From Duplicate 1 (Extreme Programming Explained: Embrace Change - Beck, Kent)

The quintessential XP book.

- Ch1: XP was conceived in resposne to risk management
- Ch2: Statement of: Test-driven development. Introduce pair programming. Integration and integration testing immediately follows development.
- Ch3: A flexible project management strategy has more economic beneifts. "Use XP when requirements are vague or changing."
- Ch4: Controlling scope is essential to controlling cost,quality, and time; hence, flexibilty in requirements is essential.
- Ch5:The techinal premise of ZP "If the cost of change rose slowly over time, you would act compeltely differently from how you do under the 
assumption that costs rise exponentially." Objects and message sending help make this possible.
- Ch6: XP is all about little corrections.
- Ch7: The four values of XP: communication, simplicity, feedback, and courage.
- Ch8: Central principles of XP: rapid feedback, assume simplicity, incremental change, embracing change, quality work.
- Ch9: Basic activities of development: coding, testing, listening, designing. Quote: "Code isn't swayed by the power and logic of rhetoric. Code isn't impressed by college degrees or large salaries. Code just sits there, happily doing exactly what you told it to do. If that isn't what you thought you told it to do, that's your problem."
- Ch10: Listing of the 12 XP practices.
- Ch11: How can this work? Dependencies of the practices on one another.
- Ch12: Management strategy.
- Ch13: Optimal facility design. Take control of the physical environment.
- Ch14: Dividing Business and Technical respoonsibility: business: scope of timing of releases, priorities of proposed features, exact scope of proposed features. development: time estimates, consequences of alternatives, development process, set of practices.
- Ch15: Planning strategy and the planning game.
- Ch16: Development strategy. Continuous integration should occur every couple of horus and is made possible by a fast integration/build/test cycle and a reasonably complete test suite. Pair programming - forces you to maintain the other processes by peer-watching. Collective code onwership.
- Ch17: Design strategy. Use the simplest thing that can possibly work. Do not guess at the future, since it almost certainly will change. Definition of "What is simplest?"
- Ch18: Testing strategy. Tests must be automatic and independent of one another. Essential to keep programmer-written tests running at 100{\%}. An XP team should have a tester to translate customer functionality requirements into tests.
- Ch21: Lifecycle of an XP project.},
author = {Beck, Kent},
edition = {Second},
isbn = {0-201-61641-6},
keywords = {XP,beck,extreme programming},
publisher = {Addison-Wesley},
title = {{Extreme Programming Explained: Embrace Change}},
year = {2000}
}
@article{Austing1979,
abstract = {Contained in this report are the recommendations for the undergraduate degree program in Computer Science of the Curriculum Committee on Computer Science (C3S) of the Association for Computing Machinery (ACM). The core curriculum common to all computer science undergraduate programs is presented in terms of elementary level topics and courses, and intermediate level courses. Elective courses, used to round out an undergraduate program, are then discussed, and the entire program including the computer science component and other material is presented. Issues related to undergraduate computer science education, such as service courses, supporting areas, continuing education, facilities, staff, and articulation are presented.},
author = {Austing, Richard H. and Barnes, Bruce H. and Bonnette, Della T. and Engel, Gerald L. and Stokes, Gordon},
doi = {10.1145/359080.359083},
issn = {15577317},
journal = {Communications of the ACM},
keywords = {CS2,computer science curriculum,computer science education,computer science undergraduate degree programs,computer sciences courses,continuing education,service courses},
mendeley-tags = {CS2},
month = {mar},
number = {3},
pages = {147--166},
title = {{Curriculum '78: Recommendations for the Undergraduate Program in Computer Science-- a report of the ACM curriculum committee on computer science}},
volume = {22},
year = {1979}
}
@inproceedings{Ihantola2015a,
abstract = {Educational data mining and learning analytics promise better understanding of student behavior and knowledge, as well as new information on the tacit factors that contribute to student actions. This knowledge can be used to inform decisions related to course and tool design and pedagogy, and to further engage students and guide those at risk of failure. This working group report provides an overview of the body of knowledge regarding the use of educational data mining and learning analytics focused on the teaching and learning of programming. In a literature survey on mining students' programming processes for 2005-2015, we observe a significant increase in work related to the field. However, the majority of the studies focus on simplistic metric analysis and are conducted within a single institution and a single course. This indicates the existence of further avenues of research and a critical need for validation and replication to better understand the various contributing factors and the reasons why certain results occur. We introduce a novel taxonomy to analyse replicating studies and discuss the importance of replicating and reproducing previous work. We describe what is the state of the art in collecting and sharing programming data. To better understand the challenges involved in replicating or reproducing existing studies, we report our experiences from three case studies using programming data. Finally, we present a discussion of future directions for the education and research community.},
address = {Vilnius, Lithuania},
author = {Ihantola, Petri and Vihavainen, Arto and Ahadi, Alireza and Butler, Matthew and B{\"{o}}rstler, J{\"{u}}rgen and Edwards, Stephen H. and Isohanni, Essi and Korhonen, Ari and Petersen, Andrew and Rivers, Kelly and Rubio, Miguel {\'{A}}ngel and Sheard, Judy and Skupas, Bronius and Spacco, Jaime and Szabo, Claudia and Toll, Daniel},
booktitle = {ITiCSE-WGP 2015 - Proceedings of the 2015 ITiCSE Conference on Working Group Reports},
doi = {10.1145/2858796.2858798},
isbn = {9781450341462},
keywords = {Educational data mining,Learning analytics,Literature review,Programming,Replication},
month = {jul},
pages = {41--63},
publisher = {Association for Computing Machinery, Inc},
title = {{Educational data mining and learning analytics in programming: Literature review and case studies}},
year = {2015}
}
@article{Turhan2011,
author = {Turhan, Burak},
doi = {10.1007/s10664-011-9182-8},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {oct},
number = {1-2},
pages = {62--74},
title = {{On the dataset shift problem in software engineering prediction models}},
url = {http://link.springer.com/10.1007/s10664-011-9182-8},
volume = {17},
year = {2011}
}
@article{Holt2011,
author = {Holt, Thomas J. and Bossler, Adam M. and May, David C.},
doi = {10.1007/s12103-011-9117-3},
issn = {1066-2316},
journal = {American Journal of Criminal Justice},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jun},
number = {3},
pages = {378--395},
title = {{Low Self-Control, Deviant Peer Associations, and Juvenile Cyberdeviance}},
url = {http://link.springer.com/10.1007/s12103-011-9117-3},
volume = {37},
year = {2011}
}
@article{Gillie1989,
author = {Gillie, Tony and Broadbent, Donald},
keywords = {interruption,psychology},
number = {4},
pages = {243--250},
publisher = {Springer-Verlag},
title = {{What Makes Interruptions Disruptive? A Study of Length, Similarity, and Complexity?}},
volume = {50},
year = {1989}
}
@inproceedings{4373504,
abstract = {Currently, detecting potential threats in air passenger baggage heavily depends on the human examination of X-ray images of individual luggage items. In order to improve the performance of airport security personnel in searching images of air passenger luggage it is important first to understand fully the requirements of the demanding task. Here, an experiment is reported where eye movements of naive observers and screeners were recorded when they searched 30 X-ray images of air passenger luggage for potential terrorist threat items such as guns, knives and improvised explosive devices. Compared with novices, the advantages of the screeners were speed and accuracy in detecting threats. Eye position data revealed that screeners were faster to fixate on target areas and once they fixated on targets their hit rate was significantly higher. Most of the lEDs were missed by both naive observers and screeners due to interpretation errors which indicated the importance of training. Stimulus salience at the first fixation locations of naive observers and screeners was compared to investigate expertise development. It was found that experience did not change attention preference on stimuli properties at the beginning of the observers visual search. The implications and further studies are discussed.},
author = {Liu, Xi and Gale, Alastair and Song, Tao},
booktitle = {2007 41st Annual IEEE International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2007.4373504},
isbn = {978-1-4244-1129-0},
keywords = {X-ray images,air passenger luggage,airport securit},
month = {oct},
pages = {301--306},
publisher = {Ieee},
title = {{Detection of Terrorist Threats in Air Passenger Luggage: Expertise Development}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4373504},
year = {2007}
}
@inproceedings{Yang2012,
address = {New York, New York, USA},
author = {Yang, Liu and Boushehrinejadmoradi, Nader and Roy, Pallab and Ganapathy, Vinod and Iftode, Liviu},
booktitle = {Proceedings of the second ACM workshop on Security and privacy in smartphones and mobile devices - SPSM '12},
doi = {10.1145/2381934.2381940},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2012 - Enhancing Users' Comprehension of Android Permissions.pdf:pdf},
isbn = {9781450316668},
keywords = {agile,android permission,crowdsourcing,mobile applications,nsf,permission understanding,record and replay},
mendeley-tags = {agile,nsf},
month = {oct},
pages = {21},
publisher = {ACM Press},
title = {{Enhancing Users' Comprehension of Android Permissions}},
url = {http://dl.acm.org/citation.cfm?id=2381934.2381940},
year = {2012}
}
@inproceedings{Zimmermann2009,
address = {Amsterdam, The Netherlands},
author = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
booktitle = {Proc. of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Sym, on the Foundations of Software Engineering (ESEC/FSE)},
doi = {10.1145/1595696.1595713},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zimmermann et al. - 2009 - Cross-project defect prediction.pdf:pdf;:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zimmermann et al. - 2009 - Cross-project defect prediction(2).pdf:pdf},
isbn = {9781605580012},
keywords = {churn,cross-project,decision trees,defect prediction,logistic regression,prediction quality},
mendeley-tags = {churn},
month = {aug},
pages = {91--100},
publisher = {ACM Press},
title = {{Cross-project defect prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1595696.1595713 http://dl.acm.org/citation.cfm?id=1595696.1595713},
year = {2009}
}
@book{Box2004,
address = {Cambridge, UK},
author = {Box-Steffensmeier, Janet M. and Jones, Bradford S.},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Cambridge University Press},
title = {{Event history modeling: A guide for social scientists}},
year = {2004}
}
@article{5492194,
abstract = {We introduce and evaluate various methods for purely automated attacks against PassPoints-style graphical passwords. For generating these attacks, we introduce a graph-based algorithm to efficiently create dictionaries based on heuristics such as click-order patterns (e.g., five points all along a line). Some of our methods combine click-order heuristics with focus-of-attention scan-paths generated from a computational model of visual attention, yielding significantly better automated attacks than previous work. One resulting automated attack finds 7{\%}-16{\%} of passwords for two representative images using dictionaries of approximately 226 entries (where the full password space is 243). Relaxing click-order patterns substantially increased the attack efficacy albeit with larger dictionaries of approximately 235 entries, allowing attacks that guessed 48{\%}-54{\%} of passwords (compared to previous results of 1{\%} and 9{\%} on the same dataset for two images with 235 guesses). These latter attacks are independent of focus-of-attention models, and are based on image-independent guessing patterns. Our results show that automated attacks, which are easier to arrange than human-seeded attacks and are more scalable to systems that use multiple images, require serious consideration when deploying basic PassPoints-style graphical passwords.},
author = {van Oorschot, P C and Salehi-Abari, A and Thorpe, J},
doi = {10.1109/TIFS.2010.2053706},
issn = {1556-6013},
journal = {Information Forensics and Security, IEEE Transactions on},
keywords = {PassPoints-style graphical password,automated atta},
number = {3},
pages = {393--405},
title = {{Purely Automated Attacks on PassPoints-Style Graphical Passwords}},
volume = {5},
year = {2010}
}
@inproceedings{Ipeirotis2010,
address = {New York, New York, USA},
author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
booktitle = {Proceedings of the ACM SIGKDD Workshop on Human Computation - HCOMP '10},
doi = {10.1145/1837885.1837906},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ipeirotis, Provost, Wang - 2010 - Quality management on Amazon Mechanical Turk.pdf:pdf},
isbn = {9781450302227},
month = {jul},
pages = {64--67},
publisher = {ACM Press},
title = {{Quality management on Amazon Mechanical Turk}},
url = {http://dl.acm.org/citation.cfm?id=1837885.1837906},
year = {2010}
}
@article{Finkenauer2005,
abstract = {Cross-sectional data from 1359 boys and girls aged 10-14 years investigated whether parenting behaviours are directly or indirectly (through building   self-control) associated with emotional (depression, stress, low self-esteem) and behavioural (delinquency, aggression) problems among adolescents. Replicating   existing findings, both types of problems were directly, negatively related to adaptive parenting behaviour (high parental acceptance, strict control and   monitoring, and little use of manipulative psychological control). Extending existing findings, self-control partially mediated the link between parenting behaviour and adolescent emotional and behavioural problems. Contrary to earlier suggestions, there was no sign that high self-control was associated with drawbacks   or increased risk of psychosocial problems.},
author = {Finkenauer, Catrin and Engels, Rutger and Baumeister, Roy},
doi = {10.1080/01650250444000333},
issn = {0165-0254},
journal = {International Journal of Behavioral Development},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jan},
number = {1},
pages = {58--69},
title = {{Parenting behaviour and adolescent behavioural and emotional problems: The role of self-control}},
url = {http://jbd.sagepub.com/content/29/1/58.short},
volume = {29},
year = {2005}
}
@inproceedings{Baumeister2002,
abstract = {Software development for applications where time-to-market is critical has to cope with, among others, imprecise requirements and reliability of the resulting software. This paper describes the positive experiences with the techniques iterative development with small increments and test-rst programming in developing software for a framework for building customized Customer Relationship Management (CRM) applications.},
annote = {Applying Test-First Programming and Iterative Development in Building an E-Business Application},
author = {Baumeister, H and Wirsing, M},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Applying Test-First Programming and Iterative Development in Building an E-Business Application}},
url = {citeulike-article-id:3934552 http://www.pst.informatik.uni-muenchen.de/{~}baumeist/publications/ssgrr2002w.pdf},
year = {2002}
}
@article{4376129,
abstract = {The Internet has become a wild place: malicious code is spread on personal computers across the world, deploying botnets ready to attack the network infrastructure. The vast number of security incidents and other anomalies overwhelms attempts at manual analysis, especially when monitoring service provider backbone links. We present an approach to interactive visualization with a case study indicating that interactive visualization can be applied to gain more insight into these large data sets. We superimpose a hierarchy on IP address space, and study the suitability of Treemap variants for each hierarchy level. Because viewing the whole IP hierarchy at once is not practical for most tasks, we evaluate layout stability when eliding large parts of the hierarchy, while maintaining the visibility and ordering of the data of interest.},
author = {Mansmann, F and Keim, D A and North, S C and Rexroad, B and Sheleheda, D},
doi = {10.1109/TVCG.2007.70522},
issn = {1077-2626},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
keywords = {IP address space,Internet,Treemap,interactive moni},
number = {6},
pages = {1105--1112},
title = {{Visual Analysis of Network Traffic for Resource Planning, Interactive Monitoring, and Interpretation of Security Threats}},
volume = {13},
year = {2007}
}
@article{Briand2005,
author = {Briand, L.C. and Labiche, Y. and Lin, Q.},
doi = {10.1109/ISSRE.2005.24},
isbn = {0-7695-2482-6},
journal = {16th IEEE International Symposium on Software Reliability Engineering (ISSRE'05)},
pages = {95--104},
publisher = {Ieee},
title = {{Improving Statechart Testing Criteria Using Data Flow Information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1544725},
year = {2005}
}
@misc{Simonsen2007,
author = {Simonsen, Erlend},
howpublished = {$\backslash$url{\{}http://www.fudgie.org{\}}},
title = {{glTail.rb - realtime logfile visualization}},
url = {http://www.fudgie.org},
year = {2007}
}
@book{Stephens2003,
address = {Berkley, CA},
author = {Stephens, M and Rosenberg, D},
keywords = {XP,extreme programming},
publisher = {APress},
title = {{Extreme Programming Refactored: The Case Against XP}},
year = {2003}
}
@book{Kececioglu1991,
address = {Englewood Cliffs, NJ},
author = {Kececioglu, Dimitri},
publisher = {Prentice Hall},
title = {{Reliability Engineering Handbook}},
year = {1991}
}
@inproceedings{Slaten2005,
abstract = {One of the reasons that undergraduate students, particularly women and minorities, can become disenchanted with computer science education is because software development is wrongly characterized as a solitary activity. We conducted a collective case study in a software engineering course at North Carolina State University to ascertain the effects of a collaborative pedagogy intervention on student perceptions. The pedagogy intervention was based upon the practices of agile software development with a focus on pair programming. Six representative students in the course participated in the study. Their perspectives helped validate a social interaction model of student views. The findings suggest that pair programming and agile software methodologies contribute to more effective learning opportunities for computer science students and that students understand and appreciate these benefits.},
address = {Denver, CO},
author = {Slaten, Kelli M and Droujkova, Maria and Berenson, Sarah B and Williams, Laurie and Layman, Lucas},
booktitle = {Agile Development Conference (ADC'05)},
doi = {10.1109/ADC.2005.48},
isbn = {0-7695-2487-7},
keywords = {Collaborative software,Collaborative work,Computer science,Computer science education,Educational programs,Iterative methods,Mathematical model,Mathematics,North Carolina State University,Programming profession,Software engineering,agile software development,collaborative pedagogy intervention,computer science students,educational courses,mypubs,pair programming,qualitative,simpp,social interaction model,software engineering course,undergraduate student perception},
mendeley-tags = {mypubs},
pages = {323--330},
publisher = {IEEE Comput. Soc},
shorttitle = {Agile Conference, 2005. Proceedings},
title = {{Undergraduate Student Perceptions of Pair Programming and Agile Software Methodologies: Verifying a Model of Social Interaction}},
year = {2005}
}
@inproceedings{1532070,
abstract = { An Internet cyber threat monitoring system detects cyber threats using network sensors deployed at particular points on the Internet, statistically analyzes the time of attack, source of attack, and type of attack, and then visualizes the result of this analysis. Existing systems, however, simply visualize country-by-country statistics of attacks or hourly changes of attacks. Using these systems, it is difficult to understand the source of attack, the diffusion of the attack, or the relation between the target and the source of the attack. This paper described a method for visualizing cyber threats by using 2-dimensional matrix representation of IP addresses. The advantages of this method are that: (1) the logical distance of IP addresses is represented intuitively; (2) Internet address space is visualized economically; (3) macroscopic information (Internet level) and microscopic information (local level) are visualized simultaneously. By using this visualization framework, propagation of the Welchia worm and the Sasser.D worm are visualized.},
author = {Koike, H. and Ohno, K. and Koizumi, K.},
booktitle = {IEEE Workshop on Visualization for Computer Security, 2005. (VizSEC 05).},
doi = {10.1109/VIZSEC.2005.1532070},
isbn = {0-7803-9477-1},
keywords = {2-dimensional matrix representation,IP address ma},
pages = {91--98},
publisher = {Ieee},
title = {{Visualizing cyber attacks using IP matrix}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1532070},
year = {2005}
}
@inproceedings{1405405,
abstract = { The image compression techniques are widely applied in the field of security technology (perimeter supervision, CCTV in facilities, faces, fingerprints, car plates etc.). The extensive and dramatically expanding use of image information requires increasing capacity of either transmission channels or retrieving devices. The image compression techniques offer a partial solution of this problem but there are some shortcomings to be avoided. During last years, we have tested and evaluated a set of different image compression techniques from the security point of view in a relation to a standard visual quality evaluation. The results have been part-by-part presented at several previous Carnahan conferences (Lucas and Kanade, 2001; Swain and Ballard,1991; Comanaciu, 2003). The paper presents summarized information about these results and based upon them a comprehensive discussion and conclusions are derived. Moreover some new results related to the VQ (vector quantization) and video coding are presented too.},
author = {Klima, M and Fliegel, K},
booktitle = {Security Technology, 2004. 38th Annual 2004 International Carnahan Conference on},
doi = {10.1109/CCST.2004.1405405},
keywords = {CCTV,image compression techniques,image informat},
pages = {278--284},
title = {{Image compression techniques in the field of security technology: examples and discussion}},
year = {2004}
}
@inproceedings{Rothermel2000,
address = {Limerick, Ireland},
author = {Rothermel, K and Cook, C and Burnett, M and Schonfeld, J and Green, T and Rothermel, G},
pages = {230--239},
title = {{WYSIWYT Testing in the spreadsheet Paradigm: An Empirical Evaluation}},
year = {2000}
}
@inproceedings{Kremenek2003,
address = {San Diego, CA},
author = {Kremenek, Ted and Engler, Dawson},
keywords = {false positive,static analysis},
pages = {295--315},
title = {{Z-Ranking: Using Statistical Analysis to Counter the Impact of Static Analysis Approximations}},
year = {2003}
}
@article{Lindvall2004,
author = {Lindvall, M and Muthig, Dirk and Dagnino, A},
journal = {Computer},
title = {{Agile software development in large organizations}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1377040},
year = {2004}
}
@inproceedings{Maximilien2003,
abstract = {In a software development group of IBM Retail Store Solutions, we built a non-trivial software system based on a stable standard specification using a disciplined, rigorous unit testing and build approach based on the test-driven development (TDD) practice. Using this practice, we reduced our defect rate by about 50 percent compared to a similar system that was built using an ad-hoc unit testing approach. The project completed on time with minimal development productivity impact. Additionally, the suite of automated unit test cases created via TDD is a reusable and extendable asset that will continue to improve quality over the lifetime of the software system. The test suite will be the basis for quality checks and will serve as a quality contract between all members of the team.},
address = {Portland, OR},
annote = {Assessing test-driven development at IBM},
author = {Maximilien, E. Michael and Williams, Laurie},
booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
doi = {10.1109/ICSE.2003.1201238},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maximilien, Williams - 2003 - Assessing test-driven development at IBM.pdf:pdf},
isbn = {0-7695-1877-X},
keywords = {TDD,agile,file-import-09-01-23},
mendeley-tags = {TDD,agile},
pages = {564--569},
publisher = {IEEE},
title = {{Assessing test-driven development at IBM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1201238 citeulike-article-id:3934708 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0038601469{\&}{\#}38 partnerID=40},
volume = {6},
year = {2003}
}
@article{Sweller1988,
author = {Sweller, John},
number = {1},
pages = {257--285},
title = {{Cognitive Load during Problem Solving: Effects on Learning}},
volume = {12},
year = {1988}
}
@inproceedings{Hwang2013,
address = {New York, New York, USA},
author = {Hwang, Myung-Hwa and Wang, Shaowen and Cao, Guofeng and Padmanabhan, Anand and Zhang, Zhenhua},
booktitle = {Proceedings of the 4th ACM SIGSPATIAL International Workshop on GeoStreaming - IWGS '13},
doi = {10.1145/2534303.2534310},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang et al. - 2013 - Spatiotemporal transformation of social media geostreams.pdf:pdf},
isbn = {9781450325325},
keywords = {data pipeline,disease surveillance,social media geostreams,spatiotemporal analysis,twitter},
mendeley-tags = {twitter},
pages = {12--21},
publisher = {ACM Press},
title = {{Spatiotemporal transformation of social media geostreams}},
url = {http://dl.acm.org/citation.cfm?doid=2534303.2534310},
year = {2013}
}
@inproceedings{Ko2004a,
address = {New York, New York, USA},
author = {Ko, Andrew J. and Myers, Brad A.},
booktitle = {Proceedings of the 2004 conference on Human factors in computing systems - CHI '04},
doi = {10.1145/985692.985712},
isbn = {1581137028},
keywords = {Alice,debugging,program comprehension,program slicing},
mendeley-tags = {debugging,program comprehension},
month = {apr},
pages = {151--158},
publisher = {ACM Press},
title = {{Designing the whyline}},
url = {http://dl.acm.org/citation.cfm?id=985692.985712},
year = {2004}
}
@inproceedings{Johnson2013,
author = {Johnson, Brittany and Song, Yoonki and Murphy-Hill, Emerson and Bowdidge, Robert},
booktitle = {2013 35th International Conference on Software Engineering (ICSE)},
doi = {10.1109/ICSE.2013.6606613},
isbn = {978-1-4673-3076-3},
keywords = {Companies,Computer bugs,Encoding,Interviews,Software,Standards,Teamwork,agile,automatic code inspection,bug detection,false positives,interactive mechanism,interactive systems,nsf,program debugging,program diagnostics,software defects,software development,software engineering,software quality,static analysis tools},
language = {English},
mendeley-tags = {agile,nsf},
month = {may},
pages = {672--681},
publisher = {IEEE},
title = {{Why don't software developers use static analysis tools to find bugs?}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6606613},
year = {2013}
}
@article{Kerievsky2003,
abstract = {Are your developers running an Extreme Programming offense against their own executives? Test-driven management can align programmers, customers and managers in the right direction. Like much of what makes up XP, test-driven management lends a new name to an old and sound practice. Test-driven management enables development managers to ensure that the software their teams produce meets the organizational intentions of business managers (also known as Gold Owners)},
annote = {Right game, wrong team [extreme programming]},
author = {Kerievsky, J},
isbn = {1070-8588},
journal = {Software Development},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {30--33},
title = {{Right game, wrong team [extreme programming]}},
url = {citeulike-article-id:3934673 {\#}},
volume = {11},
year = {2003}
}
@book{Conte1986,
address = {Menlo Park},
author = {Conte, Samuel Daniel and Dunsmore, Hubert E. and Shen, Vincent Y.},
publisher = {Benjamin-Cummings Publishing Co.},
title = {{Software Engineerning Metrics and Models}},
year = {1986}
}
@inproceedings{Ackermann2006,
address = {Washington, DC},
author = {Ackermann, Chris and Lindvall, Mikael},
booktitle = {IEEE/NASA Software Engineering Workshop},
keywords = {Software Engineering},
pages = {66--75},
series = {30th NASA/IEEE Software Engineering Workshop (SEW)},
title = {{Understanding change requests in order to predict software impact}},
year = {2006}
}
@article{Subramanyam,
author = {Subramanyam, Ramanath and Krishnan, M.S.},
journal = {IEEE Transactions on Software Engineering},
keywords = {CK metrics},
number = {4},
pages = {297--310},
title = {{Empirical Analysis of CK Metrics for Object-Oriented Design Complexity: Implications for Software Defects}},
volume = {29}
}
@inproceedings{Schechter2007a,
abstract = {We evaluate Website authentication measures that are designed to protect users from man-in-the-middle, 'phishing', and other site forgery attacks. We asked 67 bank customers to conduct common online banking tasks. Each time they logged in, we presented increasingly alarming clues that their connection was insecure. First, we removed HTTPS indicators. Next, we removed the participant's site-authentication image--the customer-selected image that many Websites now expect their users to verify before entering their passwords. Finally, we replaced the bank's password-entry page with a warning page. After each clue, we determined whether participants entered their passwords or withheld them. We also investigate how a study's design affects participant behavior: we asked some participants to play a role and others to use their own accounts and passwords. We also presented some participants with security-focused instructions. We confirm prior findings that users ignore HTTPS indicators: no participants withheld their passwords when these indicators were removed. We present the first empirical investigation of site-authentication images, and we find them to be ineffective: even when we removed them, 23 of the 25 (92{\%}) participants who used their own accounts entered their passwords. We also contribute the first empirical evidence that role playing affects participants' security behavior: role-playing participants behaved significantly less securely than those using their own passwords.},
author = {Schechter, S.E. Stuart E. and Dhamija, Rachna and Ozment, Andy and Fischer, Ian},
booktitle = {Proceedings of the 2007 IEEE Symposium on Security and Privacy (SP '07)},
doi = {10.1109/SP.2007.35},
isbn = {0-7695-2848-1},
keywords = {agile,nsf,security},
mendeley-tags = {agile,nsf,security},
month = {may},
pages = {51--65},
publisher = {IEEE},
title = {{The Emperor's New Security Indicators}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4223213},
year = {2007}
}
@inproceedings{Knodel,
abstract = {The software architecture is one of the most important artifacts created in the lifecycle of a software system. It enables, facilitates, hampers, or interferes directly the achievement of business goals, functional and quality requirements. One instrument to determine how adequate the architecture is for its intended usage is architecture compliance checking. This paper compares three static architecture compliance checking approaches (reflexion models, relation conformance rules, and component access rules) by assessing their applicability in 13 distinct dimensions. The results give guidance on when to use which approach.},
author = {Knodel, J. and Popescu, D.},
booktitle = {The Working IEEE/IFIP Conference on Software Architecture, 2007. WICSA '07.},
keywords = {TSAFE study},
mendeley-tags = {TSAFE study},
title = {{A Comparison of Static Architecture Compliance Checking Approaches}},
url = {http://ieeexplore.ieee.org.proxy-um.researchport.umd.edu/xpl/articleDetails.jsp?tp={\&}arnumber=4077029{\&}contentType=Conference+Publications{\&}searchField=Search{\_}All{\&}queryText=knodel+wicsa}
}
@inproceedings{Nikora2004,
address = {Sydney, Australia},
author = {Nikora, A.P. and Munson, J.C.},
booktitle = {Proceedings of the 9th International Software Metrics Symposium (METRICS '03)},
doi = {10.1109/METRIC.2003.1232479},
isbn = {0-7695-1987-3},
pages = {338--350},
publisher = {IEEE Comput. Soc},
title = {{Developing fault predictors for evolving software systems}},
url = {http://ieeexplore.ieee.org/document/1232479/},
year = {2004}
}
@article{Mcfall2004,
abstract = {The two principal roles of the API's Lubricants Committee are program oversight, i.e., serving as a "board of directors" for the Engine Oil Licensing and Certification System (EOLCS), and policy development. API declares that its Engine Oil Licensing and Certification System is the oil and automotive industry's "seal of approval". A key component of EOLCS is the annual Aftermarket Audit Program in which samples of marketplace oils are tested to ensure compliance with the program's rules and tests. First API licensing of GF-4, the next passenger car engine oil upgrade, will begin in July 2004, and API will cease licensing GF-3 in April 2005. Initial licensing of API's companion category to GF-4, likely called API SM, will depend on when it is defined. GF-4 will be commercialized in the middle of 2004. All previous GF categories have been accompanied by an API service category; GF-3's companion is API SL.},
annote = {Portrait of a Heavyweight: API's Lubricants Committee},
author = {Mcfall, D},
journal = {Lubes-n-Greases},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
title = {{Portrait of a Heavyweight: API's Lubricants Committee}},
url = {citeulike-article-id:3934710 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-1942442280{\&}{\#}38 partnerID=40},
volume = {10},
year = {2004}
}
@misc{Smith2013,
address = {Washington, DC},
author = {Smith, Aaron and {Pew Research Center}},
booktitle = {Pew Research Center},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smith - 2013 - Smartphone Ownership - 2013 Update.pdf:pdf},
institution = {Pew Research Center},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{Smartphone Ownership - 2013 Update}},
url = {http://pewinternet.org/Reports/2013/Smartphone-Ownership-2013.aspx},
urldate = {2013-08-01},
year = {2013}
}
@inproceedings{Mathew2006b,
address = {New York, New York, USA},
author = {Mathew, S. and Giomundo, R. and Upadhyaya, S. and Sudit, M. and Stotz, A.},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179578},
isbn = {1595935495},
keywords = {attack tracks,intrusion detection,visualization},
month = {nov},
pages = {1},
publisher = {ACM Press},
title = {{Understanding multistage attacks by attack-track based visualization of heterogeneous event streams}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179578},
year = {2006}
}
@inproceedings{Singer1997,
address = {Toronto, Ontario},
author = {Singer, Janice and Lethbridge, Timothy and Vinson, Norman and Anquetil, Nicolas},
booktitle = {Proceedings of the 1997 conference of the Centre for Advanced Studies on Collaborative Research},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singer et al. - 1997 - An examination of software engineering work practices.pdf:pdf},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {21--35},
publisher = {IBM Press},
title = {{An examination of software engineering work practices}},
url = {http://portal.acm.org/citation.cfm?id=782010.782031},
year = {1997}
}
@inproceedings{6027518,
abstract = {Sufficient and credible information security measurement in software-intensive systems requires use of a variety of security metrics offering security-related evidence from different viewpoints. Visualization is needed to facilitate management of security metrics and measurements and to increase the meaningfulness of them in decision-making such as security assurance and risk management. We introduce a novel visualization and modeling tool for hierarchical specification and deployment of security metrics and measurements. The tool connects high-level risk-driven security objectives with detailed measurements and evidence gathering. The tool facilitates the management of a large number of metrics and measurements without losing appropriate granularity that is crucial for informed security decision-making.},
author = {Savola, R M and Heinonen, P},
booktitle = {Information Security South Africa (ISSA), 2011},
doi = {10.1109/ISSA.2011.6027518},
keywords = {hierarchical specification,high-level risk-driven},
pages = {1--8},
title = {{A visualization and modeling tool for security metrics and measurements management}},
year = {2011}
}
@inproceedings{6060031,
abstract = {There is no widely accepted way of measuring the level of security of a recognition-based graphical password against guessing attacks. We aim to address this by examining the influence of predictability of user choice on the guessability and proposing a new measure of guessability. Davis et al. showed that these biases exist for schemes using faces and stories, we support this result and show these biases exist in other recognition-based schemes. In addition, we construct an attack exploiting predictability, which we term {\#}x201C;Semantic Ordered Guessing Attack {\#}x201D; (SOGA). We then apply this attack to two schemes (the Doodles scheme and a standard recognition-based scheme using photographic images) and report the results. The results show that predictability when users select graphical passwords influence the level of security to a varying degree (dependent on the distractor selection algorithm). The standard passimages scheme show an increase on guessability of up to 18 times more likely than the usual reported guessability, with a similar set up of nine images per screen and four screens, the doodles scheme shows a successful guessing attack is 3.3 times more likely than a random guess. Finally, we present a method of calculating a more accurate guessability value, which we call the revised guessability of a recognition-based scheme. Our conclusion is that to maximise the security of a recognition-based graphical password scheme, we recommend disallowing user choice of images.},
author = {English, R and Poet, R},
booktitle = {Network and System Security (NSS), 2011 5th International Conference on},
doi = {10.1109/ICNSS.2011.6060031},
keywords = {attack exploiting predictability,doodles scheme,pa},
pages = {364--368},
title = {{Measuring the revised guessability of graphical passwords}},
year = {2011}
}
@article{Krishnan1999,
abstract = {In this paper, an empirical study that links software process consistency with product defects is reported. Various measurement issues such as validity, reliability, and other challenges in measuring process consistency at the project level are discussed. A measurement scale for software process consistency is introduced. An empirical study that uses this scale to measure consistency in achieving the CMM goal questions in various key process areas (KPAs) in 45 projects at a leading software vendor is reported. The results of this analysis indicate that consistent adoption of practices specified in the CMM is associated with a lower number of defects. Even a relatively modest improvement in the consistency of implementing these practices is associated with a significant reduction in field defects},
author = {Krishnan, M.S. and Kellner, M.I.},
doi = {10.1109/32.824401},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
number = {6},
pages = {800--815},
title = {{Measuring process consistency: implications for reducing software defects}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=824401},
volume = {25},
year = {1999}
}
@inproceedings{Malaiya1999,
abstract = {Ideally the requirements for a software system should be completely and unambiguously determined before design, coding and testing rake place. In practice, often there are changes in the requirements, causing software components to be redesigned, deleted or added. This requirements volatility causes the software to have a higher defect density. In this paper we analytically examine the influence of requirement changes taking place during different times by examining the consequences of software additions, removals and modifications. We take into account interface defects which arise due to errors at the interfaces among software sections. We compare the resulting defect density in the presence of requirement volatility, with the defect density that would have resulted had requirements not changed. The results show that if the requirement changes take place close to the release date, there is a greater impact on defect density. In each case we compute the defect equivalence factor representing the overall impact of requirement volatility},
author = {Malaiya, Y.K. and Denton, J.},
booktitle = {Proceedings 10th International Symposium on Software Reliability Engineering (ISSRE '99)},
doi = {10.1109/ISSRE.1999.809334},
isbn = {0-7695-0443-4},
issn = {1071-9458},
keywords = {Computer bugs,Computer science,Debugging,Electrical capacitance tomography,Programming profession,Software reliability,System testing,defect density,defect equivalence factor,formal specification,interface defects,requirements,requirements volatility,software reliability,software system},
pages = {285--294},
publisher = {IEEE Comput. Soc},
shorttitle = {Software Reliability Engineering, 1999. Proceeding},
title = {{Requirements volatility and defect density}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=809334},
year = {1999}
}
@inproceedings{4076248,
abstract = {In this paper, we give a view-dependent dynamic terrain visualization method using strip masks, and we also present our implementations results. We partition our terrain into several blocks, and we pre-compute a set of strip masks of blocks. At run time, to render the terrain, we simply choose the mask of blocks that represent each part of the terrain to meet our desired visual fidelity. We also combine our algorithm with the physical based model of craters and ruts. Our implementations are running smooth running smoothly in real time, and our craters and ruts are very realistic. The experiments prove our method is feasible and valid},
author = {Cai, Xingquan and Li, Fengxia and Zhan, Shouyi and Sun, Haiyan},
booktitle = {2006 International Conference on Computational Intelligence and Security},
doi = {10.1109/ICCIAS.2006.295342},
isbn = {1-4244-0604-8},
keywords = {dynamic terrain visualization,strip masks,terrain},
month = {nov},
pages = {1653--1658},
publisher = {Ieee},
title = {{Dynamic Terrain Visualization Method using Strip Masks and Its Applications}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4076248},
volume = {2},
year = {2006}
}
@book{Gottfredson1990,
address = {Palo Alto, CA},
author = {Gottfredson, Michael R. and Hirschi, Travis},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Stanford University Press},
title = {{A General Theory of Crime}},
year = {1990}
}
@inproceedings{Rahman2007,
abstract = {In our research, we have introduced and implemented a new software development method, Testing Before Coding (TBC), to bring in the benefits of using software development lifecycle in computer programming and improved students program quality. TBC follows the basic concept of Agile Method (such as Test-driven Development, TDD) and makes students a "cultural shift" in developing their computer programs i.e., students must test their own codes and consider testing as an indispensable part of program development. Our method has been exercised in the introductory computer programming courses and has improved the students' programs quality at least 24{\%}. {\^{A}}{\textcopyright}2007 IEEE.},
annote = {Applying the TBC method in introductory programming courses},
author = {Rahman, S M},
booktitle = {Proceedings - Frontiers in Education Conference, FIE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Applying the TBC method in introductory programming courses}},
url = {citeulike-article-id:3934760 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-50049095640{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Read2005,
abstract = {Executable acceptance testing allows both to specify customers' expectations in the form of the tests and to compare those to actual results that the software produces. The results of an observational study identifying patterns in the use of the FIT acceptance testing framework are presented and the data on acceptance-test driven design is discussed. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {Examining usage patterns of the FIT acceptance testing framework},
author = {Read, K and Melnik, G and Maurer, F},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {127--136},
title = {{Examining usage patterns of the FIT acceptance testing framework}},
url = {citeulike-article-id:3934763 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444483839{\&}{\#}38 partnerID=40},
volume = {3556},
year = {2005}
}
@inproceedings{Bhattacharya2012a,
author = {Bhattacharya, D. and Ram, S.},
booktitle = {2012 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining},
doi = {10.1109/ASONAM.2012.170},
isbn = {978-1-4673-2497-7},
keywords = {twitter},
mendeley-tags = {twitter},
month = {aug},
pages = {966--971},
publisher = {IEEE},
title = {{Sharing News Articles Using 140 Characters: A Diffusion Analysis on Twitter}},
url = {http://ieeexplore.ieee.org/document/6425634/},
year = {2012}
}
@inproceedings{Sarcia2009,
address = {New York, New York, USA},
author = {Sarcia, Salvatore Alessandro and Basili, Victor Robert and Cantone, Giovanni},
booktitle = {Proceedings of the 5th International Conference on Predictor Models in Software Engineering - PROMISE '09},
doi = {10.1145/1540438.1540464},
isbn = {9781605586342},
keywords = {Bayesian prediction intervals,accuracy,calibration,cost estimation,cost model,model evaluation,model selection,prediction interval,uncertainty},
month = {may},
pages = {1},
publisher = {ACM Press},
title = {{Using uncertainty as a model selection and comparison criterion}},
url = {http://portal.acm.org/citation.cfm?id=1540438.1540464},
year = {2009}
}
@article{Sinha,
author = {Sinha, Avik and Paradkar, Amit and Watson, I B M Thomas},
isbn = {1595934588},
title = {{Model-Based Functional Conformance Testing of Web Services Operating on Persistent Data}}
}
@article{Murru2003,
annote = {Conducted as a research experiment. One relatively inexperienced team testing two pilot projects.

The authors talk about partially-adopting XP and how it bombed initially. They eliminated planning game and it failed. Duh. Big surprise. 

They acknowledge that tools and practices outside XP can help bolster it and make it work.

Don't recommend "customizing" XP practices.

Conducted ifnormal interviews with a number of different stakeholders, includign marketing and managers to get their feedback.},
author = {{Murru  R. Deias, and G. Mugheddu}, O and Murru, Orlando and Deias, Roberto and Mugheddu, Giampiero},
doi = {10.1109/MS.2003.1196318},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murru, Deias, Mugheddu - 2003 - Assessing xp at a european internet company.pdf:pdf},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {XP,agile,anecdotal,european,partial-adoption},
mendeley-tags = {agile},
month = {may},
number = {3},
pages = {37--43},
title = {{Assessing XP at a European Internet Company}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1196318},
volume = {20},
year = {2003}
}
@article{Collier2005,
abstract = {Integrated automated testing in database development presents a unique set of challenges. Current automated testing tools for software development are not easily adaptable to database development, and large data volumes can make automated testing a daunting task. In this article, the author presents a complete framework for building automated, integrated, test-driven development (TDD) into your agile database development. Moreover, this framework has been developed, refined, and proven out on a real commercial data warehousing project},
annote = {Agile database testing},
author = {Collier, K},
isbn = {1048-5600},
journal = {Cutter IT Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {12},
pages = {14--22},
title = {{Agile database testing}},
url = {citeulike-article-id:3934576 {\#}},
volume = {18},
year = {2005}
}
@article{Olimpiew2005a,
address = {New York, New York, USA},
author = {Olimpiew, Erika Mir and Gomaa, Hassan},
doi = {10.1145/1083274.1083279},
isbn = {1595931155},
journal = {Proceedings of the first international workshop on Advances in model-based testing - A-MOST '05},
keywords = {feature,model,product line,test,test order,uml},
pages = {1--7},
publisher = {ACM Press},
title = {{Model-based testing for applications derived from software product lines}},
url = {http://portal.acm.org/citation.cfm?doid=1083274.1083279},
year = {2005}
}
@inproceedings{Schumacher2010,
address = {Bolzano, Italy},
author = {Schumacher, Jan and Zazworka, Nico and Shull, Forrest and Seaman, Carolyn B. and Shaw, Michele},
booktitle = {International Symposium on Empirical Software Engineering and Measurement},
title = {{Building Empirical Support for Automated Code Smell Detection}},
year = {2010}
}
@inproceedings{4529398,
abstract = {In today's IT environments, there is an ever increasing demand for log file analysis solutions. Log files often contain important information about possible incidents, but inspecting the often large amounts of textual data is too time-consuming and tedious a task to perform manually. To address this issue, we propose a novel log file visualization technique called Histogram Matrix (HMAT). HMAT visualizes the content of a log file in order to enable a security administrator to efficiently spot anomalies. The system uses a combination of graphical and statistical techniques and allows even non-experts to interactively search for anomalous log messages. Contrary to other approaches, our proposal does not only work on certain special kinds of log files, but instead works on almost every textual log file. Additionally, the system allows to automatically generate security events if an anomaly is detected, similar to anomaly-based intrusion detection systems. This paper introduces HMAT, demonstrates its functionality using log files from a variety of services in real environments, and identifies strengths and limitations of the technique.},
author = {Frei, A and Rennhard, M},
booktitle = {Availability, Reliability and Security, 2008. ARES 08. Third International Conference on},
doi = {10.1109/ARES.2008.148},
keywords = {anomaly detection,histogram matrix,interactive sea},
month = {mar},
pages = {610--617},
title = {{Histogram Matrix: Log File Visualization for Anomaly Detection}},
year = {2008}
}
@inproceedings{Corney2014,
abstract = {Recent studies have linked the ability of novice (CS1) programmers to read and explain code with their ability to write code. This study extends earlier work by asking CS2 students to explain object-oriented data structures problems that involve recursion. Results show a strong correlation between ability to explain code at an abstract level and performance on code writing and code reading test problems for these object-oriented data structures problems. The authors postulate that there is a common set of skills concerned with reasoning about programs that explains the correlation between writing code and explaining code. The authors suggest that an overly exclusive emphasis on code writing may be detrimental to learning to program. Non-code writing learning activities (e.g., reading and explaining code) are likely to improve student ability to reason about code and, by extension, improve student ability to write code. A judicious mix of code-writing and code-reading activities is recommended. Copyright {\textcopyright} 2014 ACM.},
address = {Atlanta, GA, USA},
author = {Corney, Malcolm and Lister, Raymond and Fitzgerald, Sue and McCauley, Ren{\'{e}}e and Hanks, Brian and Murphy, Laurie},
booktitle = {SIGCSE 2014 - Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2538862.2538911},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Corney et al. - 2014 - 'Explain in plain English' questions revisited Data structures problems.pdf:pdf},
keywords = {CS2,Computer science education research,Data structures,Explain in plain English,Mixed methods,Neo-Piaget,Qualitative research methods,SOLO taxonomy},
pages = {591--596},
publisher = {Association for Computing Machinery},
title = {{'Explain in plain English' questions revisited: Data structures problems}},
url = {http://dl.acm.org/citation.cfm?doid=2538862.2538911},
year = {2014}
}
@inproceedings{Turnu2004,
abstract = {This paper describes a simulation model that represents the first step to modeling open source software development processes. The model has been tuned using data from a real FLOSS project: Apache. The goal of our research is to show the effects of adopting TDD (Test Driven Development) on our open source simulation model. In order to introduce the TDD practice, we have only changed some parameter values in the FLOSS simulation model. In particular, the average time required to write a line of production code increases because now automated tests are also coded, the number of defects injected during coding decreases as does the debugging time to fix a single defect. The two FLOSS development models (nonTDD and TDD) have been compared, the one incorporating the agile practice of Test Driven Development yielding better results in terms of code quality (defects/Kloc).},
address = {New York, NY, USA},
annote = {Introducing TDD on a free libre open source software project: a simulation experiment},
author = {Turnu, Ivana and Melis, Marco and Cau, Alessandra and Marchesi, Michele and Setzu, Alessio},
booktitle = {QUTE-SWAP '04: Proceedings of the 2004 workshop on Quantitative techniques for software agile process},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {59--65},
publisher = {ACM},
title = {{Introducing TDD on a free libre open source software project: a simulation experiment}},
url = {citeulike-article-id:3934821 http://dx.doi.org/10.1145/1151433.1151442},
year = {2004}
}
@phdthesis{Thijs2012,
author = {Thijs, Twente and November, Hoeve},
school = {University of Twente},
title = {{Model Based Testing of a PLC Based Interlocking System}},
year = {2012}
}
@article{Whalen2005a,
author = {Whalen, Tara and Inkpen, Kori M.},
isbn = {1-56881-265-5},
keywords = {secure web-based transactions,usable security,visual feedback,web browsing},
month = {may},
pages = {137--144},
title = {{Gathering evidence: use of visual security cues in web browsers}},
url = {http://dl.acm.org/citation.cfm?id=1089508.1089532},
year = {2005}
}
@inproceedings{Li2011,
author = {Li, Qi and Yang, Jingjing and Wu, Jinglong},
booktitle = {The 2011 IEEE/ICME International Conference on Complex Medical Engineering},
doi = {10.1109/ICCME.2011.5876760},
isbn = {978-1-4244-9323-4},
keywords = {Attention,Educational institutions,Visualization,agile,attend-designated modality,audiovisual integration,auditory attention,auditory evoked potentials,behavioral measures,bisensory audiovisual integration,centro-medial area,electro-oculography,electroencephalography,electrophysiological measures,event-related potential,event-related potentials,infrequent target stimulus,medical signal processing,nsf,right temporal area,time 280 ms to 320 ms,unisensory auditory stimuli,unisensory visual stimuli,ventral temporal area,visual attention,visual evoked potentials},
language = {English},
mendeley-tags = {agile,nsf},
month = {may},
pages = {328--333},
publisher = {IEEE},
title = {{The different effects of visual attention and auditory attention on bisensory audiovisual integration}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5876760' escapeXml='false'/{\%}3E},
year = {2011}
}
@inproceedings{Alfaro2001,
author = {Alfaro, Luca De and Henzinger, Thomas A},
booktitle = {Foundations of Software Engineering},
isbn = {1581133901},
pages = {109--120},
title = {{Interface Automata}},
year = {2001}
}
@inproceedings{Basili2010c,
abstract = {We report on a preliminary case study to examine software safety risk in the early design phase of the NASA Constellation spaceflight program. Our goal is to provide NASA quality assurance managers with information regarding the ongoing state of software safety across the program. We examined 154 hazard reports created during the preliminary design phase of three major flight hardware systems within the Constellation program. Our purpose was two-fold: 1) to quantify the relative importance of software with respect to system safety; and 2) to identify potential risks due to incorrect application of the safety process, deficiencies in the safety process, or the lack of a defined process. One early outcome of this work was to show that there are structural deficiencies in collecting valid safety data that make software safety different from hardware safety. In our conclusions we present some of these deficiencies. {\textcopyright} 2010 ACM.},
address = {Bolzano, Italy},
author = {Basili, Victor R and Zelkowitz, Marvin V and Layman, Lucas and Dangle, Kathleen and Diep, Madeline},
booktitle = {Proceedings of the 4th ACM/IEEE International Symposium on Empirical Soiftware Engineering and Measurement (ESEM '10)},
doi = {10.1145/1852786.1852846},
isbn = {9781450300391},
keywords = {NASA,case study,mypubs,nasa,risk analysis,safety metrics},
mendeley-tags = {mypubs},
pages = {Article No. 46},
title = {{Obtaining Valid Safety Data for Software Safety Measurement and Process Improvement}},
year = {2010}
}
@inproceedings{Williams2008a,
abstract = {Utilizing pair programming in the classroom requires specific classroom management techniques. We have created nine guidelines for successfully implementing pair programming in the classroom. These guidelines are based on pair programming experiences spanning seven years and over one thousand students at North Carolina State University. In Fall 2007, pair programming was adopted in the undergraduate human-computer interaction (HCI) course at Virginia Tech. We present the pair programming guidelines in the context of the HCI course, discuss how the guidelines were implemented, and evaluate the general applicability and sufficiency of the guidelines. We find that eight of the nine guidelines were applicable to the Virginia Tech experience. We amended our peer evaluation guideline to account for constantly supervised pairing, as was the case at Virginia Tech. We add two guidelines stating that a pair should always be working toward a common goal and that pairs should be encouraged to find their own answers to increase their independence and self-confidence.},
author = {Williams, Laurie and McCrickard, D. Scott and Layman, Lucas and Hussein, Khaled},
booktitle = {Agile 2008 Conference},
doi = {10.1109/Agile.2008.12},
isbn = {978-0-7695-3321-6},
keywords = {Computer science,Conference management,Costs,Guidelines,Human computer interaction,Laboratories,Programming profession,Software engineering,classroom management techniques,collaboration,education,educational computing,mypubs,pair programming,supervised pairing,undergraduate human-computer interaction course},
mendeley-tags = {mypubs},
pages = {445--452},
publisher = {IEEE},
shorttitle = {Agile, 2008. AGILE '08. Conference},
title = {{Eleven Guidelines for Implementing Pair Programming in the Classroom}},
year = {2008}
}
@inproceedings{Liang2007,
abstract = {Test-driven development (TDD) is a software development methodology for achieving high reliability. However, the practice of TDD at the integration level is rare. In this paper, we propose a test-driven component integration approach with the support of UML 2.0 Testing and Monitoring Profile (U2TMP) that is also proposed in this paper. In our approach, automated integration-level test cases are specified using U2TMP before writing the glue-code that integrates components. U2TMP enhances UML 2.0 diagrams for specifying how test scenarios should be set up and how the integrated components are expected to interact under these scenarios. Test cases written in U2TMP can be automatically transformed into test artifacts that setup the test environment, generate test stimuli, and monitor the component interactions at runtime. These test cases serve as a guideline for writing the glue-code and a regression testbed. With this approach, the software developers may produce glue-code of high reliability. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Test-driven component integration with UML 2.0 testing and monitoring profile},
author = {Liang, D and Xu, K},
booktitle = {Proceedings - International Conference on Quality Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {32--39},
title = {{Test-driven component integration with UML 2.0 testing and monitoring profile}},
url = {citeulike-article-id:3934695 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-46449098084{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Jongeling2017,
abstract = {Recent years have seen an increasing attention to social aspects of software engineering, including studies of emotions and sentiments experienced and expressed by the software developers. Most of these studies reuse existing sentiment analysis tools such as SentiStrength and NLTK. However, these tools have been trained on product reviews and movie reviews and, therefore, their results might not be applicable in the software engineering domain. In this paper we study whether the sentiment analysis tools agree with the sentiment recognized by human evaluators (as reported in an earlier study) as well as with each other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool on software engineering studies by conducting a simple study of differences in issue resolution times for positive, negative and neutral texts. We repeat the study for seven datasets (issue trackers and Stack Overflow questions) and different sentiment analysis tools and observe that the disagreement between the tools can lead to diverging conclusions. Finally, we perform two replications of previously published studies and observe that the results of those studies cannot be confirmed when a different sentiment analysis tool is used.},
author = {Jongeling, Robbert and Sarkar, Proshanta and Datta, Subhajit and Serebrenik, Alexander},
doi = {10.1007/s10664-016-9493-x},
journal = {Empirical Software Engienering},
keywords = {sentiment},
mendeley-tags = {sentiment},
month = {jan},
pages = {1--42},
title = {{On negative results when using sentiment analysis tools for software engineering research}},
url = {http://dx.doi.org/10.1007/s10664-016-9493-x},
year = {2017}
}
@inproceedings{Nelson2005,
abstract = {This paper reports on the experiences of integrating several Software Engineering techniques by the Rapid Prototyping group of Siemens Corporate Research. This experience was gained during our recent project that involved developing a web-based, workflow-driven information system. The techniques integrated for this project included agile and iterative processes, user centered design, requirements discovery and maturation, and test-driven development. These techniques were integrated and supported by a proprietary process entitled "Siemens Rapid Prototyping" (S-RaP), a software architecture, and project management techniques. This paper will detail the specific characteristics of S-RaP, the software architecture, and the project management techniques that supported the integration of the above listed software engineering techniques. We will also report on our experience with their effectiveness and our thoughts on future enhancements in all three areas. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {Integration of software engineering techniques through the use of architecture, process, and people management: An experience report},
author = {Nelson, C and Kim, J S},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1--10},
title = {{Integration of software engineering techniques through the use of architecture, process, and people management: An experience report}},
url = {citeulike-article-id:3934738 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-24944557310{\&}{\#}38 partnerID=40},
volume = {3475},
year = {2005}
}
@inproceedings{Hochstein2008,
abstract = {There is widespread belief in the computer science community that MPI is a difficult and time-intensive approach to developing parallel software. Nevertheless, MPI remains the dominant programming model for HPC systems, and many projects have made effective use of it. It remains unknown how much impact the use of MPI truly has on the productivity of computational scientists. In this paper, we examine a mature, ongoing HPC project, the Flash Center at the University of Chicago, to understand how MPI is used and to estimate the time that programmers spend on MPI-related issues during development. Our analysis is based on an examination of the source code, version control history, and regression testing history of the software. Based on our study, we estimate that about 20{\%} of the development effort is related to MPI. This implies a maximum productivity improvement of 25{\%} for switching to an alternate parallel programming model.},
author = {Hochstein, Lorin and Shull, Forrest and Reid, L},
booktitle = {ACM/IEEE International Conference for High Performance Computing, Networking, and Storage (SC08)},
month = {nov},
publisher = {IEEE Press},
title = {{The Role of MPI in Development Time: A Case Study}},
year = {2008}
}
@article{Valeur2004,
abstract = {Alert correlation is a process that analyzes the alerts produced by one or more intrusion detection systems and provides a more succinct and high-level view of occurring or attempted intrusions. Even though the correlation process is often presented as a single step, the analysis is actually carried out by a number of components, each of which has a specific goal. Unfortunately, most approaches to correlation concentrate on just a few components of the process, providing formalisms and techniques that address only specific correlation issues. This paper presents a general correlation model that includes a comprehensive set of components and a framework based on this model. A tool using the framework has been applied to a number of well-known intrusion detection data sets to identify how each component contributes to the overall goals of correlation. The results of these experiments show that the correlation components are effective in achieving alert reduction and abstraction. They also show that the effectiveness of a component depends heavily on the nature of the data set analyzed.},
author = {Valeur, F. and Vigna, G. and Kruegel, C. and Kemmerer, R.A.},
doi = {10.1109/TDSC.2004.21},
issn = {1545-5971},
journal = {IEEE Transactions on Dependable and Secure Computing},
keywords = {65,Bandwidth,Communication system security,Data analysis,Event detection,Index Terms- Intrusion detection,Intrusion detection,Local area networks,Operating systems,Performance analysis,Portable computers,Spine,alert abstraction,alert correlation,alert reduction,attempted intrusions,correlation components,correlation data sets,correlation data sets.,correlation methods,general correlation model,intrusion detection alert correlation,security of data},
month = {jul},
number = {3},
pages = {146--169},
shorttitle = {Dependable and Secure Computing, IEEE Transactions},
title = {{Comprehensive approach to intrusion detection alert correlation}},
volume = {1},
year = {2004}
}
@article{Roehm2012,
author = {Roehm, Tobias and Tiarks, Rebecca and Koschke, Rainer and Maalej, Walid},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roehm et al. - 2012 - How do professional developers comprehend software.pdf:pdf},
isbn = {978-1-4673-1067-3},
month = {jun},
pages = {255--265},
title = {{How do professional developers comprehend software?}},
url = {http://dl.acm.org/citation.cfm?id=2337223.2337254},
year = {2012}
}
@article{Offen1997,
author = {Offen, R and R Jeffery},
keywords = {framework,measurement},
number = {2},
pages = {45--53},
title = {{Establishing Software Measurement Programs}},
volume = {14},
year = {1997}
}
@article{Schreck2002,
abstract = {Researchers traditionally rely on routine activities and lifestyle theories to explain the differential risk of victimization; few studies have also explored nonsituational alternative explanations. We present a conceptual framework that links individual trait and situational antecedents of violent victimization. Individual risk factors include low self-control and weak social ties with the family and school. Situational risk factors include having delinquent peers and spending time in unstructured and unsupervised socializing activities with peers. We investigate the empirical claims proposed in this model on a sample of high school students, using LISREL to create a structural equation model. The results generally support our assertions that individual traits and situational variables each significantly and meaningfully contribute to victimization. Researchers traditionally rely on routine activities and lifestyle theories to explain the differential risk of victimization; few studies have also explored nonsituational alternative explanations. We present a conceptual framework that links individual trait and situational antecedents of violent victimization. Individual risk factors include low self-control and weak social ties with the family and school. Situational risk factors include having delinquent peers and spending time in unstructured and unsupervised socializing activities with peers. We investigate the empirical claims proposed in this model on a sample of high school students, using LISREL to create a structural equation model. The results generally support our assertions that individual traits and situational variables each significantly and meaningfully contribute to victimization.},
author = {Schreck, Christopher and Wright, Richard and Miller, J. Mitchell},
doi = {10.1080/07418820200095201},
issn = {0741-8825},
journal = {Justice Quarterly},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {mar},
number = {1},
pages = {159--180},
publisher = {Routledge},
title = {{A Study of Individual and Situational Antecedents of Violent Victimization}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07418820200095201},
volume = {19},
year = {2002}
}
@book{Jones2000,
address = {Boston, MA},
author = {Jones, C},
publisher = {Addison Wesley},
title = {{Software Assessments, Benchmarks, and Best Practices}},
year = {2000}
}
@article{Zandbergen2009,
annote = {More details on positioning technologies. 

Provides list of WiFi positioning services. Need to u demand if Google supports this and how. 

Need to understand what GPS chipsets modern androids use. 

Cites sources of cell positioning error. 
},
author = {Zandbergen, Paul A.},
doi = {10.1111/j.1467-9671.2009.01152.x},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zandbergen - 2009 - Accuracy of iPhone Locations A Comparison of Assisted GPS, WiFi and Cellular Positioning.pdf:pdf},
issn = {13611682},
journal = {Transactions in GIS},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jun},
pages = {5--25},
title = {{Accuracy of iPhone Locations: A Comparison of Assisted GPS, WiFi and Cellular Positioning}},
url = {http://doi.wiley.com/10.1111/j.1467-9671.2009.01152.x},
volume = {13},
year = {2009}
}
@inproceedings{He2008,
abstract = {This talk presents a theory of testing that integrates into Hoare and He{\&}{\#}x2019;s Unifying Theory of Programming (UTP). We give test cases a denotational semantics by viewing them as specification predicates. This reformulation of test cases allows for relating test cases via refinement to specifications and programs. Having such a refinement order that integrates test cases, we develop a testing theory for fault-based testing. Fault-based testing uses test data designed to demonstrate the absence of a set of pre-specified faults. A well-known fault-based technique is mutation testing. In mutation testing, first, faults are injected into a program by altering (mutating) its source code. Then, test cases that can detect these errors are designed. The assumption is that other faults will be caught, too. We apply the mutation technique to both specifications and programs. Using our theory of testing, two new test case generation laws for detecting injected (anticipated) faults are presented: one is based on the semantic level of design specifications, the other on the algebraic properties of a programming language.},
annote = {Refinement and test case generation in Unifying Theory of Programming},
author = {He, Ji-Feng},
booktitle = {Software Maintenance, 2008. ICSM 2008. IEEE International Conference on},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {6},
title = {{Refinement and test case generation in Unifying Theory of Programming}},
url = {citeulike-article-id:3934636 http://dx.doi.org/10.1109/ICSM.2008.4658048},
year = {2008}
}
@book{Humphrey1995,
address = {Boston, MA},
author = {Humphrey, W S},
keywords = {PSP},
publisher = {Addison Wesley},
title = {{A Discipline for Software Engineering}},
year = {1995}
}
@inproceedings{1176283,
abstract = { LOCK is an advanced development of hardware-based computer security and crypto-graphic service modules. Much of the design and some of the implementation specifications are complete. The Formal Top Level Specification (FTLS) also is complete and the advanced noninterference proofs are beginning. This hardware-based approach has brought the LOCK project into many uncharted areas in the design, verification, and evaluation of an integrated information security system. System integration promises to be the single largest programmatic problem. Our verification tools seem able to verify design only and not implementation.},
author = {{Sami Saydjari}, O},
booktitle = {Computer Security Applications Conference, 2002. Proceedings. 18th Annual},
doi = {10.1109/CSAC.2002.1176283},
issn = {1063-9527},
keywords = {Formal Top Level Specification,LOCK,access contr},
pages = {96--108},
title = {{LOCK: an historical perspective}},
year = {2002}
}
@article{Mosemann2001a,
author = {Mosemann, R.},
doi = {10.1109/WPC.2001.921716},
isbn = {0-7695-1131-7},
journal = {Program Comprehension,},
keywords = {navigation,novice programmers,program comprehension},
pages = {79--88},
publisher = {IEEE Comput. Soc},
title = {{Navigation and comprehension of programs by novice programmers}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=921716 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=921716},
year = {2001}
}
@article{Yang2006a,
author = {Yang, Junfeng and Twohey, Paul and Engler, Dawson and Musuvathi, Madanlal},
doi = {10.1145/1189256.1189259},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
month = {nov},
number = {4},
pages = {393--423},
title = {{Using model checking to find serious file system errors}},
url = {http://portal.acm.org/citation.cfm?doid=1189256.1189259},
volume = {24},
year = {2006}
}
@book{Stapleton1997,
address = {Reading, MA},
author = {Stapleton, J},
publisher = {Addison-Wesley},
title = {{DSDM, Dynamic System Development Method: The Method in Practice}},
year = {1997}
}
@inproceedings{Martin2003,
address = {Portland, OR},
author = {Martin, Johannes},
booktitle = {3rd Int'l Workshop on Adoption-Centric Software Engineering},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {7--9},
title = {{Tool Adoption: A Software Developer's Perspective}},
year = {2003}
}
@article{Orso2005a,
author = {Orso, Alessandro and Kennedy, Bryan},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Orso, Kennedy - 2005 - Selective capture and replay of program executions.pdf:pdf},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
month = {jul},
number = {4},
pages = {1},
title = {{Selective capture and replay of program executions}},
url = {http://dl.acm.org/citation.cfm?id=1082983.1083251},
volume = {30},
year = {2005}
}
@inproceedings{5453608,
abstract = {This paper studies system clipboard monitoring technology in the Graphic and Document Information Security Protection System. On the basis of the research of HOOK API, file system filter driver and other key technical which the clipboard monitoring technology involved in the windows environment, this paper studies their implementation technology in the clipboard monitoring of the electronic data protection system. The study completes effective monitoring of the clipboard data, provides effective technical support for Graphic and Document Information Security Protection System, and achieves the purpose of preventing important data from being leaked.},
author = {Li, Shaobo and Lv, Shulin and Jia, Xiaohui and Shao, Zhisheng and Lu, Shulin and Jia, Xiaohui and Shao, Zhisheng and Lv, Shulin and Jia, Xiaohui and Shao, Zhisheng},
booktitle = {Intelligent Information Technology and Security Informatics (IITSI), 2010 Third International Symposium on},
doi = {10.1109/IITSI.2010.55},
keywords = {HOOK API,clipboard monitoring technology,document},
month = {apr},
pages = {423--426},
title = {{Application of Clipboard Monitoring Technology in Graphic and Document Information Security Protection System}},
year = {2010}
}
@article{Lilliefors,
author = {Lilliefors, Hubert W},
journal = {Journal of the American Statistical Association},
number = {325},
pages = {387--389},
title = {{On the {\{}K{\}}olmogorov-{\{}S{\}}mirnov Test for the Exponential Distribution with Mean Unknown}},
volume = {64},
year = {1969}
}
@article{Green2006a,
author = {Green, N W and Hoffman, A R and Garrett, H B},
journal = {Journal of Spacecraft and Rockets},
number = {1},
pages = {218--224},
title = {{Anomaly trends for long-life robotic spacecraft}},
volume = {43},
year = {2006}
}
@inproceedings{Chien2008,
abstract = {This paper introduces a new typed mind map extension for a data model in parse-tree-based online referee system with a TDD (Test-Driven Development) model and DALM (DICE adaptive learning model) model named DICE. Typed Mind Maps and a semantic node are defined by OWL-DL. Typed Mind Maps are now widely used in the DICE system as a uniform data model for the system, instructors, learners and materials. The mind maps are used for system configuration, system deployment, learner's information, organization of training materials, answer parsing and some widgets. An implementation based on this data model has been working for years in a real teaching and learning environment.},
annote = {Using a typed mind map as data model in a TDD DICE system},
author = {Chien, L R and Buehrer, D J},
booktitle = {Proceedings of the International Conference on Information Technology Interfaces, ITI},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {931--937},
title = {{Using a typed mind map as data model in a TDD DICE system}},
url = {citeulike-article-id:3934574 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-51949085430{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Keefe2006,
abstract = {This paper reports on an Action Research project that investigated the effect of introducing a number of Extreme Programming (XP) practices as teaching techniques to introductory programming students. The focus of the study was on using the XP practices to assist students in an introductory programming subject develop object oriented programming skills, problem solving skills and teach them to become more self-sufficient in their learning. The research is concerned with applying several of the XP practices as a means of value-adding to current pedagogical approaches. The results from this first exploratory cycle have been mixed, but there have been enough positive results to feed forward into the next action research cycle.},
annote = {Adopting XP practices for teaching object oriented programming},
author = {Keefe, K and Sheard, J and Dick, M},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {91--100},
publisher = {Australian Computer Society, Inc. Darlinghurst, Australia, Australia},
title = {{Adopting XP practices for teaching object oriented programming}},
url = {citeulike-article-id:3934671 {\#}},
year = {2006}
}
@article{Kitchenham2017,
author = {Kitchenham, Barbara and Madeyski, Lech and Budgen, David and Keung, Jacky and Brereton, Pearl and Charters, Stuart and Gibbs, Shirley and Pohthong, Amnart},
doi = {10.1007/s10664-016-9437-5},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kitchenham et al. - 2017 - Robust Statistical Methods for Empirical Software Engineering.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {apr},
number = {2},
pages = {579--630},
publisher = {Springer US},
title = {{Robust Statistical Methods for Empirical Software Engineering}},
url = {http://link.springer.com/10.1007/s10664-016-9437-5},
volume = {22},
year = {2017}
}
@inproceedings{Born2010a,
address = {New York, New York, USA},
author = {Born, Kenton and Gustafson, David},
booktitle = {Proceedings of the Sixth Annual Workshop on Cyber Security and Information Intelligence Research - CSIIRW '10},
doi = {10.1145/1852666.1852718},
isbn = {9781450300179},
keywords = {DNS,anomaly detection,character frequency analysis,network traffic analysis,tunnel detection,visualization},
month = {apr},
pages = {1},
publisher = {ACM Press},
title = {{NgViz}},
url = {http://dl.acm.org/citation.cfm?id=1852666.1852718},
year = {2010}
}
@techreport{InternetCrimeComplaintCenter2011,
address = {Washington, DC},
author = {{Internet Crime Complaint Center}},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Internet Crime Complaint Center - 2012 - 2011 Internet Crime Report.pdf:pdf},
institution = {Internet Crime Complaint Center},
keywords = {agile,cybercrime,nsf,security},
mendeley-tags = {agile,cybercrime,nsf,security},
title = {{2011 Internet Crime Report}},
url = {http://www.ic3.gov/media/annualreport/2011{\_}ic3report.pdf},
year = {2012}
}
@inproceedings{Berlin1993,
author = {Berlin, Lucy},
booktitle = {Empirical studies of programmers: fifth workshop: papers presented at the Fifth Workshop on Empirical Studies of Programmers, December 3-5, 1993, Palo Alto, CA},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Berlin - 1993 - Beyond program understanding A look at programming expertise in industry.pdf:pdf},
keywords = {collaboration},
mendeley-tags = {collaboration},
organization = {Hewlett-Packard},
pages = {6},
publisher = {Ablex Publishing Corporation},
title = {{Beyond program understanding: A look at programming expertise in industry}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=rMmxq8q0CGYC{\&}oi=fnd{\&}pg=PA6{\&}dq=Beyond+program+understanding:+A+look+at+programming+expertise+in+industry{\&}ots=gK-MhAlXLD{\&}sig=n4BAODkstXjIScnL{\_}{\_}P8efonYE8},
year = {1993}
}
@misc{Flohr2006,
abstract = {For most XP techniques only a few experimental results on their effects are available. In October 2004 we started a medium-term experiment to investigate the Impact of test-first compared to a classical-testing approach. We carefully designed a controlled experiment and conducted it with 18 graduated students randomly assigned to 9 pairs. Hypotheses dealt with development speed, number of test-cases and the test-coverage when applying the testing approaches. Results show differences however not significant ones. This paper also addresses other observations we made during the experimental run. Two major problems strongly affect the results of the experiment: the low number of data points and the non-trivial question, whether students really applied test-first all the time. Although we cannot provide any new results on testing to the research community, this paper contains valuable information about further experimental studies on this topic. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {Lessons learned from an XP experiment with students: Test-first needs more teachings},
author = {Flohr, T and Schneider, T},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {305--318},
title = {{Lessons learned from an XP experiment with students: Test-first needs more teachings}},
url = {citeulike-article-id:3934607 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746256446{\&}{\#}38 partnerID=40},
volume = {4034 LNCS},
year = {2006}
}
@book{Voas1998,
author = {Voas, J. M. and McGraw, G.},
publisher = {Wiley},
title = {{Software Fault Injection: Inoculating Programs Against Errors}},
year = {1998}
}
@article{1607922,
abstract = {This article presents a framework for designing network security visualization systems as well as results from the end-to-end design and implementation of two highly interactive systems. In this article, we provide multiple contributions: we present the results of our survey of security professionals, the design framework, and lessons learned from the design of our systems as well as an evaluation of their effectiveness. Our results indicate that both systems effectively present significantly more information when compared to traditional textual approaches. We believe that the interactive, graphical techniques that we present will have broad applications in other domains seeking to deal with information overload.},
author = {Conti, G and Abdullah, K and Grizzard, J and Stasko, J and Copeland, J A and Ahamad, M and Owen, H L and Lee, C},
doi = {10.1109/MCG.2006.30},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {Computer-Assisted,Software,User-Computer Interfac,alert visualization,end-to-end design,graphical te},
number = {2},
pages = {60--70},
title = {{Countering security information overload through alert and packet visualization}},
volume = {26},
year = {2006}
}
@inproceedings{Gougen1993,
address = {San Diego, California},
annote = {Talks about the basics of requirements elicitation from a cognitive point of view. Psychological references. Interviews, discussion, introspection, etc.},
author = {Gougen, Joseph A and Charlotte Linde},
keywords = {interview,interviews,psychology,requirements},
pages = {152--164},
title = {{Techniques for Requirements Elicitation}},
year = {1993}
}
@misc{Siniaalto2008,
abstract = {It is suggested that test-driven development (TDD) is one of the most fundamental practices in agile software development, which produces loosely coupled and highly cohesive code. However, how the TDD impacts on the structure of the program code have not been widely studied. This paper presents the results from a comparative case study of five small scale software development projects where the effect of TDD on program design was studied using both traditional and package level metrics. The empirical results reveal that an unwanted side effect can be that some parts of the code may deteriorate. In addition, the differences in the program code, between TDD and the iterative test-last development, were not as clear as expected. This raises the question as to whether the possible benefits of TDD are greater than the possible downsides. Moreover, it additionally questions whether the same benefits could be achieved just by emphasizing unit-level testing activities. {\^{A}}{\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
annote = {Does test-driven development improve the program code? Alarming results from a comparative case study},
author = {Siniaalto, M and Abrahamsson, P},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {143--156},
title = {{Does test-driven development improve the program code? Alarming results from a comparative case study}},
url = {citeulike-article-id:3934797 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-50949125152{\&}{\#}38 partnerID=40},
volume = {5082 LNCS},
year = {2008}
}
@inproceedings{Schwartzman2007,
abstract = {UrbanSim is a large scale land use and transportation simulator that models the possible long-term effects of different policies on urban regions. The output is presented using indicators, which are variables that convey information on significant aspects of the simulation results. To support their use, we designed and implemented the Indicator Browser, a web-based interface for visualizing and browsing indicator results in the form of tables, charts, or maps. This interface was designed and evaluated via design techniques including Value Sensitive Design, paper prototyping, and frequent user testing. The Indicator Browser was implemented on top of an evolving system, developed using agile test-first software development strategies. This paper presents the Indicator Browser as an illustrative e-government infrastructure case study. We describe its interface and system design, implementation process, and evaluation by a set of potential users, and discuss our plans for deployment. {\^{A}}{\textcopyright}2007 IEEE.},
annote = {The indicator browser: A web-based interface for visualizing UrbanSim simulation results},
author = {Schwartzman, Y and Borning, A},
booktitle = {Proceedings of the Annual Hawaii International Conference on System Sciences},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{The indicator browser: A web-based interface for visualizing UrbanSim simulation results}},
url = {citeulike-article-id:3934785 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-39749138012{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Vaarandi2003,
abstract = {Today, event logs contain vast amounts of data that can easily overwhelm a human. Therefore, mining patterns from event logs is an important system management task. The paper presents a novel clustering algorithm for log file data sets which helps one to detect frequent patterns from log files, to build log file profiles, and to identify anomalous log file lines.},
author = {Vaarandi, R.},
booktitle = {Proceedings of the 3rd IEEE Workshop on IP Operations {\&} Management (IPOM 2003) (IEEE Cat. No.03EX764)},
doi = {10.1109/IPOM.2003.1251233},
isbn = {0-7803-8199-8},
keywords = {Association rules,Clustering algorithms,Data engineering,Data mining,Databases,Event detection,Fault detection,Monitoring,Pattern matching,Protocols,data clustering algorithm,event logs,log file data sets,network management,pattern clustering,pattern mining,simple log file clustering tool,software tools,syslog protocol,system management,telecommunication computing,telecommunication network management},
pages = {119--126},
publisher = {IEEE},
shorttitle = {IP Operations {\&} Management, 2003. (IPOM 2003). 3rd},
title = {{A data clustering algorithm for mining patterns from event logs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1251233},
year = {2003}
}
@article{Yeomans2019,
abstract = {Programming skills are an increasingly desirable asset across disciplines; however, learning to program continues to be difficult for many students. To improve pedagogy, we need to better understand the concepts that students find difficult and which have the biggest impact on their learning. Threshold-concept theory provides a potential lens on student learning, focusing on concepts that are troublesome and transformative. However, there is still a lack of consensus as to what the most relevant threshold concepts in programming are. The challenges involved are related to concept granularity and to evidencing some of the properties expected of threshold concepts. In this article, we report on a qualitative study aiming to address some of these concerns. The study involved focus groups with undergraduate students of different-year groups as well as professional software developers so as to gain insights into how perspectives on concepts change over time. Four concepts emerged from the data, where the majority of participants agreed on their troublesome nature—including abstract classes and data structures. Some of these concepts are considered transformative, too, but the evidence base is weaker. However, even though these concepts may not be considered transformative in the “big” sense of threshold concept theory, we argue the “soft” transformative effect of such concepts means they can provide important guidance for pedagogy and the design of programming courses. Further analysis of the data identified additional concepts that may hinder rather than help the learning of these threshold concepts, which we have called “accidental complexities.” We conclude the article with a critique of the use of threshold concepts as a lens for studying students' learning of programming.},
annote = {data structures

classes and inheritance

abstract vlasses

user interface architecturs

small, qualitative study

ur primary research question was “What are threshold concepts for learners of (object- oriented) programming?” As we progressed through our study, we added a secondary research question: “Are threshold concepts a suitable, and pedagogically useful, concept in understanding learning of (object-oriented) programming?”

Sanders and McCartney's recent survey [42] provides a good overview of research undertaken in this area, including a list of some concepts identified by var- ious authors, and highlights a number of open challenge

Kate Sanders and Robert McCartney. 2016. Threshold concepts in computing: Past, present, and future. In Proceedings of the 16th Koli Calling International Conference on Computing Education Research (Koli Calling'16). ACM, New York, NY, 91–100. DOI:https://doi.org/10.1145/2999541.2999546

Identified Threshold Concepts Programming Concept Dimension of Threshold Concept Theory Data Example Classes and inheritance Troublesome and transformative by practitioners, troublesome and transformative by students “Classes and inheritance for me . . . yes, getting your head around that was quite hard but then once you, it becomes quite a vital part of the programming once you get your head around it” year 1 student participant User Interface Architectures Transformative by practitioners, troublesome and transformative by students “The MVC [Model View Controller] for me it's, I don't really get it yet like how to actually do it but, yes, it actually makes the programme neater and stuff, yes.” year 1 student participant Data structures Troublesome by practitioners, troublesome and transformative by students “(Data Structures) weren't hard to under- stand, but we don't, we all understand, we should understand it a little bit better than what we currently do. And definitely the first year we felt that way” year 3 student participant Abstract classes Troublesome and transformative by practitioners, troublesome by students “And abstract classes for me anyway, I don't know about anyone else . . . Yes, I mean I understand it I just don't know how like I just I've never seen the point in using it” year 1 student participant},
author = {Yeomans, Lucy and Zschaler, Steffen and Coate, Kelly},
doi = {10.1145/3283071},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yeomans, Zschaler, Coate - 2019 - Transformative and troublesome Students' and professional programmers' perspectives on difficult conce.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {Accidental complexities,CS2,Computer science curriculum,Focus groups,Learning programming,Threshold concepts},
mendeley-tags = {CS2},
month = {jan},
number = {3},
pages = {Article 23},
publisher = {Association for Computing Machinery},
title = {{Transformative and troublesome? Students' and professional programmers' perspectives on difficult concepts in programming}},
volume = {19},
year = {2019}
}
@inproceedings{Bhat2006,
abstract = {This paper discusses software development using the Test Driven Development (TDD) methodology in two different environments (Windows and MSN divisions) at Microsoft. In both these case studies we measure the various context, product and outcome measures to compare and evaluate the efficacy of TDD. We observed a significant increase in quality of the code (greater than two times) for projects developed using TDD compared to similar projects developed in the same organization in a non-TDD fashion. The projects also took at least 15{\%} extra upfront time for writing the tests. Additionally, the unit tests have served as auto documentation for the code when libraries/APIs had to be used as well as for code maintenance. Copyright 2006 ACM.},
annote = {Evaluating the efficacy of test-driven development: Industrial case studies},
author = {Bhat, T and Nagappan, N},
booktitle = {ISCE'06 - Proceedings of the 5th ACM-IEEE International Symposium on Empirical Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {356--363},
title = {{Evaluating the efficacy of test-driven development: Industrial case studies}},
url = {citeulike-article-id:3934556 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34247347410{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@inproceedings{Denny2008,
abstract = {Common exam practice centres around two question types: code tracing (reading) and code writing. It is commonly believed that code tracing is easier than code writing, but it seems obvious that different skills are needed for each. These problems also differ in their value on an exam. Pedagogically, code tracing on paper is an authentic task whereas code writing on paper is less so. Yet, few instructors are willing to forgo the code writing question on an exam. Is there another way, easier to grade, that captures the "problem solving through code creation process" we wish to examine? In this paper we propose Parson's puzzle-style problems for this purpose. We explore their potential both qualitatively, through interviews, and quantitatively through a set of CS1 exams. We find notable correlation between Parsons scores and code writing scores. We find low correlation between code writing and tracing and between Parsons and tracing. We also make the case that marks from a Parsons problem make clear what students don't know (specifically, in both syntax and logic) much less ambiguously than marks from a code writing problem. We make recommendations on the design of Parsons problems for the exam setting, discuss their potential uses and urge further investigations of Parsons problems for assessment of CS1 students. Copyright 2008 ACM.},
address = {Sydney, Australia},
author = {Denny, Paul and Luxton-Reilly, Andrew and Simon, Beth},
booktitle = {ICER'08 - Proceedings of the ACM Workshop on International Computing Education Research},
doi = {10.1145/1404520.1404532},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Denny, Luxton-Reilly, Simon - 2008 - Evaluating a new exam question Parsons problems.pdf:pdf},
isbn = {9781605582160},
keywords = {Assessment,CS1,CS2,Code writing,Exam questions,Parsons problems,Tracing},
mendeley-tags = {CS2},
pages = {113--124},
publisher = {ACM Press},
title = {{Evaluating a new exam question: Parsons problems}},
url = {http://portal.acm.org/citation.cfm?doid=1404520.1404532},
year = {2008}
}
@article{Sheil1981,
author = {Sheil, B. A.},
doi = {10.1145/356835.356840},
issn = {03600300},
journal = {ACM Computing Surveys},
month = {jan},
number = {1},
pages = {101--120},
publisher = {ACM},
title = {{The Psychological Study of Programming}},
url = {http://dl.acm.org/citation.cfm?id=356835.356840},
volume = {13},
year = {1981}
}
@inproceedings{Mockus2009,
address = {Lake Buena Vista, FL},
author = {Mockus, Audris and Nagappan, Nachiappan and Dinh-Trong, Trung T.},
booktitle = {2009 3rd International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2009.5315981},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mockus, Nagappan, Dinh-Trong - 2009 - Test coverage and post-verification defects A multiple case study.pdf:pdf;:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mockus, Nagappan, Dinh-Trong - 2009 - Test coverage and post-verification defects A multiple case study(2).pdf:pdf},
isbn = {978-1-4244-4842-5},
issn = {1938-6451},
keywords = {Computer industry,Control systems,Industrial relations,Lead,Programming,Software engineering,Software measurement,Software quality,Software testing,System testing,churn,code complexity,cost-effective level,developer experience,industrial software project,organization development,post-verification defect,program testing,program verification,remote development team,software measure,software metrics,software quality,test coverage},
language = {English},
mendeley-tags = {churn,test coverage},
month = {oct},
pages = {291--301},
publisher = {IEEE},
title = {{Test coverage and post-verification defects: A multiple case study}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5315981 http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5315981},
year = {2009}
}
@article{Krishna2017,
abstract = {{\textcopyright} 2017 Elsevier B.V. Context: Developers use bad code smells to guide code reorganization. Yet developers, textbooks, tools, and researchers disagree on which bad smells are important. How can we offer reliable advice to developers about which bad smells to fix? Objective: To evaluate the likelihood that a code reorganization to address bad code smells will yield improvement in the defect-proneness of the code. Method: We introduce XTREE, a framework that analyzes a historical log of defects seen previously in the code and generates a set of useful code changes. Any bad smell that requires changes outside of that set can be deprioritized (since there is no historical evidence that the bad smell causes any problems). Evaluation: We evaluate XTREE's recommendations for bad smell improvement against recommendations from previous work (Shatnawi, Alves, and Borges) using multiple data sets of code metrics and defect counts. Results: Code modules that are changed in response to XTREE's recommendations contain significantly fewer defects than recommendations from previous studies. Further, XTREE endorses changes to very few code metrics, so XTREE requires programmers to do less work. Further, XTREE's recommendations are more responsive to the particulars of different data sets. Finally XTREE's recommendations may be generalized to identify the most crucial factors affecting multiple datasets (see the last figure in paper). Conclusion: Before undertaking a code reorganization based on a bad smell report, use a framework like XTREE to check and ignore any such operations that are useless; i.e. ones which lack evidence in the historical record that it is useful to make that change. Note that this use case applies to both manual code reorganizations proposed by developers as well as those conducted by automatic methods.},
archivePrefix = {arXiv},
arxivId = {1609.03614},
author = {Krishna, Rahul and Menzies, Tim and Layman, Lucas},
doi = {10.1016/j.infsof.2017.03.012},
eprint = {1609.03614},
file = {:C$\backslash$:/Users/laymanl/Desktop/layman pubs/Krishna2017.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Bad smells,Decision trees,Performance prediction},
pages = {53--66},
title = {{Less is More: Minimizing Code Reorganization using XTREE}},
url = {https://arxiv.org/pdf/1609.03614.pdf},
volume = {88},
year = {2017}
}
@article{KitchenhamS.L.PfleegerL.M.PickardP.W.JonesD.C.HoaglinK.ElEmamandJ.Rosenberg2002,
author = {{Kitchenham S. L. Pfleeger, L. M. Pickard, P. W. Jones, D. C. Hoaglin, K. El Emam, and J. Rosenberg}, B A},
journal = {Transactions on Software Engineering},
keywords = {case studies,case study,empirical},
number = {8},
pages = {721--733},
title = {{Preliminary Guidelines for Empirical Research in Software Engineering}},
volume = {28},
year = {2002}
}
@article{Blom2005,
author = {Blom, Jan and Chipchase, Jan and Lehikoinen, Jaakko},
doi = {10.1145/1070838.1070863},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jul},
number = {7},
pages = {37},
publisher = {ACM},
title = {{Contextual and cultural challenges for user mobility research}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=1070863{\&}type=html},
volume = {48},
year = {2005}
}
@inproceedings{Melnik2005,
abstract = {Research was conducted on using agile methods in software engineering education. This paper explores the perceptions of students from five different academic levels of agile practices. Information has been gathered through the collection of quantitative and qualitative data over three academic years, and analysis reveals student experiences, mainly positive but also some negative. Student opinions indicate the preference to continue to use agile practices at the workplace if allowed. A way these findings may potentially be extrapolated to the industrial settings is discussed. Finally, this report should encourage other academics considering adoption of agile methods in their computer science or software engineering curricula.},
address = {New York, NY, USA},
annote = {A cross-program investigation of students{\&}{\#}039; perceptions of agile methods},
author = {Melnik, Grigori and Maurer, Frank},
booktitle = {ICSE '05: Proceedings of the 27th international conference on Software engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {481--488},
publisher = {ACM},
title = {{A cross-program investigation of students' perceptions of agile methods}},
url = {citeulike-article-id:3934715 http://dx.doi.org/10.1145/1062455.1062543},
year = {2005}
}
@inproceedings{Johnson2007a,
abstract = {Zorro is a system designed to automatically determine whether a developer is complying with an operational definition of Test-Driven Development (TDD) practices. Automated recognition of TDD can benefit the software development community in a variety of ways, from inquiry into the "true nature" of TDD, to pedagogical aids to support the practice of test-driven development, to support for more rigorous empirical studies on the effectiveness of TDD in both laboratory and real world settings. This paper introduces the Zorro system, its operational definition of TDD, the analyses made possible by Zorro, and our ongoing efforts to validate the system. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Automated recognition of test-driven development with Zorro},
author = {Johnson, P M and Kou, H},
booktitle = {Proceedings - AGILE 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {15--24},
title = {{Automated recognition of test-driven development with Zorro}},
url = {citeulike-article-id:3934663 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-46449126920{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{6113308,
abstract = {This paper presents an approach for modeling location-based profiles of social image media based on tagging information and collaborative geo-reference annotations. We utilize pattern mining techniques for obtaining sets of tags that are specific for the specified point, landmark, or region of interest. Next, we show how these candidate patterns can be presented and visualized for interactive exploration using a combination of general pattern mining visualizations and views specialized on geo-referenced tagging data. We present a case study using publicly available data from the Flickr photo sharing application.},
author = {Lemmerich, F and Atzmueller, M},
booktitle = {Privacy, security, risk and trust (passat), 2011 ieee third international conference on and 2011 ieee third international conference on social computing (socialcom)},
doi = {10.1109/PASSAT/SocialCom.2011.186},
keywords = {Flickr photo sharing application,collaborative geo},
pages = {1356--1363},
title = {{Modeling Location-Based Profiles of Social Image Media Using Explorative Pattern Mining}},
year = {2011}
}
@article{Smith2005b,
abstract = {Last month you learned about the numerous advantages to test-driven development (TDD), which is a primary component of an agile development technique. Now you'll find out how to build a prototype TDD environment for an embedded system. In the first part of this series, the idea of test-driven development (TDD) is introduced. Briefly described, TDD involves replacing volumes of documents for customer requirements and developer ideas with a series of specified and agreed upon tests before coding starts. The TDD environment offers advantages to those who want to fully embrace the TDD concepts. It's also advantageous to those who realize that an automated test environment quickens the familiar test-last approach},
annote = {Practical application for TDD. Part 2. Automated test-driven environment},
author = {Smith, M and Banffi, M and Flaman, W and Geras, A and Huang, L and Kwan, A and Martin, A and Miller, J},
isbn = {0896-8985},
journal = {Circuit Cellar},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {177},
pages = {60--67},
title = {{Practical application for TDD. Part 2. Automated test-driven environment}},
url = {citeulike-article-id:3934799 {\#}},
year = {2005}
}
@book{Chung1999,
author = {Chung, L and Nixon, B A and Yu, E and Mylopolous, J},
publisher = {Kluwer Academic Publishing},
title = {{Non-Functional Requirements in Software Engineering}},
year = {1999}
}
@inproceedings{Nguyen2011,
address = {Lawrence, KS},
author = {Nguyen, Anh Tuan and Nguyen, Tung Thanh and Al-Kofahi, Jafar and Nguyen, Hung Viet and Nguyen, Tien N.},
booktitle = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
doi = {10.1109/ASE.2011.6100062},
isbn = {978-1-4577-1639-3},
issn = {1938-4300},
keywords = {BugScout,Defect Localization,Prediction algorithms,Software algorithms,Software systems,Synchronization,Topic Modeling,Training,Vectors,bug report,buggy code,buggy files,program debugging,search space,software development,software engineering,source code,text analysis,textual contents,topic based approach},
language = {English},
month = {nov},
pages = {263--272},
publisher = {IEEE},
title = {{A topic-based approach for narrowing the search space of buggy files from a bug report}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6100062},
year = {2011}
}
@inproceedings{Goldring2004,
address = {New York, New York, USA},
author = {Goldring, Tom},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029227},
isbn = {1581139748},
keywords = {visualization},
month = {oct},
pages = {119},
publisher = {ACM Press},
title = {{Scatter (and other) plots for visualizing user profiling data and network traffic}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029227},
year = {2004}
}
@article{Brugali09,
author = {Brugali, Davide and Scandurra, Patrizia},
journal = {IEEE Robotics and Automation Magazine},
month = {dec},
pages = {84--96},
title = {{Component-Based Robotic Engineering (Part I)}},
year = {2009}
}
@article{Freeman2007,
abstract = {"Point Argument: Mock Objects: Find Out Who Your Friends Are," by Steve Freeman and Nat Pryce. Mock objects help guide object-oriented programming by concentrating on what objects do, no what they are. "Counterpoint Argument: "TDD: Don't Much It Up with Too Many Mocks," by Joshua Kerievsky. Routinely test-driving code with mock objects leads to premature object composition, hard-to-read and fragile code, and lost time. This department is part of a special issue on test-driven development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Point/counterpoint},
author = {Freeman, S and Pryce, N and Kerievsky, J},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {80--83},
title = {{Point/counterpoint}},
url = {citeulike-article-id:3934610 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248330055{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@article{Robillard2004,
author = {Robillard, Martin P and Coelho, Wesley and Murphy, Gail C},
journal = {IEEE Transactions on Software Engineering},
number = {12},
pages = {889--903},
title = {{How effective developers investigate source code: An exploratory study}},
volume = {30},
year = {2004}
}
@article{Dyba2008,
author = {Dyb{\aa}, Tore and Dings{\o}yr, Torgeir},
number = {9-10},
pages = {833--859},
title = {{Epirical Studies of Agile Software Development: A Systematic Review}},
volume = {50},
year = {2008}
}
@article{Booch2008,
abstract = {Over the past 25 years, we've made great advances in tooling, technologies, and techniques that make software design more concrete. But design still requires careful thought. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Back to the future},
author = {Booch, G},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {20--21},
title = {{Back to the future}},
url = {citeulike-article-id:3934558 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57049172643{\&}{\#}38 partnerID=40},
volume = {25},
year = {2008}
}
@inproceedings{Ardis2004,
abstract = {Test-first development is a practice of extreme programming designed to produce reliable software quickly. Rather than writing the code first, a software engineer first creates the tests that will demonstrate that the software works correctly. Coding follows and is often guided by the tests. Practitioners of this method claim that the discipline of developing the tests before the code focuses their attention on the right problems and yields cleaner code. Test-First Teaching is a method of course development that incorporates Instructional Design methods to create more effective instruction. The instruments that will be used to test students' day-to-day learning of the course material - assignments and quizzes are created first, and instruction is developed to meet the students' needs. Components of Test-First Teaching are applied at both course and lecture levels. Test-First Teaching has been used successfully to develop courses for the new Bachelor of Science in Software Engineering program at Rose-Hulman Institute of Technology. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {Test-first teaching: Extreme programming meets instructional design in software engineering courses},
author = {Ardis, M A and Dugas, C A},
booktitle = {Proceedings - Frontiers in Education Conference, FIE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Test-first teaching: Extreme programming meets instructional design in software engineering courses}},
url = {citeulike-article-id:3934543 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-21644486810{\&}{\#}38 partnerID=40},
volume = {2},
year = {2004}
}
@inproceedings{4258673,
abstract = {A Weblog is a Web site where entries are made in diary style, maintained by its sole author - a blogger, and displayed in a reverse chronological order. Due to the freedom and convenience of publishing in Weblogs, this form of media provides an ideal environment as a propaganda platform for terrorist groups to promote their ideologies and as an operation platform for organizing crimes. In this work, we present a framework to analyze and visualize Weblog social network embedded beneath relevant Weblogs gathered through topic-specific exploration. Link analysis uses the relationships between bloggers to construct the Weblog social network. Content analysis associates similar blog messages to unveil implicit relationships found in the semantics to further improve the Weblog social network analysis. Users can use different interactive information visualization techniques to explore various aspects of the underlying social network at different levels of abstraction. With the capability of analyzing and visualizing Weblog social networks in terrorist and crime related matters, intelligence agencies and law enforcement will be able to have an additional tools and means to ensure the national security.},
author = {Yang, C C and Ng, T D},
booktitle = {Intelligence and Security Informatics, 2007 IEEE},
doi = {10.1109/ISI.2007.379533},
keywords = {Web site,content analysis,crime related Weblog,ele},
month = {may},
pages = {55--58},
title = {{Terrorism and Crime Related Weblog Social Network: Link, Content Analysis and Information Visualization}},
year = {2007}
}
@inproceedings{Hertz2010,
abstract = {Thirty-one years ago, the ACM Computing Curricula used the terms "CS1" and "CS2" to designate the first two two courses in the introductory sequence of a computer science major. While computer science education has greatly changed since that time, we still refer to introduction to programming courses as CS1 and basic data structures courses as CS2. This common shorthand is then used to enable students to transfer between institutions and as a base of many research studies. In this paper we show that while there is wide agreement on the connotation of CS1 and CS2, there is little agreement as to the denotation of these terms. Surveying CS1 and CS2 instructors, we find little agreement on how important various topics are to each of these course and less agreement on how well students master the material. Even after limiting the analysis to whether a topic has ANY important or students complete a course with ANY mastery of the material, we continue to find significant disagreements between instructors.},
address = {Milwaukee, WI, USA},
author = {Hertz, Matthew},
booktitle = {SIGCSE'10 - Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1734263.1734335},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hertz - 2010 - What do CS1 and CS2 mean Investigating differences in the early courses.pdf:pdf},
isbn = {9781605588858},
keywords = {CS1,CS2,Curriculum design,Survey},
mendeley-tags = {CS2},
pages = {199--203},
publisher = {ACM Press},
title = {{What do "CS1" and "CS2" mean? Investigating differences in the early courses}},
url = {http://portal.acm.org/citation.cfm?doid=1734263.1734335},
year = {2010}
}
@article{madigan2014,
author = {Madigan, David and Stang, Paul E and Berlin, Jesse A and Schuemie, Martijn and Overhage, J Marc and Suchard, Marc A and Dumouchel, Bill and Hartzema, Abraham G and Ryan, Patrick B},
journal = {Annual Review of Statistics and Its Application},
pages = {11--39},
title = {{A systematic statistical approach to evaluating evidence from observational studies}},
volume = {1},
year = {2014}
}
@article{carlson2009,
author = {Carlson, Melissa D A and Morrison, R Sean},
journal = {Journal of palliative medicine},
number = {1},
pages = {77--82},
title = {{Study design, precision, and validity in observational studies}},
volume = {12},
year = {2009}
}
@inproceedings{Runeson2007,
address = {Minneapolis, MN},
author = {Runeson, Per and Alexandersson, Magnus and Nyholm, Oskar},
booktitle = {29th International Conference on Software Engineering (ICSE'07)},
issn = {0270-5257},
keywords = {Failure analysis,Mobile communication,Natural language processing,Natural languages,Prototypes,Relays,Robustness,Software engineering,Software testing,Sony Ericsson Mobile Communication,Vocabulary,duplicate defect report detection,formal method,program testing,prototype tool,software prototyping,user testing},
language = {English},
month = {may},
pages = {499--510},
publisher = {IEEE},
title = {{Detection of Duplicate Defect Reports Using Natural Language Processing}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4222611},
year = {2007}
}
@inproceedings{Park2011b,
address = {New York, New York, USA},
author = {Park, Myung Ah},
booktitle = {Proceedings of the 2011 Information Security Curriculum Development Conference on - InfoSecCD '11},
doi = {10.1145/2047456.2047469},
isbn = {9781450308120},
keywords = {application,information security education,secure coding,security,security principles,visual C{\#},visual basic},
month = {sep},
pages = {84--93},
publisher = {ACM Press},
title = {{Embedding security into visual programming courses}},
url = {http://dl.acm.org/citation.cfm?id=2047456.2047469},
year = {2011}
}
@article{Apfelbaum1997,
author = {Apfelbaum, Larry and Doyle, John},
journal = {Software Quality Week Conference},
pages = {1--14},
title = {{Model based testing}},
url = {http://www.geocities.com/model{\_}based{\_}testing/sqw97.pdf},
year = {1997}
}
@inproceedings{Hertzog2006b,
address = {New York, New York, USA},
author = {Hertzog, Patrick},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179596},
isbn = {1595935495},
keywords = {alarm visualization,connection visualization,parallel-coordinates,reaction time,reactivity,security,starfield},
month = {nov},
pages = {95},
publisher = {ACM Press},
title = {{Visualizations to improve reactivity towards security incidents inside corporate networks}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179596},
year = {2006}
}
@article{Pitt-Francis2008,
abstract = {Cardiac modelling is the area of physiome modelling where the available simulation software is perhaps most mature, and it therefore provides an excellent starting point for considering the software requirements for the wider physiome community. In this paper, we will begin by introducing some of the most advanced existing software packages for simulating cardiac electrical activity. We consider the software development methods used in producing codes of this type, and discuss their use of numerical algorithms, relative computational efficiency, usability, robustness and extensibility.We then go on to describe a class of software development methodologies known as test-driven agile methods and argue that such methods are more suitable for scientific software development than the traditional academic approaches. A case study is a project of our own, Cancer, Heart and Soft Tissue Environment, which is a library of computational biology software that began as an experiment in the use of agile programming methods. We present our experiences with a review of our progress thus far, focusing on the advantages and disadvantages of this new approach compared with the development methods used in some existing packages.We conclude by considering whether the likely wider needs of the cardiac modelling community are currently being met and suggest that, in order to respond effectively to changing requirements, it is essential that these codes should be more malleable. Such codes will allow for reliable extensions to include both detailed mathematical models-of the heart and other organs-and more efficient numerical techniques that are currently being developed by many research groups worldwide. {\^{A}}{\textcopyright} 2008 The Royal Society.},
annote = {Chaste: Using agile programming techniques to develop computational biology software},
author = {Pitt-Francis, J and Bernabeu, M O and Cooper, J and Garny, A and Momtahan, L and Osborne, J and Pathmanathan, P and Rodriguez, B and Whiteley, J P and Gavaghan, D J},
journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1878},
pages = {3111--3136},
title = {{Chaste: Using agile programming techniques to develop computational biology software}},
url = {citeulike-article-id:3934755 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-48349125992{\&}{\#}38 partnerID=40},
volume = {366},
year = {2008}
}
@phdthesis{Mckenzie2006,
abstract = {Women Partnering is non-profit organization that helps women who are financially vulnerable. This organization establishes relationships with the women and connects them to support services. This project created a software system to support Women Partnering{\^{a}}€™s daily operations and reporting needs, which replaced the previous manually intensive, paper-based system. There were many problems with the previous paper-based system including the following: data duplication, data not readily available, and lack of a reporting capability. Besides these problems, the previous system was not expected to support anticipated growth. The student followed a Test-Driven Development Methodology while building the software system. This is the first time that the student has used Test-Driven Development on a project. To help with his understanding, he compared and contrasted this methodology to the Zachman Framework Methodology. The student knew that he also had to secure the application, so he researched the Rijndael cipher. The analysis, design, and testing is handled differently in Test-Driven Development. Testing happens first, and the design captures the requirements. The student found Test-Driven Development lacking in a few areas, so he used other tools that are not part of the methodology like entity relationship diagrams and a data dictionary. Since the student was new to Test-Driven Development, he shares his many lessons on this project in hopes to helping others to avoid vi the same pitfalls. The project{\^{a}}€™s next steps include getting help integrated and possible integration with other support agencies. Test-Driven Development is not a tool that should be used on all development projects. Rather, Test-Driven Development works best when the requirements are not clear, when the development team is smaller, and when the requirements are changing frequently. Most importantly, this methodology works well when the users are willing and able to participate throughout the entire project. The student suggests that software developers remain flexible in their tool choice in order to better serve their projects and avoid project failure.},
annote = {Unlocking Test-Driven Development},
author = {Mckenzie, Tim},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Unlocking Test-Driven Development}},
url = {citeulike-article-id:3934711 {\#}},
year = {2006}
}
@article{Castelli2001,
address = {Riverton, NJ, USA},
author = {Castelli, V and Harper, R E and Heidelberger, P and Hunter, S W and Trivedi, K S and Vaidyanathan, K and Zeggert, W P},
issn = {0018-8646},
journal = {IBM Journal of Research {\&} Development},
number = {2},
pages = {311--332},
publisher = {IBM Corp.},
title = {{Proactive management of software aging}},
volume = {45},
year = {2001}
}
@article{Farahmand2010,
author = {Farahmand, Fariborz and Spafford, Eugene H.},
doi = {10.1007/s10796-010-9265-x},
issn = {1387-3326},
journal = {Information Systems Frontiers},
keywords = {Behavior,Insider,Perception,Prospect theory,Risk,agile,nsf},
mendeley-tags = {agile,nsf},
month = {aug},
number = {1},
pages = {5--15},
publisher = {Kluwer Academic Publishers},
title = {{Understanding insiders: An analysis of risk-taking behavior}},
url = {http://dl.acm.org/citation.cfm?id=2451342.2451409},
volume = {15},
year = {2010}
}
@phdthesis{Kile2005,
abstract = {Test-driven development is a practice that involves the implementation of a system from its unit test cases [10, 13] with the goal of producing defect free code [9] that is both understandable and maintainable [17]. Although test-driven development appears to yield code with superior external quality [10], many of the experiments conducted to date used relatively small project sizes, mixed test-driven development with other agile development practices during the experiments, and were not normalized for varying levels of developer experience and organizational maturity. Their results, therefore, did not yield conclusive proof that test-driven development has a positive impact on programmer productivity and overall code quality. This idea paper proposes a more in depth study into test-driven development to either conclusively prove or disprove its perceived benefits.},
annote = {Test-Driven Development: Enhancing the Productivity and Quality of Software Development Teams},
author = {Kile, J F},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
publisher = {Pace University,},
title = {{Test-Driven Development: Enhancing the Productivity and Quality of Software Development Teams}},
url = {citeulike-article-id:3934674 http://utopia.csis.pace.edu/dps/2007/jkile/2005 - Summer/DCS891C/DCS891C TDD Idea Paper.pdf},
year = {2005}
}
@inproceedings{Lyu2007,
author = {Lyu, Michael R.},
booktitle = {Future of Software Engineering (FOSE '07)},
month = {may},
pages = {153--170},
publisher = {IEEE},
title = {{Software Reliability Engineering: A Roadmap}},
url = {http://dl.acm.org/citation.cfm?id=1253532.1254716},
year = {2007}
}
@article{Algan2005,
abstract = {This report aims at providing software developers with an understanding of the test driven software development practices and methods in the context of Java and J2EE {\^{a}}€{\oe}(WEB{\_}19, 2005){\^{a}}€ based technologies. The study is presented in two main stages. The first stage describes the evolution of software verification and validation techniques and how they have been applied throughout the software development lifecycle, progressing upto Agile Software Development practices and Test Driven Development. The second stage of the study presents a collection of patterns and methods for applying test driven development in real world scenarios. These techniques also aim to show how test driven development forces the classes and responsibilities to be decoupled and highly cohesive, leading to a beter object oriented design and high quality code.},
annote = {TEST DRIVEN SOFTWARE DEVELOPMENT},
author = {Algan, F},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{TEST DRIVEN SOFTWARE DEVELOPMENT}},
url = {citeulike-article-id:3934533 http://library.iyte.edu.tr/tezler/master/bilgisayaryazilimi/T000412.pdf},
year = {2005}
}
@misc{Canfora2006,
abstract = {With the growing interest for Extreme Programming, test driven development (TDD) has been increasingly investigated, and several experiments have been executed with the aim of understanding if and when it is preferable to the traditional practice of testing the code after having written it (named TAC in the paper). However, the research concerning TDD is at its beginning and the body of knowledge is largely immature. Thin paper discusses an experiment carried out within a Spanish software company with the aim of comparing productivity in TDD and TAC. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {Productivity of test driven development: A controlled experiment with professionals},
author = {Canfora, G and Cimitile, A and Garcia, F and Piattini, M and Visaggio, C A},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {383--388},
title = {{Productivity of test driven development: A controlled experiment with professionals}},
url = {citeulike-article-id:3934563 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746238094{\&}{\#}38 partnerID=40},
volume = {4034 LNCS},
year = {2006}
}
@inproceedings{Harbort2011a,
address = {New York, New York, USA},
author = {Harbort, Zach and Louthan, G. and Hale, J.},
booktitle = {Proceedings of the Seventh Annual Workshop on Cyber Security and Information Intelligence Research - CSIIRW '11},
doi = {10.1145/2179298.2179383},
isbn = {9781450309455},
month = {oct},
pages = {1},
publisher = {ACM Press},
title = {{Techniques for attack graph visualization and interaction}},
url = {http://dl.acm.org/citation.cfm?id=2179298.2179383},
year = {2011}
}
@inproceedings{Wu2003,
author = {Wu, James and Graham, T.C.N. and Smith, Paul W.},
booktitle = {Proceedings of the 2003 International Symposium on Empirical Software Engineering},
doi = {10.1109/ISESE.2003.1237991},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Graham, Smith - 2003 - A study of collaboration in software design.pdf:pdf},
isbn = {0-7695-2002-2},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {304--313},
publisher = {IEEE Comput. Soc},
title = {{A study of collaboration in software design}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1237991},
year = {2003}
}
@inproceedings{Williams2007b,
abstract = {Despite many professed benefits of collaboration, some computer science educators feel students need to master work individually, particularly in the courses early in the curriculum that feed into software engineering courses. In the natural sciences, however, students almost always work with one or more partners in the laboratory. What can computer science educators learn about collaborative lab settings from our natural science counterparts? We conducted a survey of science and computer science educators to compare views and use of collaboration in their classes. The positive and negative aspects of collaboration, as reported by the natural science educators, are strikingly similar to those of computer science educators. These results suggest that computer science educators should be more open to the use of collaborative labs, as is done in the natural sciences, for the overall benefit to students.},
address = {Dublin, Ireland},
author = {Williams, Laurie and Layman, Lucas},
booktitle = {20th Conference on Software Engineering Education and Training (CSEET'07)},
doi = {10.1109/CSEET.2007.31},
isbn = {0-7695-2893-7},
issn = {1093-0175},
keywords = {Biology,Chemistry,Collaboration,Collaborative software,Collaborative work,Computer science,Laboratories,Physics,Programming profession,Software engineering,collaborative lab settings,computer science education,computer science educators,mypubs,natural science educators,natural sciences computing,software engineering courses},
mendeley-tags = {mypubs},
month = {jul},
pages = {72--82},
publisher = {IEEE},
shorttitle = {Software Engineering Education {\&} Training, 2007. C},
title = {{Lab Partners: If They're Good Enough for the Sciences, Why Aren't They Good Enough for Us?}},
year = {2007}
}
@inproceedings{Lemos2007,
abstract = {We present CodeGenie, a tool that implements a test-driven approachto search and reuse of code available on large-scale coderepositories. While using CodeGenie developers design test cases fora desired feature first, similar to Test-driven Development (TDD).However, instead of implementing the feature as in TDD, CodeGenieautomatically searches for it based on information available in thetests. To check the suitability of the candidate results in thelocal context, each result is automatically woven into thedeveloper's project and tested using the original tests. Thedeveloper can then reuse the most suitable result. Later, reusedcode can also be unwoven from the project as wished. For the codesearching and wrapping facilities, CodeGenie relies on Sourcerer, anInternet-scale source code infrastructure that we have developed},
address = {New York, NY, USA},
annote = {CodeGenie: using test-cases to search and reuse source code},
author = {Lemos, Ot{\'{a}}vio and Bajracharya, Sushil and Ossher, Joel and Morla, Ricardo and Masiero, Paulo and Baldi, Pierre and Lopes, Cristina},
booktitle = {ASE '07: Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {525--526},
publisher = {ACM},
title = {{CodeGenie: using test-cases to search and reuse source code}},
url = {citeulike-article-id:3934692 http://dx.doi.org/10.1145/1321631.1321726},
year = {2007}
}
@article{Cao2008,
abstract = {An analysis of data from 16 software development organizations reveals seven agile requirements-engineering practices, along with their benefits and challenges. These practices include face-to-face communication, iterative RE, extreme prioritization, constant planning, prototyping, test-driven development, and reviews and tests. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Agile requirements engineering practices: An empirical study},
author = {Cao, L and Ramesh, B},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {60--67},
title = {{Agile requirements engineering practices: An empirical study}},
url = {citeulike-article-id:3934566 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-39449130107{\&}{\#}38 partnerID=40},
volume = {25},
year = {2008}
}
@inproceedings{5655057,
abstract = {We present VALET, a Visual Analytics Law Enforcement Toolkit for analyzing spatiotemporal law enforcement data. VALET provides users with a suite of analytical tools coupled with an interactive visual interface for data exploration and analysis. This system includes linked views and interactive displays that spatiotemporally model criminal, traffic and civil (CTC) incidents and allows officials to observe patterns and quickly identify regions with higher probabilities of activity. Our toolkit provides analysts with the ability to visualize different types of data sets (census data, daily weather reports, zoning tracts, prominent calendar dates, etc.) that provide an insight into correlations among CTC incidents and spatial demographics. In the spatial domain, we have implemented a kernel density estimation mapping technique that creates a color map of spatially distributed CTC events that allows analysts to quickly find and identify areas with unusually large activity levels. In the temporal domain, reports can be aggregated by day, week, month or year, allowing the analysts to visualize the CTC activities spatially over a period of time. Furthermore, we have incorporated temporal prediction algorithms to forecast future CTC incident levels within a 95{\%} confidence interval. Such predictions aid law enforcement officials in understanding how hotspots may grow in the future in order to judiciously allocate resources and take preventive measures. Our system has been developed using actual law enforcement data and is currently being evaluated and refined by a consortium of law enforcement agencies.},
author = {Malik, A and Maciejewski, R and Collins, T F and Ebert, D S},
booktitle = {Technologies for Homeland Security (HST), 2010 IEEE International Conference on},
doi = {10.1109/THS.2010.5655057},
keywords = {VALET,census data,daily weather reports,data analy},
pages = {222--228},
title = {{Visual Analytics Law Enforcement Toolkit}},
year = {2010}
}
@inproceedings{4381261,
abstract = {3D sensing and modeling is increasingly important for mobile robotics in general and safety, security and rescue robotics (SSRR) in particular. To reduce the data and to allow for efficient processing, e.g., with computational geometry algorithms, it is necessary to extract surface data from 3D point clouds delivered by range sensors. A significant amount of work on this topic exists from the computer graphics community. But the existing work relies on relatively exact point cloud data. As also shown by others, sensors suited for mobile robots are very noise-prone and standard approaches that use local processing on surface normals are doomed to fail. Hence plane fitting has been suggested as solution by the robotics community. Here, a novel approach for this problem is presented. Its main feature is that it is based on region growing and that the underlying mathematics has been re-formulated such that an incremental fit can be done, i.e., the best fit surface does not have to be completely re-computed the moment a new point is investigated in the region growing process. The worst case complexity is O(n log(n)), but as shown in experiments it tends to scale linearly with typical data. Results with real world data from a Swissranger time-of-flight camera are presented where surface polygons are always successfully extracted within about 0.3 sec.},
author = {Vaskevicius, N and Birk, A and Pathak, K and Poppinga, J},
booktitle = {Safety, Security and Rescue Robotics, 2007. SSRR 2007. IEEE International Workshop on},
doi = {10.1109/SSRR.2007.4381261},
keywords = {3D point clouds,3D sensing,computational geometry},
pages = {1--6},
title = {{Fast Detection of Polygons in 3D Point Clouds from Noise-Prone Range Sensors}},
year = {2007}
}
@inproceedings{Sarcia2009b,
address = {Lake Buena Vista, FL},
author = {Sarcia, S.A. and Basili, Victor R. and Cantone, Giovanni},
booktitle = {Proceedings of the International Symposium on Empirical Software Engineering and Measurement},
pages = {123--132},
title = {{Scope error detection and handling concerning software estimation models}},
year = {2009}
}
@inproceedings{Teoh2006a,
address = {New York, New York, USA},
author = {Teoh, Soon Tee and Ranjan, Supranamaya and Nucci, Antonio and Chuah, Chen-Nee},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179593},
isbn = {1595935495},
keywords = {BGP,network security,routing,visualization},
month = {nov},
pages = {81},
publisher = {ACM Press},
title = {{BGP eye}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179593},
year = {2006}
}
@inproceedings{6103802,
abstract = {Network security visualization is a highlighted topic of network security research in recent years, The existing research situation of network security visualization is analyzed. For the technical issues that the index of security situation is not accurate, and visual effects is not straightforward, the paper designed and implemented the security situation visualization prototype system based on geographic information systems, network topology graph, attack paths. The security situation data show in multiple views, multi-angle, multi-level display to the user by visualization technology, therefore the performance of the security situation will be more accurate and vivid, assessment of network security situation become timely and accurate, laying the foundation for rapid decision-making.},
author = {Li, Xiaoyan and Wang, Qingxian and Yang, Lin and Luo, Xiangyang},
booktitle = {Multimedia Information Networking and Security (MINES), 2011 Third International Conference on},
doi = {10.1109/MINES.2011.133},
keywords = {attack paths,geographic information system,multile},
pages = {411--415},
title = {{Network Security Situation Awareness Method Based on Visualization}},
year = {2011}
}
@article{Gino2011,
abstract = {Across four experimental studies, individuals who were depleted of their self-regulatory resources by an initial act of self-control were more likely to “impulsively cheat” than individuals whose self-regulatory resources were intact. Our results demonstrate that individuals depleted of self-control resources were more likely to behave dishonestly (Study 1). Depletion reduced people's moral awareness when they faced the opportunity to cheat, which, in turn, was responsible for heightened cheating (Study 2). Individuals high in moral identity, however, did not show elevated levels of cheating when they were depleted (Study 3), supporting our hypothesis that self-control depletion increases cheating when it robs people of the executive resources necessary to identify an act as immoral or unethical. Our results also show that resisting unethical behavior both requires and depletes self-control resources (Study 4). Taken together, our findings help to explain how otherwise ethical individuals predictably engage in unethical behavior.},
author = {Gino, Francesca and Schweitzer, Maurice E. and Mead, Nicole L. and Ariely, Dan},
journal = {Organizational Behavior and Human Decision Processes},
keywords = {Dishonesty,Ego depletion,Ethical decision making,Impulsive cheating,Moral identity,Self-control,Self-regulatory resources,Unethical behavior,agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {191--203},
title = {{Unable to resist temptation: How self-control depletion promotes unethical behavior}},
url = {http://www.sciencedirect.com/science/article/pii/S0749597811000422},
volume = {115},
year = {2011}
}
@inproceedings{Thomas2010,
address = {Timisoara, Romania},
author = {Thomas, Stephen W. and Adams, Bram and Hassan, Ahmed E. and Blostein, Dorothea},
booktitle = {2010 10th IEEE Working Conference on Source Code Analysis and Manipulation},
doi = {10.1109/SCAM.2010.13},
isbn = {978-1-4244-8655-7},
keywords = {Analytical models,Color,History,JHot draw system,Measurement,Monitoring,Software,Software evolution,Visualization,latent Dirichlet allocation,software evolution,software maintenance,software metrics,software re-engineering,source code evolution,statistical analysis,topic drops,topic evolutions,topic modeling,topic models,topic spikes},
language = {English},
month = {sep},
pages = {55--64},
publisher = {IEEE},
title = {{Validating the Use of Topic Models for Software Evolution}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5601831},
year = {2010}
}
@article{Meyer2005,
author = {Meyer, Jan H. F. and Land, Ray},
doi = {10.1007/s10734-004-6779-5},
issn = {0018-1560},
journal = {Higher Education},
keywords = {CS2},
mendeley-tags = {CS2},
month = {apr},
number = {3},
pages = {373--388},
publisher = {Kluwer Academic Publishers},
title = {{Threshold concepts and troublesome knowledge (2): Epistemological considerations and a conceptual framework for teaching and learning}},
url = {http://link.springer.com/10.1007/s10734-004-6779-5},
volume = {49},
year = {2005}
}
@inproceedings{Resch2019,
address = {Minneapolis, MN, USA},
author = {Resch, Cheryl and Gardner-McCune, Christina},
booktitle = {SIGCSE '19 Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3287324.3293802},
keywords = {CS2},
mendeley-tags = {CS2},
pages = {1253--1254},
publisher = {Association for Computing Machinery (ACM)},
title = {{Encouraging Reflection in Support of Learning Data Structures}},
year = {2019}
}
@article{Berenson2004,
author = {Berenson, Sarah B and Slaten, Kelli M and Williams, Laurie and Ho, Chih-wei},
number = {1},
pages = {1--18},
title = {{Voices of Women in a Software Engineering Course:  Reflections on Collaboration }},
volume = {4},
year = {2004}
}
@article{Tomkin2018,
abstract = {We present a methodological improvement for calculating Grade Point Averages (GPAs). Heterogeneity in grading between courses systematically biases observed GPAs for individual students: the GPA observed depends on course selection. We show how a logistic model can account for course selection by simulating how every student in a sample would perform if they took all available courses, giving a new "modeled GPA." We then use 10 years of grade data from a large university to demonstrate that this modeled GPA is a more accurate predictor of student performance in individual courses than the observed GPA. Using Computer Science (CS) as an example learning analytics application, it is found that required CS courses give significantly lower grades than average courses. This depresses the recorded GPAs of CS majors: modeled GPAs are 0.25 points higher than those that are observed. The modeled GPA also correlates much more closely with standardized test scores than the observed GPA: the correlation with Math ACT is 0.37 for the modeled GPA and is 0.20 for the observed GPA. This implies that standardized test scores are much better predictors of student performance than might otherwise be assumed.},
author = {Tomkin, Jonathan H. and West, Matthew and Herman, Geoffrey L.},
doi = {10.1145/3157086},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tomkin, West, Herman - 2018 - An improved grade point average, with applications to CS undergraduate education analytics.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {CS2,GPA,Gender disparity,Learning analytics,Women in computing},
mendeley-tags = {CS2},
month = {sep},
number = {4},
pages = {Article 17},
publisher = {Association for Computing Machinery},
title = {{An improved grade point average, with applications to CS undergraduate education analytics}},
volume = {18},
year = {2018}
}
@inproceedings{6107896,
abstract = {Emergency response operational systems often do not work as expected by users. Subject matter expertise is not always considered for system design when developing the system requirements, and interagency coordination is often absent. With the Real-time Online Game-based Use-case Engine for Validation of Interagency Doctrine in Emergency Operations (ROGUEVIDEO) project we are attempting to build a bridge between operational environments and synthetic environments. The ROGUEVIDEO approach is to capture subject matter expert (SME)-validated doctrine, plans, procedures, equipment, decision making and systems in an architectural framework, then build a transfer mechanism from the architectural framework to an open source synthetic environment. The integrity of the SME input will result in more relevant and accurate simulations to better prepare global emergency responders. Interagency and international planning and response to natural and man-made disasters require new capabilities for planners and responders. Emergency response practitioners require the ability to test, exercise and challenge existing doctrines, plans and equipment for a variety of emergency scenarios. Further, they need a system that will capture response best practices and subject matter expertise. Currently, we have found stakeholders do have limited tools to look at their own doctrines in isolation, but what is lacking is an interagency validation framework for working on the increasing complexity of multi-agency emergency response activities, as well as a sufficient training environment. One of the main tenets of the ROGUEVIDEO project is to leverage and build on existing efforts. To realize this, we have researched domestic efforts for modeling and simulation, training and exercises, and emergency planning and response frameworks. Our findings surfaced various national doctrine such as the National Response Framework (NRF) and the National Incident Management System (NIMS) as well as the Homeland- Security Exercise Evaluation Program (HSEEP) which provides a principal foundation for us to elicit data inputs from operators. To test this approach, we held an HSEEP Data Collection Workshop at the MOVES Institute on 24 February 2011. This served as our first step toward collecting SME input to inform development of the synthetic environment. By the HSEEP definition, this initial workshop was truly a hybrid of a traditional seminar and workshop, designed to acquaint participants with the ROGUEVIDEO project and approach before we move to the next step of discussing the plan or way ahead for using a table top exercise (TTX) to elicit SME data inputs. The subsequent TTX conducted in August 2011 in Long Beach, California, helped validate and highlight successes of symbiotic doctrine; tactics, techniques and procedures (TTPs); standard operating procedures (SOPs); and equipment use, as well as surface interagency gaps related to National Planning Scenario 11, a radiological dispersal event. The information from this TTX is being used to inform software development efforts to provide a visual and interactive interagency doctrine validation experience potentially applicable to all fifteen U.S. Department of Homeland Security national planning scenarios.},
author = {Walsh, W and Blais, C and Englehorn, L and McDowell, P},
booktitle = {Technologies for Homeland Security (HST), 2011 IEEE International Conference on},
doi = {10.1109/THS.2011.6107896},
keywords = {ROGUEVIDEO approach,U.S. department of homeland se},
pages = {358--364},
title = {{Real-time Online Game-based Use-case Engine for Validation of Interagency Doctrine in Emergency Operations: Research exploring the use of synthetic environments to augment disaster planning, preparation, response, and recovery}},
year = {2011}
}
@inproceedings{Nurmi2005,
author = {Nurmi, Daniel and Brevik, John and Wolski, Rich},
booktitle = {Proc.$\backslash$ 11th International Euro-Par Conference on Parallel Processing},
isbn = {3-540-28700-0, 978-3-540-28700-1},
pages = {432--441},
title = {{Modeling machine availability in enterprise and wide-area distributed computing environments}},
year = {2005}
}
@inproceedings{Robillard2002,
address = {Orlando, FL},
author = {Robillard, Martin P and Murphy, Gail C},
pages = {406--416},
title = {{Concern Graphs: Finding and Descriving Concerns Using Structural Program Dependencies}},
year = {2002}
}
@inproceedings{5209730,
abstract = {This paper consists of a survey of various e-business, e-marketplaces, graphics and imaging processing, pattern recognition, computer vision, access control, authentication, authorization, etc. applications based on the intelligent computing, and also a summary of the recent techniques such as still artificial neural networks, swarm intelligence, artificial immune systems, fractal geometry, artificial life, ant colony algorithm, genetic algorithm, evolutionary algorithm, quantum computing. Intelligent computing can be powerful tools in several fields of electronic commerce and security.},
author = {Zhang, Jing},
booktitle = {Electronic Commerce and Security, 2009. ISECS '09. Second International Symposium on},
doi = {10.1109/ISECS.2009.205},
keywords = {access control,ant colony algorithm,artificial imm},
month = {may},
pages = {398--402},
title = {{Intelligence Computing Methods in Electronic Commerce and Security}},
volume = {1},
year = {2009}
}
@article{Lippert2003a,
author = {Lippert, Martin and Becker-Pecbau, Petra and Breitling, Holger and Koch, J{\"{o}}rn and Kornstadt, A and Roock, Stefan and Schnolitzky, A and Wolf, Henning and Zullighoven, H and Kornst{\"{a}}dt, Andreas and Roock, Stefan and Schmolitzky, Axel and Wolf, Henning and Z{\"{u}}llighoven, Heinz},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lippert et al. - 2003 - Developing complex projects using XP with extensions.pdf:pdf},
journal = {Computer},
keywords = {agile},
mendeley-tags = {agile},
number = {6},
pages = {67--73},
title = {{Developing complex projects using XP with extensions}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Developing+Complex+Projects+Using+XP+with+Extensions{\#}0},
volume = {36},
year = {2003}
}
@article{Sokolsky,
author = {Sokolsky, O.},
doi = {10.1109/IRI.2004.1431509},
isbn = {0-7803-8819-4},
journal = {Proceedings of the 2004 IEEE International Conference on Information Reuse and Integration, 2004. IRI 2004.},
pages = {493--498},
publisher = {Ieee},
title = {{Specification-based testing with linear temporal logic}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1431509}
}
@inproceedings{4724691,
abstract = {In order to assess the security of network information system, many graph-based approaches have been proposed. Attack Graph is the most influential one. But attack graphs grow exponentially with the size of the network. In this paper, we propose an improved access graph based model to analyze network security. As a complement to the attack graph approach, the access graph is host-centric approach, which grows polynomially with the number of hosts and so has the benefit of being computationally feasible on large networks. Compared with the related works, our approach improves in both performance and computational cost.},
author = {Xiao, Xiaochun and Zhang, Tiange and Zhang, Gendu},
booktitle = {Computational Intelligence and Security, 2008. CIS '08. International Conference on},
doi = {10.1109/CIS.2008.68},
keywords = {access graph generation;attack graph;computer netw},
pages = {447--452},
title = {{An Improved Approach to Access Graph Generation}},
volume = {1},
year = {2008}
}
@inproceedings{5375527,
abstract = {This paper deals with the visualization of complex attacks. ¿Complex attacks¿ is used here to denote the type of attack which consists of a sequence of related events, namely a multistep, DDoS attack and alike. While there are numerous systems to visualize events that occur in the network, most of them are too complex to perceive, and require several visualization modes. This work presents a technique whereby the operator, using visualization alone, is able to display the full picture of events occurring in the network. The main feature of this method is the high recognition ratio of complex attacks as the sequence of constituent common events.},
author = {Yelizarov, A and Gamayunov, D},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375527},
keywords = {DDoS attack,compl,complex attack recognition ratio},
pages = {1--9},
title = {{Visualization of complex attacks and state of attacked network}},
year = {2009}
}
@article{Li2010,
author = {Li, Xin and Hoover, H. James and Rudnicki, Piotr},
keywords = {API conformance,TSAFE study,exception,model checking,verification},
mendeley-tags = {TSAFE study},
month = {nov},
pages = {188--203},
title = {{API conformance verification for Java programs}},
url = {http://dl.acm.org/citation.cfm?id=1939864.1939882},
year = {2010}
}
@article{Qian2017,
abstract = {Efforts to improve computer science education are underway, and teachers of computer science are challenged in introductory programming courses to help learners develop their understanding of programming and computer science. Identifying and addressing students' misconceptions is a key part of a computer science teacher's competence. However, relevant research on this topic is not as fully developed in the computer science education field as it is in mathematics and science education. In this article, we first review relevant literature on general definitions of misconceptions and studies about students' misconceptions and other difficulties in introductory programming. Next, we investigate the factors that contribute to the difficulties. Finally, strategies and tools to address difficulties including misconceptions are discussed. Based on the review of literature, we found that students exhibit various misconceptions and other difficulties in syntactic knowledge, conceptual knowledge, and strategic knowledge. These difficulties experienced by students are related to many factors including unfamiliarity of syntax, natural language, math knowledge, inaccurate mental models, lack of strategies, programming environments, and teachers' knowledge and instruction. However, many sources of students' difficulties have connections with students' prior knowledge. To better understand and address students' misconceptions and other difficulties, various instructional approaches and tools have been developed. Nevertheless, the dissemination of these approaches and tools has been limited. Thus, first, we suggest enhancing the dissemination of existing tools and approaches and investigating their long-Term effects. Second, we recommend that computing education research move beyond documenting misconceptions to address the development of students' (mis)conceptions by integrating conceptual change theories. Third, we believe that developing and enhancing instructors' pedagogical content knowledge (PCK), including their knowledge of students' misconceptions and ability to apply effective instructional approaches and tools to address students' difficulties, is vital to the success of teaching introductory programming.},
author = {Qian, Yizhou and Lehman, James},
doi = {10.1145/3077618},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian, Lehman - 2017 - Students' misconceptions and other difficulties in introductory programming A literature review.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {Conceptual Change,Constructivism,Difficulties,Introductory Programming,Misconceptions},
month = {oct},
number = {1},
pages = {Article 1},
publisher = {Association for Computing Machinery},
title = {{Students' misconceptions and other difficulties in introductory programming: A literature review}},
url = {https://doi.org/10.1145/3077618},
volume = {18},
year = {2017}
}
@misc{Hall2007,
abstract = {Longitudinal studies (LS) generate particularly valuable empirical data. There are many reasons for this, most of which are related to the fact that LS are usually large scale. This allows for a range of rich data to be collected. It also means that the scale of data collected should enable statistically significant results to be generated. Furthermore there are also strong temporal aspects to longitudinal studies. These allow changes over time to be tracked which means that the life of a system can be better understood. It also means that the temporal aspects of process change can be identified. The scale and richness of data, collected over the lifetime of a development project, makes for a valuable empirical investigation. LS can provide more opportunity for contextual data to be collected. Variation in software development environments means that contextual data is particularly important in software engineering. Collecting rigorous contextual data at the right level of granularity means that research findings are more portable. This allows organizations to customize and adapt findings from empirical research and transfer them into their own projects or environments. LS are rare in software engineering. Researchers find it difficult to access large-scale industrial software development projects over extended periods of time. Such studies are also expensive and time consuming to run. Consequently many empirical studies in software engineering are either short snapshots of industrial projects or else experiments conducted in laboratories isolated from the industrial context. The efficacy of many snapshot empirical studies is compromised by confounding factors. LS allow a sophisticated understanding of software development to emerge. We argue that data collected in such studies can contribute significantly to the maturation of empirical software engineering research. Extensive use of LS has been made in medicine, for example Remsberg used LS to identify risk factors for developing cardiovascular disease [1]. The usefulness of LS has also been experienced in a few software engineering research studies. Maximilien and Williams [2] conducted a year-long study with an IBM software development group to examine the efficiency of test-driven development. LS can produce reliable and comprehensive findings that combine technical and social factors in software development. These findings can be presented in a contextualized form that allows them to be more appropriately transferred to other environments. Such findings can directly contribute to the development of bodies of software engineering evidence. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Longitudinal studies in evidence-based software engineering},
author = {Hall, T},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {41},
title = {{Longitudinal studies in evidence-based software engineering}},
url = {citeulike-article-id:3934632 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-36349011274{\&}{\#}38 partnerID=40},
volume = {4336 LNCS},
year = {2007}
}
@inproceedings{5931382,
abstract = {This paper discusses the implementation and use of the BDMP (Boolean logic Driven Markov Processes) formalism, recently adapted to graphical attack modeling. Theoretically, it offers an attractive trade-off between readability, scalability, modeling power and quantification capabilities. In practice, efficient model construction and analysis need complementary tools and enhancements. They have been developed only once the implementation and the first security studies have been realized. In particular, attack sequence filtering based on attacker profiles and sensitivity analysis provide a significant help. Perspectives include the addition of a security pattern library or the connection with other modeling frameworks.},
author = {Pietre-Cambacedes, Ludovic and Deflesselle, Yann and Bouissou, Marc},
booktitle = {2011 Conference on Network and Information Systems Security},
doi = {10.1109/SAR-SSI.2011.5931382},
isbn = {978-1-4577-0735-3},
keywords = {BDMP,Boolean logic driven Markov processes,attack},
month = {may},
pages = {1--8},
publisher = {Ieee},
title = {{Security Modeling with BDMP: From Theory to Implementation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5931382},
year = {2011}
}
@article{Bennedsen2007,
abstract = {It is a common conception that CS1 is a very difficult course and that failure rates are high. However, until now there has only been anecdotal evidence for this claim. This article reports on a survey among institutions around the world regarding failure rates in introductory programming courses. The article describes the design of the survey and the results. The number of institutions answering the call for data was unfortunately rather low, so it is difficult to make firm conclusions. It is our hope that this article can be the starting point for a systematic collection of data in order to find solid proof of the actual failure and pass rates of CS1.},
author = {Bennedsen, Jens and Caspersen, Michael E.},
doi = {10.1145/1272848.1272879},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bennedsen, Caspersen - 2007 - Failure rates in introductory programming.pdf:pdf},
issn = {00978418},
journal = {ACM SIGCSE Bulletin},
keywords = {CS1,CS2,failure rate,introductory programming,pass rate},
mendeley-tags = {CS2},
month = {jun},
number = {2},
pages = {32},
publisher = {ACM},
title = {{Failure rates in introductory programming}},
url = {http://portal.acm.org/citation.cfm?doid=1272848.1272879},
volume = {39},
year = {2007}
}
@inproceedings{Fraser2007,
abstract = {Test Driven Design is a programming methodology that provides an iterative design cycle with integrated testing at each step, leading to reduced time and cost associated with end of cycle testing in the traditional waterfall method of development. In this paper we implemented a small portion of a PBX system to expose common problems in the implementation of Test Driven Design, and propose solutions to the same. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Test driven design challenges for faster product development},
author = {Fraser, J and Mattu, B S},
booktitle = {Proceedings of the 1st Annual 2007 IEEE Systems Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {32--36},
title = {{Test driven design challenges for faster product development}},
url = {citeulike-article-id:3934608 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34748869613{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{5984052,
abstract = {Inferring the sentiment of social media content, for instance blog posts and forum threads, is both of great interest to security analysts and technically challenging to accomplish. This paper presents two computational methods for estimating social media sentiment which address the challenges associated with Web-based analysis. Each method formulates the task as one of text classification, models the data as a bipartite graph of documents and words, and assumes that only limited prior information is available regarding the sentiment orientation of any of the documents or words of interest. The first algorithm is a semi-supervised sentiment classifier which combines knowledge of the sentiment labels for a few documents and words with information present in unlabeled data, which is abundant online. The second algorithm assumes existence of a set of labeled documents in a domain related to the domain of interest, and leverages these data to estimate sentiment in the target domain. We demonstrate the utility of the proposed methods by showing they outperform several standard techniques for the task of inferring the sentiment of online movie and consumer product reviews. Additionally, we illustrate the potential of the methods for security informatics by estimating regional public opinion regarding Egypt's unfolding revolution through analysis of Arabic, Indonesian, and Danish (language) blog posts.},
author = {Glass, K and Colbaugh, R},
booktitle = {Intelligence and Security Informatics (ISI), 2011 IEEE International Conference on},
doi = {10.1109/ISI.2011.5984052},
keywords = {Web-based analysis;bipartite graph;blog posts;cons},
month = {jul},
pages = {65--70},
title = {{Estimating the sentiment of social media content for security informatics applications}},
year = {2011}
}
@article{Hindle2012,
author = {Hindle, Abram and Ernst, Neil A. and Godfrey, Michael W. and Mylopoulos, John},
doi = {10.1007/s10664-012-9209-9},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {may},
number = {6},
pages = {1125--1155},
title = {{Automated topic naming}},
url = {http://link.springer.com/10.1007/s10664-012-9209-9},
volume = {18},
year = {2012}
}
@book{Musa2004,
address = {New York},
author = {Musa, John D.},
publisher = {McGraw-Hill Education},
title = {{Software Reliability Engineering: More Reliable Software, Faster and Cheaper}},
year = {2004}
}
@inproceedings{Kollanus2008,
abstract = {Test-Driven Development (TDD) was applied in educational setting right after it became well-known as a key practice of Extreme Programming (XP). Basically, there are many studies reporting positive experiences on TDD applied in different levels of a curriculum. In this paper, we discuss the role of TDD in education through the students' experiences. In our experiment, a challenging programming task was applied in order to see what kind of difficulties the students would encounter and discuss. The students' answers revealed several topics that require a careful treatment in teaching to avoid conceptual confusion. For example, the topics include the scalability of TDD, extent of single test, and discipline. Copyright 2008 ACM.},
annote = {Test-driven development in education: Experiences with critical viewpoints},
author = {Kollanus, S and Isom{\"{o}}tt{\"{o}}nen, V},
booktitle = {Proceedings of the Conference on Integrating Technology into Computer Science Education, ITiCSE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {124--127},
title = {{Test-driven development in education: Experiences with critical viewpoints}},
url = {citeulike-article-id:3934678 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57349143767{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Mortensen2006,
abstract = {Moving program code that implements cross-cutting concerns into aspects can improve the maintainability of legacy systems. This kind of refactoring, called aspectualization, can also introduce faults into a system. A test driven approach can identify these faults during the refactoring process so that they can be removed. We perform systematic testing as we aspectualize commercial VLSI CAD applications. The process of refactoring these applications revealed the kinds of faults that can arise during aspectualization, and helped us to develop techniques to reduce their occurrences. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Testing during refactoring: Adding aspects to legacy systems},
author = {Mortensen, M and Ghosh, S and Bieman, J M},
booktitle = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {221--230},
title = {{Testing during refactoring: Adding aspects to legacy systems}},
url = {citeulike-article-id:3934723 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34547657460{\&}{\#}38 partnerID=40},
year = {2006}
}
@article{Kim2004,
abstract = {Developing areas in China are attracting increasing investment in manufacturing. This has increased the local demand for software and, consequently, demands on local software teams. Such teams, typically small, inexperienced and suffer high personnel turnover, often produce defective products. As software process improvement models are unsuitable for such teams, research was conducted applying a test-driven development (TDD) approach. TDD quickly improved the overall team performance. Our findings are applicable in other Asian developing countries},
annote = {Test driven development and software process improvement in China},
author = {Kim and Chan, K C C},
isbn = {3540221379},
journal = {Extreme Programming and Agile Processes in Software Engineering 5th International Conference, XP 2004 Proceedings Lecture Notes in Comput Sci Vol 3092},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {219--222},
title = {{Test driven development and software process improvement in China}},
url = {citeulike-article-id:3934675 {\#}},
volume = {3092},
year = {2004}
}
@inproceedings{1532072,
abstract = { Information visualization has proven to be a valuable tool for working more effectively with complex data and maintaining situational awareness in demanding operational domains. Unfortunately, many applications of visualization technology fall short of expectations because the technology is used inappropriately - the wrong tool applied in the wrong way. A study of visualization techniques as applied to one particularly demanding area, information assurance, leads to the conclusion that there is a proper and formal way to approach designing visualization techniques for maintaining situational awareness in complex domains. Visualization techniques should be specifically designed or selected to align with one of the three identified stages of situational awareness - perception, comprehension, or projection -and with one of five standard uses of visualization - monitoring, inspecting, exploring, forecasting, or communicating. Greater value can be realized by selecting the right visualization technique to focus on each operational task, rather than searching for a single all-encompassing solution to fit every need. Examples of how visualizations can be used to support specific tasks of IA analysis are presented, with examples based on a review of available literature, a formal cognitive task analysis performed by the authors, and lessons learned from direct experience with developing IA visualizations and training analysts in their use.},
author = {D'Amico, a. and Kocka, M.},
booktitle = {IEEE Workshop on Visualization for Computer Security, 2005. (VizSEC 05).},
doi = {10.1109/VIZSEC.2005.1532072},
isbn = {0-7803-9477-1},
keywords = {computer network defense,data representation,for},
pages = {107--112},
publisher = {Ieee},
title = {{Information assurance visualizations for specific stages of situational awareness and intended uses: lessons learned}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1532072},
year = {2005}
}
@incollection{Simon1989,
address = {Hillsdale, NJ},
author = {Simon, Herbert Alexander},
booktitle = {Complex Information Processing: The Impact of Herbert A. Simon},
editor = {Klahr, David and Kotovsky, Kenneth},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Erlbaum},
title = {{The scientist as problem solver}},
year = {1989}
}
@inproceedings{4258732,
abstract = {Infectious disease informatics, as a sub-field of security informatics, is concerned with development of the science and technologies needed for collecting, sharing, reporting, analyzing, and visualizing infectious disease data; and for providing data and decision-making support for infectious disease prevention, detection, and management. Syndromic surveillance is a major study area of infectious disease informatics, focusing on identifying in a timely manner possible infectious disease outbreaks based on pre-diagnostic data. Free-text chief complaints (CCs), short phrases describing reasons for patients' emergency department visits, are a major source of data for syndromic surveillance. For surveillance purposes, CCs need to be classified into syndromic categories. However, the lack of standard vocabulary and high-quality encoding of CCs hinder effective classification. To meet this challenge, we have developed an ontology-enhanced automatic CC classification approach. Exploiting semantic relations in the UMLS, a medical ontology, this approach is motivated to address the CC word variation problem in general and to meet the specific need for a flexible classification approach capable of handling multiple sets of syndrome categories.},
author = {Lu, Hsin-Min and Zeng, D and Chen, Hsinchun},
booktitle = {Intelligence and Security Informatics, 2007 IEEE},
doi = {10.1109/ISI.2007.379506},
keywords = {UMLS,classification,data analysis,data collection},
month = {may},
pages = {372},
title = {{Medical Ontology-Enhanced Text Processing for Infectious Disease Informatics}},
year = {2007}
}
@book{Jones1996,
address = {New York, NY},
author = {Jones, Capers},
publisher = {McGraw-Hill},
title = {{Applied Software Measurement: Assuring Productivity and Quality}},
year = {1996}
}
@inproceedings{Panesar-Walawege2011,
abstract = {Certification of safety-critical systems according to well-recognised standards is the norm in many industries where the failure of such systems can harm people or the environment. Certification bodies examine such systems, based on evidence that the system suppliers provide, to ensure that the relevant safety risks have been sufficiently mitigated. The evidence is aimed at satisfying the requirements of the standards used for certification, and naturally a key prerequisite for effective collection of evidence, is that the supplier be aware of these requirements and the evidence they require. This often proves to be a very challenging task because of the sheer size of the standards and the fact that the textual standards are amenable to subjective interpretation. In this paper, we propose an approach based on UML profiles and model-driven engineering. It addresses not only the above challenge but also enables the automated verification of compliance to standards based on evidence. Specifically, a profile is created, based on a conceptual model of a given standard, which provides a succinct and explicit interpretation of the underlying standard. The profile is augmented with constraints that help system suppliers with establishing a relationship between the concepts in the safety standard of interest and the concepts in the application domain. This in turn enables suppliers to demonstrate how their system development artifacts achieve compliance to the standard. We illustrate our approach by showing how the concepts in the domain of sub-sea control systems can be aligned with the evidence requirements in the IEC61508 standard, which is one of the most commonly used certification standard for control systems.},
author = {Panesar-Walawege, Rajwinder Kaur and Sabetzadeh, Mehrdad and Briand, Lionel},
booktitle = {2011 IEEE 22nd International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.2011.11},
isbn = {978-1-4577-2060-4},
month = {nov},
pages = {30--39},
publisher = {IEEE},
title = {{A Model-Driven Engineering Approach to Support the Verification of Compliance to Safety Standards}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp={\&}arnumber=6132951{\&}contentType=Conference+Publications{\&}searchWithin=p{\_}Authors:.QT.Panesar-Walawege,+R.K..QT.{\&}searchField=Search{\_}All http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6132951},
year = {2011}
}
@article{Cialdini1990,
abstract = {Past research has generated mixed support among social scientists for the utility of social norms in accounting for human behavior. We argue that norms do have a substantial impact on human action; however, the impact can only be properly recognized when researchers (a) separate 2 types of norms that at times act antagonistically in a situation—injunctive norms (what most others approve or disapprove) and descriptive norms (what most others do)—and (b) focus Ss' attention principally on the type of norm being studied. In 5 natural settings, focusing Ss on either the descriptive norms or the injunctive norms regarding littering caused the Ss' littering decisions to change only in accord with the dictates of the then more salient type of norm. (PsycINFO Database Record (c) 2012 APA, all rights reserved)},
address = {US},
author = {Cialdini, Robert B and Reno, Raymond R and Kallgren, Carl A},
doi = {10.1037/0022-3514.58.6.1015},
issn = {1939-1315(Electronic);0022-3514(Print)},
journal = {Journal of Personality and Social Psychology},
keywords = {*Environmental Attitudes,Social Norms,agile,nsf},
mendeley-tags = {agile,nsf},
number = {6},
pages = {1015--1026},
publisher = {American Psychological Association},
title = {{A focus theory of normative conduct: Recycling the concept of norms to reduce littering in public places.}},
volume = {58},
year = {1990}
}
@article{McFarlane2002,
author = {McFarlane, Daniel C},
journal = {Human-Computer Interaction},
keywords = {alert,hci,interruption},
number = {1},
pages = {63--139},
title = {{Comparison of Four Primary Methods for Coordinating the Interruption of People in Human-Computer Interaction}},
volume = {17},
year = {2002}
}
@misc{Goldman2013,
author = {Goldman, Jeff},
booktitle = {eSecurityPlanet.com},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{Petrochem Insulation Admits Security Breach}},
url = {http://www.esecurityplanet.com/network-security/petrochem-insulation-admits-security-breach.html},
urldate = {2015-01-01},
year = {2013}
}
@inproceedings{Baumeister2004,
abstract = {Early test development and specification enhance the quality and robustness of software as experience with agile software development methods shows. The methods propagate test-first techniques and early prototyping through executable design models. We propose to enhance test-driven development to a more general property-driven development technique: Property-driven development ties together automatic tests, formal specification, and executable UML models by developing these three views together instead of one after the other as is common practice. Scenarios and properties serve as a combined basis for system specification and test cases. By extracting common properties of several scenarios we obtain invariants and pre- and post-conditions. The behavior of the system is described UML state machines. For testing we insert invariants and pre-and postconditions as assertions in the code generated from the state machines. For verification, we use model checking. For this we have to restrict the models to finite domains. Therefore we construct suitable abstractions of the scenarios and the system behavior and verify the abstractions using a model checker. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {Property-driven development},
author = {Baumeister, H and Knapp, A and Wirsing, M},
booktitle = {Proceedings of the Second International Conference on Software Engineering and Formal Methods. SEFM 2004},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {96--102},
title = {{Property-driven development}},
url = {citeulike-article-id:3934551 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-16244372091{\&}{\#}38 partnerID=40},
year = {2004}
}
@article{Wallace2004a,
author = {Wallace, Linda and Keil, Mark},
doi = {10.1145/975817.975819},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wallace, Keil - 2004 - Software project risks and their effect on outcomes.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {apr},
number = {4},
pages = {68--73},
title = {{Software project risks and their effect on outcomes}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=975819{\&}type=html},
volume = {47},
year = {2004}
}
@incollection{ziefle2004mental,
author = {Ziefle, Martina and Bay, Susanne},
booktitle = {Mobile Human-Computer Interaction-MobileHCI 2004},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {25--37},
publisher = {Springer},
title = {{Mental models of a cellular phone menu. Comparing older and younger novice users}},
year = {2004}
}
@book{Saaty1980,
address = {New York, NY},
author = {Saaty, T L},
publisher = {Wiley},
title = {{The Analytical Hierarchy Process}},
year = {1980}
}
@inproceedings{Card2002,
address = {Trento, Italy},
author = {Card, Stuart K and Nation, David},
keywords = {attention,doi},
pages = {231--245},
title = {{Degree-of-Interest Trees: A Component of an Attention-Reactive User Interface}},
year = {2002}
}
@article{Blei2003,
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, Ng, Jordan - 2003 - Latent Dirichlet Allocation.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
month = {mar},
pages = {993--1022},
publisher = {JMLR.org},
title = {{Latent Dirichlet Allocation}},
url = {http://dl.acm.org/citation.cfm?id=944919.944937},
volume = {3},
year = {2003}
}
@article{5680893,
abstract = {Cumbersome and complicated authentication procedures to access sensitive online services such as Internet banking can be a nuisance. For people with disabilities or the elderly, poorly designed identity management systems can preclude usage altogether. This article presents a secure and accessible multimodal authentication method to log in to an Internet banking service. The method uses a one-time-password (OTP) client installed on a mobile phone that replaces dedicated OTP generators. The client provides both visual and auditory output, and is based on an application approved for secure log-in to sensitive online services. It allows usage by people whose functional impairments adversely affect their ability to use existing solutions. The authors also discuss implications for development, and make several recommendations for designing usable and accessible security applications and solutions.},
author = {Fuglerud, Kristin and Dale, Oystein},
doi = {10.1109/MSP.2010.204},
issn = {1540-7993},
journal = {IEEE Security {\&} Privacy Magazine},
keywords = {Internet banking,OTP generators,identity managemen},
month = {mar},
number = {2},
pages = {27--34},
title = {{Secure and Inclusive Authentication with a Talking Mobile One-Time-Password Client}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5680893},
volume = {9},
year = {2011}
}
@article{Schuh2001,
author = {Schuh, Peter},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schuh - 2001 - Recovery, redemption, and extreme programming.pdf:pdf},
journal = {IEEE Software},
keywords = {XP,agile},
mendeley-tags = {agile},
number = {6},
pages = {34--41},
title = {{Recovery, Redemption, and Extreme Programming}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Recovery,+Redemption+and+Extreme+Programming{\#}0},
volume = {18},
year = {2001}
}
@inproceedings{Chatmon2010a,
address = {New York, New York, USA},
author = {Chatmon, Christy and Chi, Hongmei and Davis, Will},
booktitle = {2010 Information Security Curriculum Development Conference on - InfoSecCD '10},
doi = {10.1145/1940941.1940943},
isbn = {9781450302029},
keywords = {active leaning,e-learning,hands-on lab,information assurance,virtual labs},
month = {oct},
pages = {1},
publisher = {ACM Press},
title = {{Active learning approaches to teaching information assurance}},
url = {http://dl.acm.org/citation.cfm?id=1940941.1940943},
year = {2010}
}
@article{Weller1994,
author = {Weller, E F},
number = {9},
pages = {27--33},
title = {{Using Metrics to Manage Software Projects}},
volume = {27},
year = {1994}
}
@article{Pancur2003a,
abstract = {Test driven development (TDD) is an agile software development technique and it is one of the core development practices of Extreme programming (XP). In TDD, developers write automatically executable tests prior to writing the code they test. We ran a set of experiments to empirically assess different parameters of the TDD. We compared TDD to a more "traditionally" oriented iterative test-last development process (ITL). Our preliminary results show that TDD is not substantially different from ITL and our qualitative findings about a development process are different from results obtained from other researches},
annote = {Towards empirical evaluation of test-driven development in a university environment},
author = {Pan{\v{c}}ur, M and Ciglari{\v{c}}, M and Trampu{\v{s}}, M and Vidmar, T},
isbn = {078037763X},
journal = {IEEE Region 8 EUROCON 2003 Computer as a Tool Proceedings Cat No 03EX655},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Towards empirical evaluation of test-driven development in a university environment}},
url = {citeulike-article-id:3934747 {\#}},
volume = {2},
year = {2003}
}
@article{Perlow1999,
author = {Perlow, Leslie a.},
doi = {10.2307/2667031},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perlow - 1999 - The Time Famine Toward a Sociology of Work Time.pdf:pdf},
issn = {00018392},
journal = {Administrative Science Quarterly},
keywords = {collaboration},
mendeley-tags = {collaboration},
month = {mar},
number = {1},
pages = {57--81},
title = {{The Time Famine: Toward a Sociology of Work Time}},
url = {http://www.jstor.org/stable/2667031?origin=crossref},
volume = {44},
year = {1999}
}
@inproceedings{Williams2006a,
abstract = {Pair programming has been shown to be beneficial for both students and teaching staff in university courses. A two-phased study of 1350 students was conducted at North Carolina State University from 2002-2005 to determine if teaching staff can proactively form compatible pairs based upon any of the following factors: personality type, learning style, skill level, programming self esteem, work ethic, or time management preference. We examined compatibility among freshmen, advanced undergraduate and graduate student pair programmers. We have found that overall 93{\%} of students are compatible with their partners. Students notably preferred to pair with a partner that he or she perceived to be of similar or higher skill level to them, which can be predicted by grouping students with similar grade point average. Additionally, pairs comprised of a sensor and an intuitor learning style seem to be compatible, and pairs with differing work ethic are generally not compatible},
address = {Minneapolis, MN},
author = {Williams, Laurie and Layman, Lucas and Osborne, Jason and Katira, Neha},
booktitle = {AGILE 2006 (AGILE'06)},
doi = {10.1109/AGILE.2006.25},
isbn = {0-7695-2562-8},
keywords = {Collaboration,Collaborative work,Computer science,Education,Ethics,Information technology,North Carolina State University,Peer to peer computing,Programming profession,Statistics,Testing,advanced undergraduate student pair programmers,computer science education,continuing education,educational courses,ethical aspects,freshmen student pair programmers,graduate student pair programmers,human factors,intuitor learning style,mypubs,personality type,programming,programming self esteem,skill level,teaching,teaching staff,time management preference,university courses,work ethic},
mendeley-tags = {mypubs},
pages = {411--420},
publisher = {IEEE},
shorttitle = {Agile Conference, 2006},
title = {{Examining the Compatibility of Student Pair Programmers}},
year = {2006}
}
@misc{Yesko2013,
author = {Yesko, Parker},
booktitle = {San Francisco Bay Guardian Online},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{Smartphones trigger rise in crime rate as new iPhone features a fingerprint lock}},
url = {http://www.sfbg.com/politics/2013/09/10/smartphones-trigger-rise-crime-rate-new-iphone-features-fingerprint-lock},
year = {2013}
}
@inproceedings{Lukins2008,
address = {Antwerp, Belgium},
author = {Lukins, Stacy K. and Kraft, Nicholas A. and Etzkorn, Letha H.},
booktitle = {2008 15th Working Conference on Reverse Engineering},
doi = {10.1109/WCRE.2008.33},
isbn = {978-0-7695-3429-9},
issn = {1095-1350},
keywords = {Aging,Computer bugs,Costs,Indexing,Information retrieval,LDA,LDA-based static technique,LSI,Large scale integration,Linear discriminant analysis,Reverse engineering,Software maintenance,Software systems,automatic bug localization,automating bug localization,bug localization,indexing,information retrieval,latent Dirichlet allocation,latent semantic indexing,program comprehension,software engineering,source code retrieval},
language = {English},
month = {oct},
pages = {155--164},
publisher = {IEEE},
title = {{Source Code Retrieval for Bug Localization Using Latent Dirichlet Allocation}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4656405},
year = {2008}
}
@inproceedings{Slyngstad2008,
abstract = {Test Driven Development (TDD) is a software engineering technique to promote fast feedback, task- oriented development, improved quality assurance and more comprehensible low-level software design. Benefits have been shown for non-reusable software development in terms of improved quality (e.g. lower defect density). We have carried out an empirical study of a framework of reusable components, to see whether these benefits can be shown for reusable components. The framework is used in building new applications and provides services to these applications during runtime. The three first versions of this framework were developed using traditional test-last development, while for the two latest versions TDD was used. Our results show benefits in terms of reduced mean defect density (35.86{\%}), when using TDD, over two releases. Mean change density was 76.19{\%} lower for TDD than for test-last development. Finally, the change distribution for the TDD approach was 33.3{\%} perfective, 5.6{\%}) adaptive and 61.1{\%} preventive.{\^{A}}{\textcopyright} 2008 IEEE.},
annote = {The impact of test driven development on the evolution of a reusable framework of components - An industrial case study},
author = {Slyngstad, O P N and Li, J and Conradi, R and R{\o}nneberg, H and Landre, E and Wesenberg, H},
booktitle = {Proceedings - The 3rd International Conference on Software Engineering Advances, ICSEA 2008, Includes ENTISY 2008: International Workshop on Enterprise Information Systems},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {214--223},
title = {{The impact of test driven development on the evolution of a reusable framework of components - An industrial case study}},
url = {citeulike-article-id:3934798 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57649192497{\&}{\#}38 partnerID=40},
year = {2008}
}
@article{Ambler2008a,
abstract = {Agilists have developed many strategies for dealing with functional requirements effectively, leading to the higher success rates enjoyed by agile teams. Nonfunctional requirements (NFRs), also known as technical requirements or quality of service (QoS) requirements, focus on aspects that typically cross-cut functional requirements. Common NFRs include accuracy, availability, concurrency, consumability, environmental/green concerns, internationalization, operations issues, performance, regulatory concerns, reliability, security, serviceability, support, and timeliness. Although many agilists have adopted a test-first or test-driven approach to development, an incredibly good practice is Scaling Test-Driven Development. Disciplined agilists go beyond the prioritized stack approach to requirements management and adopt a more holistic approach that addresses not only functional requirements but nonfunctional requirements and constraints as well.},
annote = {Beyond functional requirements on agile projects},
author = {Ambler, S W},
journal = {Dr. Dobb's Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {10},
pages = {64--66},
title = {{Beyond functional requirements on agile projects}},
url = {citeulike-article-id:3934540 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-54749155107{\&}{\#}38 partnerID=40},
volume = {33},
year = {2008}
}
@inproceedings{Rybarczyk2019,
abstract = {Previous work demonstrated an effective strategy on how to teach two programming languages (Java and C++) side-by-side in a Computer Science II (CS2) course. The goal of this approach was twofold: first, to stress the importance of programming language selection as part of a career preparation module; secondly, to improve the overall success and satisfaction of the students. The results of this work led to additional questions, including how students (peers) could be leveraged to act as instructors in laboratory settings to aid in a peer-led learning exercise. One of the unique challenges associated with CS2 curricula is the varying degree of student experiences and expertise when entering the course. Course surveys have demonstrated that a subset of the student population enter the course with significant programming experience, while others report no prior experience with programming or programming languages. It is therefore the role of the instructor, to find the proper balance within the course in order to satisfy all students' learning needs. In the original work it was indicated, via student post-completion surveys, that peer-led discussion of coding "best practices" could improve overall student outcomes. This survey provided the motivation for this paper. In this paper, we propose using interactive peer-led code reviews to help introduce the students to widely accepted practices found throughout industry while allowing them to better master the material through a shared learning experience. We provide an outline for how this can be achieved in an existing course structure and discuss the outcomes from this study.},
address = {Minneapolis, MN, USA},
annote = {peerled vodecreviews can help transition to new language and oo concepts according to survey

students like it, lower failure rate},
author = {Rybarczyk, Ryan and Acheson, Lingma},
booktitle = {SIGCSE 2019 - Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3287324.3287442},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rybarczyk, Acheson - 2019 - Interactive peer-led code reviews in CS2 curricula.pdf:pdf},
isbn = {9781450358903},
keywords = {CS2,Career Preparation,Curriculum,Peer-led Instruction},
mendeley-tags = {CS2},
month = {feb},
pages = {659--665},
publisher = {Association for Computing Machinery, Inc},
title = {{Interactive peer-led code reviews in CS2 curricula}},
year = {2019}
}
@inproceedings{Felt2012,
address = {New York, New York, USA},
author = {Felt, Adrienne Porter and Ha, Elizabeth and Egelman, Serge and Haney, Ariel and Chin, Erika and Wagner, David},
booktitle = {Proceedings of the Eighth Symposium on Usable Privacy and Security - SOUPS '12},
keywords = {Android,mobile phones,security,smartphones,usable security},
mendeley-tags = {security},
month = {jul},
pages = {1},
publisher = {ACM Press},
title = {{Android permissions}},
url = {http://dl.acm.org/citation.cfm?id=2335356.2335360},
year = {2012}
}
@inproceedings{Huang2007,
abstract = {During our previous research conducted in the Sheffield Software Engineering Observatory [11], we found that test first programmers spent a higher percentage of their time testing than those testing after coding. However as the team allocation was based on subjects ' academic records and their preference, it was unclear if they were simply better testers. Thus this paper proposes two questionnaires to assess the testing ability of subjects, in order to reveal the factors that contribute to the previous findings. Preliminary results show that the testing ability of subjects, as measured by the survey, varies based on their professional skill level. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {How good are your testers? An assessment of testing ability},
author = {Huang, L and Thomson, C and Holcombe, M},
booktitle = {Proceedings - Testing: Academic and Industrial Conference Practice and Research Techniques, TAIC PART-Mutation 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {82--86},
title = {{How good are your testers? An assessment of testing ability}},
url = {citeulike-article-id:3934646 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-48049084990{\&}{\#}38 partnerID=40},
year = {2007}
}
@incollection{Visser1990,
abstract = {Early studies on programming have neglected design strategies actually implemented by expert programmers.  Recent studies observing designers in real(istic) situations show these strategies to be deviating from the top-down and breadth-first prescriptive model, and leading to an oppurtunistically organized design activity.  The main components of these strategies are presented here.  Consequences are drawn from the results for the specification of design support tools, as well as for programmers' training.},
address = {New York, NY},
annote = {{\textless}m:note{\textgreater}"professional programmers know that it is very difficult to implement top-down, depth-first design in practice." (pp.235-236){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}"teaching novices requires understanding not only their lack of knowledge, but that the design ideas they are being taught are not perfect"{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}"an ill-defined problem is one for which a person has a representation that cannot trigger a ready-made procedure to reach the goal." (p.236){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}-	initial problem specifications are generally not sufficient to define a goal{\textless}m:linebreak/{\textgreater}-	the resolution of conflicting constraints, often at different levels, plays an important role{\textless}m:linebreak/{\textgreater}-	specifications or constraints come from different representation and processing systems, and are often conflicting and have to be translated into a specific design domain, such as programming language used{\textless}m:linebreak/{\textgreater}-	there is no 'definite criterion for testing any proposed solution'. They are not correct or incorrect, they are acceptable or satisfying or not.{\textless}m:linebreak/{\textgreater}-	Various solutions are possible, one being possibly more satisfying in one dimension, another in another dimension (p.237){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Many design studies focused on what design "should be" rather than how it actually is. (p.237){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}There is a lack of studies of design on large projects. (p.239){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Most authors note variations in the design strategies and the solutions they lead to between experts and novices AND between experts? (p.240){\textless}m:linebreak/{\textgreater}**WHY?{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Some people believe decomposition strategies are predictable procedures, while others stress that you deviate from those structures, and others suggest that strategies do not reflect deviations but are rather caused by the opportunistic character of design. (p.240){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Whether it is deviation or opportunism may be a factor of the size/complexity of the problem. (p.247).{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Brooks suggests that only expert programming in their comfortable domain can proceed top-down, breadth-first. (p.241){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Professional programmers working top-down may find that revising the plan is difficult, and require awkwardly modifying modules to fix planning errors. (p.241){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}**Many experts proceed top down, but depth-first rather than breadth-first.  Breadth-first processing is useful for identifying potential interactions (p.241){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}**One may think of elements at a problem at different levels at one time.  Experts are capable of maintaining some elements for a later time when a related topic comes up. (p.241)  Even experts have trouble maintaining all of this information, however. (p.242){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}The language editor typically constrain subjects to express too precise expressions in the design.  This requires people to develop the problem out at a deeper level.  This sometimes forces depth-first programming.(p.242).{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Opportunistic design has been observed to be guided/influenced by the cognitive cost of actions and their importance.  Perhaps information is not available and would be 'expensive' to acquire.  They also might process a problem that is 'cheaper' because the information source is at hand, rather than looking for the information required to handle the current component.. (p.243).{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Procedural strategies - the program is generated following a mental execution strategy.  {\textless}m:linebreak/{\textgreater}Declarative strategy - the static representation guides the design{\textless}m:linebreak/{\textgreater}Prospective strategy - start from the beginning of the problem and work forward{\textless}m:linebreak/{\textgreater}Retrospective - start from the end of the problem and work backward{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Procedural-prospective - the problem statements trigger a familiar procedure * Experts can classify problems in categories for which they have available procedures.{\textless}m:linebreak/{\textgreater}Declarative-prospective - relationships between the inputs impose strong and complex constraints on the program structure{\textless}m:linebreak/{\textgreater}Declarative-retrospective - the guide can be provided by the structure of the output files and their relationships: the program can be written in reverse order{\textless}m:linebreak/{\textgreater}Procedural-retrospective - start with unknowns, search for definitions of these variables, generate new unknowns until reaching given values. *Often done by novices. (p.243){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}The environment induces retrospective strategies, and prospective strategies were more difficult to solve.  Retrospectives were also difficult.(pp.243-244){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Simulation is important for helping elaborate designs.  Simulation of what users would do. (p.244){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Need more assistance for prospective strategies. (p.246){\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}**So most, people are retrospective?  Do we need more guidance of the expected{\textless}/m:note{\textgreater}},
author = {Visser, W and Hoc, J M and Hoc, J.-M. and Green, T R G and Samurcay, R and Gilmore, D J and Gaines, B R and Monk, A},
keywords = {design,psychology,survey},
pages = {235--249},
publisher = {Harcourt Brace Jovanovich},
title = {{Expert Software Design Strategies}},
year = {1990}
}
@inproceedings{5335571,
abstract = {This document concentrates on how aviation security can be largely improved by selecting the right people for the x-ray screening task. X-ray images dispose of a different colour range, single objects superimpose each other independently of whether they are in front or behind of each other, and the single objects look very different from what they look like under natural light. Therefore, individual visual abilities such as mental rotation, figure-ground segregation, or visual search of specific patterns are very important for the correct interpretation of X-ray images. In this paper we briefly discuss job and task analysis, as well as cognitive task analysis in x-ray screening. We then discuss tests used in pre-employment assessment, including a more detailed presentation of the X-Ray Object Recognition Test (X-Ray ORT). Finally, a new project is presented in which a cognitive test battery is developed to measure important visual abilities needed in x-ray screening and related tasks.},
author = {Bolfing, A and Schwaninger, A},
booktitle = {Security Technology, 2009. 43rd Annual 2009 International Carnahan Conference on},
doi = {10.1109/CCST.2009.5335571},
keywords = {X-ray images,X-ray object recognition test,X-ray s},
pages = {5--12},
title = {{Selection and pre-employment assessment in aviation security x-ray screening}},
year = {2009}
}
@article{Grottke2006,
author = {Grottke, Michael and Li, Lei and Vaidyanathan, Kalyanaraman and Trivedi, Kishor S},
journal = {IEEE Transactions on Reliability},
pages = {411--420},
title = {{Analysis of software aging in a {\{}Web{\}} server}},
volume = {55},
year = {2006}
}
@inproceedings{Zazworka2011,
address = {Honolulu, HI},
author = {Zazworka, Nico and Shaw, Michele A. and Shull, Forrest and Seaman, Carolyn B.},
booktitle = {International Workshop on Managing Technical Debt},
doi = {10.1145/1985362.1985366},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zazworka et al. - 2011 - Investigating the impact of design debt on software quality.pdf:pdf},
isbn = {9781450305860},
pages = {17--23},
title = {{Investigating the Impact of Design Debt on Software Quality}},
url = {http://portal.acm.org/citation.cfm?doid=1985362.1985366},
year = {2011}
}
@article{Brinkkemper1996,
author = {Brinkkemper, S},
number = {4},
pages = {275--280},
title = {{Method Engineering: Engineering of Information Systems Development Methods and Tools}},
volume = {38},
year = {1996}
}
@article{Layman2008,
abstract = {The software industry uses a mixture of plan-driven and agile techniques, and educators must prepare students for industry needs while creating an effective educational environment that appeals to a diverse student population. We describe the undergraduate course in software engineering at North Carolina State University, which teaches both agile and plan-driven practices while emphasizing collaborative and active learning. We present demographics, personality types, and learning styles from 400 students, and provide statistical analyses and student testimonials on the impact of our course. Students have reacted favorably to the course and are better prepared to meet the diverse needs of industry. {\textcopyright} 2008 TEMPUS Publications.},
author = {Layman, Lucas and Williams, Laurie and Slaten, Kelli and Berenson, Sarah and Vouk, Mladen},
issn = {0949149X},
journal = {International Journal of Engineering Education},
keywords = {Agile methods,Learning styles,Personality types,Software engineering education,mypubs},
mendeley-tags = {mypubs},
number = {4},
pages = {659--670},
title = {{Addressing Diverse Needs through a Balance of Agile and Plan-driven Software Development Methodologies in the Core Software Engineering Course}},
volume = {24},
year = {2008}
}
@article{Lamsweerde1998,
author = {van Lamsweerde, A and Darimont, R and Leiter, E},
keywords = {formal,requirements,verification},
number = {11},
pages = {908--926},
title = {{Managing Conflicts in Goal-Driven Requirements Engineering}},
volume = {24},
year = {1998}
}
@inproceedings{Geras2004,
abstract = {Test Driven Development (TDD) is a relatively new software development process. On the strength of anecdotal evidence and a number of empirical evaluations, TDD is starting to gain momentum as the primary means of developing software in organizations worldwide. In traditional development, tests are for verification and validation purposes and are built after the target product feature exists. In testdriven development, tests are used for specification purposes in addition to verification and validation. An experiment was devised to investigate the distinction between test-driven development and traditional, testlast development from the perspective of developer productivity and software quality. The results of the experiment indicate that while there is little or no difference in developer productivity in the two processes, there are differences in the frequency of unplanned test failures. This may lead to less debugging and more time spent on forward progress within a development project. As with many new software development technologies however, this requires further study, in particular to determine if the positive results translate into lower total costs of ownership. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {A prototype empirical evaluation of test driven development},
author = {Geras, A and Smith, M and Miller, J},
booktitle = {Proceedings - International Software Metrics Symposium},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {405--416},
title = {{A prototype empirical evaluation of test driven development}},
url = {citeulike-article-id:3934621 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-14844311233{\&}{\#}38 partnerID=40},
year = {2004}
}
@inproceedings{Cannizzo2008,
abstract = {Agile and eXtreme Programming practices have popularised concepts of test driven development and continuous build cycles to the software community. Such practices are typically adopted to implement and deliver functionality early in the development process. However, some types of applications such as the one described in this report also require continuous tests for performance and robustness. This report shows the experiences of the authors in extending the continuous build loop to include additional tests for performance and robustness, with the intention of overcoming limitations of standard testing frameworks when applied to highly concurrent and real time applications. It also describes how they went about building the appropriate framework to support the execution and verification of the test results. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Pushing the boundaries of testing and continuous integration},
author = {Cannizzo, F and Clutton, R and Ramesh, R},
booktitle = {Proceedings - Agile 2008 Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {501--505},
title = {{Pushing the boundaries of testing and continuous integration}},
url = {citeulike-article-id:3934565 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-52949117215{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{6059963,
abstract = {Secure and reliable authentication is an essential prerequisite for many online systems, yet achieving this in a way which is acceptable to customers remains a challenge. GrIDsure, a one-time PIN scheme using random grids and personal patterns, has been proposed as a way to overcome some of these challenges. We present an analytical study which demonstrates that GrIDsure in its current form is vulnerable to interception. To strengthen the scheme, we propose a way to fortify GrIDsure against Man-in-the-Middle attacks through (i) an additional secret transmitted out-of-band and (ii) multiple patterns. Since the need to recall multiple patterns increases user workload, we evaluated user performance with multiple captures with 26 participants making 15 authentication attempts each over a 3-week period. In contrast with other research into the use of multiple graphical passwords, we find no significant difference in the usability of GrIDsure with single and with multiple patterns.},
author = {Jhawar, Ravi and Inglesant, Philip and Courtois, Nicolas and Sasse, M. Angela},
booktitle = {2011 5th International Conference on Network and System Security},
doi = {10.1109/ICNSS.2011.6059963},
isbn = {978-1-4577-0458-1},
keywords = {GrIDsure,graphical one-time PIN authentication sec},
month = {sep},
pages = {81--88},
publisher = {Ieee},
title = {{Make mine a quadruple: Strengthening the security of graphical one-time PIN authentication}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6059963},
year = {2011}
}
@article{6007132,
abstract = {Security Visualization is a very young term. It expresses the idea that common visualization techniques have been designed for use cases that are not supportive of security-related data, demanding novel techniques fine tuned for the purpose of thorough analysis. Significant amount of work has been published in this area, but little work has been done to study this emerging visualization discipline. We offer a comprehensive review of network security visualization and provide a taxonomy in the form of five use-case classes encompassing nearly all recent works in this area. We outline the incorporated visualization techniques and data sources and provide an informative table to display our findings. From the analysis of these systems, we examine issues and concerns regarding network security visualization and provide guidelines and directions for future researchers and visual system developers.},
author = {Shiravi, H and Shiravi, A and Ghorbani, A},
doi = {10.1109/TVCG.2011.144},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shiravi, Shiravi, Ghorbani - 2012 - A Survey of Visualization Systems for Network Security.pdf:pdf},
issn = {1077-2626},
journal = {Visualization and Computer Graphics, IEEE Transactions on},
number = {99},
pages = {1},
title = {{A Survey of Visualization Systems for Network Security}},
volume = {PP},
year = {2012}
}
@inproceedings{6107916,
abstract = {Despite more than a decade of significant government investment in network defense research and technology development, there have been relatively few successful transitions across the chasm between research and operational use. Prior work describes approaches to crossing the {\#}x201C;valley of death {\#}x201D; from the perspective of the government sponsor or independent tester. The researcher and developer's perspective offered in this paper adds to our understanding of the challenges faced and solutions applied to deployment of advanced technologies into operational environments. The paper describes lessons learned from recent transitions of two information assurance technologies - the VIAssist {\#}x00AE; netflow visualization tool and the MeerCAT {\#}x00AE; wireless vulnerability analysis tool - into operational use by the Department of Homeland Security (DHS) and the Department of Defense (DoD).},
author = {O'Brien, B F and D'Amico, A and Larkin, M E},
booktitle = {Technologies for Homeland Security (HST), 2011 IEEE International Conference on},
doi = {10.1109/THS.2011.6107916},
keywords = {MeerCAT wireless vulnerability analysis tool,VIAss},
pages = {481--486},
title = {{Technology transition of network defense visual analytics: Lessons learned from case studies}},
year = {2011}
}
@inproceedings{1594861,
abstract = {Tracking is a basic process for security and surveillance problems that need to follow an object of interest during a certain period of time. Visual tracking algorithms based on pattern matching techniques constitute the basis of many, and the most used, current tracking systems (Brown, 1992). However, the main drawback of template matching consists in its lack of automatic adaptation to any environmental condition. Usually, the object of interest or the environment change their visual aspect and the template should, in a robust way, adapt to them. Therefore, a proper updating technique of the template is a crucial matter of all these systems. This paper proposes a new approach to the updating problem in order to achieve a better tracking performance and robustness. This is carried out by using an internal representation technique that makes use of second order isomorphisms (Shepard and Chipman, 1970). This allows to establish a representation space where an object of interest can be more easily distinguished from the representations of the objects of the context. The most important improvements of this approach are its parameter-free working, therefore no parameters have to be set manually in order to tune the process, and a better performance in robustness compared with other methods. Besides, objects to be tracked can be rigid or deformable, the system is adapted automatically to any situation},
author = {Guerra, C. and Hernandez, M. and Hernandez, D. and Isern, J.},
booktitle = {Proceedings 39th Annual 2005 International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2005.1594861},
isbn = {0-7803-9245-0},
keywords = {internal representation technique,object tracking},
pages = {267--270},
publisher = {Ieee},
title = {{Visual tracking of targets in unrestricted environments}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1594861},
year = {2005}
}
@inproceedings{McDermid1995,
abstract = {This paper summarises the experience gained from application of Hazard and Operability Studies (HAZOP) and related techniques to four computer-based systems. Emphasis is placed on working practices and the integration of HAZOP-style analysis into a safety-oriented lifecycle. Two of the case studies are described in some detail. An industrial study is used to investigate working practices, highlighting a number of areas of concern with the traditional team approach. A second example is described using an alternative process known as Software Hazard Analysis and Resolution in Design (SHARD), showing its effectiveness on a technology demonstrator case study. This example also demonstrates the integration of our approach with other techniques such as our Failure Propagation and Transformation Notation (FPTN) and Software Fault Trees},
author = {McDermid, J.A. and Nicholson, M. and Pumfrey, D.J. and Fenelon, P.},
booktitle = {COMPASS '95 Proceedings of the Tenth Annual Conference on Computer Assurance Systems Integrity, Software Safety and Process Security'},
doi = {10.1109/CMPASS.1995.521885},
isbn = {0-7803-2680-6},
pages = {37--48},
publisher = {IEEE},
title = {{Experience with the application of HAZOP to computer-based systems}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=521885},
year = {1995}
}
@inproceedings{Melnik2004,
abstract = {The paper summarizes three years of experience of introducing agile practices in academic environments. The perceptions of students from four different academic programs (Diploma, Applied Bachelor's, Bachelor's and Master's) from two institutions are analyzed. Specifically, pair programming, test-driven development and project planning using the planning game were studied in detail. Overwhelmingly, students ' experiences are positive and their opinions indicate the preference to continue to use agile practices if allowed. No major problems with agile techniques appeared in the evaluation contexts and benefits in these contexts have been seen.},
annote = {Introducing agile methods: Three years of experience},
author = {Melnik, G and Maurer, F},
booktitle = {Conference Proceedings of the EUROMICRO},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {334--341},
title = {{Introducing agile methods: Three years of experience}},
url = {citeulike-article-id:3934714 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-11844297780{\&}{\#}38 partnerID=40},
volume = {30},
year = {2004}
}
@article{Barriocanal2002,
abstract = {Unit testing is one of the core practices in the Extreme Programming lightweight software development method, and it is usually carried out with the help of software frameworks that ease the construction of test cases as an integral part of programming tasks. This work describes our first results in studying the integration of automated unit testing practices in conventional 'introduction to programming' laboratories. Since the work used a classical procedural language in the course's assignments, we had to design a specific testing framework called tpUnit. The results of the experiment points out that a straightforward approach for the integration of unit testing in first-semester courses do not result in the expected outcomes in terms of student's engagement in the practice.},
annote = {An experience in integrating automated unit testing practices in an introductory programming course},
author = {Barriocanal, E G and Urb{\'{a}}n, Miguel-{\'{A}}ngel and Cuevas, I A and P{\'{e}}rez, P D},
journal = {ACM SIGCSE Bulletin},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {34},
pages = {125--128},
title = {{An experience in integrating automated unit testing practices in an introductory programming course}},
url = {citeulike-article-id:3934548 {\#}},
year = {2002}
}
@article{Nagappan2008a,
abstract = {Test-driven development (TDD) is a software development practice that has been used sporadically for decades. With this practice, a software engineer cycles minute-by-minute between writing failing unit tests and writing implementation code to pass those tests. Test-driven development has recently re-emerged as a critical enabling practice of agile software development methodologies. However, little empirical evidence supports or refutes the utility of this practice in an industrial context. Case studies were conducted with three development teams at Microsoft and one at IBM that have adopted TDD. The results of the case studies indicate that the pre-release defect density of the four products decreased between 40{\%} and 90{\%} relative to similar projects that did not use the TDD practice. Subjectively, the teams experienced a 15-35{\%} increase in initial development time after adopting TDD. {\^{A}}{\textcopyright} 2008 Springer Science+Business Media, LLC.},
annote = {Realizing quality improvement through test driven development: Results and experiences of four industrial teams},
author = {Nagappan, N and Maximilien, E M and Bhat, T and Williams, L},
journal = {Empirical Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {289--302},
title = {{Realizing quality improvement through test driven development: Results and experiences of four industrial teams}},
url = {citeulike-article-id:3934735 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-44649166368{\&}{\#}38 partnerID=40},
volume = {13},
year = {2008}
}
@inproceedings{Vihavainen2014,
abstract = {Decades of effort has been put into decreasing the high failure rates of introductory programming courses. Whilst numerous studies suggest approaches that provide effective means of teaching programming, to date, no study has at- tempted to quantitatively compare the impact that differ- ent approaches have had on the pass rates of programming courses. In this article, we report the results of a system- atic review on articles describing introductory programming teaching approaches, and provide an analysis of the effect that various interventions can have on the pass rates of intro- ductory programming courses. A total of 60 pre-intervention and post-intervention pass rates, describing thirteen differ- ent teaching approaches were extracted from relevant ar- ticles and analyzed. The results showed that on average, teaching interventions can improve programming pass rates by nearly one third when compared to a traditional lecture and lab based approach.},
address = {Glasgow, Scotland},
author = {Vihavainen, Arto and Airaksinen, Jonne and Watson, Christopher},
booktitle = {ICER 2014 - Proceedings of the 10th Annual International Conference on International Computing Education Research},
doi = {10.1145/2632320.2632349},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vihavainen, Airaksinen, Watson - 2014 - A systematic review of approaches for teaching introductory programming and their influence on s.pdf:pdf},
isbn = {9781450327558},
keywords = {Analysis,CS2,Cs1,Introductory programming,Programming education,Systematic review,Teaching interventions},
mendeley-tags = {CS2},
pages = {19--26},
publisher = {ACM Press},
title = {{A systematic review of approaches for teaching introductory programming and their influence on success}},
url = {http://dl.acm.org/citation.cfm?doid=2632320.2632349},
year = {2014}
}
@article{Bandyopadhyay2009a,
author = {Bandyopadhyay, Aritra and Ghosh, Sudipto},
doi = {10.1109/ICST.2009.23},
isbn = {978-1-4244-3775-7},
journal = {2009 International Conference on Software Testing Verification and Validation},
keywords = {alistic applications,class models,in most re-,model-based testing,models,practical use of the,sequence,sequences is too,state machine models,system,test input generation,that form a complete,the number of possible},
month = {apr},
pages = {121--130},
publisher = {Ieee},
title = {{Test Input Generation Using UML Sequence and State Machines Models}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4815344},
year = {2009}
}
@inproceedings{Gotel1994,
address = {Colorado Springs, CO},
author = {Gotel, O C Z and Finkelstein, A C W},
keywords = {requirements,traceability},
pages = {94--101},
title = {{An Analysis of the Requirements Traceability Problem}},
year = {1994}
}
@inproceedings{Edwards2004,
abstract = {Introductory computer science students rely on a trial and error approach to fixing errors and debugging for too long. Moving to a reflection in action strategy can help students become more successful. Traditional programming assignments are usually assessed in a way that ignores the skills needed for reflection in action, but software testing promotes the hypothesis-forming and experimental validation that are central to this mode of learning. By changing the way assignments are assessed - where students are responsible for demonstrating correctness through testing, and then assessed on how well they achieve this goal - it is possible to reinforce desired skills. Automated feedback can also play a valuable role in encouraging students while also showing them where they can improve. Copyright 2004 ACM.},
annote = {Using software testing to move students from trial-andor to reflectlon-in-action},
author = {Edwards, S H},
booktitle = {SIGCSE Bulletin (Association for Computing Machinery, Special Interest Group on Computer Science Education)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {26--30},
title = {{Using software testing to move students from trial-andor to reflectlon-in-action}},
url = {citeulike-article-id:3934597 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33646852418{\&}{\#}38 partnerID=40},
volume = {36},
year = {2004}
}
@inproceedings{Flohr2005,
abstract = {Designing experiments to be carried out with students as subjects in an XP setup is a difficult task: Students lack experiences with XP, there are limited resources, the experiment might not be taken seriously and other effects interfere. This paper presents an experiment using student subjects examining test-first in comparison to classical-testing. We proved several hypotheses about test coverage, number of test-cases, contacts with customer, acceptance for test-first, development speed and not required features. While designing the experiment we noticed that it is useful to include some additional XP techniques on top of test first, because of our special setup and the demands we had. Despite careful planning and conduction of the experiment we still faced a number of problems. In this paper we also discuss the problems with our experimental setup. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {An XP experiment with students - Setup and problems},
author = {Flohr, T and Schneider, T},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {474--486},
title = {{An XP experiment with students - Setup and problems}},
url = {citeulike-article-id:3934606 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444558037{\&}{\#}38 partnerID=40},
volume = {3547},
year = {2005}
}
@article{5411758,
abstract = {The least-significant-bit (LSB)-based approach is a popular type of steganographic algorithms in the spatial domain. However, we find that in most existing approaches, the choice of embedding positions within a cover image mainly depends on a pseudorandom number generator without considering the relationship between the image content itself and the size of the secret message. Thus the smooth/flat regions in the cover images will inevitably be contaminated after data hiding even at a low embedding rate, and this will lead to poor visual quality and low security based on our analysis and extensive experiments, especially for those images with many smooth regions. In this paper, we expand the LSB matching revisited image steganography and propose an edge adaptive scheme which can select the embedding regions according to the size of secret message and the difference between two consecutive pixels in the cover image. For lower embedding rates, only sharper edge regions are used while keeping the other smoother regions as they are. When the embedding rate increases, more edge regions can be released adaptively for data hiding by adjusting just a few parameters. The experimental results evaluated on 6000 natural images with three specific and four universal steganalytic algorithms show that the new scheme can enhance the security significantly compared with typical LSB-based approaches as well as their edge adaptive ones, such as pixel-value-differencing-based approaches, while preserving higher visual quality of stego images at the same time.},
author = {Luo, Weiqi and Huang, Fangjun and Huang, Jiwu},
doi = {10.1109/TIFS.2010.2041812},
issn = {1556-6013},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {LSB matching revisited,data hiding,edge adaptive i},
month = {jun},
number = {2},
pages = {201--214},
title = {{Edge Adaptive Image Steganography Based on LSB Matching Revisited}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5411758},
volume = {5},
year = {2010}
}
@inproceedings{5473583,
abstract = {One of the important objectives of information hiding research is that secret information is embed as much as possible without the perception of the carrier is affected. A hiding information algorithm of adaptive multiple plane-bit based on space domain in color image is proposed, which has low computing complexity and high capacity of information hiding. The main idea of this algorithm is to judge each pixel at first, then embed hidden information in the designated point behind the highest non-zero bit until another designated point according to the human visual characteristics. The experimental results show hidden information capacity of this algorithm is much larger than LSB algorithm, and security is much higher than LSB algorithm on the premise of that the visual effect of image is not affected.},
author = {Qing, Xie and Jianquan, Xie and Yunhua, Xiao},
booktitle = {e-Business and Information System Security (EBISS), 2010 2nd International Conference on},
doi = {10.1109/EBISS.2010.5473583},
keywords = {LSB algorithm,adaptive multiple plane-bit,color im},
month = {may},
pages = {1--4},
title = {{A High Capacity Information Hiding Algorithm in Color Image}},
year = {2010}
}
@techreport{NationalAeronauticsandSpaceAdministration2009,
address = {CXP-70038, Revision B},
author = {{National Aeronautics and Space Administration}},
institution = {National Aeronautics and Space Administration},
title = {{Constellation Program Hazard Analyses Methodology}},
year = {2009}
}
@book{DAgostinoStephens,
address = {New York},
author = {D'Agostino, R B and Stephens, M A},
publisher = {M. Dekker},
title = {{Goodness-of-Fit Techniques}},
year = {1986}
}
@article{Chee1993,
abstract = {This research empirically tests the postulations of Gentner concerning the properties of explanatory analogy. It does so in the context of teaching programming. The factor analogy was operationalized by varying the clarity and systematicity/abstractness of the analogies used. The dependent variables were score obtained on program comprehension and program composition tasks and the time taken to perform the tasks. Research subjects were 15- to 17-year-olds without prior exposure to computer programming. Differences in age were controlled. The results provide empirical support for Gentner's postulations on the relative goodness of competing analogies. In particular, good explanatory analogies are characterized by clarity and high systematicity/abstractness. {\textcopyright} 1993 Academic Press. All rights reserved.},
author = {Chee, Yam San},
doi = {10.1006/imms.1993.1016},
issn = {00207373},
journal = {International Journal of Man-Machine Studies},
month = {mar},
number = {3},
pages = {347--368},
publisher = {Academic Press},
title = {{Applying Gentner's theory of analogy to the teaching of computer programming}},
volume = {38},
year = {1993}
}
@article{Janzen2006,
abstract = {Test-driven development (TDD) is an agile software development strategy that addresses both design and testing. This paper describes a controlled experiment that examines the effects of TDD on internal software design quality. The experiment was conducted with undergraduate students in a software engineering course. Students in three groups completed semester-long programming projects using either an iterative Test-First (TDD), iterative Test-Last, or linear Test-Last approach. Results from this study indicate that TDD can be an effective software design approach improving both code-centric aspects such as object decomposition, test coverage, and external quality, and developer-centric aspects including productivity and confidence. In addition, iterative development approaches that include automated testing demonstrated benefits over a more traditional linear approach with manual tests. This study demonstrates the viability of teaching TDD with minimal effort in the context of a relatively traditional development approach. Potential dangers with TDD are identified regarding programmer motivation and discipline. Pedagogical implications and instructional techniques which may foster TDD adoption will also be referenced.},
annote = {On the influence of test-driven development on software design},
author = {Janzen, D and Saiedian, H},
journal = {CSEE{\&}T, pages},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{On the influence of test-driven development on software design}},
url = {citeulike-article-id:3934654 http://users.csc.calpoly.edu/{~}djanzen/pubs/janzen-TestDrivenDevelopment.pdf},
year = {2006}
}
@article{Turnu2006,
abstract = {The goal of this work is to study the effects of the adoption of agile practices on open source development. In particular, we started to evaluate the effects of TDD (Test Driven Development) since it is easer to apply in a distributed environment than most other agile practices. In order to reach this goal we used the simulation modeling approach. We developed a simulation model of open source software development process. The model was tuned using data from a real FLOSS project: Apache HTTP Server. To introduce the TDD practice in our FLOSS simulation model, we made some assumptions based on empirical results. The two FLOSS development models (nonTDD and TDD) were compared. The one incorporating the agile practice yields better results in terms of code quality. {\^{A}}{\textcopyright} 2006 Elsevier B.V. All rights reserved.},
annote = {Modeling and simulation of open source development using an agile practice},
author = {Turnu, I and Melis, M and Cau, A and Setzu, A and Concas, G and Mannaro, K},
journal = {Journal of Systems Architecture},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {11},
pages = {610--618},
title = {{Modeling and simulation of open source development using an agile practice}},
url = {citeulike-article-id:3934822 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33749629486{\&}{\#}38 partnerID=40},
volume = {52},
year = {2006}
}
@book{Juran2010,
address = {New York, NY},
author = {Juran, Joseph M. and {De Feo}, Joseph A.},
edition = {6th},
isbn = {978-0-07-162973-7},
keywords = {quality},
publisher = {McGraw Hill},
title = {{Juran's Quality Handbook: The Complete Guide to Performance Excellence}},
year = {2010}
}
@incollection{boehm2012,
author = {Boehm, Barry W},
booktitle = {Making Software: What Really Works, and Why We Believe It},
editor = {Oram, Andy and Wilson, Greg},
pages = {161--186},
publisher = {O'Reilly},
title = {{Architecting: How Much and When}},
year = {2012}
}
@techreport{ISO25010,
author = {ISO/IEC},
institution = {ISO/IEC},
title = {{ISO/IEC 25010:2011 Systems and software engineering -- Systems and software Quality Requirements and Evaluation (SQuaRE) -- System and software quality models}},
year = {2011}
}
@inproceedings{4105313,
abstract = {Currently, airport perimeter intrusion detection primarily relies on visual surveillance by security personnel and is often augmented with video cameras. This approach is limited to day light hours and degrades with bad weather. We are developing a proof of concept system, mobile RAVIN, that detects intrusions as small as a human, works at all hours and all weather conditions, and provides rapid situational awareness to security personnel. The mobile RAVIN (radar and video integrated on mobile object architecture) system has been installed and tested at Seattle-Tacoma International Airport (SeaTac) in February 2006. It uses the airport security display processor (ASDP) - an integrated radar signal processor, track processor, and display processor system that derives threat information from the FAA's airport surface detection equipment (ASDE-3) ground surveillance radar systems. This approach leverages existing airport assets to provide a cost effective suite of security sensors. The mobile RAVIN system performs filtering and tracking on the ASDE-3 radar data, initiates and maintains video tracks of objects, and fuses radar and video tracks for operator display. It also allows operators to slew a video camera to a radar track location which reduces false alarms and nuisance alarms. Finally, we developed a display to show the radar and video tracks overlaid on a map of the airport},
author = {Mazel, David and Barry, Ann},
booktitle = {Proceedings 40th Annual 2006 International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2006.313426},
isbn = {1-4244-0174-7},
keywords = {airport security display processor,airport surface},
month = {oct},
pages = {30--33},
publisher = {Ieee},
title = {{Mobile Ravin: Intrusion Detection and Tracking with Organic Airport Radar and Video Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4105313},
year = {2006}
}
@article{Vanhanen2003,
abstract = {This paper discusses the adoption level of and experiences from using agile practices in three software development projects in a large telecom company. The use of agile practices was more emergent than planned. Project managers and developers simply used practices they considered efficient and natural. The most widely adopted agile practices were to measure progress by working code, to have developers estimate task efforts, to use coding standards, having no continuous overtime, to have the team develop its own processes, to use limited documentation, and to have the team in one physical location. The projects used conventional testing approaches. Adoption of agile testing practices, i.e., test first and automated unit tests, was low. Some agile practices can just emerge without conscious adoption, because developers find them useful. However, it seems that an emergent process aiming for agility may also neglect important agile practices},
annote = {Practical experiences of agility in the telecom industry},
author = {Vanhanen, J and Jartti, J and Kahkonen, T},
isbn = {3540402152},
journal = {Extreme Programming and Agile Processes in Software Engineering 4th International Conference, XP 2003 Proceedings Lecture Notes in Computer Science Vol 2675},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {279--287},
title = {{Practical experiences of agility in the telecom industry}},
url = {citeulike-article-id:3934826 {\#}},
volume = {2675},
year = {2003}
}
@article{Hertzum2002,
author = {Hertzum, Morten},
doi = {10.1016/S1471-7727(01)00007-0},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hertzum - 2002 - The importance of trust in software engineers' assessment and choice of information sources.pdf:pdf},
issn = {14717727},
journal = {Information and Organization},
keywords = {collaboration,communication by engineers,information seeking,information sources,trust},
mendeley-tags = {collaboration},
month = {jan},
number = {1},
pages = {1--18},
title = {{The importance of trust in software engineers' assessment and choice of information sources}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1471772701000070},
volume = {12},
year = {2002}
}
@article{Mockus2002,
abstract = {According to its proponents, open source style software development has the capacity to compete successfully, and perhaps in many cases displace, traditional commercial development methods. In order to begin investigating such claims, we examine data from two major open source projects, the Apache web server and the Mozilla browser. By using email archives of source code change history and problem reports we quantify aspects of developer participation, core team size, code ownership, productivity, defect density, and problem resolution intervals for these OSS projects. We develop several hypotheses by comparing the Apache project with several commercial projects. We then test and refine several of these hypotheses, based on an analysis of Mozilla data. We conclude with thoughts about the prospects for high-performance commercial/open source process hybrids.},
author = {Mockus, Audris and Fielding, Roy T and Herbsleb, James D},
doi = {10.1145/567793.567795},
isbn = {1049-331X},
issn = {1049331X},
journal = {ACM Transactions on Software Engineering and Methodology (TOSEM)},
keywords = {OSS},
number = {3},
pages = {309--346},
title = {{Two Case Studies of Open Source Software Development: Apache and Mozilla}},
url = {http://portal.acm.org/citation.cfm?doid=567793.567795},
volume = {11},
year = {2002}
}
@inproceedings{Trautsch2016,
address = {Austin, TX},
author = {Trautsch, Fabian and Herbold, Steffen and Makedonski, Philip and Grabowski, Jens},
booktitle = {Proceedings of the 13th International Workshop on Mining Software Repositories - MSR '16},
doi = {10.1145/2901739.2901753},
isbn = {9781450341868},
pages = {97--108},
title = {{Adressing problems with external validity of repository mining studies through a smart data platform}},
url = {http://dl.acm.org/citation.cfm?doid=2901739.2901753},
year = {2016}
}
@inproceedings{4757489,
abstract = {A security policy constitutes one of the major actors in the protection of communication networks. However, it can be one of their weaknesses if it is inadequate according to the network security requirements. For this, a security policy has to be validated before its deployment. Unfortunately, in the literature, there is no well established validation mechanisms ensuring the well founded of such security policies. This paper proposes a validation framework for security policies based on the concept of executable specifications and applied to the firewall case. The main contributions provided by this paper concerns the adaptation of some concepts and mechanisms traditionally used in software engineering for validation aims, such as specification, executable specification or reachability graph.},
author = {Abassi, R and {El Fatmi}, S G},
booktitle = {Risks and Security of Internet and Systems, 2008. CRiSIS '08. Third International Conference on},
doi = {10.1109/CRISIS.2008.4757489},
keywords = {agile,automated firewall security policy validation;comm,nsf},
mendeley-tags = {agile,nsf},
pages = {267--272},
title = {{Towards an automated firewall security policies validation process}},
year = {2008}
}
@misc{VanDijk2007,
abstract = {Test-Driven Development (TDD) is a practice that can be applied in almost all software development projects. It is a great tool to write working code and end up with clean designs. When writing code we make choices about the technologies we use and the underlying architecture. Sometimes the consequences of unfortunate choices do not show up for a while. TDD does not prevent us from making big mistakes. This paper is about such an unfortunate choice, the process of writing code based on this choice and the result. Finally, we discuss the lessons learned that can help us to avoid making big mistakes or to get a quick indication of such a mistake. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Test driving the wrong car},
author = {{Van Dijk}, I and Wijnands, R},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {250--252},
title = {{Test driving the wrong car}},
url = {citeulike-article-id:3934825 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38149061325{\&}{\#}38 partnerID=40},
volume = {4536 LNCS},
year = {2007}
}
@inproceedings{Vu2009,
abstract = {Test-Driven Development (TDD) is an agile development process wherein automated tests are created before production code is designed or constructed in short, rapid iterations. This paper discusses an experiment conducted with undergraduate students in a year-long software engineering capstone course. In this course the students designed, implemented, deployed, and maintained a software system to meet the requirements of an industry sponsor who served as the customer. The course followed an incremental process in which features were added incrementally under the direction of the industry sponsor and the professor. The fourteen students observed in the study were divided into three teams. Among the three teams were two experimental groups. One group consisted of two teams that applied a Test-First (TDD) methodology, while a control group applied a traditional Test-Last methodology. Unlike Test-First, the tests in Test-Last are written after the design and construction of the production code being tested. Results from this experiment differ from many previous studies. In particular, the Test-Last team was actually more productive and wrote more tests than their Test-First counterparts. Anecdotal evidence suggests that factors other than development approach such as individual ambition and team motivation may have more affect than the development approach applied. Although more students indicated a preference for the Test-First approach, concerns regarding learning and applying TDD with unfamiliar technologies are noted.},
address = {Las Vegas, NV},
annote = {Evaluating Test-Driven Development in an Industry-sponsored Capstone Project},
author = {Vu, John and Frojd, Niklas and Shenkel-Therolf, Clay and Janzen, David},
booktitle = {6th International Conference on Information Technology: New Generations (ITNG'09)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Evaluating Test-Driven Development in an Industry-sponsored Capstone Project}},
url = {citeulike-article-id:3934829 {\#}},
year = {2009}
}
@article{McFarlane2002a,
author = {McFarlane, Daniel C and Latorella, Kara A},
journal = {Human-Computer Interaction},
keywords = {agile,alert,hci,interruption,nsf},
mendeley-tags = {agile,nsf},
number = {1},
pages = {1--61},
title = {{The Scope and Importance of Human Interruption in Human-Computer Interaction Design}},
volume = {17},
year = {2002}
}
@inproceedings{Chen2012,
address = {Zurich, Switzerland},
author = {Chen, Tse-Hsun and Thomas, S. W. and Nagappan, M. and Hassan, A. E.},
booktitle = {2012 9th IEEE Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2012.6224280},
isbn = {978-1-4673-1761-0},
issn = {2160-1852},
keywords = {Correlation,Eclipse,Fires,History,I/O tasks,Java,Measurement,Mozilla Firefox,Mylyn,Software systems,business logic,code lines,code quality,defect-proneness,historical metrics,software concerns,software defects,software development,software fault tolerance,software metrics,software project,software quality,source code entities,statistical analysis,statistical topic modeling technique,structural metrics,topic modeling},
language = {English},
month = {jun},
pages = {189--198},
publisher = {IEEE},
title = {{Explaining software defects using topic models}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6224280},
year = {2012}
}
@inproceedings{Reichlmayr2003,
abstract = {The rise in popularity of agile software development methodologies such as Extreme Programming (XP), Crystal, DSDM and Feature-Driven Development has opened an opportunity for the software engineering education community. How can one capitalize on the strengths of agile development models while still appealing to established software engineering practices? The typical introductory software engineering course makes use of a team-based project to reinforce software process activities. The project normally runs for one academic term during which students a re led through life-cycle activities using a modified waterfall approach to software development. While useful in teaching software engineering process concepts, this approach limits the team's ability to utilize feedback from downstream process activities. It also limits the students' opportunity to understand process improvement from their own experiences. The ability to respond to project change is also dampened by the fact that teams do not have the time or resources in this format to modify, or refactor the design of a project component let alone incorporate a new or modified customer requirement. Agile methodologies promote an evolutionary approach to development using short incremental release cycles. We report on the experiences of conducting a team project in an introductory software engineering course using agile development techniques a t the Rochester Institute of Technology. Teams have the opportunity to experience multiple iterations of the software engineering life cycle and evolve a product design that allows for discoveries made during implementation and through the introduction of changing customer requirements. The project integrates the concept of test-driven development. This agile technique addresses testing early in the development process and reinforces the value of unit testing. The incorporation of agile techniques is not only useful for students in an introductory course, but may also be applied to upper division software engineering courses.},
annote = {The agile approach in an undergraduate software engineering course project},
author = {Reichlmayr, T},
booktitle = {Proceedings - Frontiers in Education Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{The agile approach in an undergraduate software engineering course project}},
url = {citeulike-article-id:3934764 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-1642519867{\&}{\#}38 partnerID=40},
volume = {3},
year = {2003}
}
@inproceedings{Allenby2001,
address = {Toronto, Canada},
annote = {{\textless}m:note{\textgreater}This paper presents a new technique for eliciting safety-requirements during the analysis phase so that they may be incorporated into development.  The new technique is based on an existing technique (HAZOP), and employs a combination of use cases and scenarios.  The integration of these three elements allows a requirements team to specify various failure scenarios and relate them to system-level design elements.  This method provides an easily-understood way to generate safety requirements early in the product lifecycle, but lacks the completeness of a more formal specification.  More work needs to be done before this method can be applied to complex failure scenarios that may involve multiple use cases.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Keywords: safety, safety-critical, scenario, requirements{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}How would this technique handle a changing environment?  It's success seems predicated on well-defined functional requirements, as well as a limited set of system and environment constraints.  If the safety-critical aspect of a system is the transmission of data across a volatile network, it seems to me that it would be impossible to generate even modestly complete safety requirements using this technique.  {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}How do safety requirements deal with the unknown?  Is there a catch-all state that is available?  If a malfunction occurs in a scenario that hasn't been thought of, can the requirements model this in any way?{\textless}/m:note{\textgreater}},
author = {Allenby, K and Kelly, T},
keywords = {requirements,safety,safety-critical,scenario,scenarios},
pages = {228--235},
title = {{Deriving Safety Requirements Using Scenarios}},
year = {2001}
}
@inproceedings{Myers2006,
address = {Montreal, Canada},
author = {Myers, B A and Weitzman, D and Ko, A J and Chau, D H},
pages = {397--406},
title = {{Answering Why and Why Not Questions in User Interfaces}},
year = {2006}
}
@article{Baumeister2002a,
abstract = {Self‐control is a promising concept for consumer research, and self‐control failure may be an important cause of impulsive purchasing. Three causes of self‐control failure are described. First, conflicting goals and standards undermine control, such as when the goal of feeling better immediately conflicts with the goal of saving money. Second, failure to keep track of (monitor) one's own behavior renders control difficult. Third, self‐control depends on a resource that operates like strength or energy, and depletion of this resource makes self‐control less effective. Trait differences in self‐control predict many behaviors. Implications for theory and research in consumer behavior are discussed.},
author = {Baumeister, Roy F.},
doi = {10.1086/338209},
issn = {0093-5301},
journal = {Journal of Consumer Research},
keywords = {Affect/Emotions/Mood,Compulsive/Impulsive Consumption,Judgment and Decision Making,Motivation/Desires/Goals,Shopping Behavior,agile,nsf},
language = {en},
mendeley-tags = {agile,nsf},
month = {mar},
number = {4},
pages = {670--676},
publisher = {The University of Chicago Press},
title = {{Yielding to Temptation: Self-Control Failure, Impulsive Purchasing, and Consumer Behavior}},
url = {http://www.jstor.org/stable/10.1086/338209},
volume = {28},
year = {2002}
}
@techreport{Bitglass2014,
author = {Bitglass},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{2014 Healthcare Breach Report}},
url = {http://pages.bitglass.com/pr-2014-healthcare-breach-report.html},
year = {2014}
}
@article{Henze2013,
abstract = {Ubiquitous computing technologies enable new in situ data-collection opportunities that can reveal insights into mobile users' behaviors and preferences.},
author = {Henze, Niels and Shrazi, Alireza Sahami and Schmidt, Albrecht and Pielot, Martin and Michahelles, Florian},
doi = {10.1109/MC.2013.202},
issn = {0018-9162},
journal = {Computer},
keywords = {Context awareness,Games,Human computer interaction,Mobile communication,Smart phones,Ubiquitous computing,human-computer interaction,invisible computing,mobile computing,research in the large,usability studies},
language = {English},
month = {jun},
number = {6},
pages = {74--76},
publisher = {IEEE},
title = {{Empirical Research through Ubiquitous Data Collection}},
url = {http://www.computer.org/csdl/mags/co/2013/06/mco2013060074.html},
volume = {46},
year = {2013}
}
@techreport{NASA2009,
author = {{Dvorak, Ed.}, Daniel},
institution = {NASA},
number = {NASA Office of Chief Engineer},
title = {{{\{}NASA{\}} study on flight software complexity}},
url = {http://www.nasa.gov/pdf/418878main{\_}FSWC{\_}Final{\_}Report.pdf},
year = {2009}
}
@inproceedings{Baxley2006b,
address = {New York, New York, USA},
author = {Baxley, Thomas and Xu, Jinsheng and Yu, Huiming and Zhang, Jinghua and Yuan, Xiaohong and Brickhouse, Joseph},
booktitle = {Proceedings of the 3rd annual conference on Information security curriculum development - InfoSecCD '06},
doi = {10.1145/1231047.1231072},
isbn = {1595934375},
keywords = {computers and education,educational software,network security,visualization},
month = {sep},
pages = {118},
publisher = {ACM Press},
title = {{LAN attacker}},
url = {http://dl.acm.org/citation.cfm?id=1231047.1231072},
year = {2006}
}
@inproceedings{Hanks2008,
abstract = {Agile software development is gaining popularity in industry. Anecdotal evidence to support this includes the emergence of agile practitioner journals and increasing attendance at agile-focused conferences. Many organizations are using agile methods because previously used techniques were not effective. Pair programming is probably the best known agile practice. Although controversial in both industry and academe, its use in education has been increasing, particularly in CS1. While some instructors fear that students may get a 'free ride' when pairing, empirical evidence shows that this is not the case. CS1 students who pair do as well in CS2 as students who worked by themselves in CS1, even when working alone in CS2 [1]. Pairing in CS1 leads to greater retention in the CS major, and also shows promise at reducing the gender gap in computing - the increase in retention among paired students is greater for women than it is for men [2]. The members of this panel argue that many other agile techniques can be used effectively in the CS curriculum. Collectively, we have more than 60 years of industrial experience; we feel that this gives us a clear understanding of the realities faced by students as they enter the workforce. Because of this, we strongly believe that students should be extensively exposed to agile practices. We have successfully taught agile techniques in courses ranging from introductory programming to the senior capstone. The benefits of this include increased student confidence and successful project delivery. We believe that agile techniques can be applied fruitfully in many other parts of the curriculum.},
annote = {Integrating agility in the CS curriculum: Practices through Values},
author = {Hanks, B and Wellington, C and Reichlmayr, T and Coupal, C},
booktitle = {SIGCSE'08 - Proceedings of the 39th ACM Technical Symposium on Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {19--20},
title = {{Integrating agility in the CS curriculum: Practices through Values}},
url = {citeulike-article-id:3934633 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57449116984{\&}{\#}38 partnerID=40},
year = {2008}
}
@article{Lampson2009,
author = {Lampson, Butler},
issn = {00010782},
journal = {Communications of the ACM},
month = {nov},
number = {11},
pages = {25},
title = {{Privacy and securityUsable security}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=1592773{\&}type=html},
volume = {52},
year = {2009}
}
@inproceedings{Robinson2005,
abstract = {XP is a social activity as well as a technical activity. The social side of XP is emphasized typically in the values and principles which underlie the technical practices. However, the fieldwork studies we have carried out with mature XP teams have shown that the technical practices themselves are also intensely social: they have social dimensions that arise from and have consequences for the XP approach. In this paper, we report on elements of XP practice that show the social side of several XP practices, including test-first development, simple design, refactoring and on-site customer. We also illustrate the social side of the practices in combination through a thematic view of progress. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {The social side of technical practices},
author = {Robinson, H and Sharp, H},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {100--108},
title = {{The social side of technical practices}},
url = {citeulike-article-id:3934769 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444446485{\&}{\#}38 partnerID=40},
volume = {3556},
year = {2005}
}
@inproceedings{Bellotti1996,
address = {Cambridge, MA},
author = {Bellotti, Victoria and Bly, Sara},
booktitle = {Proceedings of the 1996 ACM conference on Computer supported cooperative work},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellotti, Bly - 1996 - Walking away from the desktop computer distributed collaboration and mobility in a product design team.pdf:pdf},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {209--218},
publisher = {ACM},
title = {{Walking away from the desktop computer: distributed collaboration and mobility in a product design team}},
url = {http://portal.acm.org/citation.cfm?id=240080.240256{\&}type=series},
year = {1996}
}
@inproceedings{1532075,
abstract = { Networked systems still suffer from poor firewall configuration and monitoring. VisualFirewall seeks to aid in the configuration of firewalls and monitoring of networks by providing four simultaneous views that display varying levels of detail and time-scales as well as correctly visualizing firewall reactions to individual packets. The four implemented views, real-time traffic, visual signature, statistics, and IDS alarm, provide the levels of detail and temporality that system administrators need to properly monitor their systems in a passive or an active manner. We have visualized several attacks, and we feel that even individuals unfamiliar with networking concepts can quickly distinguish between benign and malignant traffic patterns with a minimal amount of introduction.},
author = {Lee, C P and Trost, J and Gibbs, N and Beyah, Raheem and Copeland, J A},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532075},
keywords = {VisualFirewall,firewall configuration,firewall m},
pages = {129--136},
title = {{Visual firewall: real-time network security monitor}},
year = {2005}
}
@inproceedings{Cao2004,
address = {Honolulu, HI},
annote = {XP for large projects},
author = {Cao, Lan and Mohan, Kannan and Xu, Peng and Ramesh, Balasubramaniam},
booktitle = {Proceedings of the 37th Hawaii International Conference on System Sciences},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2004 - How Extreme Does Extreme Programming Have to Be Adapting XP Practices to Large-scale Projects.pdf:pdf},
keywords = {XP,agile,large projects,large teams},
mendeley-tags = {agile},
pages = {83--92},
publisher = {IEEE},
title = {{How Extreme Does Extreme Programming Have to Be? Adapting XP Practices to Large-scale Projects}},
year = {2004}
}
@article{Maimon2013,
abstract = {Cybercrime has been the focus of public attention during the last decade. However, within the criminological field, no prior research initiatives have been launched in an effort to better understand this phenomenon using computer network data. Addressing this challenge, we employ the classical routine-activities and lifestyle perspective to raise hypotheses regarding the trends and origin of computer-focused crime incidents (i.e. computer exploits, port scans, and Denial of Service (DoS) attacks) against a large university computer network. We first propose that computer-focused crimes against a university network are determined by the university users' daily activity patterns. In addition, we hypothesize that the social composition of the network users determines the origin of computer attacks against the university network. We use data recorded between the years 2007 and 2009 by an Intrusion Prevention System (IPS) to test these claims. Consistently with our theoretical expectations, two important findings emerge. First, computer attacks are more likely to occur during university official business hours. Second, an increase in the number of foreign network users substantially increases the number of computer-focused crimes originating from Internet Protocol (IP) addresses linked with these users' countries of origin. Future directions for subsequent studies are discussed.},
author = {Maimon, David and Kamerdze, Amy and Cukier, Michel and Sobesto, Bertrand},
doi = {10.1093/bjc/azs067},
issn = {0007-0955},
journal = {British Journal of Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jan},
number = {2},
pages = {319--343},
title = {{Daily Trends and Origin of Computer-Focused Crimes Against a Large University Computer Network: An Application of the Routine-Activities and Lifestyle Perspective}},
url = {http://bjc.oxfordjournals.org/content/53/2/319.short},
volume = {53},
year = {2013}
}
@inproceedings{6061226,
abstract = {Inferring the sentiment of social media content, for instance blog posts and forum threads, is both of great interest to security analysts and technically challenging to accomplish. This paper presents a new method for estimating social media sentiment which addresses the challenges associated with Web-based analysis. The approach formulates the task as one of learning-based text classification, models the data as a bipartite graph of documents and words, and provides accurate sentiment estimation using only a small lexicon of words of known sentiment orientation, in particular, good performance is obtained without the need for labeled training documents. This capability for effective learning without (labeled) exemplar documents is realized by 1.)exploiting the information present in unlabeled documents and words, which are abundant online, and 2.) appropriately smoothing the sentiment polarity estimates for documents and words in the bipartite graph data model. The utility of the proposed algorithm is demonstrated through implementation with a "standard"sentiment analysis task involving online consumer product reviews. Additionally, we illustrate the potential of the method for security informatics by inferring regional public opinion regarding the Egyptian revolution via analysis of Arabic, Indonesian, and Danish blog posts.},
author = {Colbaugh, R and Glass, K},
booktitle = {Intelligence and Security Informatics Conference (EISIC), 2011 European},
doi = {10.1109/EISIC.2011.65},
keywords = {Arabic blog post;Danish blog post;Egyptian revolut},
pages = {327--331},
title = {{Agile Sentiment Analysis of Social Media Content for Security Informatics Applications}},
year = {2011}
}
@article{Ostroff2004,
abstract = {We present an agile approach to specification-driven development, which combines features of test-driven development and the plan-based approach of design-by-contract. We argue that both tests and contracts are different types of specifications, and both are useful and complementary for building high quality software. We conclude that it is useful for being able to switch between writing tests and writing contracts, and explain how specification-driven development supports this capability},
annote = {Agile specification-driven development},
author = {Ostroff, J S and Makalsky, D and Paige, R F},
isbn = {3540221379},
journal = {Extreme Programming and Agile Processes in Software Engineering 5th International Conference, XP 2004 Proceedings Lecture Notes in Comput Sci Vol 3092},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {104--112},
title = {{Agile specification-driven development}},
url = {citeulike-article-id:3934742 {\#}},
volume = {3092},
year = {2004}
}
@inproceedings{Morschhuser2007,
author = {Morschhaeuser, Iris and Lindvall, Mikael},
series = {IEEE Aerospace Conference},
title = {{Model-Based Validation {\&} Verification Integrated with SW Architecture Analysis: A Feasibility Study}},
year = {2007}
}
@inproceedings{Watson2014,
abstract = {Whilst working on an upcoming meta-analysis that synthesized fifty years of research on predictors of programming performance, we made an interesting discovery. Despite several studies citing a motivation for research as the high failure rates of introductory programming courses, to date, the majority of available evidence on this phenomenon is at best anecdotal in nature, and only a single study by Bennedsen and Caspersen has attempted to determine a worldwide pass rate of introductory programming courses. In this paper, we answer the call for further substantial evidence on the CS1 failure rate phenomenon, by performing a systematic review of introductory programming literature, and a statistical analysis on pass rate data extracted from relevant articles. Pass rates describing the outcomes of 161 CS1 courses that ran in 15 different countries, across 51 institutions were extracted and analysed. An almost identical mean worldwide pass rate of 67.7{\%} was found. Moderator analysis revealed significant, but perhaps not substantial differences in pass rates based upon: grade level, country, and class size. However, pass rates were found not to have significantly differed over time, or based upon the programming language taught in the course. This paper serves as a motivation for researchers of introductory programming education, and provides much needed quantitative evidence on the potential difficulties and failure rates of this course.},
address = {Uppsala, Sweden},
author = {Watson, Christopher and Li, Frederick W.B.},
booktitle = {ITICSE 2014 - Proceedings of the 2014 Innovation and Technology in Computer Science Education Conference},
doi = {10.1145/2591708.2591749},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Watson, Li - 2014 - Failure rates in introductory programming revisited.pdf:pdf},
isbn = {9781450328333},
keywords = {CS1,CS2,Failure rate,Introductory programming,Pass rate,Programming,Statistics},
mendeley-tags = {CS2},
pages = {39--44},
publisher = {ACM Press},
title = {{Failure rates in introductory programming revisited}},
url = {http://dl.acm.org/citation.cfm?doid=2591708.2591749},
year = {2014}
}
@article{Hamill2009,
author = {Hamill, M. and Goseva-Popstojanova, K.},
doi = {10.1109/TSE.2009.3},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamill, Goseva-Popstojanova - 2009 - Common Trends in Software Fault and Failure Data.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Software faults and failures,coding fault,complex system,empirical studies.,fault location,fault types,requirement fault,software development,software failure data,software fault analysis,software fault distribution,software fault tolerance,software life cycle activity,software reliability,software system,system recovery,systems analysis},
language = {English},
month = {jul},
number = {4},
pages = {484--496},
publisher = {IEEE},
title = {{Common Trends in Software Fault and Failure Data}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4760152},
volume = {35},
year = {2009}
}
@inproceedings{5066528,
abstract = {Notice of Violation of IEEE Publication Principles

"Novel Algorithms for Subgroup Detection in Terrorist Networks"
by Nasrullah Memon, Abdul Rasool Qureshi, Uffe Kock Wiil, and David L. Hicks
in the Proceedings of the 2009 International Conference on Availability, Reliability and Security, March 2009, pp. 572-577

After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.

This paper contains significant portions of original text from the papers cited below. The original text was copied with insufficient attribution (including appropriate references to the original author(s) and/or paper titles) and without permission.

Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following articles:

"Balancing Systematic and Flexible Exploration of Social Networks"
by Adam Perer, Ben Shneiderman
in the IEEE Transactions on Visualization and Computer Graphics, Vol. 12, No. 5 Sept/Oct 2006, pp. 693-700

"Mining for Offender Group Detection and Story of a Police Operation"
by Fatih Ozgul, Julian Bondy, Hakan Aksoy
in the Proceedings of the Sixth Australasian Data Mining Conference (AusDM 2007), Gold Coast, Australia. Conferences in Research and Practice in Information Technology (CRPIT), Vol. 70, December 2007, pp. 185-189

"The Exploratory Construction of Database Views"
by M.N. Smith, P.J.H. King
in Research Report BBKCS-02-02, School of Computer Science and Information Systems, Birbeck College, University of London. 2002. http://www.dcs.bbk.ac.uk/TriStarp/pubs/ECoDV2002.pdf

Discovery of the organizational structure of terrorist networks leads investigators to terrorist cells. Therefore, detection of covert networks from terrorists' data is important to ter- orism investigation and prevention of future terrorist activity. In this paper, we discuss this important area of subgroup detection in terrorist networks, propose novel algorithms for subgroup detection, and present a demonstration system that we have implemented.},
author = {Memon, N and Qureshi, A R and Wiil, U K and Hicks, D L},
booktitle = {Availability, Reliability and Security, 2009. ARES '09. International Conference on},
doi = {10.1109/ARES.2009.168},
keywords = {covert networks,organizational structure,subgroup},
month = {mar},
pages = {572--577},
title = {{Notice of Violation of IEEE Publication Principles Novel Algorithms for Subgroup Detection in Terrorist Networks}},
year = {2009}
}
@article{Jin2012,
author = {Jin, Wei and Orso, Alessandro},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jin, Orso - 2012 - BugRedux reproducing field failures for in-house debugging.pdf:pdf},
isbn = {978-1-4673-1067-3},
month = {jun},
pages = {474--484},
title = {{BugRedux: reproducing field failures for in-house debugging}},
url = {http://dl.acm.org/citation.cfm?id=2337223.2337279},
year = {2012}
}
@inproceedings{390642,
abstract = {Connection of a local area network to the Internet brings with it a certain amount of risk. Once connected, local computer systems are subject to attacks by other users on the Internet. To maintain system integrity, system administrators need tools to accurately display the status of communication between local hosts and remote hosts. The paper describes an ongoing research and development project for Surveying Network Information Flow (SNIF). The SNIF tool provides system administrators and security officers an encapsulated view of network communication and information flow for a set of monitored subnetworks. The tool provides a real-time graphical interface, configurable monitoring, alarms and signals, connection statistics, and an extensible interface to permit links to external systems (LES). This tool is one of many in a suite of security tools being developed at the University of Idaho},
author = {Alves-Foss, J.},
booktitle = {Proceedings of the Symposium on Network and Distributed System Security},
doi = {10.1109/NDSS.1995.390642},
isbn = {0-8186-7027-4},
keywords = {Internet,LAN connection,SNIF,alarms,attacks,commun},
month = {feb},
pages = {94--101},
publisher = {IEEE Comput. Soc. Press},
title = {{An overview of SNIF: a tool for Surveying Network Information Flow}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=390642},
year = {1995}
}
@article{Jørgensen2004,
author = {J{\o}rgensen, Magne and Mol{\o}kken-{\O}stvold, Kjetil},
number = {12},
pages = {993--1007},
title = {{Reasons for Software Effort Estimation Error: Impact of Respondent Role, Information Collection Approach, and Data Analysis Method}},
volume = {30},
year = {2004}
}
@inproceedings{Park2008,
abstract = {In this paper, we argue that executable acceptance test driven development (EATDD) allows tighter integration between the software requirements and the implementation. We argue that EATDD improves communication between all project stakeholders. We give an overview of why previous approaches to requirements specifications are less than impressive and how executable acceptance tests help fix problems. In addition, we argue for multi-modal executable acceptance tests and how it can help improve the requirements specification. We provide some of the immediate research questions that need to be addressed in order to push forward more wide-spread use of executable acceptance test driven development.},
address = {New York, NY, USA},
annote = {The benefits and challenges of executable acceptance testing},
author = {Park, Shelly and Maurer, Frank},
booktitle = {APOS '08: Proceedings of the 2008 international workshop on Scrutinizing agile practices or shoot-out at the agile corral},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {19--22},
publisher = {ACM},
title = {{The benefits and challenges of executable acceptance testing}},
url = {citeulike-article-id:3934749 http://dx.doi.org/10.1145/1370143.1370148},
year = {2008}
}
@article{Brownstein2009,
abstract = {John Brownstein, Clark Freifeld, and Lawrence Madoff write that a new generation of disease-surveillance “mashups” can mine, categorize, filter, and visualize online intelligence about epidemics in real time.},
author = {Brownstein, John S. and Freifeld, Clark C. and Madoff, Lawrence C.},
doi = {10.1056/NEJMp0900702},
issn = {0028-4793},
journal = {New England Journal of Medicine},
keywords = {twitter},
mendeley-tags = {twitter},
month = {may},
number = {21},
pages = {2153--2157},
publisher = { Massachusetts Medical Society },
title = {{Digital Disease Detection — Harnessing the Web for Public Health Surveillance}},
url = {http://www.nejm.org/doi/abs/10.1056/NEJMp0900702},
volume = {360},
year = {2009}
}
@inproceedings{1405402,
abstract = { A central aspect of airport security is reliable detection of forbidden objects in passenger bags using X-ray screening equipment. Human recognition involves visual processing of the X-ray image and matching items with object representations stored in visual memory. Thus, without knowing which objects are forbidden and what they look like, prohibited items are difficult to recognize (aspect of visual knowledge). In order to measure whether a screener has acquired the necessary visual knowledge, we have applied the prohibited items test (PIT). This test contains different forbidden items according to international prohibited items lists. The items are placed in X-ray images of passenger bags so that the object shapes can be seen relatively well. Since all images can be inspected for 10 seconds, failing to recognize a threat item can be mainly attributed to a lack of visual knowledge. The object recognition test (ORT) is more related to visual processing and encoding. Three image-based factors can be distinguished that challenge different visual processing abilities. First, depending on the rotation within a bag, an object can be more or less difficult to recognize (effect of viewpoint). Second, prohibited items can be more or less superimposed by other objects, which can impair detection performance (effect of superposition). Third, the number and type of other objects in a bag can challenge visual search and processing capacity (effect of bag complexity). The ORT has been developed to measure how well screeners can cope with these image-based factors. This test contains only guns and knives, placed into bags in different views with different superposition and complexity levels. Detection performance is determined by the ability of a screener to detect threat items despite rotation, superposition and bag complexity. Since the shapes of guns and knives are usually known well even by novices, the aspect of visual threat object knowledge is of minor importance in this test.},
author = {Schwaninger, A and Hardmeier, D and Hofer, F},
booktitle = {Security Technology, 2004. 38th Annual 2004 International Carnahan Conference on},
doi = {10.1109/CCST.2004.1405402},
keywords = {X-ray imaging,X-ray screening equipment,airport},
pages = {258--264},
title = {{Measuring visual abilities and visual knowledge of aviation security screeners}},
year = {2004}
}
@article{Rasmussen2003a,
author = {Rasmussen, Jonathan},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rasmussen - 2003 - Introducing XP into greenfield projects lessons learned.pdf:pdf},
journal = {IEEE Software},
keywords = {XP,agile},
mendeley-tags = {agile},
number = {3},
pages = {21--28},
title = {{Introducing XP into greenfield projects: lessons learned}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Introducing+XP+into+Greenfield+Projects+:{\#}0},
volume = {20},
year = {2003}
}
@inproceedings{5384681,
abstract = {Network packet traces, despite having a lot of noise, contain priceless information, especially for investigating security incidents. However, given the gigabytes of flow crossing a typical medium sized enterprise network every day, spotting malicious activity and analyzing trends in network behavior becomes a tedious task. Computational mechanisms for analyzing such data usually take substantial time to detect interesting patterns and often mislead the analyst into reaching false positives or false negatives. Therefore, the appropriate representation of network traffic data to the human user has been an issue of concern. Much of the focus, however, has been on visualizing TCP traffic alone while adapting visualization techniques for the fields that are relevant to this protocol's traffic, rather than on the multivariate nature of network security data, in general, and the fact that forensic analysis, in order to be fast and effective, has to take into consideration different parameters for each protocol. In this paper, we bring together two powerful tools: SiLK (system for Internet-level knowledge), for command-based network trace analysis; and ComVis, a generic information visualization tool. We integrate the powers of both tools by aiding simplified interaction between them, using a simple GUI, for the purpose of visualizing network traces, characterizing interesting patterns, and fingerprinting related activity. We applied the visualizations on anonymized packet traces from Lawrence Berkley National Laboratory, captured on selected hours across three months. We used a sliding window approach in visually examining traces for two transport-layer protocols: ICMP and UDP. The main contribution of this research is a protocol-specific framework of visualization for ICMP and UDP traffic data.},
author = {El-Shehaly, M and Gracanin, D and Abdel-Hamid, A and Matkovic, K},
booktitle = {New Technologies, Mobility and Security (NTMS), 2009 3rd International Conference on},
doi = {10.1109/NTMS.2009.5384681},
keywords = {ComVis,GUI,ICMP,TCP traffic visualization,UDP,anon},
pages = {1--6},
title = {{A Visualization Framework for Traffic Data Exploration and Scan Detection}},
year = {2009}
}
@inproceedings{5688764,
abstract = {To evaluate the security situation of hierarchical network, a novel evaluation algorithm based on the method of constructing a security risk function is proposed. The proposed algorithm is the aggregation of qualitative evaluation and quantitative evaluation. We quantify the asset loss (AL) and the threat value of each attack step (TVA) basing on attack graph, and adjust the loss of assets that are of the same type and have different uses with coefficient of asset importance (CAI). Then, we construct a risk evaluation function which is based on the above three parameters. Thus, the assessment score gotten through the evaluation function can comprehensively reflect the risk value including loss, threat of an attack step, and importance of the suffering entity. Finally, we get the risk value by fusing all subnets' risk value in one area, and divide the risk value into 4 security levels. Seen from the case study, the model solves the security evaluation problem for hierarchical network simply and efficiently.},
author = {Ge, Haihui and Gu, Lize and Yang, Yixian and Liu, Kewei},
booktitle = {Information Theory and Information Security (ICITIS), 2010 IEEE International Conference on},
doi = {10.1109/ICITIS.2010.5688764},
keywords = {asset importance coefficient;asset loss;attack gra},
pages = {208--211},
title = {{An attack graph based network security evaluation model for hierarchical network}},
year = {2010}
}
@inproceedings{Margolis1997,
author = {Margolis, J and Fisher, A},
title = {{Geek Mythology and Attracting Undergraduate Women to Computer Science}},
year = {1997}
}
@article{lane2011,
author = {Lane, Wilburn and Manner, Chris},
journal = {International Journal of Business and Social Science},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {17},
pages = {22--28},
publisher = {Centre for Promoting Ideas, USA},
title = {{The impact of personality traits on smartphone ownership and use}},
volume = {2},
year = {2011}
}
@inproceedings{Bachmann2010,
address = {Cape Town, South Africa},
author = {Bachmann, Adrian and Bernstein, Abraham},
booktitle = {2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010)},
doi = {10.1109/MSR.2010.5463286},
isbn = {978-1-4244-6802-7},
month = {may},
pages = {62--71},
title = {{When process data quality affects the number of bugs: Correlations in software engineering datasets}},
url = {http://ieeexplore.ieee.org/document/5463286/},
year = {2010}
}
@misc{Mark2005,
address = {Portland, OR},
author = {Mark, Gloria and Gonzalez, Victor M and Harris, Justin},
pages = {321--330},
title = {{No Task Left Behind? Examining the Nature of Fragmented Work}},
year = {2005}
}
@inproceedings{6045956,
abstract = {Privacy and security are relevant topics in both -- research and practice. Although they are often used together, implicitly assuming that they represent the same concept, they actually represent different concepts that are closely related. First, this paper presents a way to differentiate between these two topics from a conceptual point of view. Furthermore, it depicts some commonly accepted privacy regulations that exist in the OECD, EU and US. Second, we show how privacy and security are defined and implemented in practice, based on three interviews, conducted in different Austrian companies. The interviews picture the specific situation in the companies. Similarities and differences between the three interviews as well as between the interviews as a whole and the conceptual considerations were found and are described. To explain the maturity of these companies in terms of their understanding of privacy and security, we analyzed and visualized the interviews.},
author = {Krumay, B and Oetzel, M C},
booktitle = {Availability, Reliability and Security (ARES), 2011 Sixth International Conference on},
doi = {10.1109/ARES.2011.53},
keywords = {EU,OECD,US,privacy regulation,qualitative Analysis},
pages = {313--320},
title = {{Security and Privacy in Companies: State-of-the-art and Qualitative Analysis}},
year = {2011}
}
@techreport{Smith2015,
address = {Washington DC},
author = {Smtih, Aaron},
institution = {Pew Research Center},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{U.S. Smartphone Use in 2015}},
url = {http://www.pewinternet.org/2015/04/01/us-smartphone-use-in-2015/},
year = {2015}
}
@misc{Layman2020a,
address = {Wilmington, NC},
author = {Layman, Lucas and Sigmund, Kinsley and Taylor, Amy R. and Kubasko, Dennis S. and Owen, Avery C.},
title = {{Coastal Eco Explorer}},
url = {https://github.com/uncw-hfcs/cbsp-ecosystem-explorer},
year = {2020}
}
@inproceedings{Anton1996,
address = {Colorado Springs, CO},
author = {Ant{\'{o}}n, A},
pages = {136--144},
title = {{Goal-Based Requirements Analysis}},
year = {1996}
}
@inproceedings{Nataraj2011,
address = {New York, New York, USA},
author = {Nataraj, L. and Karthikeyan, S. and Jacob, G. and Manjunath, B. S.},
booktitle = {Proceedings of the 8th International Symposium on Visualization for Cyber Security - VizSec '11},
doi = {10.1145/2016904.2016908},
isbn = {9781450306799},
keywords = {computer security,image processing,image texture,malware,malware classification,malware visualization,visualization},
month = {jul},
pages = {1--7},
publisher = {ACM Press},
title = {{Malware images}},
url = {http://dl.acm.org/citation.cfm?id=2016904.2016908},
year = {2011}
}
@inproceedings{Berry2002,
address = {Essen, Germany},
annote = {Good paper. Discusses some of the flaws in XP. MAkes the point that refactoring slacks off and that customer acceptance tests are hard to write. Also points out that XP assumes the customer knows up front what the system should do. Points out that customers begin to rely on test cases written by the developers.},
author = {Berry, D M},
keywords = {XP,problems,refactoring,requirements},
title = {{The Inevitable Pain of Software Development, Including of Extreme Programming, Caused by Requirements Volatility}},
year = {2002}
}
@inproceedings{Cutrell2001,
address = {Tokyo, Japan},
author = {Cutrell, Edward and Czerwinski, Mary and Horvitz, Eric},
booktitle = {Proceedings of Human-Computer Interaction 2001 (INTERACT '01)},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {263--269},
publisher = {IOS Press},
title = {{Notification, Distruption, and Memory: Effectors of Messaging Interruptions on Memory and Performance}},
year = {2001}
}
@inproceedings{Choudhury2013,
abstract = {Major depression constitutes a serious challenge in personal and public health. Tens of millions of people each year suffer from depression and only a fraction receives adequate treatment. We explore the potential to use social media to detect and diagnose major depressive disorder in individuals. We first employ crowdsourcing to compile a set of Twitter users who report being diagnosed with clinical depression, based on a standard psychometric instrument. Through their social media postings over a year preceding the onset of depression, we measure behavioral attributes relating to social engagement, emotion, language and linguistic styles, ego network, and mentions of antidepressant medications. We leverage these behavioral cues, to build a statistical classifier that provides estimates of the risk of depression, before the reported onset. We find that social media contains useful signals for characterizing the onset of depression in individuals, as measured through decrease in social activity, raised negative affect, highly clustered egonetworks, heightened relational and medicinal concerns, and greater expression of religious involvement. We believe our findings and methods may be useful in developing tools for identifying the onset of major depression, for use by healthcare agencies; or on behalf of individuals, enabling those suffering from depression to be more proactive about their mental health.},
address = {Ann Arbor, MI},
author = {Choudhury, Munmun De and Gamon, Michael and Counts, Scott and Horvitz, Eric},
booktitle = {Seventh International AAAI Conference on Weblogs and Social Media},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choudhury et al. - 2013 - Predicting Depression via Social Media.pdf:pdf},
keywords = {depression,healthcare,mental health,prediction,social media,twitter},
mendeley-tags = {twitter},
month = {jun},
pages = {128--137},
title = {{Predicting Depression via Social Media}},
url = {https://www.aaai.org/ocs/index.php/ICWSM/ICWSM13/paper/viewPaper/6124},
year = {2013}
}
@article{Potter2004,
abstract = {Testing software security is a commonly misunderstood task. Done properly, it goes deeper than simple black-box probing on the presentation layer (the sort performed by so-called application security tools) - and even beyond the functional testing of security apparatus. Testers must use risk-based approaches, grounded in both the system's architectural reality and the attacker's mindset, to gauge software security adequately. By identifying risks in the system and creating tests driven by those risks, a software security tester can properly focus on areas of code in which an attack is likely to succeed. This approach provides a higher level of software security assurance than is possible with classical black-box testing.},
annote = {Software security testing},
author = {Potter, B and Mcgraw, G},
journal = {Security {\&} Privacy, IEEE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {5},
pages = {81--85},
title = {{Software security testing}},
url = {citeulike-article-id:3934757 http://dx.doi.org/10.1109/MSP.2004.84},
volume = {2},
year = {2004}
}
@inproceedings{Bacchelli2008,
abstract = {The importance of testing has recently seen a significant growth, thanks to its benefits to software design (e.g. think of test-driven development), implementation and maintenance support. As a consequence of this, nowadays it is quite common to introduce a test suite into an existing system, which was not designed for it. The software engineer must then decide whether using tools which automatically generate unit tests (test suites necessary foundations) and how. This paper tries to deal with the issue of choosing the best approach. We will describe how different generation techniques (both manual and automatic) have been applied to a real case study. We will compare achieved results using several metrics in order to identify different approaches benefits and shortcomings. We will conclude showing the measure how the adoption of tools for automatic test creation can shift the trade-off between time and quality. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {On the effectiveness of manual and automatic unit test generation},
author = {Bacchelli, A and Ciancarini, P and Rossi, D},
booktitle = {Proceedings - The 3rd International Conference on Software Engineering Advances, ICSEA 2008, Includes ENTISY 2008: International Workshop on Enterprise Information Systems},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {252--257},
title = {{On the effectiveness of manual and automatic unit test generation}},
url = {citeulike-article-id:3934546 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57649227757{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Chu2010a,
address = {New York, New York, USA},
author = {Chu, Matthew and Ingols, Kyle and Lippmann, Richard and Webster, Seth and Boyer, Stephen},
booktitle = {Proceedings of the Seventh International Symposium on Visualization for Cyber Security - VizSec '10},
doi = {10.1145/1850795.1850798},
isbn = {9781450300131},
keywords = {attack graph,attack path,client-side vulnerability,treemap,visualization},
month = {sep},
pages = {22--33},
publisher = {ACM Press},
title = {{Visualizing attack graphs, reachability, and trust relationships with NAVIGATOR}},
url = {http://dl.acm.org/citation.cfm?id=1850795.1850798},
year = {2010}
}
@techreport{OfficeforNationalStatistics2013,
address = {London, UK},
author = {{Office for National Statistics}},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Office for National Statistics - 2013 - Crime Statistics, Focus on Property Crime, 20112012.pdf:pdf},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {May},
pages = {1--22},
title = {{Crime Statistics, Focus on Property Crime, 2011/2012}},
year = {2013}
}
@inproceedings{Teoh2004a,
address = {New York, New York, USA},
author = {Teoh, Soon Tee and Zhang, Ke and Tseng, Shih-Ming and Ma, Kwan-Liu and Wu, S. Felix},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029215},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Teoh et al. - 2004 - Combining visual and automated data mining for near-real-time anomaly detection and analysis in BGP.pdf:pdf},
isbn = {1581139748},
keywords = {information visualization,internet routing,network visualization},
month = {oct},
pages = {35},
publisher = {ACM Press},
title = {{Combining visual and automated data mining for near-real-time anomaly detection and analysis in BGP}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029215},
year = {2004}
}
@article{4288052,
abstract = {An attack graph is a visual aid used to document the known security risks of a particular architecture; in short, it captures the paths attackers could use to reach their goals. The graph's purpose is to document the risks known at the time the system is designed, which helps architects and analysts understand the system and find good trade-offs that mitigate these risks. Once the risks are identified and understood in this way, the design can he refined iteratively until the risk becomes acceptable.},
author = {Gupta, Suvajit and Winstead, Joel},
doi = {10.1109/MSP.2007.100},
issn = {1540-7993},
journal = {Security Privacy, IEEE},
keywords = {attack graphs,data visu,security risks,visual aids},
number = {4},
pages = {80--83},
title = {{Using Attack Graphs to Design Systems}},
volume = {5},
year = {2007}
}
@inproceedings{Ren2006b,
address = {New York, New York, USA},
author = {Ren, Pin and Kristoff, John and Gooch, Bruce},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179582},
isbn = {1595935495},
keywords = {domain name system,information visualization,network security visualization,visual metaphor},
month = {nov},
pages = {23},
publisher = {ACM Press},
title = {{Visualizing DNS traffic}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179582},
year = {2006}
}
@incollection{Shull2013,
abstract = {{\textcopyright} 2013 Springer-Verlag Berlin Heidelberg. All rights are reserved. In this chapter, we discuss recent progress and opportunities in empirical software engineering by focusing on a particular technology, Technical Debt (TD), which ties together many recent developments in the field. Recent advances in TD research are providing empiricists the chance to make more sophisticated recommendations that have observable impact on practice. TD uses a financial metaphor and provides a framework for articulating the notion of tradeoffs between the short-term benefits and the long-term costs of software development decisions. TD is seeing an explosion of interest in the practitioner community, and research in this area is quickly having an impact on practice. We argue that this is due to several strands of empirical research reaching a level of maturity that provides useful benefits to practitioners, who in turn provide excellent data to researchers. They key is providing observable benefit to practitioners, such as the ability to tie technical debt measures to business goals, and the ability to articulate more sophisticated value-based propositions regarding how to prioritize rework. TD is an interesting case study in how the maturing field of empirical software engineering research is paying dividends. It is only a little hyperbolic to call this a watershed moment for empirical study, where many areas of progress are coming to a head at the same time.},
author = {Shull, Forrest and Falessi, Davide and Seaman, Carolyn and Diep, Madeline and Layman, Lucas},
booktitle = {Perspectives on the Future of Software Engineering: Essays in Honor of Dieter Rombach},
doi = {10.1007/978-3-642-37395-4_12},
editor = {M{\"{u}}nch, J{\"{u}}rgen and Schmid, Klaus},
isbn = {9783642373954},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {179--190},
publisher = {Elsevier},
title = {{Technical Debt: Showing the Way for Better Transfer of Empirical Results}},
volume = {9783642373},
year = {2013}
}
@inproceedings{Thompson2006,
abstract = {The objective of this paper is to argue for an approach to the scholarship of teaching based on conceptions of learning and task representation. A case study based on learning to implement a data structure using test-driven development is presented to illustrate the issues. Documented phenomenographic investigations into concepts of programming are referenced to lay a foundation for future research into improving learning and teaching of programming themes.},
address = {Darlinghurst, Australia, Australia},
annote = {Exploring learner conceptions of programming},
author = {Thompson, Errol and Hunt, Lynn and Kinshuk, Kinshuk},
booktitle = {ACE '06: Proceedings of the 8th Austalian conference on Computing education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {205--211},
publisher = {Australian Computer Society, Inc.},
title = {{Exploring learner conceptions of programming}},
url = {citeulike-article-id:3934815 http://portal.acm.org/citation.cfm?id=1151869.1151896},
year = {2006}
}
@incollection{Wertsch1995,
address = {Mahwah, NJ},
author = {Wertsch, J L and Toma, C and Steffe, L and Gale, J},
pages = {159--174},
publisher = {Lawrence Erlbaum},
title = {{Discourse and learning in the classroom: A sociocultural approach}},
year = {1995}
}
@inproceedings{Saito2005a,
address = {Monterey, CA},
author = {Saito, Yasushi},
booktitle = {Proceedings of the Sixth International symposium on Automated analysis-driven debugging - AADEBUG'05},
keywords = {Linux,checkpointing,debugging,execution record and replay,jockey,keywords,x86},
month = {sep},
pages = {69--76},
publisher = {ACM Press},
title = {{Jockey}},
url = {http://dl.acm.org/citation.cfm?id=1085130.1085139},
year = {2005}
}
@article{Shepperd1997,
author = {Shepperd, Martin and Schofield, Chris},
number = {11},
pages = {736--743},
title = {{Estimating Software Project Effort Using Analogies}},
volume = {23},
year = {1997}
}
@article{Sobczak2004,
abstract = {A syntax-oriented library for C++ programmers to use embedded SQL is discussed. The oprerator overloading and templates provides the effect of 'embedding' SQL code within C++ application. The library is built to set up the intended syntax before writing a single line of code. The syntax-first library development and test-first development can be used together in a library development which is both expressive and well tested.},
annote = {A simple oracle call interface},
author = {Sobczak, M},
journal = {Dr. Dobb's Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {12},
pages = {52--55},
title = {{A simple oracle call interface}},
url = {citeulike-article-id:3934803 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-8444229362{\&}{\#}38 partnerID=40},
volume = {29},
year = {2004}
}
@inproceedings{Karpierz2014,
abstract = {In this paper, we triangulate evidence for five misconceptions concerning binary search trees and hash tables. In addition, we design and validate multiple-choice concept inventory questions to measure the prevalence of four of these misconceptions. We support our conclusions with quantitative analysis of grade data and closed-ended problems, and qualitative analysis of interview data and open-ended problems. Instructors and researchers can inexpensively measure the impact of pedagogical changes on these misconceptions by using these questions in a larger concept inventory.},
address = {Atlanta, GA, USA},
author = {Karpierz, Kuba and Wolfman, Steven A.},
booktitle = {SIGCSE 2014 - Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2538862.2538902},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karpierz, Wolfman - 2014 - Misconceptions and concept inventory questions for binary search trees and hash tables.pdf:pdf},
isbn = {9781450326056},
keywords = {Concept inventory,Data structures,Misconceptions},
pages = {109--114},
publisher = {ACM Press},
title = {{Misconceptions and concept inventory questions for binary search trees and hash tables}},
url = {http://dl.acm.org/citation.cfm?doid=2538862.2538902},
year = {2014}
}
@inproceedings{6061183,
abstract = {Summary form only given. How should we communicate the results of our analysis to decision-makers? This talk will argue that visualisations and infographics can play a very important role, not only for analytical processes of data analysts, but also for explaining our analytical results to decision-makers at the highest of levels. Some care must be taken to avoid various common pitfalls when designing such visuals: the talk will cover bad examples as well as good in order to uncover design guidelines and practical advice for those wishing to pursue a more visual approach.},
author = {Parry, J},
booktitle = {Intelligence and Security Informatics Conference (EISIC), 2011 European},
doi = {10.1109/EISIC.2011.88},
keywords = {analytical processes,data analysis,decision makers},
pages = {8},
title = {{Visualisation for Decision-Makers}},
year = {2011}
}
@inproceedings{6102492,
abstract = {We present a multiple views visualization for the security data in the VAST 2010 Mini Challenge 2. The visualization is used to monitor log event activity on the network log data included in the challenge. Interactions are provided that allow analysts to investigate suspicious activity and escalate events as needed. Additionally, a database application is used to allow SQL queries for more detailed investigation.},
author = {Harrison, L and Dou, Wenwen and Lu, Aidong and Ribarsky, W and Wang, Xiaoyu},
booktitle = {Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on},
doi = {10.1109/VAST.2011.6102492},
keywords = {SQL queries,VAST 2010 Mini Challenge 2,data securi},
pages = {317--318},
title = {{Guiding security analysis through visualization}},
year = {2011}
}
@article{Basili2008g,
author = {Basili, Victor and Marotta, Frank and Dangle, Kathleen and Esker, Linda and Rus, Ioana},
journal = {CrossTalk},
month = {nov},
number = {October},
pages = {4--8},
publisher = {Ieee},
title = {{Measures and Risk Indicators for Early Insight into Software Safety}},
year = {2008}
}
@article{Kwapisz2011,
author = {Kwapisz, Jennifer R. and Weiss, Gary M. and Moore, Samuel A.},
doi = {10.1145/1964897.1964918},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
keywords = {accelerometer,activity recognition,cell phone,induction,sensor mining,sensors},
month = {mar},
number = {2},
pages = {74},
publisher = {ACM},
title = {{Activity recognition using cell phone accelerometers}},
url = {http://dl.acm.org/citation.cfm?id=1964897.1964918},
volume = {12},
year = {2011}
}
@article{Edwards1998,
author = {Edwards, Mark B and Gronlund, Scott D},
keywords = {interruption,psychology},
number = {6},
pages = {665--687},
title = {{Task Interruption and its Effects on Memory}},
volume = {6},
year = {1998}
}
@inproceedings{6107867,
abstract = {Battelle is the systems integrator on the Hazard Mitigation, Materiel and Equipment Restoration (HaMMER) Advanced Technology Demonstration (ATD) which is demonstrating integrated systems of decontaminants, applicators, and processes for recovery after a chemical or biological (CB) attack. Solutions have been developed using live agent testing and feedback from the user community. The HaMMER ATD is funded by the DTRA Joint Science and Technology Office and sponsored by the U.S. Pacific Command. The Joint Project Manager for Protection is the Transition Manager, and the U.S. Army Edgewood Chemical Biological Center serves as the Technical Manager. The HaMMER ATD focuses on three areas: {\#}x00B7; Agent disclosure - to provide a visual indication of the extent of contamination {\#}x00B7; Strippable coatings - pre-applied to mitigate contact and vapor hazards {\#}x00B7; Custom decontamination solutions that provide a dial-a-decon capability To date, the work has identified technologies that, when collectively applied, reduce or eliminate CB hazards following an attack. Technologies are grouped into suites targeted for mobile, stationary, and preparatory employment. HaMMER products will be considered for transition into the Decontamination Family of Systems (DFoS), using HaMMER test results for risk reduction. Beyond testing new technologies and systems, the program also provides a test bed to introduce new test protocols and operational methods. The HaMMER FoS consists of mobile, stationary, and support suites. The mobile suites focus on technologies that can be carried and employed on-the-move, while the stationary suite focuses on technologies that can be deployed to support field operations. The support suite provides the capability to apply preparatory measures prior to mission deployment. For the military, program benefits include: {\#}x00B7; New components and technologies that may replace or supplement existing decontamination equipment under current doctrine (near-- erm) {\#}x00B7; New FoS employed under new Concept of Operations (CONOPS) (mid-term) {\#}x00B7; Flexible configurations to address specific problems and operational needs {\#}x00B7; Incorporation of user feedback to ensure the suitability of the products These benefits and lessons learned from the program are easily translated into similar benefits for emergency responders and other homeland security professionals.},
author = {Saxon, M and Rossman, R and Yontosh, K and Merboth, G},
booktitle = {Technologies for Homeland Security (HST), 2011 IEEE International Conference on},
doi = {10.1109/THS.2011.6107867},
keywords = {Battelle,CONOPS,DFoS,DTRA Joint Science and Techno},
pages = {179--184},
title = {{Hazard Mitigation, Materiel and Equipment Restoration (HaMMER) Advanced Technology Demonstration (ATD)}},
year = {2011}
}
@inproceedings{6061836,
abstract = {Currently, NFC phones are coming in the handheld market, providing facilities to perform m-transactions. Obviously, this type of operation requires special security precautions. Indeed, a malicious code could intercept and hijack the system, even if there is a smart card. For example, the amount of the payment displayed in the terminal can be hijacked by an attacker to fool the user, or user's credential can be stolen thanks to a keylogger (and thus malicious codes can perform unwanted m-transactions automatically). This paper describes a security mechanism based on a graphical Turing test to prevent m-transactions submission by malwares. Firstly it introduces current m-transactions solutions. Then it explains the security mechanism that we propose to tackle the problem of untrusted handheld devices. It also underlines a proof of concept we implemented, to test its feasibility on a SIM card. Finally, it gives information on performances corresponding to the implementation that we made.},
author = {Pequegnot, D and Cart-Lamy, L and Thomas, A and Tigeon, T and Iguchi-Cartigny, J and Lanet, J.-L.},
booktitle = {Risk and Security of Internet and Systems (CRiSIS), 2011 6th International Conference on},
doi = {10.1109/CRiSIS.2011.6061836},
keywords = {NFC phones,SIM card,graphical Turing test,handheld},
pages = {1--8},
title = {{A security mechanism to increase confidence in m-transactions}},
year = {2011}
}
@article{weyeuker08,
author = {Weyuker, Elaine J. and Ostrand, Thomas J. and Bell, Robert M.},
doi = {10.1007/s10664-008-9082-8},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weyuker, Ostrand, Bell - 2008 - Do too many cooks spoil the broth Using the number of developers to enhance defect prediction models.pdf:pdf},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {oct},
number = {5},
pages = {539--559},
publisher = {Springer US},
title = {{Do too many cooks spoil the broth? Using the number of developers to enhance defect prediction models}},
url = {http://link.springer.com/10.1007/s10664-008-9082-8},
volume = {13},
year = {2008}
}
@article{Curtis1988,
author = {Curtis, Bill and Krasner, Herb and Iscoe, Neil},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Curtis, Krasner, Iscoe - 1988 - A field study of the software design process for large systems.pdf:pdf},
journal = {Communications of the ACM},
keywords = {collaboration},
mendeley-tags = {collaboration},
number = {11},
pages = {1268--1287},
publisher = {ACM},
title = {{A field study of the software design process for large systems}},
url = {http://portal.acm.org/citation.cfm?id=50089},
volume = {31},
year = {1988}
}
@article{Lister2004,
abstract = {A study by a ITiCSE 2001 working group ("the McCracken Group") established that many students do not know how to program at the conclusion of their introductory courses. A popular explanation for this incapacity is that the students lack the ability to problem-solve. That is, they lack the ability to take a problem description, decompose it into sub-problems and implement them, then assemble the pieces into a complete solution. An alternative explanation is that many students have a fragile grasp of both basic programming principles and the ability to systematically carry out routine programming tasks, such as tracing (or "desk checking") through code. This ITiCSE 2004 working group studied the alternative explanation, by testing students from seven countries, in two ways. First, students were tested on their ability to predict the outcome of executing a short piece of code. Second, students were tested on their ability, when given the desired function of short piece of near-complete code, to select the correct completion of the code from a small set of possibilities. Many students were weak at these tasks, especially the latter task, suggesting that such students have a fragile grasp of skills that are a prerequisite for problem-solving.},
author = {Lister, Raymond and Fone, William and McCartney, Robert and Sepp{\"{a}}l{\"{a}}, Otto and Adams, Elizabeth S. and Hamer, John and Mostr{\"{o}}m, Jan Erik and Simon, Beth and Fitzgerald, Sue and Lindholm, Morten and Sanders, Kate and Thomas, Lynda},
doi = {10.1145/1041624.1041673},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lister et al. - 2004 - A multi-national study of reading and tracing skills in novice programmers.pdf:pdf},
issn = {00978418},
journal = {SIGCSE Bulletin (Association for Computing Machinery, Special Interest Group on Computer Science Education)},
month = {dec},
number = {4},
pages = {119--150},
title = {{A multi-national study of reading and tracing skills in novice programmers}},
url = {https://dl.acm.org/doi/10.1145/1041624.1041673},
volume = {36},
year = {2004}
}
@inproceedings{Lutz1999a,
address = {Boca Raton, FL},
author = {Lutz, Robyn R. and Shaw, Hui-Tin},
booktitle = {10th Int'l Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.1999.809309},
isbn = {0-7695-0443-4},
number = {Mm},
pages = {42--49},
publisher = {IEEE Computer Society},
title = {{Applying adaptive safety analysis techniques}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/ISSRE.1999.809309},
year = {1999}
}
@inproceedings{5678729,
abstract = {As scientific studies have shown, the performance of airport security screening officers in x-ray image interpretation depends critically on individual abilities and visual knowledge acquired through class-room, computer-based (CBT) and on-the job training. The effectiveness of adaptive CBT in increasing the efficiency in x-ray image interpretation of screening officers could be verified in several studies. Individually adaptive CBT systems, like X-Ray Tutor (XRT), are constructed in such a way that with increasing performance of each individual the difficulty level will increase and become more challenging with respect to three imaged-based factors: viewpoint (depending on rotation of threat items in a bag), superposition by other items and bag complexity. The focus of this study was to examine to what extend the achieved training level in XRT positively correlates with performance in the X-Ray Competency Assessment Test (X-Ray CAT) or, more specifically, whether screeners that have acquired higher difficulty levels also achieved better test results. Knowledge about an acquired difficulty level could then, subsequently, provide an indication about the screeners' x-ray image interpretation competency. Furthermore, analysis of these correlations would suggest which XRT difficulty level screening officers should have mastered in order to be acknowledged as proficient, and for them to be well prepared to pass initial and recurrent certification with the X-Ray CAT. The latter is of particular interest as the X-Ray CAT is being used for certification at several European airports. A further goal of this study was to reproduce results of previous studies confirming the effectiveness of adaptive CBT for increasing detection performance of screeners. The study was carried out at one international airport. 199 screening officers underwent training with XRT for 20 months. Assessments were taken before the start, after 13 months and finally after 20 months of training. Analyse- - s showed high positive correlations between the achieved XRT training levels and the individual's performance in the X-Ray CAT. Based on these results, recommendations on which XRT level should be reached before initial and recurrent certification of x-ray image interpretation competency are derived. Furthermore, large increases in detection performance were found when comparing performance obtained at the three measurement dates, supporting results of earlier studies.},
author = {Michel, Stefan and Mendes, Marcia and Schwaninger, Adrian},
booktitle = {44th Annual 2010 IEEE International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2010.5678729},
isbn = {978-1-4244-7403-5},
issn = {1071-6572},
keywords = {X-ray competency assessment test,X-ray tutor,airpo},
month = {oct},
pages = {148--154},
publisher = {Ieee},
title = {{Can the difficulty level reached in computer-based training predict results in x-ray image interpretation tests?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5678729},
year = {2010}
}
@inproceedings{Balebako2013,
address = {New York, New York, USA},
author = {Balebako, Rebecca and Jung, Jaeyeon and Lu, Wei and Cranor, Lorrie Faith and Nguyen, Carolyn},
booktitle = {Proceedings of the Ninth Symposium on Usable Privacy and Security - SOUPS '13},
doi = {10.1145/2501604.2501616},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Balebako et al. - 2013 - Little brothers watching you.pdf:pdf},
isbn = {9781450323192},
keywords = {Android permissions,agile,data sharing,just-in-time notifications,mobile,nsf,privacy,smartphones,usable privacy},
mendeley-tags = {agile,nsf},
month = {jul},
pages = {1},
publisher = {ACM Press},
title = {{"Little brothers watching you"}},
url = {http://dl.acm.org/citation.cfm?id=2501604.2501616},
year = {2013}
}
@inproceedings{Tarvo2008,
address = {Redmond, WA},
author = {Tarvo, Alexander},
booktitle = {2008 19th International Symposium on Software Reliability Engineering (ISSRE)},
doi = {10.1109/ISSRE.2008.21},
month = {nov},
pages = {259--264},
title = {{Using Statistical Models to Predict Software Regressions}},
url = {http://ieeexplore.ieee.org/document/4700331/},
year = {2008}
}
@inproceedings{Goddard2000,
address = {Los Angeles, CA},
author = {Goddard, Peter L.},
booktitle = {Proceedings of the Annual Reliability and Maintainability Symposium, 2000},
pages = {118--123},
publisher = {IEEE},
title = {{Software FMEA Techniques}},
year = {2000}
}
@article{Paternoster2010,
author = {Paternoster, Ray and Pogarsky, Greg and Zimmerman, Gregory},
doi = {10.1007/s10940-010-9095-5},
issn = {0748-4518},
journal = {Journal of Quantitative Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {may},
number = {1},
pages = {1--26},
title = {{Thoughtfully Reflective Decision Making and the Accumulation of Capital: Bringing Choice Back In}},
url = {http://link.springer.com/10.1007/s10940-010-9095-5},
volume = {27},
year = {2010}
}
@article{Muller2002,
abstract = {Test-first programming is one of the central techniques of extreme programming. Programming test-first means (i) write down a test-case before coding and (ii) make all the tests executable for regression testing. Thus far, knowledge about test-first programming is limited to experience reports. Nothing is known about the benefits of test-first compared to traditional programming (design, implementation, test). This paper reports an experiment comparing test-first to traditional programming. It turns out that test-first does not accelerate the implementation, and the resulting programs are not more reliable, but test-first seems to support better program understanding.},
annote = {Experiment about test-first programming},
author = {Muller, MM and Hagner, O. and M{\"{u}}ller, M M and Hagner, O.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Muller et al. - 2002 - Experiment about test-first programming.pdf:pdf},
journal = {IEE Proceedings: Software},
keywords = {TDD,agile,file-import-09-01-23},
mendeley-tags = {TDD,agile},
number = {5},
pages = {131--136},
title = {{Experiment about test-first programming}},
url = {citeulike-article-id:3934726 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0036807037{\&}{\#}38 partnerID=40 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Experiment+about+test-first+programming{\#}0},
volume = {149},
year = {2002}
}
@inproceedings{4126216,
abstract = {In monitoring security of enterprise or campus networks, detecting attacks from internal network to external network is becoming more and more important. After detecting such attacks, finding the location of the target PC is sometimes needed. This paper describes a visual security monitoring system for large-scale local area network. The system integrates three information, logical, temporal, and geographical information, in one 3D visualization. The system also provides effective interaction capabilities and filtering mechanism. IDS logs obtained at the computer center of our university were visualized, and typical examples such as botnet activities and SSH brute force attack were discussed.},
author = {Mukosaka, S and Koike, H},
booktitle = {Visualization, 2007. APVIS '07. 2007 6th International Asia-Pacific Symposium on},
doi = {10.1109/APVIS.2007.329273},
keywords = {3D visualization,IP address,filtering mechanism,ge},
pages = {41--44},
title = {{Integrated visualization system for monitoring security in large-scale local area network}},
year = {2007}
}
@inproceedings{5375539,
abstract = {Reverse engineering compiled executables is a task with a steep learning curve. It is complicated by the task of translating assembly into a series of abstractions that represent the overall flow of a program. Most of the steps involve finding interesting areas of an executable and determining their overall functionality. This paper presents a method using dynamic analysis of program execution to visually represent the overall flow of a program. We use the Ether hypervisor framework to covertly monitor a program. The data is processed and presented for the reverse engineer. Using this method the amount of time needed to extract key features of an executable is greatly reduced, improving productivity. A preliminary user study indicates that the tool is useful for both new and experienced users.},
author = {Quist, Daniel a. and Liebrock, Lorie M.},
booktitle = {2009 6th International Workshop on Visualization for Cyber Security},
doi = {10.1109/VIZSEC.2009.5375539},
isbn = {978-1-4244-5413-6},
keywords = {Ether hypervisor framework,compiled executables,ma},
pages = {27--32},
publisher = {Ieee},
title = {{Visualizing compiled executables for malware analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5375539},
year = {2009}
}
@inproceedings{Green2006b,
author = {Green, N W and Hoffman, A R and Schow, T K M and Garrett, H B},
booktitle = {Proc.$\backslash$ 44th AIAA Aerospace Sciences Meeting and Exhibit},
pages = {1--9},
title = {{Anomaly trends for robotic missions to {\{}M{\}}ars: Implications for mission reliability}},
year = {2006}
}
@inproceedings{Leite2000,
author = {Leite, J},
pages = {73--74},
title = {{Is There a Gap between RE Research and RE Practice?}},
year = {2000}
}
@article{bossler2009line,
author = {Bossler, Adam M. and Holt, Thomas J.},
journal = {International Journal of Cyber Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {1},
pages = {400--420},
title = {{On-line activities, guardianship, and malware infection: an examination of routine activities theory}},
volume = {3},
year = {2009}
}
@inproceedings{Qiao2007,
abstract = {Test-driven development (TDD), a core Extreme Programming business-world practice, has been shown as a viable approach towards reducing product defects. The production of an "executable specification (test) document" permits the customer, with developer assistance, to explore the design parameters for the project before code is created. Given the "zero-tolerance" to defects required in biomedical products, an investigation was made to see whether the TDD business-world advantages move into the biomedical area. One key difference between the two environments is that biomedical product development is often preceded by fundamental research as the scientific solution is unknown or uncertain. In this paper, we report on the use of MUnit, a test framework for MATLAB. A key reason for using a TDD approach to MATLAB development is that the same tests, appropriately translated, can be used in the verification of the final (non-MATLAB) code running on an embedded biomedical device. Using the development of a new MR dynamic susceptibility contrast algorithm as an example, we demonstrate why the standard TDD constructs used in business TDD JUnit, and echoed in MUnit, are insufficient for a design process that involves scientific exploration. Our extended version of MUnit is compared with MTest, a commercially available model-based testing framework for MATLAB/Simulink. {\^{A}}{\textcopyright}2007 IEEE.},
annote = {On moving test-driven development from the business world into a biomedical engineering environment},
author = {Qiao, J and Smith, M and Miller, J},
booktitle = {Canadian Conference on Electrical and Computer Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1554--1557},
title = {{On moving test-driven development from the business world into a biomedical engineering environment}},
url = {citeulike-article-id:3934759 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-48749108641{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{McDonald2007,
author = {McDonald, Sharon and Edwards, Helen M},
keywords = {mbti,personality},
number = {1},
pages = {66--71},
title = {{Who should test whom?}},
volume = {50},
year = {2007}
}
@techreport{Talagala1999,
author = {Talagala, Nisha and Patterson, David},
institution = {EECS Department, University of California, Berkeley},
number = {UCB/CSD-99-1042},
title = {{An Analysis of Error Behavior in a Large Storage System}},
year = {1999}
}
@inproceedings{Petersen2009,
author = {Petersen, Kai and Wohlin, Claes},
booktitle = {3rd International Symposium on Empirical Software Engineering and Measurement (ESEM '09)},
doi = {10.1109/ESEM.2009.5316010},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Petersen, Wohlin - 2009 - Context in industrial software engineering research.pdf:pdf},
isbn = {978-1-4244-4842-5},
month = {oct},
pages = {401--404},
title = {{Context in industrial software engineering research}},
url = {http://dl.acm.org/citation.cfm?id=1671248.1671293},
year = {2009}
}
@inproceedings{Elssamadisy2001,
author = {Elssamadisy, A},
title = {{XP on a Large Project - A Developer's View}},
year = {2001}
}
@book{Brooks1995,
annote = {{\textless}m:note{\textgreater}Brook's Law:{\textless}m:linebreak/{\textgreater}Adding manpower to a late software project makes it later.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Hence the man-month as a unit for measuring the size of a job is a dangerous and deceptive myth.  It implies that men and months are interchangeable.  Men and months are interchangeabile commodities only when a task can be partitioned with no communication among them . . . In tasks that can be partitioned but which require communication among the subtasks, the effort of communication must be added to the amount of work done.  Therefore, the best that can be done is somewhat poorer than an even trade of men for months . . . If each part of the task must be separately coordinated with each other part, the effort increases as n(n-1)/2 . . . the added effort of communication may fully counteract the division of the original task.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}p. 7 "The programmer, like the poet, works only slightly removed from pure thought-stuff.  He builds his castles in the air, from air, creating by exertion of the imagination."{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}p. 232 "A team of two, with one leader, is often the best use of minds. [Note God's plan for marriage.]{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}p. 250 "Software systems are perhaps the most intricate and complex (in terms of number of distinct kinds of parts) of the things humanity makes."{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}p. 203 How to grow great designers?  Space does not permit a lengthy discussion, but some steps are obvious:{\textless}m:linebreak/{\textgreater}*  Systematically identify top designers as early as possible.  The best are often not the most experienced.{\textless}m:linebreak/{\textgreater}*  Assign a career mentor to be responsible for the development of the prospect, and keep a careful career file.{\textless}m:linebreak/{\textgreater}*  Devise and maintain a career development plan for each prospect, including carefully selected apprenticeships with top designers, episodes of advanced formal education, and short courses, all interspersed with solo design and technical leadership assignments.{\textless}m:linebreak/{\textgreater}*  Provide opportunities for growing designers to interact with and stimulate each other.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}p. 187{\textless}m:linebreak/{\textgreater}"The slow turnaround of batch programming means that we inevitably forget the minutiae, if not the very thrust, of what we were thinking when we stopped programming and called for compilation and execution.  This interruption of consciousness is costly in time, for we must refresh.  The most serious effect may well be the decay of grasp of all that is going on in a complex system."{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}p. 271{\textless}m:linebreak/{\textgreater}"Harlan Mills has argued persuasively that "programming should be a public process," that exposing all the work to everybody's gaze helps quality control, both by peer pressure to do things well and by peers actually spotting flaws and bugs."{\textless}/m:note{\textgreater}},
author = {Brooks, Frederick P},
edition = {2nd},
isbn = {0-201-00650-2},
publisher = {Addison-Wesley Publishing Company},
title = {{The Mythical Man-Month: Essays on Software Engineering, Anniversary Edition}},
year = {1995}
}
@inproceedings{Amershi2011,
address = {New York, New York, USA},
author = {Amershi, Saleema and Lee, Bongshin and Kapoor, Ashish and Mahajan, Ratul and Christian, Blaine},
booktitle = {Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11},
keywords = {interactive machine learning,triage,visualization},
month = {may},
pages = {157},
publisher = {ACM Press},
title = {{CueT}},
url = {http://dl.acm.org/citation.cfm?id=1978942.1978966},
year = {2011}
}
@article{Bron1986,
abstract = {Reviews research on the influence of external environments on the functioning of families as contexts of human development. Investigations of the interaction of genetics and environment in family processes; transitions and linkages between the family and other major settings influencing development, such as hospitals, daycare, peer groups, school, social networks, the world of work (both for parents and children), and neighborhoods and communities; and public policies affecting families and children are included. A 2nd major focus is on the patterning of environmental events and transitions over the life course as these affect and are affected by intrafamilial processes. External systems affecting the family are categorized as meso-, exo-, and chronosystem models. Identified as areas for future research are ecological variations in the expression of genotypes, relations between the family and other child settings, relations between family processes and parental participation in other settings of adult life, and families in broader social contexts. (4 p ref)},
author = {Bronfenbrenner, Urie},
journal = {Developmental Psychology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {6},
pages = {723--742},
title = {{Ecology of the family as a context for human development: Research perspectives.}},
volume = {22},
year = {1986}
}
@article{Carver2008a,
author = {Carver, J.C. and Nagappan, Nachiappan and Page, Alan},
doi = {10.1109/TSE.2008.49},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carver, Nagappan, Page - 2008 - The Impact of Educational Background on the Effectiveness of Requirements Inspections An Empirical Stud.docx:docx},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {collaboration},
mendeley-tags = {collaboration},
month = {nov},
number = {6},
pages = {800--812},
title = {{The Impact of Educational Background on the Effectiveness of Requirements Inspections: An Empirical Study}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:The+Impact+of+Educational+Background+on+the+Effectiveness+of+Requirements+Inspections:+An+Empirical+Study{\#}0},
volume = {34},
year = {2008}
}
@inproceedings{Ahern2007,
address = {New York, New York, USA},
author = {Ahern, Shane and Eckles, Dean and Good, Nathaniel S. and King, Simon and Naaman, Mor and Nair, Rahul},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '07},
doi = {10.1145/1240624.1240683},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahern et al. - 2007 - Over-exposed.pdf:pdf},
isbn = {9781595935939},
keywords = {agile,context-aware,location-aware,nsf,online content,photo sharing,photos,privacy,social software},
mendeley-tags = {agile,nsf},
month = {apr},
pages = {357},
publisher = {ACM Press},
title = {{Over-exposed?}},
url = {http://dl.acm.org/citation.cfm?id=1240624.1240683},
year = {2007}
}
@article{Prasad2013,
abstract = {OBJECTIVE: To identify medical practices that offer no net benefits. METHODS: We reviewed all original articles published in 10 years (2001-2010) in one high-impact journal. Articles were classified on the basis of whether they addressed a medical practice, whether they tested a new or existing therapy, and whether results were positive or negative. Articles were then classified as 1 of 4 types: replacement, when a new practice surpasses standard of care; back to the drawing board, when a new practice is no better than current practice; reaffirmation, when an existing practice is found to be better than a lesser standard; and reversal, when an existing practice is found to be no better than a lesser therapy. This study was conducted from August 1, 2011, through October 31, 2012. RESULTS: We reviewed 2044 original articles, 1344 of which concerned a medical practice. Of these, 981 articles (73.0{\%}) examined a new medical practice, whereas 363 (27.0{\%}) tested an established practice. A total of 947 studies (70.5{\%}) had positive findings, whereas 397 (29.5{\%}) reached a negative conclusion. A total of 756 articles addressing a medical practice constituted replacement, 165 were back to the drawing board, 146 were medical reversals, 138 were reaffirmations, and 139 were inconclusive. Of the 363 articles testing standard of care, 146 (40.2{\%}) reversed that practice, whereas 138 (38.0{\%}) reaffirmed it. CONCLUSION: The reversal of established medical practice is common and occurs across all classes of medical practice. This investigation sheds light on low-value practices and patterns of medical research.},
author = {Prasad, Vinay and Vandross, Andrae and Toomey, Caitlin and Cheung, Michael and Rho, Jason and Quinn, Steven and Chacko, Satish Jacob and Borkar, Durga and Gall, Victor and Selvaraj, Senthil and Ho, Nancy and Cifu, Adam},
doi = {10.1016/j.mayocp.2013.05.012},
issn = {1942-5546},
journal = {Mayo Clinic Proceedings},
keywords = {Biomedical Research,Biomedical Research: methods,Biomedical Research: standards,Biomedical Research: trends,Evidence-Based Medicine,Evidence-Based Medicine: standards,Humans,Journal Impact Factor,Peer Review,Periodicals as Topic,Professional Practice,Professional Practice: standards,Professional Practice: trends,Research,Research Design,Research Design: standards,Standard of Care,agile,nsf},
mendeley-tags = {agile,nsf},
month = {aug},
number = {8},
pages = {790--8},
pmid = {23871230},
title = {{A decade of reversal: an analysis of 146 contradicted medical practices.}},
url = {http://www.sciencedirect.com/science/article/pii/S0025619613004059},
volume = {88},
year = {2013}
}
@inproceedings{Knodel2006,
abstract = {The software architecture is one of the most crucial artifacts within the lifecycle of a software system. Decisions made at the architectural level directly enable, facilitate, hamper, or interfere with the achievement of business goals, functional and quality requirements. Architecture evaluations play an important role in the development and evolution of software systems since they determine how adequate the architecture is for its intended usage. This paper summarizes our practical experience with using architecture evaluations and gives an overview on when and how static architecture evaluations contribute to architecture development. We identify ten distinct purposes and needs for static architecture evaluations and illustrate them using a set of industrial and academic case studies. In particular, we show how subsequent steps in architecture development are influenced by the results from architecture evaluations.},
address = {Bari, Italy},
author = {Knodel, Jens and Lindvall, Mikael and Muthig, Dirk and Naab, Matthias},
booktitle = {Conference on Software Maintenance and Reengineering (CSMR'06)},
keywords = {Software architecture},
pages = {279--294},
series = {Conference on Software Maintenance and Reengineering (CSMR)},
title = {{Static Evaluation of Software Architectures}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/CSMR.2006.53},
year = {2006}
}
@incollection{Basili1994d,
author = {Basili, V and Caldiera, G and Rombach, D H},
booktitle = {Encyclopedia of Software Engineering},
edition = {1st},
editor = {Marciniak, John J},
keywords = {GQM,framework},
pages = {528--532},
publisher = {John Wiley and Sons, Inc.},
title = {{The Goal Question Metric Paradigm}},
volume = {2},
year = {1994}
}
@inproceedings{962820,
abstract = {This paper introduces a multiple view X-ray line-scan imaging technique for the screening of carry-on-luggage and hold cargo encountered in aviation security. The three-dimensional information inherent in the resultant perspective images can be visualised as a smooth object rotation on a video display monitor. It has also been demonstrated that this rotational effect can be incorporated into a dynamic binocular stereoscopic display. In order to produce the closely matched perspective images required by this technique an integrated multiple view dual-energy X-ray camera is under development},
author = {Evans, P and Robinson, M and Hon, H W},
booktitle = {Security Technology, 2001 IEEE 35th International Carnahan Conference on},
doi = {10.1109/.2001.962820},
keywords = {3D information,aviation security,carry-on-luggage},
month = {oct},
pages = {103--107},
title = {{Multiple view dual-energy X-ray imaging}},
year = {2001}
}
@inproceedings{Turner2011,
abstract = {Computer science students need experience with essential concepts and professional activities. Peer review is one way to meet these goals. In this work, we examine the students' attitudes towards and engagement in the peer review process, in early, object-oriented, computer science courses. To do this, we used peer review exercises in two CS2 classes at neighboring universities over the course of a semester. Using three groups (one reviewing their peers, one reviewing the instructor, and one completing small design or coding exercises), we measured the students' attitudes, their perceptions of their abilities, and how many of the reviews they completed. We found moderately positive attitudes that generally increased over time but were not significantly different between groups. We also saw a lower completion rate for students reviewing peers than for the other groups. The students' internal motivation, as measured by their need for cognition, was not shown to be strongly related to their attitudes nor to the number of assignments completed. Overall, our results show a strong need for external motivation to help engage students in peer reviews.},
address = {Dallas, TX, USA},
author = {Turner, Scott and P{\'{e}}rez-Qui{\~{n}}ones, Manuel A. and Edwards, Stephen and Chase, Joseph},
booktitle = {SIGCSE'11 - Proceedings of the 42nd ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1953163.1953268},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turner et al. - 2011 - Student attitudes and motivation for peer review in CS2.pdf:pdf},
isbn = {9781450305006},
keywords = {Attitude,CS2,Computer science education,Engagement,Learning,Peer assessment,Peer review},
mendeley-tags = {CS2},
pages = {347--352},
publisher = {ACM Press},
title = {{Student attitudes and motivation for peer review in CS2}},
url = {http://portal.acm.org/citation.cfm?doid=1953163.1953268},
year = {2011}
}
@inproceedings{Canfora2007,
address = {Minneapolis, MN},
author = {Canfora, Gerardo and Cerulo, Luigi and Penta, Massimiliano Di},
booktitle = {Fourth International Workshop on Mining Software Repositories (MSR'07:ICSE Workshops 2007)},
doi = {10.1109/MSR.2007.14},
isbn = {0-7695-2950-X},
month = {may},
pages = {Article No. 14},
publisher = {IEEE},
title = {{Identifying Changed Source Code Lines from Version Repositories}},
url = {http://ieeexplore.ieee.org/document/4228651/},
year = {2007}
}
@inproceedings{Jones2006,
abstract = {This paper introduces test-driven specification , whereby the specification process is aided by the use of test cases. We also introduce an automated tool, the test-driven specification assistant (TDSA), which supports this approach. Test cases reveal specification anomalies such as incorrectness, incompleteness and ambiguity. Specification-based test coverage criteria are applied to reveal deficiencies in the set of test cases. Decision tables are used as a lightweight specification language capable of modeling black-box and Mills' state box specifications.},
address = {New York, NY, USA},
annote = {Test-driven specification: paradigm and automation},
author = {Jones, Edward},
booktitle = {ACM-SE 44: Proceedings of the 44th annual Southeast regional conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {796--797},
publisher = {ACM},
title = {{Test-driven specification: paradigm and automation}},
url = {citeulike-article-id:3934666 http://dx.doi.org/10.1145/1185448.1185641},
year = {2006}
}
@article{Ostroff2005,
abstract = {We describe a contract-aware unit testing framework, E-Tester, for the Eiffel programming language. The framework differs from JUnit in its first-class support for lightweight formal methods, through test support for contracts and assertions. As well, it supports a form of negative test, called violation cases, which aim at validating contracts. It also differs based on its use of agents for expressing tests and test cases. We compare E-Tester with JUnit and suggest several advantages it offers, with the additional aim of making recommendations for improving JUnit's support for testing software with contracts. We also explain how it can be applied within a test-driven process for building reliable systems. {\^{A}}{\textcopyright} JOT, 2005.},
annote = {E-Tester: A contract-aware and agent-based unit testing framework for Eiffel},
author = {Ostroff, J S and Paige, R F and Makalsky, D and Brooke, P J},
journal = {Journal of Object Technology},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {7},
pages = {97--114},
title = {{E-Tester: A contract-aware and agent-based unit testing framework for Eiffel}},
url = {citeulike-article-id:3934743 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-25844520500{\&}{\#}38 partnerID=40},
volume = {4},
year = {2005}
}
@article{Saurer2006,
abstract = {Today's business climate requires to constantly evolve IT strategies for responding to new opportunities or threats. While the fundamentals of IT — reliability, availability, security and manageability — are still crucial, rapid results are mandatory for business success. These business challenges can be solved by acting with agility – striking the proper balance between the introduction of leading-edge technology and the pragmatic application of IT. In this paper, we introduce a testing framework for business solutions dealing with complex and dynamic IT environments. Our framework enables a test-driven development and adaptation of business processes in order to implement flexible and reliable business solutions. We compare our testing framework with model-driven development approaches and show how we applied it to an event-driven process management platform called SARI},
annote = {Testing Event-Driven Business Processes},
author = {Saurer, Gerd and Schiefer, Josef and Schatten, Alexander},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {69--80},
title = {{Testing Event-Driven Business Processes}},
url = {citeulike-article-id:3934779 {\#}},
year = {2006}
}
@inproceedings{Guerra2007,
abstract = {Test Driven Development is a technique in which the refactoring occurs all the time, in the application code and in the test code. But there is not a method to guarantee that the test code behavior after one refactoring remains unchanged. This paper presents a representation based on the JUnit unit test structure, as well as a classification of test code refactorings that may ease the analysis to verify if he test code refactoring was carried out safely, i.e., if the observable behavior of the refactored test code has been kept unchanged. The use of this proposed technique may safely improve and speed up the production of test code refactorings. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Refactoring test code safely},
author = {Guerra, E M and Fernandes, C T},
booktitle = {2nd International Conference on Software Engineering Advances - ICSEA 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Refactoring test code safely}},
url = {citeulike-article-id:3934629 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-47849126879{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Chinn2007,
abstract = {Active and problem-based learning environments strive to improve students' problem solving skills. To better understand students' problem solving processes and thus guide the structure and development Of Such environments, we asked students to solve data structures and algorithms problems and to verbalize their thoughts as they solved them. In this paper, we discuss methodological issues associated with the analysis of their verbalizations. We then analyze and discuss the relationship between statistics that describe Students' problem solving process and their performance in the course they were taking at the time, either the data structures or algorithms course.},
address = {Dundee, Scotland},
author = {Chinn, Donald and Spencer, Catherine and Martin, Kristofer},
booktitle = {ITiCSE 2007: 12th Annual Conference on Innovation and Technology in Computer Science Education - Inclusive Education in Computer Science},
doi = {10.1145/1268784.1268854},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chinn, Spencer, Martin - 2007 - Problem solving and student performance in data structures and algorithms.pdf:pdf},
isbn = {1595936106},
issn = {0097-8418},
keywords = {CS2,Collaborative learning environments,Problem solving,Problem-based learning,Treisman workshops},
mendeley-tags = {CS2},
pages = {241--245},
publisher = {ACM Press},
title = {{Problem solving and student performance in data structures and algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=1268784.1268854},
year = {2007}
}
@inproceedings{5283057,
abstract = {Image denoising and magnification play an important role in most visual applications such as visual material examination for public security and image-based medical diagnosis. We propose a 1-D kernel fitting algorithm for denoising in space domain and wavelet transformed (WT) domain, and for magnification in space domain. In the algorithm, the values of a column or a row from an image or its transformed version are taken as the measured results of a fitting function. The fitting coefficients are estimated by least square (LS) method. An image is denoised or magnified by resampling the fitted function, or followed by inverse transform if fitting is carried out in a transformed domain. We also discuss a modified singular value decomposition (SVD) method for comparison. To illustrate the application feasibility, the presented methods are experimentally compared with the basic wavelet-thresholding algorithm for image denoising, and with the standard bicubic interpolation method for magnification.},
author = {Liu, Benyong and Liao, Xiang},
booktitle = {2009 Fifth International Conference on Information Assurance and Security},
doi = {10.1109/IAS.2009.29},
isbn = {978-0-7695-3744-3},
keywords = {1-D kernel fitting algorithm,SVD,i,image denoising},
pages = {521--524},
publisher = {Ieee},
title = {{Image Denoising and Magnification via Kernel Fitting and Modified SVD}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5283057},
volume = {2},
year = {2009}
}
@article{Bliss1995,
abstract = {The goals of this research were to substantiate the existence of the cry-wolf effect for alarm responses, quantifying its effect on operator performance. A total of 138 undergraduate students performed two blocks of a cognitively demanding psychomotor primary task; at the same time, they were presented with alarms of varying reliabilities (25, 50 and 75{\%} true alarms) and urgencies (green, yellow and red visual alarms presented concurrently with low-, medium- and high-urgency auditory civilian aircraft cockpit alarms). Alarm response frequencies were observed and analysed, and t-tests and repeated-measures MANOVAs were used to assess the effects of increasing alarm reliability on alarm response frequencies, speed and accuracy. The results indicate that most subjects (about 90{\%}) do not respond to all alarms but match their response rates to the expected probability of true alarms (probability matching). About 10{\%} of the subjects responded in the extreme, utilizing an all-or-none strategy. Implications of these results for alarm design instruction and further research are discussed.},
author = {Bliss, J P and Gilson, R D and Deaton, J E},
doi = {10.1080/00140139508925269},
issn = {0014-0139},
journal = {Ergonomics},
keywords = {Adult,Cooperative Behavior,Humans,Safety Management},
language = {en},
month = {nov},
number = {11},
pages = {2300--12},
pmid = {7498189},
publisher = {Taylor {\&} Francis Group},
title = {{Human probability matching behaviour in response to alarms of varying reliability.}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00140139508925269},
volume = {38},
year = {1995}
}
@inproceedings{Tourani2017,
address = {Klagenfurt, Austria},
author = {Tourani, Parastou and Adams, Bram and Serebrenik, Alexander},
booktitle = {2017 IEEE 24th International Conference on Software Analysis, Evolution and Reengineering (SANER)},
doi = {10.1109/SANER.2017.7884606},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tourani, Adams, Serebrenik - 2017 - Code of conduct in open source projects.pdf:pdf},
isbn = {978-1-5090-5501-2},
keywords = {sentiment},
mendeley-tags = {sentiment},
month = {feb},
pages = {24--33},
publisher = {IEEE},
title = {{Code of conduct in open source projects}},
url = {http://ieeexplore.ieee.org/document/7884606/},
year = {2017}
}
@inproceedings{Beach2010,
address = {New York, New York, USA},
author = {Beach, Aaron and Gartrell, Mike and Xing, Xinyu and Han, Richard and Lv, Qin and Mishra, Shivakant and Seada, Karim},
booktitle = {Proceedings of the Eleventh Workshop on Mobile Computing Systems {\&} Applications - HotMobile '10},
doi = {10.1145/1734583.1734599},
isbn = {9781450300056},
month = {feb},
pages = {60},
publisher = {ACM Press},
title = {{Fusing mobile, sensor, and social data to fully enable context-aware computing}},
url = {http://dl.acm.org/citation.cfm?id=1734583.1734599},
year = {2010}
}
@inproceedings{Li2004,
address = {New York, New York, USA},
author = {Li, Wei-Jen and Hershkop, Shlomo and Stolfo, Salvatore J.},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029229},
isbn = {1581139748},
keywords = {email,spam,virus},
month = {oct},
pages = {128},
publisher = {ACM Press},
title = {{Email archive analysis through graphical visualization}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029229},
year = {2004}
}
@article{Leveson1993,
author = {Leveson, N.G. and Turner, C.S.},
doi = {10.1109/MC.1993.274940},
issn = {0018-9162},
journal = {Computer},
month = {jul},
number = {7},
pages = {18--41},
title = {{An investigation of the Therac-25 accidents}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:An+Investigation+of+the+Therac-25+Accidents{\#}0},
volume = {26},
year = {1993}
}
@misc{USCERT2010a,
address = {Washington, DC},
author = {{US CERT}},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/US CERT - 2010 - Cyber Threats to Mobile Devices.pdf:pdf},
institution = {US CERT, Department of Homeland Security},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {1--16},
title = {{Cyber Threats to Mobile Devices}},
url = {https://www.us-cert.gov/sites/default/files/publications/cyber{\_}threats-to{\_}mobile{\_}phones.pdf},
year = {2010}
}
@inproceedings{Alabiso1988,
address = {San Diego, CA},
author = {Alabiso, Bruno},
pages = {335--353},
title = {{Transformation of Data Flow Analysis Models to Object Oriented Design}},
year = {1988}
}
@book{Pressman2001,
address = {Boston},
author = {Pressman, Roger S},
edition = {Fifth},
publisher = {McGraw Hill},
title = {{Software engineering: a practitioner's approach}},
year = {2001}
}
@article{Adams1999,
author = {Adams, Anne and Sasse, Martina Angela},
doi = {10.1145/322796.322806},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Adams, Sasse - 1999 - Users are not the enemy.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {agile,nsf,security},
mendeley-tags = {agile,nsf,security},
month = {dec},
number = {12},
pages = {40--46},
title = {{Users are not the enemy}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=322806{\&}type=html},
volume = {42},
year = {1999}
}
@inproceedings{Elbaum1998,
address = {Bethesda, MD},
author = {Elbaum, S.G. and Munson, J.C.},
booktitle = {Proceedings. International Conference on Software Maintenance (ICSM '98)},
doi = {10.1109/ICSM.1998.738486},
isbn = {0-8186-8779-7},
keywords = {churn},
mendeley-tags = {churn},
pages = {24--31},
title = {{Code churn: a measure for estimating the impact of code change}},
url = {http://ieeexplore.ieee.org/document/738486/},
year = {1998}
}
@inproceedings{5588311,
abstract = {Intrusion detection relies on the ability to obtain reliable and trustworthy measurements, while adversaries will inevitably target such monitoring and security systems to prevent their detection. This has led to a number of proposals for using coprocessors as protected monitoring instances. However, such coprocessors suffer from two problems, namely the ability to perform measurements without relying on the host system and the speed at which such measurements can be performed. The availability of smart, high-performance subsystems in commodity computer systems such as graphics processing units (GPU) strongly motivates an investigation into novel ways of achieving the twin objectives of self-protected observation and monitoring systems and sufficient measurement frequency. This, however, gives rise to performance penalties imposed by memory synchronization particularly in non-uniform memory architectures (NUMA) even for the case of direct memory access (DMA) transfers. Based on prior work detailing a cost model for synchronization of memory access in such advanced architectures, we report an experimental validation of the cost model using an IEEE 1394 DMA bus mastering environment, which provides full access to the measurement target's main memory and involves multiple bus bridges and concomitant synchronization mechanisms. We observed up to 25{\%} performance degradation, highlighting the need for efficient sampling strategies for both, memory size and a preference for quiescent data structures for monitoring executed by off-host devices.},
author = {Seeger, M M and Wolthusen, S D and Busch, C and Baier, H},
booktitle = {Information Security for South Africa (ISSA), 2010},
doi = {10.1109/ISSA.2010.5588311},
keywords = {DMA bus mastering environment,IEEE 1394,commodity},
pages = {1--8},
title = {{The cost of observation for intrusion detection: Performance impact of concurrent host observation}},
year = {2010}
}
@article{4218558,
abstract = {Microsoft's release of Windows Vista marks the arrival of a new era for software security. Fundamental changes have gradually occurred, bringing us to a point now where the threat landscape no longer resembles what it was just a few years ago. Vista's release is ideal to consider as a culmination point; it's from here that software attack strategies will move into new directions. In this article, the author examines some of these new directions, as well as some of the changes related to Vista that most encapsulate the current threat landscape for software security. Eight characterirstics most strongly define the new software security threat landscape. Let's take a look at them: actualization of Web vulnerability threats; advances in code analysis; more advanced techniques; client-side vulnerabilities; remote exploitation; targeted attacks; sale of vulnerability information; and anti-exploitation technology.},
author = {Ahmad, D},
doi = {10.1109/MSP.2007.73},
issn = {1540-7993},
journal = {Security Privacy, IEEE},
keywords = {Web vulnerability threat,Windows Vista,agile,antiexploit,nsf},
mendeley-tags = {agile,nsf},
number = {3},
pages = {75--77},
title = {{The Contemporary Software Security Landscape}},
volume = {5},
year = {2007}
}
@inproceedings{McMillan2013,
address = {New York, New York, USA},
author = {McMillan, Donald and Morrison, Alistair and Chalmers, Matthew},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems - CHI '13},
doi = {10.1145/2470654.2466245},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McMillan, Morrison, Chalmers - 2013 - Categorised ethical guidelines for large scale mobile HCI.pdf:pdf},
isbn = {9781450318990},
keywords = {agile,app stores,ethics,large-scale trials,mass participation,nsf},
mendeley-tags = {agile,nsf},
month = {apr},
pages = {1853},
publisher = {ACM Press},
title = {{Categorised ethical guidelines for large scale mobile HCI}},
url = {http://dl.acm.org/citation.cfm?id=2470654.2466245},
year = {2013}
}
@inproceedings{Linstead2008,
address = {San Diego, CA},
author = {Linstead, Erik and Lopes, Cristina and Baldi, Pierre},
booktitle = {2008 Seventh International Conference on Machine Learning and Applications},
doi = {10.1109/ICMLA.2008.47},
isbn = {978-0-7695-3495-4},
keywords = {Application software,Argo UML,Computer bugs,Eclipse,History,Information analysis,Java,Linear discriminant analysis,Machine learning,Open source software,Project management,Software engineering,latent Dirichlet allocation,latent dirichlet allocation,open source Java projects,public domain software,software design,software engineering,software evolution,software evolution analysis,software mining,software mining community,statistical analysis,topic models,unsupervised statistical topic models},
language = {English},
pages = {813--818},
publisher = {IEEE},
title = {{An Application of Latent Dirichlet Allocation to Analyzing Software Evolution}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4725072},
year = {2008}
}
@inproceedings{6106763,
abstract = {We have developed three simple simulations for range cameras (also called 3D cameras or flash LIDARs) that may be integrated into existing 3D robot visualisation and simulation frameworks. They are able to generate data at better than video framerates with minimal additional processing by leveraging the OpenGL graphics engine. We have used these simulations in our work on applying machine learning to the task of autonomous complex terrain robot mobility and 3D SLAM. We found that with a simple noise model it is possible to obtain realistic results.},
author = {Sheh, R and Sammut, C},
booktitle = {Safety, Security, and Rescue Robotics (SSRR), 2011 IEEE International Symposium on},
doi = {10.1109/SSRR.2011.6106763},
keywords = {3D SLAM,3D cameras,3D robot simulation,3D robot vi},
pages = {184--189},
title = {{Simulating range cameras for complex terrain robot mobility}},
year = {2011}
}
@book{efron93,
address = {London},
author = {Efron, Bradley and Tibshirani, Robert J},
publisher = {Chapman and Hall},
series = {Mono. Stat. Appl. Probab.},
title = {{An introduction to the bootstrap}},
year = {1993}
}
@phdthesis{Layman2008e,
author = {Layman, Lucas},
keywords = {mypubs},
mendeley-tags = {mypubs},
school = {North Carolina State University},
title = {{Information Needs of Developers for Program Comprehension during Software Maintenance Tasks}},
type = {Ph.D. Dissertation},
url = {http://repository.lib.ncsu.edu/ir/bitstream/1840.16/3703/1/etd.pdf},
year = {2008}
}
@article{Stotts2002,
abstract = {The JUnit testing tool is widely used to support the central XP concept of “test first” software development. While JUnit provides Java classes for expressing test cases and test suites, it does not provide or proscribe per se any guidelines for deciding what test cases are good ones for any particular class. We have developed a method for systematically creating complete and consistent test classes for JUnit. Called JAX (for Junit Axioms), the method is based on Guttag's algebraic specification of abstract data types. We demonstrate an informal use of ADT semantics for guiding JUnit test method generation; the programmer uses no formal notation other than Java, and the procedure meshes with XP test-as-design principles. Preliminary experiments show that informal JAX-based testing finds more errors than an ad hoc form of JUnit testing. SE  - Extreme Programming and Agile Methods XP/Agile Universe 2002 Second XP Universe and First Agile Universe Conference Proceedings Lecture Notes in Computer Science Vol 2418},
annote = {English An informal formal method for systematic JUnit test case generation Conference Paper},
author = {Stotts, D and Lindsey, M and Antley, A},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{An informal formal method for systematic JUnit test case generation}},
url = {citeulike-article-id:3934809 {\#}},
year = {2002}
}
@inproceedings{BowersJ.MayE.MelanderM.BaarmanandA.Ayoob2002,
address = {Chicago, IL},
author = {{Bowers  J. May, E. Melander, M. Baarman, and A. Ayoob}, J and Wells, J D and Williams, L},
pages = {100--111},
publisher = {Springer},
title = {{Tailoring XP for Large System Mission-Critical Software Development}},
year = {2002}
}
@inproceedings{4389007,
abstract = {This article presents SpiralView, a visualization tool for helping system administrators to assess network policies. The tool is meant to be a complementary support to the routine activity of network monitoring, enabling a retrospective view on the alarms generated during and extended period of time. The tool permits to reason about how alarms distribute over time and how they correlate with network resources (e.g., users, IPs, applications, etc.), supporting the analysts in understanding how the network evolves and thus in devising new security policies for the future. The spiral visualization plots alarms in time, and, coupled with interactive bar charts and a users/applications graph view, is used to present network data and perform queries. The user is able to segment the data in meaningful subsets, zoom on specific related information, and inspect for relationships between alarms, users, and applications. In designing the visualizations and their interaction, and through tests with security experts, several ameliorations over the standard techniques have been provided.},
author = {Bertini, Enrico and Hertzog, Patrick and Lalanne, Denis},
booktitle = {2007 IEEE Symposium on Visual Analytics Science and Technology},
doi = {10.1109/VAST.2007.4389007},
isbn = {978-1-4244-1659-2},
keywords = {corporate network,interactive bar chart,network ma},
month = {oct},
pages = {139--146},
publisher = {IEEE},
title = {{SpiralView: Towards Security Policies Assessment through Visual Correlation of Network Resources with Evolution of Alarms}},
url = {http://dl.acm.org/citation.cfm?id=1549822.1549970},
year = {2007}
}
@book{Bron1979,
address = {Cambridge, MA},
author = {Bronfenbrenner, Urie},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Harvard University Press},
title = {{The Ecology of Human Development}},
year = {1979}
}
@inproceedings{Patel2008,
abstract = {Software quality is the main factor for the software reliability and software performance. Extreme programming strongly argues that it improves the quality of the software through feedback from iterative software development and by practicing pair programming and test driven development. Software quality is mainly depends on non-functional requirements. In most of cases non-functional requirements are not covered in the exploration phase. Especially in XP it is difficult to address non-functional requirements on the story cards. Our objective of this paper is knowledge based support to story cards to improve story cards and other XP practices by bridging them with the design of testability to improve non-functional user requirements, and predefined the factors for software testability on story cards with acceptance tests. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Bridging best traditional SWD practices with XP to improve the quality of XP projects},
author = {Patel, C and Ramachandran, M},
booktitle = {Proceedings - International Symposium on Computer Science and Its Applications, CSA 2008},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {357--360},
title = {{Bridging best traditional SWD practices with XP to improve the quality of XP projects}},
url = {citeulike-article-id:3934751 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-56649105341{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{6122786,
abstract = {This paper reports on the usability study carried out to assess the feasibility of combining two graphical password methods for better security. The methods involved clicking on the image (i.e. click-based) and selecting a series of images (i.e. choice-based). A graphical password prototype was developed (Enhanced Graphical Authentication System) and tested by thirty participants, who were randomly chosen from the authors' university. Two evaluations were made; namely user performance of the combined method and the feasibility of authentication strategies towards the introduced method itself. From both evaluations, it is found that positive results have been obtained, which suggest that these methods could be combined together effectively without giving impediment to users. However, there are issues relating to predictability as a consequence of insecure user behaviour.},
author = {Jali, M Z and Furnell, S M and Dowland, P S},
booktitle = {Information Assurance and Security (IAS), 2011 7th International Conference on},
doi = {10.1109/ISIAS.2011.6122786},
keywords = {end user performance assessment,enhanced graphical},
pages = {7--12},
title = {{Multifactor graphical passwords: An assessment of end-user performance}},
year = {2011}
}
@inproceedings{Burkhardt1998,
author = {Burkhardt, JM},
booktitle = {Comprehension, 1998.},
title = {{The effect of object-oriented programming expertise in several dimensions of comprehension strategies}},
url = {http://www.springerlink.com/index/AB8W1FCYGFCUR4V4.pdf http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=693294},
year = {1998}
}
@inproceedings{Layman2004,
address = {Salt Lake City, UT},
author = {Layman, L and Williams, L and Cunningham, L},
booktitle = {Agile Development Conference 2004 (ADC'04)},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {32--41},
title = {{Exploring Extreme Programming in Context: An Industrial Case Study}},
year = {2004}
}
@misc{Tiryaki2007,
abstract = {Complex and distributed nature of multi-agent systems (MASs) makes it almost impossible to identify of all requirements at the beginning of the development. Hence, development of such systems needs an iterative and incremental process to handle complexity and the continuously changing na-ture of the requirements. In this paper, a test driven multi-agent system devel-opment approach that naturally supports iterative and incremental MAS con-struction is proposed. Also a testing framework called as SUnit which supports the proposed approach by extending JUnit framework is introduced. This framework allows writing automated tests for agent behaviors and interactions between agents. The framework also includes the necessary mock agents to model the organizational aspects of the MAS. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {SUNIT: A unit testing framework for test driven development of multi-agent systems},
author = {Tiryaki, A M and {\"{O}}ztuna, S and Dikenelli, O and Erdur, R C},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {156--173},
title = {{SUNIT: A unit testing framework for test driven development of multi-agent systems}},
url = {citeulike-article-id:3934819 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38049129010{\&}{\#}38 partnerID=40},
volume = {4405 LNCS},
year = {2007}
}
@inproceedings{Baziuk1995,
address = {Toulouse, France},
author = {Baziuk, Walter},
booktitle = {Proceedings of Sixth International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.1995.497665},
isbn = {0-8186-7131-9},
keywords = {deming,dr,is09001,metrics,process improvement},
pages = {256--262},
publisher = {IEEE Comput. Soc. Press},
title = {{BNR/NORTEL:  Path to improve product quality, reliability, and customer satisfaction}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=497665},
year = {1995}
}
@article{Xu2006b,
address = {New York, New York, USA},
author = {Xu, Weifeng and Xu, Dianxiang},
doi = {10.1145/1146374.1146376},
isbn = {1595934154},
journal = {Proceedings of the 2nd workshop on Testing aspect-oriented programs - WTAOP '06},
keywords = {aspect-oriented modeling,aspect-oriented programming,integration aspects,integration testing,model-based testing,state model},
pages = {7--14},
publisher = {ACM Press},
title = {{State-based testing of integration aspects}},
url = {http://portal.acm.org/citation.cfm?doid=1146374.1146376},
year = {2006}
}
@inproceedings{Francis2004,
abstract = {Recent research has addressed the problem of providing automated assistance to software developers in classifying reported instances of software failures so that failures with the same cause are grouped together. In this paper, two new tree-based techniques are presented for refining an initial classification of failures. One of these techniques is based on the use of dendrograms, which are rooted trees used to represent the results of hierarchical cluster analysis. The second technique employs a classification tree constructed to recognize failed executions. With both techniques, the tree representation is used to guide the refinement process. We also report the results of experimentally evaluating these techniques on several subject programs.},
author = {Francis, P. and Leon, D. and Minch, M. and Podgurski, A.},
booktitle = {15th International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.2004.43},
isbn = {0-7695-2215-7},
issn = {1071-9458},
keywords = {Classification tree analysis,Clustering algorithms,Computer science,Data analysis,Data mining,Failure analysis,Information analysis,Iterative algorithms,Pattern classification,Software testing,dendrograms,fault trees,hierarchical cluster analysis,pattern classification,pattern clustering,program diagnostics,refinement process,software failure,software fault tolerance,tree representation,tree-based method},
pages = {451--462},
publisher = {IEEE},
shorttitle = {Software Reliability Engineering, 2004. ISSRE 2004},
title = {{Tree-Based Methods for Classifying Software Failures}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1383139},
year = {2004}
}
@article{Tversky1974,
abstract = {This article described three heuristics that are employed in making judgements under uncertainty: (i) representativeness, which is usually employed when people are asked to judge the probability that an object or event A belongs to class or process B; (ii) availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development; and (iii) adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available. These heuristics are highly economical and usually effective, but they lead to systematic and predictable errors. A better understanding of these heuristics and of the biases to which they lead could improve judgements and decisions in situations of uncertainty.},
author = {Tversky, A and Kahneman, D},
doi = {10.1126/science.185.4157.1124},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tversky, Kahneman - 1974 - Judgment under Uncertainty Heuristics and Biases.pdf:pdf},
issn = {0036-8075},
journal = {Science},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {4157},
pages = {1124--31},
pmid = {17835457},
title = {{Judgment under Uncertainty: Heuristics and Biases.}},
url = {http://www.sciencemag.org/content/185/4157/1124},
volume = {185},
year = {1974}
}
@misc{Paige2004,
abstract = {Metamodels precisely define the constructs and underlying well-formedness rules for modelling languages. They are vital for tool vendors, who aim to provide support so that concrete models can be checked formally and automatically against a metamodel for conformance. This paper describes how an executable metamodel – which supports fully automated conformance checking – was developed using a model-driven extension of test-driven development. The advantages and disadvantages of this approach to building metamodels are discussed.},
annote = {Specification-driven development of an executable metamodel in Eiffel},
author = {Paige, R and Brooke, P and Ostroff, J},
booktitle = {In Proc. Workshop in Software Model Engineering 2004, co-located with UML},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Specification-driven development of an executable metamodel in Eiffel}},
url = {citeulike-article-id:3934745 {\#}},
year = {2004}
}
@article{4908965,
abstract = {A novel data hiding scheme, denoted as unseen visible watermarking (UVW), is proposed. In UVW schemes, hidden information can be embedded covertly and then directly extracted using the human visual system as long as appropriate operations (e.g., gamma correction provided by almost all display devices or changes in viewing angles relative to LCD monitors) are performed. UVW eliminates the requirement of invisible watermarking that specific watermark extractors must be deployed to the receiving end in advance, and it can be integrated with 2-D barcodes to transmit machine-readable information that conventional visible watermarking schemes fail to deliver. We also adopt visual cryptographic techniques to guard the security of hidden information and, at the same time, increase the practical value of visual cryptography. Since UVW can be alternatively viewed as a mechanism for visualizing patterns hidden with least-significant-bit embedding, its security against statistical steganalysis is proved by empirical tests. Limitations and other potential extensions of UVW are also addressed.},
author = {Huang, Chun-Hsiang and Chuang, Shang-Chih and Huang, Yen-Lin and Wu, Ja-Ling},
doi = {10.1109/TIFS.2009.2020778},
issn = {1556-6013},
journal = {Information Forensics and Security, IEEE Transactions on},
keywords = {2D barcodes,auxiliary information delivery,human v},
month = {jun},
number = {2},
pages = {193--206},
title = {{Unseen Visible Watermarking: A Novel Methodology for Auxiliary Information Delivery via Visual Contents}},
volume = {4},
year = {2009}
}
@inproceedings{Tsai2009,
address = {New York, New York, USA},
author = {Tsai, Janice Y. and Kelley, Patrick and Drielsma, Paul and Cranor, Lorrie Faith and Hong, Jason and Sadeh, Norman},
booktitle = {Proceedings of the 27th international conference on Human factors in computing systems - CHI 09},
doi = {10.1145/1518701.1519005},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsai et al. - 2009 - Who's viewed you.pdf:pdf},
isbn = {9781605582467},
keywords = {agile,context-awareness,information disclosure,mobile location sharing technology,mobile social,nsf,privacy},
mendeley-tags = {agile,nsf},
month = {apr},
pages = {2003},
publisher = {ACM Press},
title = {{Who's viewed you?}},
url = {http://dl.acm.org/citation.cfm?id=1518701.1519005},
year = {2009}
}
@article{Huang2009,
abstract = {The Test First (TF) programming, which is based on an iterative process of "setting up test cases, implementing the functionality, and having all test cases passed", has been put forward for decades, however knowledge of the evidence of the Test First programming's success is limited. This paper describes a controlled experiment that investigated the distinctions between the effectiveness of Test First and that of Test Last (TL) (the traditional approach). The experimental results showed that Test First teams spent a larger percentage of time on testing. The achievable minimum external quality of delivered software applications increased with the percentage of time spent on testing regardless of the testing strategy (TF or TL) applied, although there does not exist a linear correlation between them. With four years' data, it is also found that a strong linear correlation between the amount of effort spent on testing and coding in Test First teams, while this phenomenon was not observed in Test Last teams. {\^{A}}{\textcopyright} 2008 Elsevier B.V. All rights reserved.},
annote = {Empirical investigation towards the effectiveness of Test First programming},
author = {Huang, L and Holcombe, M},
journal = {Information and Software Technology},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {182--194},
title = {{Empirical investigation towards the effectiveness of Test First programming}},
url = {citeulike-article-id:3934645 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-56649121999{\&}{\#}38 partnerID=40},
volume = {51},
year = {2009}
}
@inproceedings{Zingaro2018,
abstract = {To be effective instructors and CS education researchers, we must identify and understand student difficulties surrounding core computing topics. This study examines student difficulties with the basic data structures commonly found in CS2 courses. Initial exploration of student thinking began with think-aloud interviews with students. These interviews centered on open-ended questions that were iteratively improved upon based on analysis of interview transcripts. The revised open-ended questions were then posed to 249 students during an end-of-term final exam study session. Using the explanations and justifications included by students, responses to the questions were coded and summarized. This work characterizes the difficulties revealed by student responses, and provides details of their prevalence among the examined student population.},
address = {Espoo, Finland},
annote = {This work aims to further our understanding of student difficul-
ties regarding Basic Data Structures, including ArrayLists, singly- and doubly-linked lists, and binary search trees. We began by re- cruiting students to participate in think-aloud interviews about Basic Data Structures problems. The interview results informed our authoring of a series of questions aimed to elicit student difficulties. To gather a larger data set, we then presented the questions to 249 students during a final exam study session. The students responded with an answer and a justification for that answer, which we coded to identify common difficulties. We explore these difficulties here.},
author = {Zingaro, Daniel and Taylor, Cynthia and Porter, Leo and Clancy, Michael and Lee, Cynthia and Liao, Soohyun Nam and Webb, Kevin C.},
booktitle = {ICER 2018 - Proceedings of the 2018 ACM Conference on International Computing Education Research},
doi = {10.1145/3230977.3231005},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zingaro et al. - 2018 - Identifying student difficulties with basic data structures.pdf:pdf},
isbn = {9781450356282},
keywords = {CS2,Data structures,Difficulties},
pages = {169--177},
publisher = {ACM Press},
title = {{Identifying student difficulties with basic data structures}},
url = {http://dl.acm.org/citation.cfm?doid=3230977.3231005},
year = {2018}
}
@article{Basili1994f,
abstract = {The Software Engineering Laboratory of the National Aeronautics and Space Administration's Goddard Space Flight Center has been adapting, analyzing, and evolving software processes for the last 18 years (1976-94). Their approach is based on the Quality Improvement Paradigm, which is used to evaluate process effects on both product and people. The authors explain this approach as it was applied to reduce defects in code. In examining and adapting reading techniques, we go through a systematic process of evaluating the candidate process and refining its implementation through lessons learned from previous experiments and studies. As a result of this continuous, evolutionary process, we determined that we could successfully apply key elements of the cleanroom development method in the SEL environment, especially for projects involving fewer than 50000 lines of code (all references to lines of code refer to developed, not delivered, lines of code). We saw indications of lower error rates, higher productivity, a more complete and consistent set of code comments, and a redistribution of developer effort. Although we have not seen similar reliability and cost gains for larger efforts, we continue to investigate the cleanroom method's effect on them},
author = {Basili, V. and Green, S.},
doi = {10.1109/52.300090},
issn = {07407459},
journal = {IEEE Software},
keywords = {QIP,framework,measurement,quality improvement paradigm},
month = {jul},
number = {4},
pages = {58--66},
title = {{Software process evolution at the SEL}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=300090 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=300090},
volume = {11},
year = {1994}
}
@misc{Belli2005,
abstract = {Testing is the most common validation method in the software industry. It entails the execution of the software system in the real environment. Nevertheless, testing is a cost-intensive process. Because of its conceptual simplicity the combination of formal methods and test methods has been widely advocated. Model checking belongs to the promising candidates for this marriage. The present paper modifies and extends the existing approaches in that, after the test case generation, a model checking step supports the manual test process. Based on the holistic approach to specification-based construction of test suites, this paper proposes to generate test cases to cover both the specification model and its complement. This helps also to clearly differentiate the correct system outputs from the faulty ones as the test cases based on the specification are to succeed the test, and the ones based on the complement of the specification are to fail. Thus, the approach handles the oracle problem in an effective manner. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {A holistic approach to test-driven model checking},
author = {Belli, F and G{\"{u}}ldali, B},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {321--331},
title = {{A holistic approach to test-driven model checking}},
url = {citeulike-article-id:3934554 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26944468638{\&}{\#}38 partnerID=40},
volume = {3533 LNAI},
year = {2005}
}
@article{Griffiths2004,
abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. {\&} Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying "hot topics" by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
author = {Griffiths, Thomas L and Steyvers, Mark},
doi = {10.1073/pnas.0307752101},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Griffiths, Steyvers - 2004 - Finding scientific topics.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Databases, Factual,Documentation,Factual,Models, Statistical,Monte Carlo Method,National Academy of Sciences (U.S.),Probability,Publishing,Science,Science: classification,Statistical,United States},
month = {apr},
number = {Supplement 1},
pages = {5228--35},
pmid = {14872004},
title = {{Finding scientific topics.}},
url = {http://www.pnas.org/content/101/suppl{\_}1/5228.short},
volume = {101},
year = {2004}
}
@inproceedings{Saurer2006a,
abstract = {Today's business climate requires you to constantly evolve IT strategies responding to new opportunities or threats. While the fundamentals of IT - reliability, availability, security and manageability - are still crucial, rapid results are mandatory for business success. These business challenges can be solved by acting with agility - striking the proper balance between the introduction of leading-edge technology and the pragmatic application of IT. In this paper, we introduce a testing framework for business solutions dealing with complex and dynamic IT environments. Our framework supports test-driven development which facilitates an incremental construction of reliable business solutions that can be adapted to changes of a business environment easily. We compare our testing framework with model-driven development approaches and show how we applied our framework to an event-driven process management platform called SARI (Sense And Respond Infrastructure). {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Testing complex business process solutions},
author = {Saurer, G and Schiefer, J and Schatten, A},
booktitle = {Proceedings - First International Conference on Availability, Reliability and Security, ARES 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {593--600},
title = {{Testing complex business process solutions}},
url = {citeulike-article-id:3934780 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33750941010{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@article{StaabR.StuderH.p.SchnurrandY.Sure2001,
author = {{Staab  R. Studer, H. p. Schnurr, and Y. Sure}, S},
keywords = {ontologies,ontology},
number = {1},
pages = {26--34},
title = {{Knowledge Process and Ontologies}},
volume = {16},
year = {2001}
}
@article{Hsieh2007,
author = {Hsieh, Gary and Tang, Karen P. and Low, Wai Yong and Hong, Jason I.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsieh et al. - 2007 - Field deployment of IMBuddy a study of privacy control and feedback mechanisms for contextual IM.pdf:pdf},
isbn = {978-3-540-74852-6},
keywords = {IM,agile,context-aware,contextual instant messaging,nsf,privacy},
mendeley-tags = {agile,nsf},
month = {sep},
pages = {91--108},
publisher = {Springer-Verlag},
title = {{Field deployment of IMBuddy: a study of privacy control and feedback mechanisms for contextual IM}},
url = {http://dl.acm.org/citation.cfm?id=1771592.1771598},
year = {2007}
}
@inproceedings{Mcdaid2008,
abstract = {It is widely documented that the absence of a structured approach to spreadsheet engineering is a key factor in the high level of spreadsheet errors. In this paper we propose and investigate the application of Test-Driven Development to the creation of spreadsheets. Test-Driven Development is an emerging development technique in software engineering that has been shown to result in better quality software code. It has also been shown that this code requires less testing and is easier to maintain. Through a set of case studies we demonstrate that Test-Driven Development can be applied to the development of spreadsheets. We present the detail of these studies preceded by a clear explanation of the technique and its application to spreadsheet engineering. A supporting tool under development by the authors is also documented along with proposed research to determine the effectiveness of the methodology and the associated tool. Copyright 2008 ACM.},
annote = {Test-Driven Development: Can it work for spreadsheets?},
author = {Mcdaid, K and Rust, A and Bishop, B},
booktitle = {Proceedings - International Conference on Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {25--29},
title = {{Test-Driven Development: Can it work for spreadsheets?}},
url = {citeulike-article-id:3934709 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57149119506{\&}{\#}38 partnerID=40},
year = {2008}
}
@article{Ma2006c,
author = {Ma, Kwan-Liu},
isbn = {1-920682-41-4},
month = {jan},
pages = {3--7},
title = {{Cyber security through visualization}},
url = {http://dl.acm.org/citation.cfm?id=1151903.1151904},
year = {2006}
}
@book{wohlin2012,
address = {Berlin Heidelberg},
author = {Wohlin, Claes and Runeson, Per and H{\"{o}}st, Martin and {C. Ohlsson}, Magnus and Regnell, Bj{\"{o}}rn and Wessl{\'{e}}n, Anders},
doi = {https://doi-org.liblink.uncw.edu/10.1007/978-3-642-29044-2},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wohlin et al. - 2012 - Experimentation in Software Engineering.pdf:pdf},
isbn = {978-3-642-29043-5},
publisher = {Springer},
title = {{Experimentation in Software Engineering}},
url = {http://www.mendeley.com/catalog/experimentation-software-engineering-introduction-3/},
year = {2012}
}
@misc{Kazanzides2008b,
abstract = {This paper describes the application of component-based software engineering concepts to the development of a C++ software library for multi-threaded robot control. The components in this design are tasks; each task encapsulates a thread and contains a state table and message queues for efficient thread-safe data exchange. A task can provide multiple interfaces, which is useful when the underlying hardware offers diverse capabilities. Communication between tasks is loosely coupled and relies on command objects that can implement both commands and events. The system is dynamically configurable because all components are self-describing. This enables an effective Interactive Research Environment (IRE), which provides a Python shell to configure and interact with the executing software. This library is part of the open source cisst software package for computer-assisted surgery systems and can be obtained from www.cisst.org/cisst.},
author = {Kazanzides, P and Deguet, A and Kapoor, A},
booktitle = {2008 IEEE International Conference on Technologies for Practical Robot Applications},
doi = {10.1109/TEPRA.2008.4686679},
isbn = {9781424427918},
pages = {89--93},
title = {{An architecture for safe and efficient multi-threaded robot software}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4686679},
year = {2008}
}
@article{VanDerAalst2012,
author = {{Van Der Aalst}, Wil},
doi = {10.1145/2240236.2240257},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Van Der Aalst - 2012 - Process mining.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {aug},
number = {8},
pages = {76--83},
title = {{Process mining}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=2240257{\&}type=html},
volume = {55},
year = {2012}
}
@article{Potts1993,
author = {Potts, C},
keywords = {industray-as-laboratory},
pages = {19--28},
title = {{Software Engineering Research Revisited}},
year = {1993}
}
@inproceedings{Sanford2014,
abstract = {In this paper we present an initial study of how metaphors are used by university-level Computer Science instructors. The goal of this research is to gain a better understanding of the role that metaphors play in Computer Science education, to catalog the kinds of metaphors that are used, and to assess their effectiveness in supporting learning. We interviewed 10 educators in Computer Science about the metaphors they have used in the classroom, with a focus on introductory "CS1" programming courses. We analyze these interviews with an existing theory of metaphors, which provides a framework for describing their structure and features. The theory predicts that most metaphors have limitations, and eventually fall apart. Therefore, we also asked educators to assess how far they could push their metaphors with and to describe what happens at the breaking point. Our preliminary findings provide a foundation to inform and guide more in-depth analyses in the future.},
address = {Atlanta, GA, USA},
author = {Sanford, Joseph P. and Tietz, Aaron and Farooq, Saad and Guyer, Samuel and Shapiro, R. Benjamin},
booktitle = {SIGCSE 2014 - Proceedings of the 45th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2538862.2538945},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanford et al. - 2014 - Metaphors we teach by.pdf:pdf},
keywords = {CS1,Metaphors,Pedagogical content knowledge,Teaching},
pages = {585--590},
publisher = {Association for Computing Machinery},
title = {{Metaphors we teach by}},
url = {http://dl.acm.org/citation.cfm?doid=2538862.2538945},
year = {2014}
}
@article{Felder2002,
author = {Felder, R M and Felder, G N and Dietz, E J},
number = {1},
pages = {3--17},
title = {{The Effects of Personality Type on Engineering Student Performance and Attitudes}},
volume = {91},
year = {2002}
}
@article{Boehm2003a,
author = {Boehm, B and Turner, R},
number = {6},
pages = {57--66},
title = {{Using Risk to Balance Agile and Plan-Driven Methods}},
volume = {36},
year = {2003}
}
@article{Treisman1992,
author = {Treisman, U},
number = {5},
pages = {362--372},
title = {{Studying students studying calculus: A look at the lives of minority mathematics students in college.}},
volume = {23},
year = {1992}
}
@article{George2002,
abstract = {Software industry is increasingly becoming more demanding on development schedules and resources. Often, software production deals with ever-changing requirements and with development cycles measured in weeks or months. To respond to these demands and still produce high quality software, over years, software practitioners have developed a number of strategies. One of the more recent one is Test Driven Development (TDD). This is an emerging object-oriented development practice that purports to aid in producing high quality software quickly. TDD has been popularized through the Extreme Programming (XP) methodology. TDD proponents profess that, for small to midsize software, the technique leads to quicker development of higher quality code. Anecdotal evidence supports this. However, until now there has been little quantitative empirical support for this TDD claim. The work presented in this thesis is concerned with a set of structured TDD experiments on very small programs with pair programmers. Programmers were both students and professionals. In each programmer category (students and professionals), one group used TDD and the other (control group) a waterfall-like software development approach. The experiments provide some interesting observations regarding TDD. When TDD was used, both student and professional TDD developers appear to achieve higher code quality, as measured using functional black box testing. The TDD student pairs passed 16{\%} more test cases while TDD professional pair passed 18{\%} more test cases than the their corresponding control groups. However, professional TDD developer pairs did spent about 16{\%} more time on development. It was not established whether the increase in the quality was due to extra development time, or due to the TDD development process itself. On the other hand, the student experiments were time-limited. Both the TDD and the non-TDD student programmers had to complete the assignment in 75 minutes. Professional programmers took about 285 minutes on the average, to complete the same assignment. Consequently, the development cycle of the student-developed software was severely constrained and the resulting code was underdeveloped and of much poorer quality than the professional code. Still, it is interesting to note that the code developed using the TDD approach under these severe restrictions appears to be less faulty than the one developed using the more classical waterfall-like approach. It is conjectured that this may be due to the granularity of the TDD process, one to two test cases per feedback loop, which may encourage more frequent and tighter verification and validation episodes. These tighter cycles may result in a code that is better when compared to that developed by a coarser granularity waterfall-like model. As part of the study, a survey was conducted of the participating programmers. The majority of the programmers thought that TDD was an effective approach, which improved their productivity.},
annote = {Analysis and Quantification of Test Driven Development Approach},
author = {George, B},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Analysis and Quantification of Test Driven Development Approach}},
url = {citeulike-article-id:3934618 http://www.lib.ncsu.edu/theses/available/etd-08202002-095244/unrestricted/etd.pdf},
year = {2002}
}
@article{Yang2015,
author = {Yang, Ming and Kiang, Melody and Shang, Wei},
doi = {10.1016/J.JBI.2015.01.011},
issn = {1532-0464},
journal = {Journal of Biomedical Informatics},
keywords = {twitter},
mendeley-tags = {twitter},
month = {apr},
pages = {230--240},
publisher = {Elsevier},
title = {{Filtering big data from social media – Building an early warning system for adverse drug reactions}},
url = {http://www.sciencedirect.com/science/article/pii/S1532046415000131},
volume = {54},
year = {2015}
}
@article{Grieskamp2002,
address = {New York, New York, USA},
author = {Grieskamp, Wolfgang and Gurevich, Yuri and Schulte, Wolfram and Veanes, Margus},
doi = {10.1145/566189.566190},
isbn = {1581135629},
issn = {01635948},
journal = {Proceedings of the international symposium on Software testing and analysis - ISSTA '02},
keywords = {abstract state machine,asm,executable specification,finite state machine,fsm,generation,test case},
pages = {112},
publisher = {ACM Press},
title = {{Generating finite state machines from abstract state machines}},
url = {http://portal.acm.org/citation.cfm?doid=566172.566190},
year = {2002}
}
@article{Cohen1979,
abstract = {In this paper we present a "routine activity approach" for analyzing crime rate trends and cycles. Rather than emphasizing the characteristics of offenders, with this approach we concentrate upon the circumstances in which they carry out predatory criminal acts. Most criminal acts require convergence in space and time of likely offenders, suitable targets and the absence of capable guardians against crime. Human ecological theory facilitates an investigation into the way in which social structure produces this convergence, hence allowing illegal activities to feed upon the legal activities of everyday life. In particular, we hypothesize that the dispersion of activities away from households and families increases the opportunity for crime and thus generates higher crime rates. A variety of data is presented in support of the hypothesis, which helps explain crime rate trends in the United States 1947-1974 as a byproduct of changes in such variables as labor force participation and single-adult households.},
author = {Cohen, Lawrence E. and Felson, Marcus},
issn = {00031224},
journal = {American Sociological Review},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {4},
pages = {588--608},
publisher = {American Sociological Association},
title = {{Social Change and Crime Rate Trends: A Routine Activity Approach}},
url = {http://www.jstor.org/stable/2094589},
volume = {44},
year = {1979}
}
@article{Glazer2001,
author = {Glazer, Hillel},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Glazer - 2001 - Dispelling the process myth Having a process does not mean sacrificing agility or creativity.pdf:pdf},
journal = {Crosstalk},
keywords = {agile},
mendeley-tags = {agile},
pages = {27--30},
title = {{Dispelling the process myth: Having a process does not mean sacrificing agility or creativity}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Dispelling+the+Process+Myth+:+Having+a+Process+Does+Not+Mean+Sacrificing+Agility+or+Creativity{\#}0},
year = {2001}
}
@inproceedings{Becker1996,
address = {Niagara on the Lake, ON, Canada},
author = {Becker, James C. and Flick, Glenn},
booktitle = {IEEE High-Assurance Systems Engineering Workshop, 1996. Proceedings.},
pages = {228--236},
title = {{A practical approach to failure mode, effects and criticalityanalysis (FMECA) for computing systems}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:A+Practical+approach+to+failure+mode,+effects+and+criticality+analysis+(FMECA)+for+computing+systems{\#}0},
year = {1996}
}
@inproceedings{5438041,
abstract = {Security modeling is an important part of software security, especially when it comes to making security knowledge more easily accessible. The purpose of this paper is to give an overview of some of the current approaches to graphical security modeling and present an initial study related to benefits of tool support.Our working hypothesis is that specialized security modeling tools will substantially outperform more general, prevailing tools, and we have sought indications of evidence for this claim. The study consisted of the following steps; (1) Investigate state-of-the-art security modeling formalisms and tools, (2) Select a security modeling formalism for further analysis and implement dedicated tool support for it, (3) Perform testing related to usability and performance aspects, comparing the tool to a general purpose drawing/modeling tool, and (4) Compare and analyze the results. The study included ten test subjects with a similar background and education, and we got clear indications that our hypothesis is valid.},
author = {Baadshaug, E T and Erdogan, G and Meland, P H},
booktitle = {Availability, Reliability, and Security, 2010. ARES '10 International Conference on},
doi = {10.1109/ARES.2010.11},
keywords = {graphical security modeling,security modeling form},
pages = {537--542},
title = {{Security Modeling and Tool Support Advantages}},
year = {2010}
}
@inproceedings{5484753,
abstract = {Object detecting and tracking at a distance has been a big problem in the research field of wide area security. This paper introduces a real time active vision system which can track a moving object from 1 meter to 200 meters. This visual servo system is mainly structured of three mechanical freedoms of pan, tilt and zoom. An eagle's eye mechanism is proposed to obtain a wide field of view (FOV) in high resolution. A fusion schemes that combines the results of the three separate zoom cameras for detecting and tracking a moving object at distance is also developed. A high performance pan-tilt unit and an algorithm of high frequency motor control is proposed to smooth the tracking. This system has the tracking and zooming functions to monitor an unexpected object at a long distance. The camera can zoom in to capture the high resolution detail of a pedestrian and record a series of available images while tracking. In order to demonstrate the effectiveness of the approaches adopted, an experiment of tracking a pedestrian from a distance of 100 {\#}x2013;200 meters has been carried out. Experimental results show that the proposed system is valid for tracking a pedestrian at a distance.},
author = {Gao, Yan and Zhang, XiaoLin},
booktitle = {Intelligence and Security Informatics (ISI), 2010 IEEE International Conference on},
doi = {10.1109/ISI.2010.5484753},
month = {may},
pages = {162--164},
title = {{Real time tracking of a remote moving object by active zoom cameras}},
year = {2010}
}
@article{Ohba2005,
author = {Ohba, Masaru and Gondow, Katsuhiko},
doi = {10.1145/1082983.1083151},
isbn = {1-59593-123-6},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {TF/IDF,concept keywords,identifiers,program understanding},
month = {jul},
number = {4},
pages = {1},
publisher = {ACM},
title = {{Toward mining "concept keywords" from identifiers in large software projects}},
url = {http://dl.acm.org/citation.cfm?id=1082983.1083151},
volume = {30},
year = {2005}
}
@incollection{Clancy2004,
address = {London, UK},
author = {Clancy, Michael},
booktitle = {Computer Science Education Research},
editor = {Fincher, Sally and Petre, Marian},
pages = {85--100},
publisher = {Taylor {\&} Francis Group},
title = {{Misconceptions and attitudes that interfere with learning to program}},
year = {2004}
}
@inproceedings{6123137,
abstract = {We introduce a novel approach to cross camera people counting that can adapt itself to a new environment without the need of manual inspection. The proposed counting model is composed of a pair of collaborative Gaussian processes (GP), which are respectively designed to count people by taking the visible and occluded parts into account. While the first GP exploits multiple visual features to result in better accuracy, the second GP instead investigates the conflicts among these features to recover the underestimate caused by occlusions. Our contributions are threefold. First, we establish a cross camera people counting system that can facilitate forensics investigation and security preservation. Second, a principled way is proposed to estimate the degree of occlusions. Third, our system is comprehensively evaluated on two benchmark datasets. The promising performance demonstrates the effectiveness of our system.},
author = {Lin, Tsung-Yi and Lin, Yen-Yu and Weng, Ming-Fang and Wang, Yu-Chiang and Hsu, Yu-Feng and Liao, H.-Y.M.},
booktitle = {Information Forensics and Security (WIFS), 2011 IEEE International Workshop on},
doi = {10.1109/WIFS.2011.6123137},
keywords = {automatic video surveillance systems,collaborative},
pages = {1--6},
title = {{Cross camera people counting with perspective estimation and occlusion handling}},
year = {2011}
}
@inproceedings{Downs2010a,
address = {New York, New York, USA},
author = {Downs, Julie S. and Holbrook, Mandy B. and Sheng, Steve and Cranor, Lorrie Faith},
booktitle = {Proceedings of the 28th international conference on Human factors in computing systems - CHI '10},
doi = {10.1145/1753326.1753688},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Downs et al. - 2010 - Are your participants gaming the system.pdf:pdf},
isbn = {9781605589299},
keywords = {crowdsourcing,mechanical turk,screening,survey},
month = {apr},
pages = {2399--2402},
publisher = {ACM Press},
title = {{Are your participants gaming the system?}},
url = {http://dl.acm.org/citation.cfm?id=1753326.1753688},
year = {2010}
}
@book{jacobson1999unified,
author = {Jacobson, Ivar and Booch, Grady and Rumbaugh, James},
publisher = {Addison-Wesley Reading},
title = {{The unified software development process}},
year = {1999}
}
@inproceedings{DeSouza2003a,
address = {Sanibel Island, FL},
author = {de Souza, Cleidson R.B. and Redmiles, David and Dourish, Paul},
booktitle = {Proceedings of the 2003 international ACM SIGGROUP conference on Supporting group work},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Souza, Redmiles, Dourish - 2003 - Breaking the code, moving between private and public work in collaborative software development.pdf:pdf},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {105--114},
publisher = {ACM},
title = {{Breaking the code, moving between private and public work in collaborative software development}},
url = {http://portal.acm.org/citation.cfm?id=958160.958177{\&}type=series},
year = {2003}
}
@inproceedings{367307,
abstract = {Fielding secure computer systems requires tradeoffs between functionality, flexibility, and security to meet the users' needs. Multilevel secure (MLS) computer systems provide better control over classified information than traditional systems and allow users from a diverse population access to information they need while protecting sensitive data. Users want the functionality of non-MLS computer systems; graphical user interfaces, a rich assortment of software, and electronic connectivity with other systems. Compartmented mode workstations (CMW) can provide such an environment. An overview of secure system architectures and an example MLS network provide the framework for discussing the risks associated with interconnecting MLS systems and unclassified networks, and approaches for mitigating those risks. A secure Email gateway, using a high-assurance (AI) network component, provides the necessary safeguards for protecting the MLS network from external attacks},
author = {Smith, R.E.},
booktitle = {Tenth Annual Computer Security Applications Conference},
doi = {10.1109/CSAC.1994.367307},
isbn = {0-8186-6795-8},
keywords = {MLS network,RCAS external interface,classified inf},
month = {dec},
pages = {202--211},
publisher = {IEEE Comput. Soc. Press},
title = {{A secure Email gateway (building an RCAS external interface)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=367307},
year = {1994}
}
@book{Strauss1998,
address = {Thousand Oaks, CA},
author = {Strauss, Anselm L and Corbin, Juliet M},
edition = {Second},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Sage Publications},
title = {{Basics of Qualitative Research: Techniques and Procedures of Developing Grounded Theory}},
year = {1998}
}
@article{Lutz1997,
author = {Lutz, Robyn R. and Woodhouse, Robert M.},
journal = {Annals of Software Engineering},
number = {1},
pages = {459--475},
publisher = {Springer},
title = {{Requirements analysis using forward and backward search}},
url = {http://www.springerlink.com/index/XJ687117MJ56U0U5.pdf},
volume = {3},
year = {1997}
}
@misc{Christensen2008,
abstract = {Software of high quality is a major concern in teaching programming: simply making any program that fulfills the requirements is not enough. Yet the way teachers often state exercises tends to make the students focus more on functionality requirements and deadlines than on keeping the program quality high. This chapter discusses some concrete teaching guidelines that help in keeping the learning focus on quality and reports on our experiences in applying them. It furthermore presents an important observation relating to the use of test-driven development as a process that focus on high quality, namely that it tend to improve also the flexibility and reusability of the production code. This issue is presented and argued by a concrete development example. {\^{A}}{\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
annote = {Experiences with a focus on testing in teaching},
author = {Christensen, H B},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {147--165},
title = {{Experiences with a focus on testing in teaching}},
url = {citeulike-article-id:3934575 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-50949127319{\&}{\#}38 partnerID=40},
volume = {4821 LNCS},
year = {2008}
}
@inproceedings{5386493,
abstract = {In this paper, we propose a secure watermarking scheme based on spread transform dither modulation (STDM) method for digital cinema. The embedding is performed in the JPEG2000 decoding pipeline after the de-quantization and prior to the inverse discrete wavelet transform (IDWT). We exploit the wavelet properties related to the human visual system (HVS) in order to have a trade-off between fidelity and robustness, while preserving security. We design a pixel-wise masking vector that modulates the spreading vector in such a way that preserves its security. Our results show that the proposed method is robust against traditional image processing attacks. The proposed scheme can also survive the camecording attack, a pre-processing step is done in the detection for this end.},
author = {Darazi, Rony and Callau, Pilar and Macq, Benoit},
booktitle = {2009 First IEEE International Workshop on Information Forensics and Security (WIFS)},
doi = {10.1109/WIFS.2009.5386493},
isbn = {978-1-4244-5279-8},
keywords = {JPEG2000 decoding pipeline,adaptive exhibition spr},
month = {dec},
pages = {1--5},
publisher = {Ieee},
title = {{Secure and HVS-adaptive exhibition Spread Transform Dither Modulation watermarking for Digital Cinema}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5386493},
year = {2009}
}
@inproceedings{Menzies2008,
abstract = {In mission critical systems, such as those developed by NASA, it is very important that the test engineers properly recognize the severity of each issue they identify during testing. Proper severity assessment is essential for appropriate resource allocation and planning for fixing activities and additional testing. Severity assessment is strongly influenced by the experience of the test engineers and by the time they spend on each issue. The paper presents a new and automated method named SEVERIS (severity issue assessment), which assists the test engineer in assigning severity levels to defect reports. SEVERIS is based on standard text mining and machine learning techniques applied to existing sets of defect reports. A case study on using SEVERIS with data from NASApsilas Project and Issue Tracking System (PITS) is presented in the paper. The case study results indicate that SEVERIS is a good predictor for issue severity levels, while it is easy to use and efficient.},
address = {Beijing, China},
author = {Menzies, Tim and Marcus, Andrian},
booktitle = {2008 IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.2008.4658083},
isbn = {978-1-4244-2613-3},
issn = {1063-6773},
keywords = {Automatic testing,Computer bugs,Computer science,Costs,NASA,Personnel,Robots,Software testing,System testing,Text mining,automated severity assessment,data mining,defect reports,learning (artificial intelligence),machine learning,machine learning techniques,mission critical systems,resource allocation,severity issue assessment,software defect reports,software engineering,text mining,triage},
language = {English},
mendeley-tags = {defect reports,machine learning,triage},
month = {sep},
pages = {346--355},
publisher = {IEEE},
title = {{Automated severity assessment of software defect reports}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4658083 http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4658083},
year = {2008}
}
@inproceedings{Muller2003,
abstract = {Prom a project economics point of view, the most important practices of Extreme Programming (XP) are Pair Programming and Test-Driven Development. Pair Programming leads to a large increase in the personnel cost, and Test-Driven Development adds to the development effort. On the other hand, Pair Programming can speed the project up; both Pair Programming and Test-Driven Development can reduce the defect density of the code. Can the increased cost of XP be balanced by its shorter time to market and higher code quality? To answer this question, we construct a new model for the business value of software projects. We then analyze the cost and benefit of XP by applying our model to a realistic sample project. We systematically vary important model parameters to provide a sensitivity analysis. Our analysis shows that the economic value of XP strongly depends on how large the XP speed and defect advantage really are. We also find that the market pressure is an important factor when assessing the business value of XP. Our study provides clear guidelines for managers when to consider using XP or better not.},
address = {Helsinki, Finland},
annote = {On the Economic Evaluation of XP Projects},
author = {M{\"{u}}ller, Matthias M and Padberg, Frank},
booktitle = {Proceedings of the Joint European Software Engineering Conference (ESEC) and SIGSOFT Symposium on the Foundations of Software Engineering (FSE-11)},
doi = {10.1145/949952.940094},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/M{\"{u}}ller, Padberg - 2003 - On the Economic Evaluation of XP Projects.pdf:pdf},
issn = {01635948},
keywords = {TDD,agile,file-import-09-01-23},
mendeley-tags = {TDD,agile},
month = {sep},
number = {5},
pages = {168--177},
publisher = {ACM},
title = {{On the Economic Evaluation of XP Projects}},
url = {citeulike-article-id:3934729 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-1542317065{\&}{\#}38 partnerID=40 http://portal.acm.org/citation.cfm?id=940071.940094},
volume = {28},
year = {2003}
}
@inproceedings{Lundh2002,
address = {Essen, Germany},
annote = {Pretty worhtless paper. Another XP success story.  Doesn{\&}{\#}039;t really address requirements.},
author = {Lundh, E and Sandberg, M},
keywords = {XP,requirements},
title = {{Time Constrained Requirements Eingineering with Extreme Programming -- An Experience Report}},
year = {2002}
}
@article{Evanco,
author = {Evanco, William M.},
doi = {10.1111/j.1540-6210.2009.02105.x},
journal = {IEEE Transactions on Software Engineering},
number = {7},
pages = {670--672},
title = {{Comments on "The confounding effect of class size on the validity of object-oriented metrics"}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Comments+on+?The+Confounding+Effect+of+Class+Size+on+the+Validity+of+Object-Oriented+Metrics?{\#}0},
volume = {29}
}
@inproceedings{4908300,
abstract = {With the multiplication of attacks on computer networks, system administrators need to monitor carefully the networks. But all the techniques or tools that they use still heavily rely on human detection. In this paper a visual interactive network connection system called NetViewer is designed in 3D view for representing traffic activities that reside in network flows and their patterns. The experiments show that NetViewer can not only monitor all the activities in the network, but also detect DDoS attacks, network scans and port scans etc.},
author = {Jiawan, Zhang and Peng, Yang and Liangfu, Lu and Lei, Chen},
booktitle = {Networks Security, Wireless Communications and Trusted Computing, 2009. NSWCTC '09. International Conference on},
doi = {10.1109/NSWCTC.2009.353},
keywords = {DDoS attacks,NetViewer,computer networks,network f},
month = {apr},
pages = {434--437},
title = {{NetViewer: A Visualization Tool for Network Security Events}},
volume = {1},
year = {2009}
}
@article{Jackson1997,
author = {Jackson, M},
keywords = {requirements,semantics,verification},
pages = {5--21},
title = {{The Meaning of Requirements}},
volume = {3},
year = {1997}
}
@article{Wilson1982,
author = {Wilson, James Q. and Kelling, George L.},
journal = {Atlantic Monthly},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {3},
pages = {29--38},
title = {{Broken windows}},
volume = {249},
year = {1982}
}
@inproceedings{Mckinney2007,
abstract = {The research on teaching and learning over the past 50 years suggests that the early use of collaborative learning leads to higher interest, higher retention, and higher academic performance in students. Early use of these techniques can also increase the sense of belonging for students and can lead to the early development of collaborative skills to prepare students for team experiences in subsequent courses and future careers. During the weekly lab sessions of a second semester introduction to programming course students engaged in collaborative learning experiences through team-based problem solving, project planning, pair programming, and other agile software development practices. Course objectives provided specific goals and criteria for assessment relative to these skills. The assessment in the authors' prior work identified several problem areas which led to specific initiatives to address those problems: (a) instructor-chosen teams, (b) early instruction and reflection on team skills, (c) feedback on team performance, and (d) the use of an IDE that incorporates an automated test-driven development tool. This paper describes the implementation and assessment of these efforts. A significant increase in student team skills from the middle of the semester to the end of the semester was observed. Copyright 2006 ACM.},
annote = {Developing collaborative skills early in the CS curriculum in a laboratory environment},
author = {Mckinney, D and Denton, L F},
booktitle = {Proceedings of the Thirty-Seventh SIGCSE Technical Symposium on Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {138--142},
title = {{Developing collaborative skills early in the CS curriculum in a laboratory environment}},
url = {citeulike-article-id:3934712 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-37849028708{\&}{\#}38 partnerID=40},
year = {2007}
}
@misc{Robinson,
author = {Robinson, H.},
title = {{Model-based testing}},
url = {http://www.geocities.com/model{\_}based{\_}testing/}
}
@inproceedings{Ramakrishnan2014,
address = {New York, New York, USA},
author = {Ramakrishnan, Naren and Korkmaz, Gizem and Kuhlman, Chris and Marathe, Achla and Zhao, Liang and Hua, Ting and Chen, Feng and Lu, Chang Tien and Huang, Bert and Srinivasan, Aravind and Trinh, Khoa and Butler, Patrick and Getoor, Lise and Katz, Graham and Doyle, Andy and Ackermann, Chris and Zavorin, Ilya and Ford, Jim and Summers, Kristen and Fayed, Youssef and Arredondo, Jaime and Gupta, Dipak and Muthiah, Sathappan and Mares, David and Self, Nathan and Khandpur, Rupinder and Saraf, Parang and Wang, Wei and Cadena, Jose and Vullikanti, Anil},
booktitle = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '14},
doi = {10.1145/2623330.2623373},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ramakrishnan et al. - 2014 - 'Beating the news' with EMBERS.pdf:pdf},
isbn = {9781450329569},
keywords = {civil unrest,event forecasting,open source indicators,twitter},
mendeley-tags = {twitter},
pages = {1799--1808},
title = {{'Beating the news' with EMBERS}},
url = {http://dl.acm.org/citation.cfm?doid=2623330.2623373},
year = {2014}
}
@inproceedings{Elmqvist2006,
address = {New York, New York, USA},
author = {Elmqvist, Niklas and Tsigas, Philippas},
booktitle = {Proceedings of the 2006 ACM symposium on Software visualization - SoftVis '06},
doi = {10.1145/1148493.1148538},
isbn = {1595934642},
keywords = {security visualization,trust visualization},
month = {sep},
pages = {189},
publisher = {ACM Press},
title = {{TrustNeighborhoods in a nutshell}},
url = {http://dl.acm.org/citation.cfm?id=1148493.1148538},
year = {2006}
}
@inproceedings{Kongsli2007,
abstract = {Selenium is a tool for creating and running automated web tests and is a good fit for agile projects where it can be used for creating acceptance tests corresponding to the web application's user stories. This demonstration will show how Selenium additionally can be leveraged to create security tests. First, we model security threats as misuse stories, similar to user stories except that we focus on illegal or non-normative use of the application. Subsequently, we create security tests in Selenium that manifest the misuse stories by exploiting vulnerabilities in the application. This approach can be seen as a contribution to strengthening the security focus in agile projects by trying to apply familiar agile concepts, methods, and tools to the security aspects of the application. We have found that several of the most common security vulnerabilities in web applications can be addressed with this approach, such as cross site scripting (XSS), broken authentication and access management, information leakage, and improper error handling. This demonstration will show examples of such vulnerabilities and corresponding tests, in addition to discussing the cases where there are shortcomings.},
annote = {Security testing with selenium},
author = {Kongsli, V},
booktitle = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {862--863},
title = {{Security testing with selenium}},
url = {citeulike-article-id:3934680 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42149185683{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Destefanis2016,
abstract = {A successful software project is the result of a complex process involving, above all, people. Developers are the key factors for the success of a software development process, not merely as executors of tasks, but as protagonists and core of the whole development process. This paper investigates social aspects among developers working on software projects developed with the support of Agile tools. We studied 22 open-source software projects developed using the Agile board of the JIRA repository. All comments committed by developers involved in the projects were analyzed and we explored whether the politeness of comments affected the number of developers involved and the time required to fix any given issue. Our results showed that the level of politeness in the communication process among developers does have an effect on the time required to fix issues and, in the majority of the analysed projects, it had a positive correlation with attractiveness of the project to both active and potential developers. The more polite developers were, the less time it took to fix an issue.},
author = {Destefanis, Giuseppe and Ortu, Marco and Counsell, Steve and Swift, Stephen and Marchesi, Michele and Tonelli, Roberto},
doi = {10.7717/peerj-cs.73},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Destefanis et al. - 2016 - Software development do good manners matter.pdf:pdf},
issn = {2376-5992},
journal = {PeerJ Computer Science},
keywords = {Issue fixing time,Mining software repositories,Politeness,Social and human aspects,Software development,sentiment},
mendeley-tags = {sentiment},
month = {jul},
pages = {e73},
publisher = {PeerJ Inc.},
title = {{Software development: do good manners matter?}},
url = {https://peerj.com/articles/cs-73},
volume = {2},
year = {2016}
}
@article{Damian2000,
author = {Damian, D and Eberlein, A and Shaw, M},
keywords = {negotiation,requirements},
number = {3},
pages = {28--35},
title = {{Using Different Communication Media in Requirements Negotiation}},
volume = {17},
year = {2000}
}
@inproceedings{Schneider1978,
abstract = {The first course in the overwhelming majority of Computer Science Departments is an introductory course in high-level language programming [1]. Because of this a number of papers have appeared, in the SIGCSE Bulletin and elsewhere, describing details of the introductory programming course at a number of schools. These papers are useful in comparing one ' s own approach with that of other schools and possibly learning from them. [2,3 ].},
author = {Schneider, G. Michael},
booktitle = {Papers of the SIGCSE/CSA Technical Symposium on Computer Science Education, SIGCSE 1978},
doi = {10.1145/990555.990598},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schneider - 1978 - The introductory programming course in computer science-ten principles.pdf:pdf},
month = {feb},
pages = {107--114},
publisher = {Association for Computing Machinery, Inc},
title = {{The introductory programming course in computer science-ten principles}},
year = {1978}
}
@article{Kindberg2005,
author = {Kindberg, T. and Spasojevic, M. and Fleck, R. and Sellen, A.},
doi = {10.1109/MPRV.2005.42},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kindberg et al. - 2005 - The Ubiquitous Camera An In-Depth Study of Camera Phone Use.pdf:pdf},
issn = {1536-1268},
journal = {IEEE Pervasive Computing},
keywords = {Camera phone,MMS,Multimedia Messaging Systems,agile,nsf,taxonomy,ubiquitous camera},
mendeley-tags = {agile,nsf},
month = {apr},
number = {2},
pages = {42--50},
publisher = {IEEE Educational Activities Department},
title = {{The Ubiquitous Camera: An In-Depth Study of Camera Phone Use}},
url = {http://dl.acm.org/citation.cfm?id=1070601.1070627},
volume = {4},
year = {2005}
}
@article{Ostrand2002,
author = {Ostrand, Thomas J. and Weyuker, Elaine J.},
doi = {10.1145/566171.566181},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ostrand, Weyuker - 2002 - The distribution of faults in a large industrial software system.pdf:pdf},
isbn = {1-58113-562-9},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {empirical study,fault-prone,pareto,software faults,software testing},
month = {jul},
number = {4},
pages = {55--64},
publisher = {ACM},
title = {{The distribution of faults in a large industrial software system}},
url = {http://dl.acm.org/citation.cfm?id=566171.566181},
volume = {27},
year = {2002}
}
@article{Lejeune2006,
abstract = {Extreme Programming (XP), one of many models for software development, has challenged some traditional software engineering practices while taking others to the extreme. The controversial practices raise questions about the role of XP in teaching undergraduate software engineering courses, especially capstone project courses. Can an XP model for software development be successfully used in teaching software engineering practices? Specific educational goals for the course are that 1) students recognize the strengths and weaknesses of XP as a model for software development and 2) that students evaluate XP practices in the larger context of other software engineering practices. Students should also complete a software development project while following a defined set of processes and practices.Students in a 4 credit, one-semester software engineering practices course used XP for their project. A survey of fifteen practices representing both XP and more traditional software engineering practices was given to students upon completion of the course to determine the students' perceptions and experiences.Overall, students were successful in using XP for their projects. Students liked pair programming, automated testing, and test-first and believed it contributed to their success. The XP practices of simple design and refactoring did not work well. XP's practices for embracing change accommodates changing requirements, design, and code but do not manage change well for the overall goals of the product. Survey results and anecdotal evidence suggests that students were aware of the strengths and weaknesses of XP and its relationship to more traditional software engineering practices. AD  -, USA},
annote = {Teaching software engineering practices with Extreme Programming},
author = {Lejeune, Noel},
isbn = {1937-4771},
journal = {J. Comput. Small Coll.},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {107--117},
title = {{Teaching software engineering practices with Extreme Programming}},
url = {citeulike-article-id:3934690 http://portal.acm.org/citation.cfm?id=1089182.1089196},
volume = {21},
year = {2006}
}
@inproceedings{Ferreira2008,
abstract = {The high rate of systems development (SD) failure is often attributed to the complexity of traditional SD methodologies (e.g. Waterfall) and their inability to cope with changes brought about by today's dynamic and evolving business environment. Agile methodologies (AM) have emerged to challenge traditional SD and overcome their limitations. Yet empirical research into AM is sparse. This paper develops and tests a research model that hypothesizes the effects of five characteristics of agile systems development (iterative development; continuous integration; test-driven design; feedback; and collective ownership) on two dependent stakeholder satisfaction measures, namely stakeholder satisfaction with the development process and with the development outcome. An empirical study of 59 South African development projects (using self reported data) provided support for all hypothesized relationships and generally supports the efficacy of AM. Iteration and integration together with collective ownership have the strongest effects on the dependent satisfaction measures.},
address = {New York, NY, USA},
annote = {Agile systems development and stakeholder satisfaction: a South African empirical study},
author = {Ferreira, Carlos and Cohen, Jason},
booktitle = {SAICSIT '08: Proceedings of the 2008 annual research conference of the South African Institute of Computer Scientists and Information Technologists on IT research in developing countries},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {48--55},
publisher = {ACM},
title = {{Agile systems development and stakeholder satisfaction: a South African empirical study}},
url = {citeulike-article-id:3934604 http://dx.doi.org/10.1145/1456659.1456666},
year = {2008}
}
@misc{Oblinger2005,
address = {Boulder, CO},
author = {Oblinger, Diana and Oblinger, James and Oblinger, Diana G and Oblinger, James L},
publisher = {Educause},
title = {{Is It Age or IT: First Steps Toward Understanding the Net Generation}},
year = {2005}
}
@inproceedings{Layman2013,
abstract = {We know surprisingly little about how professional developers define debugging and the challenges they face in industrial environments. To begin exploring professional debugging challenges and needs, we conducted and analyzed interviews with 15 professional software engineers at Microsoft. The goals of this study are: 1) to understand how professional developers currently use information and tools to debug; 2) to identify new challenges in debugging in contemporary software development domains (web services, multithreaded/multicore programming); and 3) to identify the improvements in debugging support desired by these professionals that are needed from research. The interviews were coded to identify the most common information resources, techniques, challenges, and needs for debugging as articulated by the developers. The study reveals several debugging challenges faced by professionals, including: 1) the interaction of hypothesis instrumentationand software environment as a source of debugging difficulty; 2) the impact of log file information on accurate debugging of web services; and 3) the mismatch between the sequential human thought process and the non-sequential execution of multithreaded environments as source of difficulty. The interviewees also describe desired improvements to tools to support debugging, many of which have been discussed in research but not transitioned to practice.},
address = {Baltimore, Maryland, USA},
author = {Layman, Lucas and Diep, Madeline and Nagappan, Meiyappan and Singer, Janice and DeLine, Robert and Venolia, Gina},
booktitle = {2013 ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2013.43},
isbn = {978-0-7695-5056-5},
issn = {1938-6451},
keywords = {debugging,interview,microsoft,mypubs,professionals,program comprehension,qualitative analysis,software engineering},
mendeley-tags = {debugging,interview,microsoft,mypubs},
month = {oct},
pages = {383--392},
publisher = {IEEE},
shorttitle = {Empirical Software Engineering and Measurement, 20},
title = {{Debugging Revisited: Toward Understanding the Debugging Needs of Contemporary Software Developers}},
year = {2013}
}
@article{Oulasvirta2011,
author = {Oulasvirta, Antti and Rattenbury, Tye and Ma, Lingyi and Raita, Eeva},
doi = {10.1007/s00779-011-0412-2},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jun},
number = {1},
pages = {105--114},
title = {{Habits make smartphone use more pervasive}},
url = {http://link.springer.com/10.1007/s00779-011-0412-2},
volume = {16},
year = {2011}
}
@inproceedings{Dagenais2007,
abstract = {The benefits of Test-Driven Development (TDD) can be appealing to many seeking to reduce the amount of time spent on a software project, but not all projects can implement the TDD methodology fully. A technique similar to TDD - test oriented development is introduced and a prototype software implementation in the form of an Eclipse plug-in is presented to support the adoption of this more pragmatic approach by developers to help them write high quality software. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {TODD: Test-oriented development and debugging},
author = {Dagenais, O and Deugo, D},
booktitle = {Proceedings - SERA 2007: Fifth ACIS International Conference on Software Engineering Research, Management, and Applications},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {839--846},
title = {{TODD: Test-oriented development and debugging}},
url = {citeulike-article-id:3934580 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38649107030{\&}{\#}38 partnerID=40},
year = {2007}
}
@misc{Feathers2001,
abstract = {Let's say that you are a test case. One of the things that you can do is pass yourself to the objects you are testing so that you can get more information.},
annote = {The "Self-Shunt" Unit Testing Pattern},
author = {Feathers, M},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{The "Self-Shunt" Unit Testing Pattern}},
url = {citeulike-article-id:3934602 {\#}},
year = {2001}
}
@inproceedings{Porter2014,
abstract = {Recent research suggests that the rst weeks of a CS1 course have a strong in uence on end-of-course student performance. The present work aims to re ne the understanding of this phenomenon by using in-class clicker questions as a source of student performance. Clicker questions generate per-lecture and per-question data with which to assess student under- standing. This work demonstrates that clicker question per- formance early in the term predicts student outcomes at the end of the term. The predictive nature of these questions applies to code-writing questions, multiple choice questions, and the nal exam as a whole. The most predictive clicker questions are identi ed and the relationships between these questions and nal exam performance are examined.},
address = {Glasgow, Scotland},
author = {Porter, Leo and Zingaro, Daniel and Lister, Raymond},
booktitle = {ICER 2014 - Proceedings of the 10th Annual International Conference on International Computing Education Research},
doi = {10.1145/2632320.2632354},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Porter, Zingaro, Lister - 2014 - Predicting student success using fine grain clicker data.pdf:pdf},
isbn = {9781450327558},
keywords = {Assessment,CS1,Clickers},
mendeley-tags = {CS1},
pages = {51--58},
publisher = {ACM Press},
title = {{Predicting student success using fine grain clicker data}},
url = {http://dl.acm.org/citation.cfm?doid=2632320.2632354},
year = {2014}
}
@book{Schwaber2001,
address = {Englewood Cliffs, NJ},
author = {Schwaber, K and Beedle, M},
publisher = {Prentice Hall},
title = {{Agile Software Development with Scrum}},
year = {2001}
}
@article{Nelson1996,
author = {Nelson, Craig E},
number = {2},
pages = {165--175},
title = {{Student Diversity Requires Different Approaches to College Teaching, Even in Math and Science}},
volume = {40},
year = {1996}
}
@inproceedings{Benenson2013,
address = {New York, New York, USA},
author = {Benenson, Zinaida and Gassmann, Freya and Reinfelder, Lena},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems on - CHI EA '13},
doi = {10.1145/2468356.2468502},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benenson, Gassmann, Reinfelder - 2013 - Android and iOS users' differences concerning security and privacy.pdf:pdf},
isbn = {9781450319522},
keywords = {agile,android,ios,iphone,nsf,personal data,privacy awareness,security awareness,smartphone},
mendeley-tags = {agile,nsf},
month = {apr},
pages = {817},
publisher = {ACM Press},
title = {{Android and iOS users' differences concerning security and privacy}},
url = {http://dl.acm.org/citation.cfm?id=2468356.2468502},
year = {2013}
}
@inproceedings{Sato2008,
abstract = {A Coding Dojo is a meeting where a group of programmers gets together to learn, practice, and share experiences. This report describes the authors' experience of creating and running an active Coding Dojo in Sa?o Paulo, Brazil, sharing the lessons learned from the experience. The role of the Dojo in the learning process is discussed, showing how it creates an environment for fostering and sharing Agile practices such as Test-Driven Development, Refactoring and Pair Programming, among others. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Coding Dojo: An environment for learning and sharing Agile practices},
author = {Sato, D and Corbucci, H and Bravo, M},
booktitle = {Proceedings - Agile 2008 Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {459--464},
title = {{Coding Dojo: An environment for learning and sharing Agile practices}},
url = {citeulike-article-id:3934778 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-52949083255{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{1532062,
abstract = { This paper presents the intrusion detection toolkit (IDtk), an information visualization tool for intrusion detection (ID). IDtk was developed through a user-centered design process, in which we identified design guidelines to support ID users. ID analysts protect their networks by searching for evidence of attacks in ID system output, firewall and system logs, and other complex, textual data sources. Monitoring and analyzing these sources incurs a heavy cognitive load for analysts. The use of information visualization techniques offers a valuable addition to the toolkit of the ID analyst. Several visualization techniques for ID have been developed, but few usability or field studies have been completed to assess the needs of ID analysts and the usability and usefulness of these tools. We intended to fill this gap by applying a user-centered design process in the development and evaluation of IDtk, a 3D, glyph-based visualization tool that gives the user maximum flexibility in setting up how the visualization display represents ID data. The user can also customize whether the display is a simple, high-level overview to support monitoring, or a more complex 3D view allowing for viewing the data from multiple angles and thus supporting analysis and diagnosis. This flexibility was found crucial in our usability evaluation. In addition to describing the tool, we report the findings of our user evaluation and propose new guidelines for the design of information visualization tools for ID.},
author = {Komlodi, A and Rheingans, P and Ayachit, Utkarsha and Goodall, J R and Joshi, Amit},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532062},
keywords = {glyph-based security visualization,information vi},
pages = {21--28},
title = {{A user-centered look at glyph-based security visualization}},
year = {2005}
}
@inproceedings{1254333,
abstract = { The emergence of online games has fundamentally changed security requirements for computer games, which previously were largely concerned with copy protection. We examine how new security requirements impact the design of online games by using online bridge, a simple client-server game, as our case study. We argue that security is emerging as an inherent design issue for online games, after graphics and artificial intelligence, which have become important issues of the design of most games for decades. The most important new security concern in online game design is fairness enforcement, and most security mechanisms all contribute to a single objective, namely, making the play fair for each user.},
author = {Yan, J},
booktitle = {Computer Security Applications Conference, 2003. Proceedings. 19th Annual},
doi = {10.1109/CSAC.2003.1254333},
keywords = {client-server game,computer games,copy protectio},
pages = {286--295},
title = {{Security design in online games}},
year = {2003}
}
@book{Lanza2006,
author = {Lanza, Michele and Marinescu, Radu},
pages = {207},
publisher = {Springer Verlag},
title = {{Object-Oriented Metrics in Practice: Using Software Metrics to Characterize, Evaluate, and Improve the Design of Object-Oriented Systems}},
year = {2006}
}
@inproceedings{Salo2004,
address = {Kansai Science City, Japan},
author = {Salo, Outi and Abrahamsson, Pekka},
booktitle = {Product Focused Software Process Improvement (PROFES)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Salo, Abrahamsson - 2004 - Empirical evaluation of agile software development The controlled case study approach.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
pages = {408--423},
publisher = {Springer},
title = {{Empirical evaluation of agile software development: The controlled case study approach}},
url = {http://www.springerlink.com/index/ql4cwua10aghv6pc.pdf},
year = {2004}
}
@article{Jiang2008,
author = {Jiang, Weihang and Hu, Chongfeng and Zhou, Yuanyuan and Kanevsky, Arkady},
journal = {The USENIX Magazine},
keywords = {2008,disk,faulttolerance,storage},
number = {3},
pages = {22--31},
title = {{Don't Blame Disks for Every Storage Subsystem Failure}},
volume = {33},
year = {2008}
}
@inproceedings{Breitman2002,
address = {Essen, Germany},
annote = {Talks about how to track requirements change in XP and how to track evolving requirements. They introduce a traceability framework, but probably underestimate the overhead associated with it. No validation.},
author = {Breitman, K and Leite, J},
keywords = {XP,requirements,user stories},
title = {{Managing User Stories}},
year = {2002}
}
@inproceedings{saw2008,
address = {Midas Journal},
author = {Vagvolgyi, Balazs and DiMaio, Simon and Deguet, Anton and Kazanzides, Peter and Kumar, Rajesh and Hasser, Chris and Taylor, Russell},
booktitle = {MICCAI Workshop on Systems and Arch. for Computer Assisted Interventions},
month = {sep},
title = {{The Surgical Assistant Workstation: a software framework for telesurgical robotics research}},
url = {http://hdl.handle.net/10380/1466},
year = {2008}
}
@misc{Chengyao2007,
abstract = {We conducted a survey on Executable Acceptance Test Driven Development (or: Story Test Driven Development). The results show that there is often a substantial delay between defining an acceptance test and its first successful pass. Therefore, it becomes important for teams to easily be able to distinguish between tasks that were never tackled before and tasks that were already completed but whose tests are now failing again. We then describe our FitClipse tool that extends Fit by maintaining a history of acceptance test results. Based on the history, FitClipse is able to generate reports that show when an acceptance test is suddenly failing again. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {FitClipse: A fit-based eclipse plug-in for executable acceptance test driven development},
author = {Chengyao, D and Wilson, P and Maurer, F},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {93--100},
title = {{FitClipse: A fit-based eclipse plug-in for executable acceptance test driven development}},
url = {citeulike-article-id:3934573 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38149052332{\&}{\#}38 partnerID=40},
volume = {4536 LNCS},
year = {2007}
}
@inproceedings{6107851,
abstract = {Cyber security threats require rapid identification of imminent or potential attacks to support deterrence and protection. Thomas Friedman theorized that technology has leveled or {\#}x201C;flattened {\#}x201D; the global playing field that once existed. This flattening has happened as a result of what he calls the {\#}x201C;triple convergence {\#}x201D; of platform, process and people. However, there is a general lack of understanding about how to describe, assess the complex and dynamic nature of cyber security related information to improve overall cyber security task performance. Ever since the first computer virus traversed the Internet it has been apparent that attacks can spread rapidly across national boundaries. This paper presents research that utilize bioinformatics techniques, to forecast cyber security attacks by identifying emerging threats based on analysis of computer infection, and incursions models based on human disease models.},
author = {Walker, J J and Jones, T and Blount, R},
booktitle = {Technologies for Homeland Security (HST), 2011 IEEE International Conference on},
doi = {10.1109/THS.2011.6107851},
keywords = {Internet,attack modeling,attack predictive analysi},
pages = {81--85},
title = {{Visualization, modeling and predictive analysis of cyber security attacks against cyber infrastructure-oriented systems}},
year = {2011}
}
@inproceedings{Williams2003,
abstract = {Test-driven development is a software development practice that has been used sporadically for decades. With this practice, test cases (preferably automated) are incrementally written before production code is implemented. Test-driven development has recently re-emerged as a critical enabling practice of the Extreme Programming software development methodology. We ran a case study of this practice at IBM. In the process, a thorough suite of automated test cases was produced after UML design. In this case study, we found that the code developed using a test-driven development practice showed, during functional verification and regression tests, approximately 40{\%} fewer defects than a baseline prior product developed in a more traditional fashion. The productivity of the team was not impacted by the additional focus on producing automated test cases. This test suite will aid in future enhancements and maintenance of this code. The case study and the results are discussed in detail.},
address = {Denver, CO},
annote = {From Duplicate 1 (Test-driven development as a defect-reduction practice - Williams, L; Maximilien, E M; Vouk, M)

Test-driven development as a defect-reduction practice},
author = {{Williams  E. M. Maximilien, and M. Vouk}, L and Rosenberg, Linda and Cukic, Bojan and Williams, L and Maximilien, E M and Vouk, M},
booktitle = {In Proceedings of the 14th IEEE International Symposium on Software Reliability Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {34--45},
publisher = {IEEE Computer Society},
title = {{Test-driven development as a defect-reduction practice}},
url = {citeulike-article-id:3934839 {\#}},
year = {2003}
}
@inproceedings{6128313,
abstract = {The visualization of explosion fields is an important issue in explosion science and technology. This paper presents a streamline-based method to visualize 3D explosion fields. Given the velocity data in a 3D explosion field, is surfaces of the magnitude of velocity are firstly extracted. Seeds of streamlines are then uniformly distributed on the is surfaces. And streamlines are generated with the placed seeds. By this way, the density of streamlines is controlled by a predefined value. When the is surfaces and the density value are properly set, the coverage of streamlines can be sufficient. To reduce visual cluttering, a technique of line illumination is used to render streamlines. Moreover, a color coding technique is applied to visualize the magnitude of velocity and distinguish the distribution of media in 3D explosion fields. Test results show that the method proposed in this paper can achieve effective visualization of 3D explosion fields. The visual clarity and the spatial perception are improved in the final visualized images.},
author = {Wang, Yi and Zhang, Wenyao and Ning, JianGuo},
booktitle = {Computational Intelligence and Security (CIS), 2011 Seventh International Conference on},
doi = {10.1109/CIS.2011.271},
keywords = {3D explosion field visualization,color coding tech},
pages = {1224--1228},
title = {{Streamline-based Visualization of 3D Explosion Fields}},
year = {2011}
}
@article{Steinberg2001,
abstract = {When using unit testing in an XP study group, we noticed that none of the groups introduced a main()method until it was time to deploy the application. In teaching programming in the Java programming language, there are many advantages to not introducing main()until later. In this paper we look at some of the reasons for not introducing main(). We then examine some of the related advantages of using unit tests in an introductory course.},
annote = {The Effect of Unit Tests on Entry Points, Coupling and Cohesion in an Introductory Java Programming Course},
author = {Steinberg, D H},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{The Effect of Unit Tests on Entry Points, Coupling and Cohesion in an Introductory Java Programming Course}},
url = {citeulike-article-id:3934807 {\#}},
year = {2001}
}
@article{Grun2011,
author = {Gr{\"{u}}n, Bettina and Hornik, Kurt},
journal = {Journal of Statistical Software},
number = {13},
pages = {1--30},
title = {{topicmodels: An R Package for Fitting Topic Models}},
volume = {40},
year = {2011}
}
@inproceedings{Curtis1984,
address = {Orlando, Florida},
annote = {Individual differences between programmers and the structures that underly them.

"to measure mental abilities, a task must be devised which exercises the theoretical mental construct. The critical factors for this approach are: 1) a clear definition of the metnal construct, 2) a carefully developed performance scale, and 3) a scientifically sound validation of both the construct and the scale."

Short term memory as an impediment to developing large scale, complex systems. 7+-2.

Short term memory is a major impediement to developing large scale systems because of limited resources. "Chunking" can help. Programming maturation is in part due to how much scope they can "chunk" at once. 

Program design can be broken down into a hierarchical structure of propositions. More experienced programmers can recall more propositions at a greater depth in the hierarchy than novices. More experienced programmers have more elaborate structures in long-term memory for encoding.

Decomposition of design is relative to the depth fo the knowledge base which a design is being compared to.

Client/analyst requirements sessions were broken into cycles which represented the decomposition of the problem. However, these cycles did not decompose in a top-down fashion, but were linear or sequential where the sub-problem to be attacked in the next cycle was cued by the results of the last cycle.

People do not transfer (design) solution structures across problem isomorphs.

Temporal problems are more difficult to solve than spatial problems in part because they do not transalte easily into a graphical representation.

Requirements that are clustered close to their inherent structure are easier to decompose into design.},
author = {Curtis, Bill},
booktitle = {7th international conference on Software engineering},
keywords = {psychology},
mendeley-tags = {psychology},
pages = {97--106},
title = {{Fifteen Years of Psychology in Software Engineering: Individual Differences and Cognitive Science}},
url = {http://dl.acm.org/citation.cfm?id=801956},
year = {1984}
}
@inproceedings{Alles2006,
abstract = {Presenter First (PF) is a technique for organizing source code and development activities to produce fully tested GUI applications from customer stories using test-driven development. The three elements of Presenter First are a strategy for how applications are developed and tested, a variant on the Model View Presenter (MVP) design pattern, and a particular means of composing MVP triads. Presenter tests provide an economical alternative to automated GUI system tests. We have used Presenter First on projects ranging in size from several to a hundred MVP triads. This paper describes MVP creation, composition, scaling, and the tools and process we use. An example C{\#} application illustrates the application of the Presenter First technique. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Presenter first: Organizing complex GUI applications for test-driven development},
author = {Alles, M and Crosby, D and Erickson, C and Harleton, B and Marsiglia, M and Pattison, G and Stienstra, C},
booktitle = {Proceedings - AGILE Conference, 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {276--285},
title = {{Presenter first: Organizing complex GUI applications for test-driven development}},
url = {citeulike-article-id:3934534 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34247556653{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@inproceedings{ramzan2007phishing,
author = {Ramzan, Zulfikar and W{\"{u}}est, Candid},
organization = {Citeseer},
title = {{Phishing attacks: Analyzing trends in 2006}}
}
@misc{BangaloreBenchmarkingSpecialInterestGroup2001,
annote = {Contains industry average data for defect information, productivity, etc. for several high profile companies},
author = {{Bangalore Benchmarking Special Interest Group}},
keywords = {averages,benchmark,industry average,industry standard},
publisher = {Bangalore Software Process Improvement Network},
title = {{Benchmarking of Software Engineering Practices at High Maturity Organizations}},
url = {http://www.bangaloreit.com/html/itscbng/Softengcon2001/B2SIG.pdf},
year = {2001}
}
@misc{Layman2010a,
address = {NASA Technical Report 20100031198},
author = {Layman, Lucas and Basili, Victor R. and Zelkowitz, Marvin V.},
keywords = {mypubs},
mendeley-tags = {mypubs},
publisher = {NASA},
title = {{The Role and Quality of Software Safety in the NASA Constellation Program}},
year = {2010}
}
@inproceedings{Basili2007l,
abstract = {GQM+Strategies is a measurement approach that builds on the well-tested GQM approach to planning and implementing software measurement. Although GQM has proven itself useful in a variety of industrial settings, one recognized weakness is the difficulty for GQM users to link software measurement goals to higher-level goals of the organization in which the software is being developed. This linkage is important, as it helps to justify software measurement efforts and allows measurement data to contribute to higher-level decisions. GQM+strategies provides mechanisms for explicitly linking software measurement goals, to higher-level goals for the software organization, and further to goals and strategies at the level of the entire business.},
author = {Basili, Victor and Heidrich, Jens and Lindvall, Mikael and Munch, Jurgen and Regardie, Myrna and Trendowicz, Adam},
booktitle = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
doi = {10.1109/ESEM.2007.66},
isbn = {978-0-7695-2886-1},
issn = {1938-6451},
month = {sep},
pages = {488--490},
publisher = {IEEE},
shorttitle = {Empirical Software Engineering and Measurement, 20},
title = {{GQM+Strategies -- Aligning Business Strategies with Software Measurement}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4343788},
year = {2007}
}
@article{Zhu1997,
author = {Zhu, Hong and Hall, Patrick A. V. and May, John H. R.},
doi = {10.1145/267580.267590},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu, Hall, May - 1997 - Software unit test coverage and adequacy.pdf:pdf},
issn = {03600300},
journal = {ACM Computing Surveys},
keywords = {agile,comparing testing effectiveness,fault detection,nsf,software unit test,test adequacy criteria,test coverage,testing methods},
mendeley-tags = {agile,nsf,test coverage},
month = {dec},
number = {4},
pages = {366--427},
publisher = {ACM},
title = {{Software unit test coverage and adequacy}},
url = {http://dl.acm.org/citation.cfm?id=267580.267590 http://portal.acm.org/citation.cfm?doid=267580.267590},
volume = {29},
year = {1997}
}
@inproceedings{683160,
abstract = {The authors describe two state reduction techniques for finite-state models of security protocols. The techniques exploit certain protocol properties that they have identified as characteristic of security protocols. They prove the soundness of the techniques by demonstrating that any violation of protocol invariants is preserved in the reduced state graph. In addition, they describe an optimization method for evaluating parameterized rule conditions, which are common in models of security protocols. All three techniques have been implemented in the Mur phi; verifier},
author = {Shmatikov, V and Stern, U},
booktitle = {Computer Security Foundations Workshop, 1998. Proceedings. 11th IEEE},
doi = {10.1109/CSFW.1998.683160},
issn = {1063-6900},
keywords = {Mur phi; verifier;efficient finite-state analysis;},
month = {jun},
pages = {106--115},
title = {{Efficient finite-state analysis for large security protocols}},
year = {1998}
}
@inproceedings{Giger2011,
address = {Honolulu, HI},
author = {Giger, Emanuel and Pinzger, Martin and Gall, Harald C.},
booktitle = {Proceeding of the 8th working conference on Mining software repositories - MSR '11},
doi = {10.1145/1985441.1985456},
isbn = {9781450305747},
keywords = {churn},
mendeley-tags = {churn},
pages = {83},
title = {{Comparing fine-grained source code changes and code churn for bug prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1985441.1985456},
year = {2011}
}
@inproceedings{6107884,
abstract = {In collaboration with the Transportation Security Laboratory, the National Institute of Standards and Technology has been developing a prototype shoe sampling system that relies on aerodynamic sampling for liberating, transporting, and collecting explosive contamination. Here, we focus on the measurement science of aerodynamic sampling with the goal of uncovering the underlying physics of the flow fields within these sampling systems. This paper will cover the results of a series of experiments that were used to help with the design of our prototype shoe sampling system. Laser light-sheet flow visualization revealed the bulk fluid motion inside and around the sampling system. Polymer microsphere particle standards were used to quantify the particle release efficiency of the shoe sampling system. Patches containing a known mass of explosives were also used to determine the effectiveness of particle release in the shoe sampler. Results from these experiments indicate that particle removal efficiency at a specific location is strongly influenced by its distance from an air jet and the type of explosive or material on the surface. The successful application of these flow visualization techniques and other metrology tools has helped us construct the sampling portion of a shoe screening prototype. The hope is that these tools will be useful to others who are developing next-generation aerodynamic sampling technologies.},
author = {Staymates, Matthew and Gillen, Greg and Grandner, Jessica and Lukow, Stefan},
booktitle = {2011 IEEE International Conference on Technologies for Homeland Security (HST)},
doi = {10.1109/THS.2011.6107884},
isbn = {978-1-4577-1376-7},
keywords = {National Institute of Standards and Technology,aer},
month = {nov},
pages = {276--281},
publisher = {Ieee},
title = {{The development of an aerodynamic shoe sampling system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6107884},
year = {2011}
}
@inproceedings{Sillito2006,
address = {Portland, OR},
author = {Sillito, Jonathan and Murphy, Gail C and Volder, Kris De},
booktitle = {Proceedings of the 14th ACM SIGSOFT Int'l Symp on Foundations of Software Engineering},
pages = {23--33},
title = {{Questions programmers ask during software evolution tasks}},
year = {2006}
}
@article{Shin2011,
author = {Shin, Yonghee and Meneely, Andrew and Williams, Laurie and Osborne, Jason A.},
doi = {10.1109/TSE.2010.81},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {churn},
mendeley-tags = {churn},
month = {nov},
number = {6},
pages = {772--787},
title = {{Evaluating Complexity, Code Churn, and Developer Activity Metrics as Indicators of Software Vulnerabilities}},
url = {http://ieeexplore.ieee.org/document/5560680/},
volume = {37},
year = {2011}
}
@inproceedings{Lutz1996,
address = {Colorado Springs, CO},
author = {Lutz, Robyn R. and Woodhouse, Robert M.},
booktitle = {Proceedings of the Second IEEE International Conference on Requirements Engineering},
doi = {10.1145/1411203.1411223},
month = {sep},
number = {9},
pages = {44--51},
publisher = {IEEE Computer Society},
title = {{Experience report: Contributions of SFMEA to requirements analysis}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/ICRE.1996.491428},
volume = {43},
year = {1996}
}
@article{Dai2004,
abstract = {In late 2001, the Object Management Group issued a Request for Proposal to develop a testing profile for UML 2.0. In June 2003, the work on the UML 2.0 Testing Profile was finally adopted by the OMG. Since March 2004, it has become an official standard of the OMG. The UML 2.0 Testing Profile provides support for UML based model-driven testing. This paper introduces a methodology on how to use the testing profile in order to modify and extend an existing UML design model for test issues. The application of the methodology will be explained by applying it to an existing UML Model for a Bluetooth device.},
author = {Dai, Zhen Ru and Grabowski, Jens and Neukirchen, Helmut and Pals, Holger},
doi = {10.1631/jzus.2004.1327},
issn = {1009-3095},
journal = {Journal of Zhejiang University. Science},
keywords = {Algorithms,Computer Communication Networks,Computer Communication Networks: instrumentation,Computer Communication Networks: standards,Computer Peripherals,Computer Peripherals: standards,Equipment Design,Equipment Design: methods,Equipment Design: standards,Equipment Failure Analysis,Equipment Failure Analysis: methods,Equipment Failure Analysis: standards,Germany,Programming Languages,Telecommunications,Telecommunications: instrumentation},
month = {nov},
number = {11},
pages = {1327--35},
pmid = {15495324},
title = {{Model-based testing with UML applied to a roaming algorithm for bluetooth devices.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15495324},
volume = {5},
year = {2004}
}
@article{Sabottke2019,
abstract = {Cyber attackers constantly craft new attacks previously unknown to the security community. There are two approaches for detecting such attacks: (1) employing human analysts who can observe the data and identify anomalies that correspond to malicious intent; and (2) utilizing unsupervised automated techniques, such as clustering, that do not rely on ground truth. We conduct a security analysis of the two approaches, utilizing attacks against a real-world website. Through two experiments—a user study with 65 security analysts and an experimental analysis of attack discovery using DBSCAN clustering—we compare the strategies and features employed by human analysts and clustering system for detecting attacks. Building on these observations, we propose threat models for the human analysis process and for the unsupervised techniques when operating in adversarial settings. Based on our analysis, we propose and evaluate two attacks against the DBSCAN clustering algorithm and a defense. Finally, we discuss the implications of our insights for hybrid systems that utilize the strengths of automation and of human analysis to complement their respective weaknesses.},
author = {Sabottke, Carl and Chen, Daniel and Layman, Lucas and Dumitraş, Tudor},
doi = {10.1016/J.COSE.2018.07.022},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sabottke et al. - 2019 - How to trick the Borg threat models against manual and automated techniques for detecting network attacks.pdf:pdf},
issn = {0167-4048},
journal = {Computers {\&} Security},
month = {mar},
pages = {25--40},
publisher = {Elsevier Advanced Technology},
title = {{How to trick the Borg: threat models against manual and automated techniques for detecting network attacks}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818311283},
volume = {81},
year = {2019}
}
@article{Mead2009,
abstract = {The opportunity to profit from dishonesty evokes a motivational conflict between the temptation to cheat for selfish gain and the desire to act in a socially appropriate manner. Honesty may depend on self-control given that self-control is the capacity that enables people to override antisocial selfish responses in favor of socially desirable responses. Two experiments tested the hypothesis that dishonesty would increase when people's self-control resources were depleted by an initial act of self-control. Depleted participants misrepresented their performance for monetary gain to a greater extent than did non-depleted participants (Experiment 1). Perhaps more troubling, depleted participants were more likely than non-depleted participants to expose themselves to the temptation to cheat, thereby aggravating the effects of depletion on cheating (Experiment 2). Results indicate that dishonesty increases when people's capacity to exert self-control is impaired, and that people may be particularly vulnerable to this effect because they do not predict it.},
author = {Mead, Nicole L. and Baumeister, Roy F. and Gino, Francesca and Schweitzer, Maurice E. and Ariely, Dan},
journal = {Journal of Experimental Social Psychology},
keywords = {Dishonesty,Motivation,Prosocial behavior,Self-control,agile,nsf},
mendeley-tags = {agile,nsf},
number = {3},
pages = {594--597},
title = {{Too tired to tell the truth: Self-control resource depletion and dishonesty}},
url = {http://www.sciencedirect.com/science/article/pii/S0022103109000365},
volume = {45},
year = {2009}
}
@inproceedings{VonWatzdorf2010,
address = {New York, New York, USA},
annote = {Distribution of accuracy measurements across earlier iPhone. Shows a relationship between accuracy and distance from nearest populated area. 
},
author = {von Watzdorf, Stephan and Michahelles, Florian},
booktitle = {Proceedings of the 3rd International Workshop on Location and the Web - LocWeb '10},
doi = {10.1145/1899662.1899664},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/von Watzdorf, Michahelles - 2010 - Accuracy of positioning data on smartphones.pdf:pdf},
isbn = {9781450304122},
keywords = {agile,experiment,location based service,nsf,smartphone},
mendeley-tags = {agile,nsf},
month = {nov},
pages = {1--4},
publisher = {ACM Press},
title = {{Accuracy of positioning data on smartphones}},
url = {http://dl.acm.org/citation.cfm?id=1899662.1899664},
year = {2010}
}
@article{Basarke2007,
abstract = {When a large number of people with heterogeneous knowledge and skills run a project together, it is important to use a sensible engineering process. This especially holds for a project building an intelligent autonomously driving car to participate in the 2007 DARPA Urban Challenge. In this article, we present essential elements of a software and systems engineering process for the development of artificial intelligence capable of driving autonomously in complex urban situations. The process includes agile concepts, like test first approach, continuous integration of every software module and a reliable release and configuration management assisted by software tools in integrated development environments. However, the most important ingredients for an efficient and stringent development are the ability to efficiently test the behavior of the developed system in a flexible and modular simulator for urban situations.},
annote = {Software {\&} systems engineering process and tools for the development of autonomous driving intelligence},
author = {Basarke, C and Berger, C and Rumpe, B},
journal = {Journal of Aerospace Computing, Information and Communication},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {12},
pages = {1158--1174},
title = {{Software {\&} systems engineering process and tools for the development of autonomous driving intelligence}},
url = {citeulike-article-id:3934549 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38549101566{\&}{\#}38 partnerID=40},
volume = {4},
year = {2007}
}
@inproceedings{Enriquez2002,
author = {Enriquez, Patricia and Brown, Aaron and Patterson, David},
booktitle = {Proc.$\backslash$ Workshop on Self-Healing, Adaptive, and Self-Managed Systems},
title = {{Lessons from the {\{}PSTN{\}} for dependable computing �- a study of {\{}FCC{\}} disruption reports}},
year = {2002}
}
@inproceedings{113337,
abstract = {The authors give an overview of how the Ulysses system can be used for security modeling. The default theory of security permits the security analysis of complex designs by decomposing them into their parts. System specifications may be made by using a specialized graphical language interface and a textual interface. In addition, there are a number of support tools which aid the modeler. One of these tools is the natural language component, which allows users to automatically generate short English descriptions of a graphical design. There is also a library component that facilitates the reuse of secure designs. In addition, there are mechanisms for adding security theories and other mathematical facts to augment the power of the theorem prover},
author = {Korelsky, T and Dean, B and Eichenlaub, C and Hook, J and Klapper, C and Lam, M and McCullough, D and Pottinger, G and Rambow, O and Rosenthal, D and Seldin, J P and Weber, D G},
booktitle = {Aerospace Computer Security Applications Conference, 1988., Fourth},
doi = {10.1109/ACSAC.1988.113337},
keywords = {English descriptions,Ulysses system,graphical desi},
month = {dec},
pages = {386--392},
title = {{Security modeling in the Ulysses environment}},
year = {1988}
}
@misc{Williams2003c,
address = {North Carolina State University, Department of Computer Science TR-2003-18},
author = {Williams, Laurie and Krebs, William and Layman, Lucas and Anton, Annie I.},
keywords = {mypubs},
mendeley-tags = {mypubs},
title = {{Toward an XP Evaluation Framework}},
year = {2003}
}
@inproceedings{Naps2002,
abstract = {Visualization technology can be used to graphically Illustrate various concepts in computer science. We argue that such technology, no matter how well it is designed, is of little educational value unless it engages learners in an active learning activity. Drawing on a review of experimental studies of visualization effectiveness, we motivate this position against the backdrop of current attitudes and best practices with respect to visualization use. We suggest a new taxonomy of learner engagement with visualization technology. Grounded in Bloom's well recognized taxonomy of understanding, we suggest metrics for assessing the learning outcomes to which such engagement may lead. Based on these taxonomies of engagement and effectiveness metrics, we present a framework for experimental studies of visualization effectiveness. Interested computer science educators are invited to collaborate with us by carrying out studies within this framework.},
address = {Aarhus, Denmark},
author = {Naps, Thomas L. and Fleischer, Rudolf and McNally, Myles and R{\"{o}}{\ss}ling, Guido and Hundhausen, Chris and Rodger, Susan and Almstrum, Vicki and Korhonen, Ari and Vel{\'{a}}zquez-Iturbide, J. {\'{A}}ngel and Dann, Wanda and Malmi, Lauri},
booktitle = {Proceedings of the Conference on Integrating Technology into Computer Science Education, ITiCSE},
doi = {10.1145/960568.782998},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Naps et al. - 2002 - Exploring the role of visualization and engagement in computer science education.pdf:pdf},
pages = {131--152},
publisher = {ACM Press},
title = {{Exploring the role of visualization and engagement in computer science education}},
url = {http://portal.acm.org/citation.cfm?doid=960568.782998},
year = {2002}
}
@article{Zhou2016,
author = {Zhou, Minghui and Mockus, Audris and Ma, Xiujuan and Zhang, Lu and Mei, Hong},
doi = {10.1145/2876443},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2016 - Inflow and Retention in OSS Communities with Commercial Involvement.pdf:pdf},
issn = {1049331X},
journal = {ACM Transactions on Software Engineering and Methodology},
keywords = {Hybrid project,commercial involvement,contributor inflow,contributor retention,extent and intensity of involvement,natural experiment},
month = {apr},
number = {2},
pages = {1--29},
publisher = {ACM},
title = {{Inflow and Retention in OSS Communities with Commercial Involvement}},
url = {http://dl.acm.org/citation.cfm?doid=2913009.2876443},
volume = {25},
year = {2016}
}
@inproceedings{4751286,
abstract = {Security screening systems based on imaging technology, like X-ray and gamma-ray based screening systems, are widely used in the field of aviation security. The image quality of the systems reflects detection performance directly. In the paper a set of image quality assessment software based-on human visual sensitivity (HVS) has been developed. The software integrated HVS method in the objective method and overcomes the shortage of both objective and subjective method of image processing. Experiments show that the assessment results are consistent with the perception of operators. With the help of the image quality assessment software, the image quality assessment of the security screening system can be finished justly and efficiently, which contributes the safety of aviation transportation.},
author = {Wu, Wei and Liu, Gang and Zhou, Zhenggan and Gu, Zhu},
booktitle = {Security Technology, 2008. ICCST 2008. 42nd Annual IEEE International Carnahan Conference on},
doi = {10.1109/CCST.2008.4751286},
keywords = {human visual sensitivity,image processing,image qu},
pages = {107--111},
title = {{Image quality assessment software of security screening system}},
year = {2008}
}
@inproceedings{6095896,
abstract = {This paper addresses the problem of QoE evaluation of multimedia image sequence streaming generated by IP-based security systems and impaired by coding factors and error-prone communication channel. The image quality assessment, communication channels and their performance are of extreme importance in the field of security imaging systems. In this paper, the authors adopt objective quality assessment metrics, in conjunction with a subjective user's evaluation, to study the perceived quality of image multimedia service generated from IP-based camera security systems and subjected to communication network error conditions - to benchmark a minimal visual object identification level (QoE threshold) on the degraded contents. Network transmission errors are introduced into the generated contents using a simulation tool.},
author = {Ibekwe, M and Klima, M and Soucek, P},
booktitle = {Security Technology (ICCST), 2011 IEEE International Carnahan Conference on},
doi = {10.1109/CCST.2011.6095896},
issn = {1071-6572},
keywords = {IP based camera security system,QoE,QoE evaluation},
pages = {1--5},
title = {{The objective image quality criteria and QoE in the security technology}},
year = {2011}
}
@book{schwaber2004agile,
author = {Schwaber, Ken},
publisher = {Microsoft Press},
title = {{Agile project management with Scrum}},
year = {2004}
}
@article{kocaguneli2012value,
author = {Kocaguneli, Ekrem and Menzies, Tim and Keung, Jacky W},
journal = {Software Engineering, IEEE Transactions on},
number = {6},
pages = {1403--1416},
publisher = {IEEE},
title = {{On the value of ensemble effort estimation}},
volume = {38},
year = {2012}
}
@inproceedings{Mockus2000,
address = {Limerick, Ireland},
author = {Mockus, Audris and Fielding, Roy T. and Herbsleb, James},
booktitle = {Proceedings of the 2000 International Conference on Software Engineering},
doi = {10.1109/ICSE.2000.870417},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mockus, Fielding, Herbsleb - 2000 - A case study of open source software development the Apache server.pdf:pdf},
isbn = {1-58113-206-9},
keywords = {code,collaboration,defect density,open source,ownership,repair interval,software process},
mendeley-tags = {collaboration},
pages = {263--272},
publisher = {ACM},
title = {{A case study of open source software development: the Apache server}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=870417},
year = {2000}
}
@inproceedings{4373507,
abstract = {The detection performance of X-ray security inspection systems can be reflected from the image quality generated by the system. The image quality assessment method proposed in the paper combines human visual system (HVS) with objective method to evaluation the image quality of X-ray system. The assessment process and method simulate the human vision characteristics to separate and analyze regions of interest which correspondent to the technical specification under the Chinese national standard from the entire image. The technical specifications of detection performance of X-ray system include two aspects: image resolution and material differentiation ability which correspondent to different technical specifications. The existence and integrality, the hue and saturation are achieved respectively according to different specification requirements using objective method. The assessment results are achieved compared with the corresponding threshold values obtained by a series of special designed experiments implemented by experienced operators. Simulations and experiments verified the method.},
author = {Wei, Wu},
booktitle = {2007 41st Annual IEEE International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2007.4373507},
isbn = {978-1-4244-1129-0},
keywords = {HVS,X-ray security inspection systems,hue,human vi},
month = {oct},
pages = {320--324},
publisher = {Ieee},
title = {{An Image Quality Assessment Method based on HVS}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4373507},
year = {2007}
}
@inproceedings{Kim2011a,
address = {New York, New York, USA},
author = {Kim, Yoohwan and Sheldon, Frederick},
booktitle = {Proceedings of the Seventh Annual Workshop on Cyber Security and Information Intelligence Research - CSIIRW '11},
doi = {10.1145/2179298.2179386},
isbn = {9781450309455},
month = {oct},
pages = {1},
publisher = {ACM Press},
title = {{Anomaly detection in multiple scale for insider threat analysis}},
url = {http://dl.acm.org/citation.cfm?id=2179298.2179386},
year = {2011}
}
@book{Chrissis2011,
address = {Boston, MA},
author = {Chrissis, Mary Beth and Konrad, Mike and Shrum, Sandra},
edition = {Third Edit},
publisher = {Addison-Wesley Professional},
title = {{CMMI for Development: Guidelines for Process Integration and Product Improvement}},
year = {2011}
}
@inproceedings{Curtis1989,
address = {Pittsburgh, PA},
author = {Curtis, B},
pages = {398--399},
publisher = {ACM},
title = {{Three Problems Overcome with Behavioral Models of the Software Development Process (Panel)}},
year = {1989}
}
@misc{ISO/IEC1996,
annote = {ISO standard on evaluation},
author = {ISO/IEC},
keywords = {evaluation,standard},
title = {{DIS 14598-1 Information Technology - Software Product Evaluation}},
year = {1996}
}
@techreport{Chaudhuri2004,
author = {Chaudhuri, Kamalika and Kothari, Anshul and Swaminathan, Ram and Tarjan, Robert and Zhang, Alex and Zhou, Yunhong},
institution = {HP Labs},
number = {HPL-2004-151},
title = {{Server Allocation Problem for Multi-Tiered Applications}},
year = {2004}
}
@article{Preece2009,
abstract = {With the advent of miniaturized sensing technology, which can be body-worn, it is now possible to collect and store data on different aspects of human movement under the conditions of free living. This technology has the potential to be used in automated activity profiling systems which produce a continuous record of activity patterns over extended periods of time. Such activity profiling systems are dependent on classification algorithms which can effectively interpret body-worn sensor data and identify different activities. This article reviews the different techniques which have been used to classify normal activities and/or identify falls from body-worn sensor data. The review is structured according to the different analytical techniques and illustrates the variety of approaches which have previously been applied in this field. Although significant progress has been made in this important area, there is still significant scope for further work, particularly in the application of advanced classification techniques to problems involving many different activities.},
author = {Preece, Stephen J and Goulermas, John Y and Kenney, Laurence P J and Howard, Dave and Meijer, Kenneth and Crompton, Robin},
doi = {10.1088/0967-3334/30/4/R01},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Preece et al. - 2009 - Activity identification using body-mounted sensors--a review of classification techniques.pdf:pdf},
issn = {0967-3334},
journal = {Physiological Measurement},
keywords = {Biosensing Techniques,Electronics,Humans,Medical,Motor Activity,Motor Activity: physiology,agile,nsf},
language = {en},
mendeley-tags = {agile,nsf},
month = {apr},
number = {4},
pages = {R1--33},
pmid = {19342767},
publisher = {IOP Publishing},
title = {{Activity identification using body-mounted sensors--a review of classification techniques.}},
url = {http://iopscience.iop.org/0967-3334/30/4/R01},
volume = {30},
year = {2009}
}
@inproceedings{Siniaalto2007,
abstract = {Test-driven development (TDD) is a programming technique in which the tests are written prior to the source code. It is proposed that TDD is one of the most fundamental practices enabling the development of software in an agile and iterative manner. Both the literature and practice suggest that TDD practice yields several benefits. Essentially, it is claimed that TDD leads to an improved software design, which has a dramatic impact on the maintainability and further development of the system. The impact of TDD on program design has seldom come under the researchers' focus. This paper reports the results from a comparative case study of three software development projects where the effect of TDD on program design was measured using object-oriented metrics. The results show that the effect of TDD on program design was not as evident as expected, but the test coverage was significantly superior to iterative test-last development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {A comparative case study on the impact of test-driven development on program design and test coverage},
author = {Siniaalto, M and Abrahamsson, P},
booktitle = {Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {275--284},
title = {{A comparative case study on the impact of test-driven development on program design and test coverage}},
url = {citeulike-article-id:3934796 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-47949108130{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Maurer2002,
author = {Maurer, F and Martel, S},
number = {1},
pages = {86--90},
title = {{Extreme Programming: Rapid Development for Web-Based Applications}},
volume = {6},
year = {2002}
}
@inproceedings{Robillard2008,
author = {Robillard, Martin P. and Manggala, Putra},
booktitle = {2008 The 16th IEEE International Conference on Program Comprehension},
doi = {10.1109/ICPC.2008.10},
isbn = {978-0-7695-3176-2},
month = {jun},
number = {C},
pages = {202--211},
publisher = {Ieee},
title = {{Reusing Program Investigation Knowledge for Code Understanding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4556132},
volume = {2008},
year = {2008}
}
@book{DeMarco1999,
address = {New York, NY},
author = {DeMarco, Tom and Lister, Timothy R},
publisher = {Dorset House Publishers},
title = {{Peopleware: Productive Projects and Teams}},
year = {1999}
}
@article{Bellovin1989,
author = {Bellovin, S. M.},
doi = {10.1145/378444.378449},
issn = {01464833},
journal = {ACM SIGCOMM Computer Communication Review},
month = {apr},
number = {2},
pages = {32--48},
publisher = {ACM},
title = {{Security problems in the TCP/IP protocol suite}},
url = {http://dl.acm.org/citation.cfm?id=378444.378449},
volume = {19},
year = {1989}
}
@inproceedings{Geiger2005,
abstract = {In the last years, SCESM community has studied a number of synthesis approaches that turn scenario descriptions into some kind of state machine. In our story driven modeling approach, the statechart synthesis is done manually. Many other approaches rely on human interaction, too. Frequently, the resulting state machines are just the starting point for further system development. The manual steps and the human interaction and the subsequent development steps are subject to the introduction of errors. Thus, it is not guaranteed that the final implementation still covers the initial scenarios. Therefore, this paper proposes the exploitation of scenarios for the derivation of automatic tests. These tests may be used to force the implementation to implement at least the behavior outlined in the requirements scenarios. In addition, this approach raises the value of formal scenarios for requirements elicitation and analysis since such scenarios are turned into automatic tests that may be used to drive iterative development processes according to test-first principles.},
annote = {Story driven testing-SDT},
author = {Geiger, L and Z{\"{u}}ndorf, A},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1--6},
publisher = {ACM New York, NY, USA},
title = {{Story driven testing-SDT}},
url = {citeulike-article-id:3934617 {\#}},
year = {2005}
}
@misc{ReutersStaff2016,
author = {{Reuters Staff}},
booktitle = {Reuters},
keywords = {twitter},
mendeley-tags = {twitter},
title = {{ECB to set up cyber attack warning system for banks}},
url = {http://www.reuters.com/article/ecb-cyber/ecb-to-set-up-cyber-attack-warning-system-for-banks-idUSL5N1896US},
urldate = {2017-10-12},
year = {2016}
}
@article{Freire2008a,
abstract = {The problem of systematically capturing and managing provenance for computational tasks has recently received significant attention because of its relevance to a wide range of domains and applications. The authors give an overview of important concepts related to provenance management, so that potential users can make informed decisions when selecting or designing a provenance solution.},
author = {Freire, Juliana and Koop, David and Santos, Emanuele and Silva, Cl},
issn = {1521-9615},
journal = {Computing in Science {\&} Engineering},
month = {may},
number = {3},
pages = {11--21},
publisher = {IEEE Computer Society},
title = {{Provenance for computational tasks: A survey}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/MCSE.2008.79},
volume = {10},
year = {2008}
}
@article{Paulk2001,
author = {Paulk, Mark C.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Paulk - 2001 - Extreme programming from a CMM perspective.pdf:pdf},
journal = {IEEE software},
keywords = {agile},
mendeley-tags = {agile},
number = {6},
pages = {19--26},
title = {{Extreme Programming from a CMM Perspective}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Extreme+Programming+from+a+CMM+Perspective{\#}0},
volume = {18},
year = {2001}
}
@book{Palmer2002,
address = {Englewood Cliffs, NJ},
author = {Palmer, S R and Felsing, J M},
publisher = {Prentice Hall},
title = {{A Practical Guide to Feature-Driven Development}},
year = {2002}
}
@techreport{Gray1985,
author = {Gray, Jim},
institution = {Tandem Computers},
number = {85.7, PN87614},
title = {{Why do computers stop and what can be done about it?}},
year = {1985}
}
@article{Stern2002,
abstract = {We have developed a framework for pedagogically-oriented animations, designed to help students learn new algorithms. Recursive sorting and searching algorithms pose a particular challenge, as it can be difficult to find visual representations that help students develop a mental model of how the recursion proceeds. Relatively complex representations, such as thumbnail sketches or explicitly showing the function stack along with the data structure are appropriate for some algorithms, while simpler representations suffice for others. We have found it useful to classify recursive algorithms according to the way they navigate through a data structure and manipulate data items within it, sometimes with further subdivision according to the kind of recursion. Within each category there are common strategies for visual representation. While there may be no single, general way to represent recursive algorithms, classification is a useful guide to picking an appropriate strategy when animating recursive algorithms.},
author = {Stern, Linda and Naish, Lee},
doi = {10.1145/563517.563414},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stern, Naish - 2002 - Visual representations for recursive algorithms.pdf:pdf},
issn = {00978418},
journal = {SIGCSE Bulletin (Association for Computing Machinery, Special Interest Group on Computer Science Education)},
month = {mar},
number = {1},
pages = {196--200},
publisher = {Association for Computing Machinery (ACM)},
title = {{Visual representations for recursive algorithms}},
url = {http://portal.acm.org/citation.cfm?doid=563517.563414},
volume = {34},
year = {2002}
}
@article{Heinecke2011,
author = {Heinecke, Andreas and Griebe, Tobias and Gruhn, Volker and Flemig, Holger},
pages = {603--614},
title = {{Business Process-Based Testing of Web Applications}},
year = {2011}
}
@inproceedings{Kersten2005,
address = {Chicago, IL},
author = {Kersten, Mik and Murphy, Gail C},
keywords = {doi,ide,mylar},
pages = {159--168},
title = {{Mylar: a degree-of-interest model for IDEs}},
year = {2005}
}
@inproceedings{Huang1995,
author = {Huang, Yennun and Kintala, Chandra and Kolettis, Nick and Fulton, N Dudley},
booktitle = {Proc.$\backslash$ Twenty-Fifth International Symposium on Fault-Tolerant Computing},
pages = {381--390},
title = {{Software Rejuvenation: Analysis, Module and Applications}},
year = {1995}
}
@inproceedings{Simons2008,
abstract = {JWalk is a lazy systematic unit-testing tool for Java, which supports dynamic inference of specifications from code and systematic testing from the acquired specification. This paper describes the feedback-based development methodology that is possible using the JWalk Editor, an original Java-sensitive editor and compiler coupled to JWalk, which helps programmers to prototype Java class designs, generating novel test cases as they code. Systematic exploratory testing alerts the programmer to unusual consequences in the design; and confirmed test results become part of the evolving specification, which adapts continuously to modified classes and extends to subclasses. The cycle of coding, inferring and testing systematically exposes test cases that are often missed in other test-driven development approaches, which rely on programmer intuition to create test cases. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Feedback-based specification, coding and testing with JWalk},
author = {Simons, A J H and Griffiths, N and Thomson, C},
booktitle = {Proceedings - Testing: Academic and Industrial Conference Practice and Research Techniques, TAIC PART 2008},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {69--73},
title = {{Feedback-based specification, coding and testing with JWalk}},
url = {citeulike-article-id:3934793 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57649183611{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Johnson1994,
address = {Sorrento, Italy},
author = {Johnson, Phillip M.},
booktitle = {Proceedings of the 16th International Conference on Software Engineering},
pages = {113--122},
title = {{An Instrumented Approach Formal to Improving Technical Software Review Quality through Formal Technical Review}},
year = {1994}
}
@article{Bass2000a,
author = {Bass, Tim},
doi = {10.1145/332051.332079},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bass - 2000 - Intrusion detection systems and multisensor data fusion.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {agile,ids,nsf,security},
mendeley-tags = {agile,ids,nsf,security},
month = {apr},
number = {4},
pages = {99--105},
title = {{Intrusion detection systems and multisensor data fusion}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=332079{\&}type=html},
volume = {43},
year = {2000}
}
@article{Zingaro2015,
abstract = {Computer Science 1 (CS1), the first course taken by college-level computer science (CS) majors, has traditionally suffered from high failure rates. Efforts to understand this phenomenon have considered a wide range of predictors of CS success, such as prior programming experience, math ability, learning style, and gender, with findings that are suggestive but inconclusive. The current quasiexperimental study extends this research by exploring how the pedagogical approach of the course (traditional lecture vs. Peer Instruction (PI) and clickers) in combination with student achievement goals (mastery goals vs. performance goals) relates to exam grades, interest in the subject matter, and course enjoyment. The research revealed that students with performance goals scored significantly lower on final exams in both the lecture and PI conditions. However, students with performance goals reported higher levels of subject matter interest when taught through PI. Students with mastery goals, in both conditions, scored significantly higher on the final exam, had higher levels of interest, and reported higher levels of course enjoyment than their performance-oriented counterparts. The results suggest that PI may improve the level of subject-matter interest for some students, thereby indicating the importance of studying pedagogical approach as we seek to understand student outcomes in CS1.},
author = {Zingaro, Daniel},
doi = {10.1145/2802752},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zingaro - 2015 - Examining interest and grades in computer science 1 A study of pedagogy and achievement goals.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {Achievement goals,CS1,CS2,Peer instruction},
mendeley-tags = {CS2},
month = {jul},
number = {3},
pages = {Article 14},
publisher = {Association for Computing Machinery},
title = {{Examining interest and grades in computer science 1: A study of pedagogy and achievement goals}},
volume = {15},
year = {2015}
}
@misc{WilliamsL.LaymanandW.Krebs2004,
address = {North Carolina State University, Department of Computer Science TR-2004-3 (http://goo.gl/9lz5Yb)},
author = {Williams, Laurie and Layman, Lucas and Krebs, William and Anton, Annie I.},
isbn = {TR-2004-3},
keywords = {mypubs},
mendeley-tags = {mypubs},
publisher = {North Carolina State University Department of Computer Science},
title = {{Exploring the Use of a Safe Subset of Extreme Programming: An Industrial Case Study}},
year = {2004}
}
@inproceedings{6107845,
abstract = {Those responsible for the Maritime Security of North America face a daunting task. The breadth of activities in the maritime domain makes it very difficult to achieve an acceptable level of understanding and control. This leaves decision makers to deal with a multi-national challenge that is very much connected to the aerospace, land and cyberspace domains. In order for the decision makers responsible for the Maritime Security and Defense of North America to effectively execute their missions, they require both an understanding of their current capabilities and gaps, and a means to determine the capability requirements to close any identified gaps. This paper presents the Maritime Timeline Analysis and Requirements Toolset (M-TART) as a modeling and simulation solution to address this problem. The M-TART is a deterministic, scenario-based model that accounts for the various states of the maritime event timeline and provides decision makers with statistical and graphical information regarding current capabilities and gaps.},
author = {Carson, Neil},
booktitle = {2011 IEEE International Conference on Technologies for Homeland Security (HST)},
doi = {10.1109/THS.2011.6107845},
isbn = {978-1-4577-1376-7},
keywords = {Maritime Security of North America,aerospace domai},
month = {nov},
pages = {43--48},
publisher = {Ieee},
title = {{Modeling and simulation in support of understanding maritime security and defense capabilities and requirements (The maritime timeline and analysis requirements toolset (M-TART))}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6107845},
year = {2011}
}
@book{Leveson1995,
address = {Boston, MA},
author = {Leveson, Nancy G},
publisher = {Addison-Wesley Professional},
title = {{Safeware: System Safety and Computer}},
year = {1995}
}
@article{Menzies2013,
abstract = {Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.},
author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
doi = {10.1109/TSE.2012.83},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Context,Data mining,Data models,Estimation,Java,Measurement,PROMISE repository,Software,Telecommunications,agile,automated clustering tools,automatic test pattern generation,clustering,data mining,data source,defect dataset,defect prediction,effort estimation,global lessons,learned lesson generated rule,local lessons,mypubs,nsf,pattern clustering},
language = {English},
mendeley-tags = {agile,clustering,data mining,defect prediction,effort estimation,mypubs,nsf},
month = {jun},
number = {6},
pages = {822--834},
publisher = {IEEE},
title = {{Local versus Global Lessons for Defect Prediction and Effort Estimation}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6363444},
volume = {39},
year = {2013}
}
@misc{Binder,
author = {Binder, R.},
title = {{Model-based testing user survey: Results and analysis}},
url = {http://www.robertvbinder.com/docs/arts/MBT-User-Survey.pdf}
}
@misc{Lui2006,
abstract = {This paper proposes an implementation roadmap that shows how in-experienced software teams in industrial developing areas in China can adopt eXtreme Programming (XP) to produce software applications. Inexperienced teams unfamiliar with XP can face difficulties in adopting at once all twelve XP practices in a "big bang" implementation strategy. Intuitively, a step-by-step approach might seem more practical; however, XP practices are heavily intertwined and mutually dependent, creating problems in terms of prioritizing and justifying one instructional sequence over another. We propose a way to technically analyze the complex interrelationships between XP practices by identifying cluster patterns. These patterns can then be used to assist us in sequencing the introduction XP practices, helping both inexperienced teams and classroom learners in using XP. This work has value in both industrial and educational contexts. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {A road map for implementing eXtreme programming},
author = {Lui, K M and Chan, K C C},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {474--481},
title = {{A road map for implementing eXtreme programming}},
url = {citeulike-article-id:3934697 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33745165208{\&}{\#}38 partnerID=40},
volume = {3840 LNCS},
year = {2006}
}
@inproceedings{1377217,
abstract = { This paper discusses VIEWS, a specification for building diagrams that describe the security features of systems. The authors' recent experience with providing security architecture and engineering support to organizations with large, distributed applications suggests that security architecture and assurance efforts could benefit by following other engineering disciplines, where using graphical models is the norm. Security diagrams can help security architects understand a system 's security posture and can assist them in detecting vulnerabilities. Additionally, diagrams facilitate communications about the security features of a design. The output of a modeling effort using VIEWS is a diagram depicting a system's security features as well as those of the environment in which the system operates. A goal of VIEWS is to allow the display of important security features without injecting cluttering detail. This paper presents examples of security diagrams built with VIEWS.},
author = {Brennan, J J and Faatz, D and Rudell, M and Zimmerman, C},
booktitle = {Computer Security Applications Conference, 2004. 20th Annual},
doi = {10.1109/CSAC.2004.49},
issn = {1063-9527},
keywords = {graphical models,security diagrams,visualizing e},
pages = {71--79},
title = {{Visualizing enterprise-wide security (VIEWS)}},
year = {2004}
}
@misc{Goldman2013a,
author = {Goldman, Jeff},
booktitle = {eSecurityPlanet.com},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{UCSF Medical Center Admits Security Breach}},
url = {http://www.esecurityplanet.com/network-security/ucsf-medical-center-admits-security-breach.html},
urldate = {2015-01-01},
year = {2013}
}
@inproceedings{Saff2004,
address = {Boston, MA},
author = {Saff, David and Ernst, Michael D},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saff, Ernst - 2004 - An Experimental Evaluation of Continuous Testing during Development.pdf:pdf},
keywords = {TDD,continuous testing},
mendeley-tags = {TDD},
pages = {76--85},
title = {{An Experimental Evaluation of Continuous Testing during Development}},
year = {2004}
}
@article{Herbsleb1998,
author = {Herbsleb, J D},
pages = {18--20},
title = {{Hard Problems and Hard Science: On the Practical Limits of Experimentation}},
year = {1998}
}
@inproceedings{Ostrand2004,
abstract = {In earlier research we identified characteristics of files in large software systems that tend to make them particularly likely to contain faults. We then developed a statistical model that uses historical fault information and file characteristics to predict which files of a system are likely to contain the largest numbers of faults. Testers can use that information to prioritize their testing and focus their efforts to make the testing process more efficient and the resulting software more dependable. In this paper we describe a proposed new tool to automate this prediction process, and discuss issues involved in its design and implementation. The goal is to produce an automated tool that mines the project defect tracking system and that can be used by testers without requiring any particular statistical expertise or subjective judgements.},
address = {Edinburgh, Scotland},
author = {Ostrand, T.J. and Weyuker, Elaine},
booktitle = {Int'l Wkshp on Mining Software Repositories (MSR 2004)},
doi = {10.1049/ic:20040482},
isbn = {0 86341 432 X},
month = {jan},
pages = {85--89},
title = {{A tool for mining defect-tracking systems to predict fault-prone files}},
volume = {2004},
year = {2004}
}
@article{Wallace2004,
author = {Wallace, Linda and Keil, Mark and Rai, Arun},
doi = {10.1111/j.00117315.2004.02059.x},
issn = {0011-7315},
journal = {Decision Sciences},
month = {may},
number = {2},
pages = {289--321},
title = {{How Software Project Risk Affects Project Performance: An Investigation of the Dimensions of Risk and an Exploratory Model*}},
url = {http://doi.wiley.com/10.1111/j.00117315.2004.02059.x},
volume = {35},
year = {2004}
}
@inproceedings{Yuan2011,
address = {New York, New York, USA},
author = {Yuan, Ding and Zheng, Jing and Park, Soyeon and Zhou, Yuanyuan and Savage, Stefan},
booktitle = {Proceedings of the sixteenth international conference on Architectural support for programming languages and operating systems - ASPLOS '11},
doi = {10.1145/1950365.1950369},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yuan et al. - 2011 - Improving software diagnosability via log enhancement.pdf:pdf},
isbn = {9781450302661},
issn = {0362-1340},
keywords = {log,software diagnosability,static analysis},
month = {mar},
number = {3},
pages = {3},
publisher = {ACM Press},
title = {{Improving software diagnosability via log enhancement}},
url = {http://dl.acm.org/citation.cfm?id=1950365.1950369},
volume = {46},
year = {2011}
}
@article{Heath2002,
author = {Heath, Taliver and Martin, Richard P and Nguyen, Thu D},
issn = {0163-5999},
journal = {SIGMETRICS Perform. Eval. Rev.},
number = {1},
pages = {217--227},
title = {{Improving cluster availability using workstation validation}},
volume = {30},
year = {2002}
}
@inproceedings{Far2007,
abstract = {In software industry there is a common assumption that deployment of software reliability engineering (SRE) contributes to huge overhead in development and its practice does not match the agile software development which puts emphasis on traveling light and generating a minimal amount of project artifacts. However measuring and/or assessing reliability of developed software is nevertheless a very important task and there seems to be a natural fit between SRE and agile in driving certification testing. SRE typically focuses on directing test efforts after software has been created in order to achieve a pre-determined failure intensity objective (FIO). It also uses reliability growth models in order to predict when the software will meet the FIO, and therefore can be released. While these ideas may initially seem incompatible with the test-driven emphasis of agile methods, we show that SRE does in fact have value within the agile process context. {\^{A}}{\textcopyright}2007 IEEE.},
annote = {Software reliability engineering for agile software development},
author = {Far, B},
booktitle = {Canadian Conference on Electrical and Computer Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {694--697},
title = {{Software reliability engineering for agile software development}},
url = {citeulike-article-id:3934601 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-48749112091{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Zimmerman2005,
author = {Zimmerman, Thomas and Weissgerber, Peter and Diehl, Stephan and Zeller, Andreas},
number = {6},
pages = {429--445},
title = {{Mining Version Histories to Guide Software Changes}},
volume = {31},
year = {2005}
}
@article{sutcliffe2007multimedia,
author = {Sutcliffe, Alistair},
journal = {The human-computer interaction handbook: fundamentals, evolving technologies and emerging applications},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {393},
publisher = {CRC Press},
title = {{Multimedia user interface design}},
year = {2007}
}
@inproceedings{5276984,
abstract = {At present, most of the intrusion detection systems (IDS) don't have enough flexibility, intelligentization and maintainability in actual application. Through studying and analyzing the intelligence recognition characteristics of ldquoSoftManrdquo (SM), a novel cooperation model for intrusion detection system based on multiSoftMan alliance (MSMIDS) is proposed to solve these problems. This paper discusses the concepts and various intelligence recognition characteristics in which SoftMan could be applied to the problem of detecting and responding to intrusions. At the same time, the paper visualizes the architecture and system behaviors of the multiSoftMan intrusion detection system. Relevant negotiation control process and cooperation control strategy are presented respectively. The experimental results show that MSMIDS is a more adaptive and efficient system. The purpose of the MSMIDS is to bring intelligent negotiation mechanism based on multiSoftMan alliance into cooperation system so as to improve the performance and cooperation control capability of the systems. In this way, the system will match the requests. MSMIDS also provides a novel way for implementation of network security system.},
author = {Ma, Zhanfei and Zheng, Xuefeng},
booktitle = {Anti-counterfeiting, Security, and Identification in Communication, 2009. ASID 2009. 3rd International Conference on},
doi = {10.1109/ICASID.2009.5276984},
keywords = {adaptive system,coope,cooperation control strategy},
pages = {493--496},
title = {{Cooperation modeling for intrusion detection system based on Multi-SoftMan}},
year = {2009}
}
@article{Capretz2003,
author = {Capretz, L F},
keywords = {mbti,myers-briggs},
number = {2},
pages = {207--214},
title = {{Personality Types in Software Engineering}},
volume = {58},
year = {2003}
}
@incollection{Choi2011,
author = {Choi, Kyung-Shick},
booktitle = {Cyber Criminology: Exploring Internet Crimes and Criminal Behavior},
editor = {Karuppannan, Jaishankar},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {229--253},
publisher = {CRC Press, Taylor and Francis Group},
title = {{Cyber-Routine Activities: Empirical Examination of Online Lifestyle, Digital Guardians, and Computer-Crime Victimization}},
year = {2011}
}
@inproceedings{steck04,
author = {Stecklein, Jonette and Dabney, Jim and Dick, Brandon and Haskins, Bil and Lovell, Randy and Moroney, Gregory},
booktitle = {14th Annual INCOSE International Symposium; 19-24 Jun. 2004; Toulouse; France},
title = {{Error Cost Escalation Through the Project Life Cycle}},
year = {2004}
}
@inproceedings{Gaspar2007,
abstract = {This descriptive study discusses two conceptual difficulties encountered by students in introductory programming courses regardless of the chosen language or pedagogical approach (e.g. objects, classes or fundamentals first). Firstly, students tend to learn programming by memorizing correct code examples instead of acquiring the programming thought process. Secondly, they tend to read code by "flying" over it at a comfortable altitude while thinking to its assumed intent. While relaxing, this practice fails to train students to develop the rigor to catch bugs in others' or their own code. Both trends result in an almost complete loss of intentionality in the programming activity; un-innovative code is generated by analogy with (or cut and paste from) existing solutions and is then almost randomly modified until "it fits" the minimal tests requirements without real analysis of its flaws. We review and evaluate pedagogical strategies which can be leveraged by instructors to address the above mentioned issues. Namely, we discuss the benefits of various forms of "Live coding" and test-driven pair programming active learning practices.},
address = {New York, NY, USA},
annote = {Restoring "coding with intention" in introductory programming courses},
author = {Gaspar, Alessio and Langevin, Sarah},
booktitle = {SIGITE '07: Proceedings of the 8th ACM SIGITE conference on Information technology education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {91--98},
publisher = {ACM},
title = {{Restoring "coding with intention" in introductory programming courses}},
url = {citeulike-article-id:3934616 http://dx.doi.org/10.1145/1324302.1324323},
year = {2007}
}
@inproceedings{Sinha2016,
address = {Austin, TX},
author = {Sinha, Vinayak and Lazar, Alina and Sharif, Bonita},
booktitle = {Proceedings of the 13th International Workshop on Mining Software Repositories - MSR '16},
doi = {10.1145/2901739.2903501},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sinha, Lazar, Sharif - 2016 - Analyzing developer sentiment in commit logs.pdf:pdf},
isbn = {9781450341868},
keywords = {sentiment},
mendeley-tags = {sentiment},
pages = {520--523},
publisher = {ACM Press},
title = {{Analyzing developer sentiment in commit logs}},
url = {http://dl.acm.org/citation.cfm?doid=2901739.2903501},
year = {2016}
}
@article{Kopec1999,
address = {Cracow, Poland},
author = {Kopec, Danny and Close, Richard and Aman, Jim},
doi = {10.1145/384267.305913},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kopec, Close, Aman - 1999 - How should data structures and algorithms be taught.pdf:pdf},
isbn = {1581130872},
issn = {00978418},
journal = {ACM SIGCSE Bulletin},
keywords = {CS2,Computer Science Education Research,closed laboratory experience,evaluating teaching methods,impact of technology on the curriculum,innovative instructional methods},
mendeley-tags = {CS2},
number = {3},
pages = {175--176},
publisher = {ACM Press},
title = {{How should data structures and algorithms be taught}},
url = {http://portal.acm.org/citation.cfm?doid=305786.305913},
volume = {31},
year = {1999}
}
@article{1183816,
abstract = {Internet connectivity is defined by a set of routing protocols which let the routers that comprise the Internet backbone choose the best route for a packet to reach its destination. One way to improve the security and performance of Internet is to routinely examine the routing data. In this case study, we show how interactive visualization of Border Gateway Protocol (BGP) data helps characterize routing behavior, identify weaknesses in connectivity which could potentially cripple the Internet, as well as detect and explain actual anomalous events.},
author = {Teoh, Soon Tee and Ma, Kwan-Liu Liu and Wu, S. Felix and Zhao, Xiaoliang},
doi = {10.1109/VISUAL.2002.1183816},
isbn = {0-7803-7498-3},
journal = {Visualization, 2002. VIS 2002. IEEE},
keywords = {Internet security,anomaly detection,border gateway,graph drawing,information visualization,network security},
month = {oct},
pages = {505--508},
title = {{Case study: interactive visualization for internet security}},
url = {http://dl.acm.org/citation.cfm?id=602099.602181},
year = {2002}
}
@article{1528425,
abstract = { Interest in visualization has grown in recent years, producing rapid advances in the diversity of research and in the scope of proposed techniques. Much of the initial focus in computer-based visualization concentrated on display algorithms, often for specific domains. For example, volume, flow, and terrain visualization techniques have generated significant insights into fundamental graphics and visualization theory, aiding the application experts who use these techniques to advance their own research. More recent work has extended visualization to abstract data sets like network intrusion detection, recommender systems, and database query results. This article describes our initial end-to-end system that starts with data management and continues through assisted visualization design, display, navigation, and user interaction. The purposes of this discussion are to (i) promote a more comprehensive visualization framework; (ii) describe how to apply expertise from human psychophysics, databases, rational logic, and artificial intelligence to visualization; and (iii) illustrate the benefits of a more complete framework using examples from our own experiences.},
author = {Dennis, B and Kocherlakota, S and Sawant, Amit and Tateosian, L and Healey, C G},
doi = {10.1109/MCG.2005.128},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {Factual,Humans,Information Storage and Retrieval,computer-based visualization,data management,end},
number = {6},
pages = {10--15},
title = {{Designing a visualization framework for multidimensional data}},
volume = {25},
year = {2005}
}
@inproceedings{Leitner2007,
abstract = {Although unit tests are recognized as an important tool in software development, programmers prefer to write code, rather than unit tests. Despite the emergence of tools like JUnit which automate part of the process, unit testing remains a time-consuming, resource-intensive, and not particularly appealing activity.This paper introduces a new development method, called Contract Driven Development. This development method is based on a novel mechanism that extracts test cases from failure-producing runs that the programmers trigger. It exploits actions that developers perform anyway as part of their normal process of writing code. Thus, it takes the task of writing unit tests off the developers' shoulders, while still taking advantage of their knowledge of the intended semantics and structure of the code. The approach is based on the presence of contracts in code, which act as the oracle of the test cases. The test cases are extracted completely automatically, are run in the background, and can easily be maintained over versions. The tool implementing this methodology is called Cdd and is available both in binary and in source form. Copyright 2007 ACM.},
annote = {Contract driven development = test driven development: Writing test cases},
author = {Leitner, A and Ciupa, I and Manuel, O and Meyer, B and Fiva, A},
booktitle = {6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {425--434},
title = {{Contract driven development = test driven development: Writing test cases}},
url = {citeulike-article-id:3934689 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-37849053128{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Paolacci2010a,
abstract = {Although Mechanical Turk has recently become popular among social scientists as a source of experimental data, doubts may linger about the quality of data provided by participants recruited from online labor markets. We address these potential concerns by presenting new demographic data about the Mechanical Turk subject population, reviewing the strengths of Mechanical Turk relative to other online and offline methods of recruiting participants, and comparing the magnitude of effects obtained using Mechanical Turk and traditional subject pools. We further discuss some additional benefits such as the possibility of longitudinal, cross cultural and prescreening designs, and offer some advice on how to best manage a common subject pool.},
author = {Paolacci, Gabriele and Chandler, Jesse and Ipeirotis, Panagiotis G.},
journal = {Judgement and Decision Making},
keywords = {Experimentation,Judgment and decision-making,Online research},
month = {jun},
number = {5},
pages = {411--419},
title = {{Running Experiments on Amazon Mechanical Turk}},
url = {http://papers.ssrn.com/abstract=1626226},
volume = {5},
year = {2010}
}
@article{Pardo2011,
abstract = {Adapting software quality assurance processes to product requirements will result in greater product quality and compliance, and thus increased customer satisfaction.},
author = {Pardo, C{\'{e}}sar and Pino, Francisco J. and Garc{\'{i}}a, F{\'{e}}lix and Piattini, Mario},
doi = {10.1109/MC.2011.178},
issn = {0018-9162},
journal = {Computer},
month = {jun},
number = {6},
pages = {94--96},
publisher = {IEEE Computer Society},
title = {{Harmonizing Quality Assurance Processes and Product Characteristics}},
url = {http://www.computer.org/csdl/mags/co/2011/06/mco2011060094-abs.html},
volume = {44},
year = {2011}
}
@inproceedings{Bird2011,
address = {Szeged, Hungary},
author = {Bird, Christian and Nagappan, Nachiappan and Murphy, Brendan and Gall, Harald and Devanbu, Premkumar},
booktitle = {Proc. of the 19th ACM SIGSOFT Sym. and the 13th European Conf. on Foundations of Software Engineering (ESEC/FSE '11)},
doi = {10.1145/2025113.2025119},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bird et al. - 2011 - Don't touch my code!.pdf:pdf},
isbn = {9781450304436},
keywords = {emprical software engineering,expertise,ownership,quality},
month = {sep},
pages = {4--14},
publisher = {ACM Press},
title = {{Don't touch my code!}},
url = {http://dl.acm.org/citation.cfm?doid=2025113.2025119 http://dl.acm.org/citation.cfm?id=2025113.2025119},
year = {2011}
}
@inproceedings{Becher2011,
abstract = {We are currently moving from the Internet society to a mobile society where more and more access to information is done by previously dumb phones. For example, the number of mobile phones using a full blown OS has risen to nearly 200{\%} from Q3/2009 to Q3/2010. As a result, mobile security is no longer immanent, but imperative. This survey paper provides a concise overview of mobile network security, attack vectors using the back end system and the web browser, but also the hardware layer and the user as attack enabler. We show differences and similarities between "normal" security and mobile security, and draw conclusions for further research opportunities in this area.},
address = {Oakland, CA},
author = {Becher, Michael and Freiling, Felix C. and Hoffmann, Johannes and Holz, Thorsten and Uellenbeck, Sebastian and Wolf, Christopher},
booktitle = {2011 IEEE Symposium on Security and Privacy},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {may},
pages = {96--111},
publisher = {IEEE},
title = {{Mobile Security Catching Up? Revealing the Nuts and Bolts of the Security of Mobile Devices}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=5958024},
year = {2011}
}
@inproceedings{Wieczorek2008,
abstract = {This paper suggests a holistic design and development method combining test-driven and modeldriven development for SOA architectures. It uses testdriven development on component level and modelbased testing on system level. Moreover, monitored performance parameters during test execution serve as input for a model-driven performance analysis of the business application, providing early indication of possible performance issues. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Enhancing test driven development with model based testing and performance analysis},
author = {Wieczorek, S and Stefanescu, A and Fritzsche, M and Schnitter, J},
booktitle = {Proceedings - Testing: Academic and Industrial Conference Practice and Research Techniques, TAIC PART 2008},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {82--86},
title = {{Enhancing test driven development with model based testing and performance analysis}},
url = {citeulike-article-id:3934836 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57649233309{\&}{\#}38 partnerID=40},
year = {2008}
}
@article{Andrews2005,
author = {Andrews, Anneliese a. and Offutt, Jeff and Alexander, Roger T.},
doi = {10.1007/s10270-004-0077-7},
issn = {1619-1366},
journal = {Software {\&} Systems Modeling},
keywords = {communicated by a,finite state machines,jackson,system testing,testing of web applications,wills and m},
month = {jan},
number = {3},
pages = {326--345},
title = {{Testing Web applications by modeling with FSMs}},
url = {http://link.springer.com/10.1007/s10270-004-0077-7},
volume = {4},
year = {2005}
}
@incollection{Mendling2007,
address = {Berlin},
author = {Mendling, Jan and Neumann, Gustaf and {Van Der Aalst}, Wil},
booktitle = {Lecture Notes in Computer Science},
doi = {10.1007/978-3-540-76848-7},
editor = {Curbera, F and Leymann, F and Weske, M},
isbn = {978-3-540-76846-3},
keywords = {process mining},
pages = {113--130},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Understanding the Occurrence of Errors in Process Models Based on Metrics}},
url = {http://www.springerlink.com/index/10.1007/978-3-540-76848-7},
volume = {4803},
year = {2007}
}
@article{Miller1956,
author = {Miller, George A},
number = {2},
title = {{"The Magical Number Seve, Plus or Minus Two: Some Limits on Our Capacity for Processing Information}},
volume = {63},
year = {1956}
}
@article{Atkinson1953,
author = {Atkinson, John W},
journal = {Journal of Experimental Psychology},
keywords = {agile,interrupt,interruption,nsf},
mendeley-tags = {agile,nsf},
number = {6},
pages = {381--390},
title = {{The Achievement Motive and Recall of Interrupted and Completed Tasks}},
volume = {46},
year = {1953}
}
@article{Reyns2011,
abstract = {Building upon Eck and Clarke's (2003) ideas for explaining crimes in which there is no face-to-face contact between victims and offenders, the authors developed an adapted lifestyle-routine activities theory. Traditional conceptions of place-based environments depend on the convergence of victims and offenders in time and physical space to explain opportunities for victimization. With their proposed cyberlifestyle-routine activities theory, the authors moved beyond this conceptualization to explain opportunities for victimization in cyberspace environments where traditional conceptions of time and space are less relevant. Cyberlifestyle-routine activities theory was tested using a sample of 974 college students on a particular type of cybervictimization--cyberstalking. The study's findings provide support for the adapted theoretical perspective. Specifically, variables measuring online exposure to risk, online proximity to motivated offenders, online guardianship, online target attractiveness, and online deviance were significant predictors of cyberstalking victimization. Implications for advancing cyberlifestyle-routine activities theory are discussed.},
author = {Reyns, Bradford W. and Henson, Billy and Fisher, Bonnie S.},
doi = {10.1177/0093854811421448},
issn = {0093-8548},
journal = {Criminal Justice and Behavior},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {sep},
number = {11},
pages = {1149--1169},
title = {{Being Pursued Online: Applying Cyberlifestyle-Routine Activities Theory to Cyberstalking Victimization}},
url = {http://cjb.sagepub.com/content/38/11/1149.short},
volume = {38},
year = {2011}
}
@article{Vodde2007,
abstract = {Test-driven development is an agile development practice that changes every minute of developers' daily lives. That's a big change! How can you best train developers in such a pervasive practice? One method is to write tests for counting lines of code. While developing the tests, developers run into problems that force them to reevaluate their design. This experience provides valuable insights into TDD and its benefits. This article is part of a special issue on test-driven development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Learning test-driven development by counting lines},
author = {Vodde, B and Koskela, L},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {74--79},
title = {{Learning test-driven development by counting lines}},
url = {citeulike-article-id:3934828 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248387354{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@inproceedings{4041160,
abstract = {Attack graphs are a valuable tool to network defenders, illustrating paths an attacker can use to gain access to a targeted network. Defenders can then focus their efforts on patching the vulnerabilities and configuration errors that allow the attackers the greatest amount of access. We have created a new type of attack graph, the multiple-prerequisite graph, that scales nearly linearly as the size of a typical network increases. We have built a prototype system using this graph type. The prototype uses readily available source data to automatically compute network reachability, classify vulnerabilities, build the graph, and recommend actions to improve network security. We have tested the prototype on an operational network with over 250 hosts, where it helped to discover a previously unknown configuration error. It has processed complex simulated networks with over 50,000 hosts in under four minutes},
author = {Ingols, Kyle and Lippmann, Richard and Piwowarski, Keith},
booktitle = {Computer Security Applications Conference, 2006. ACSAC '06. 22nd Annual},
doi = {10.1109/ACSAC.2006.39},
issn = {1063-9527},
keywords = {attack graph generation;multiple-prerequisite grap},
pages = {121--130},
title = {{Practical Attack Graph Generation for Network Defense}},
year = {2006}
}
@article{Holbrook1990,
author = {Holbrook, H},
number = {1},
pages = {95--104},
title = {{A Scenario-Based Methodology for Conducting Requirements Elicitation}},
volume = {15},
year = {1990}
}
@inproceedings{6059257,
abstract = {Slowly but surely, academia and industry are fully accepting the importance of the human element as it pertains to achieving security and trust. Undoubtedly, one of the main motivations for this is the increase in attacks (e.g., social engineering and phishing) which exploit humans and exemplify why many authors regard them as the weakest link in the security chain. As research in the socio-technical security and trust fields gains momentum, it is crucial to intermittently pause and reflect on their progress while also considering related domains to determine whether there are any established principles which may be transferred. Comparison of the states-of-the-arts may assist in planning work going forward and identifying useful future directions for the less mature socio-technical field. This paper seeks to fulfil several of these goals, particularly as they relate to the emerging cybersecurity-risk communication domain. The literature reviews which we conduct here are beneficial and indeed noteworthy as they pull together a number of the key aspects which may affect the trustworthiness and effectiveness of communications on cybersecurity risks. In particular, we draw on information-trustworthiness research and the established field of risk communication. An appreciation of these aspects and precepts is imperative if systems are to be designed that play to individuals' strengths and assist them in maintaining security and protecting their applications and information.},
author = {Nurse, J R C and Creese, S and Goldsmith, M and Lamberts, K},
booktitle = {Socio-Technical Aspects in Security and Trust (STAST), 2011 1st Workshop on},
doi = {10.1109/STAST.2011.6059257},
keywords = {cybersecurity risk communication,information secur},
pages = {60--68},
title = {{Trustworthy and effective communication of cybersecurity risks: A review}},
year = {2011}
}
@book{Lawrence1994,
address = {Gainesville, FL},
author = {Lawrence, G},
edition = {3rd},
keywords = {briggs,mbti,myers,myers-briggs},
publisher = {Center for Applications of Psychological Types},
title = {{People Types and Tiger Stripes}},
year = {1994}
}
@inproceedings{Cao2016,
abstract = {Although computing students may enjoy when their instructors teach using analogies, it is unknown to what extent these analogies are useful for their learning. This study examines the value of analogies when used to introduce three introductory computing topics. The value of these analogies may be evident during the teaching process itself (short term), in subsequent exams (long term), or in students' ability to apply their understanding to related non-technical areas (transfer). Comparing results between an experimental group (analogy) and control group (no analogy), we find potential value for analogies in short term learning. However, no solid evidence was found to support analogies as valuable for students in the long term or for knowledge transfer. Speciffic demographic groups were examined and promising preliminary findings are presented.},
address = {Melbourne, Australia},
author = {Cao, Yingjun and Porter, Leo and Zingaro, Daniel},
booktitle = {ICER 2016 - Proceedings of the 2016 ACM Conference on International Computing Education Research},
doi = {10.1145/2960310.2960313},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao, Porter, Zingaro - 2016 - Examining the value of analogies in introductory computing.pdf:pdf},
isbn = {9781450344494},
keywords = {Analogies,CS1,CS2,Programming},
month = {aug},
pages = {231--239},
publisher = {Association for Computing Machinery, Inc},
title = {{Examining the value of analogies in introductory computing}},
url = {https://dl.acm.org/doi/10.1145/2960310.2960313},
year = {2016}
}
@article{Freese2003,
abstract = {Although test-driven development is a well established part of extreme programming, there are still unsolved issues if using it for library development or in team environments. This paper describes how these issues may be addressed by a software configuration management tool},
annote = {Software configuration management for test-driven development},
author = {Freese, T},
isbn = {3540402152},
journal = {Extreme Programming and Agile Processes in Software Engineering 4th International Conference, XP 2003 Proceedings Lecture Notes in Computer Science Vol 2675},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {431--432},
title = {{Software configuration management for test-driven development}},
url = {citeulike-article-id:3934611 {\#}},
volume = {2675},
year = {2003}
}
@inproceedings{5949399,
abstract = {Wireless networks are a common point of entry for computer network attacks. Due to high traffic volumes, network mission assurance requires tools that can usefully display network traffic data, automatically detect, and identify attacks to provide increased situational awareness to a network administrator. Many metrics used to analyze wireless network traffic and security depend on full access to all nodes. This is impractical in fielded networks. To address these issues, we propose a new set of metrics based on wireless network packet interarrival times. These metrics are displayed in a novel way to provide administrators with a mechanism for identifying possible attacks and their impact on the network. The performance of this visualizer is validated by the use of a linear classifier system, which shows that the chosen metrics can be used to accurately identify attacks. We further argue that the classifier could be used in conjunction with the visualizer as an effective decision support system to aid in maintaining mission assurance.},
author = {Harmer, P and Thomas, R and Christel, B and Martin, R and Watson, C},
booktitle = {Computational Intelligence in Cyber Security (CICS), 2011 IEEE Symposium on},
doi = {10.1109/CICYBS.2011.5949399},
keywords = {attack identification decision support system,comp},
month = {apr},
pages = {144--151},
title = {{Wireless security situation awareness with attack identification decision support}},
year = {2011}
}
@inproceedings{Saff2003,
address = {Denver, CO},
author = {Saff, David and Ernst, Michael D.},
booktitle = {14th International Symposium on Software Reliability Engineering,},
keywords = {continuous testing},
pages = {281--292},
title = {{Reducing wasted development time via continuous testing}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Reducing+Wasted+Development+Time+via+Continuous+Testing{\#}0 http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Reducing+wasted+development+time+via+continuous+testing{\#}0 http://ieeexplo},
volume = {pages},
year = {2003}
}
@article{Huang2006,
abstract = {The test-first approach, which is based on an iterative process of "setting up test cases, implementing the functionality, and having all test cases passed", has been put forward for decades however knowledge on test-first approach is limited. This research abstract states the limitations that previous studies in this area have, describes a controlled experiment with undergraduate students in the context of Sheffield Software Engineering Observatory (SSEO) environment to investigate the distinction between test-first and test-last approach (traditional approach), presents research questions and hypotheses set up before the experiment from the quality and productivity perspective, and expounds methods of data collection and validation},
annote = {Empirical assessment of test-first approach},
author = {Huang, L and Holcombe, M},
isbn = {0769526721},
journal = {Proceedings Testing: Academic and Industrial Conference Practice and Research Techniques},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {p},
title = {{Empirical assessment of test-first approach}},
url = {citeulike-article-id:3934644 {\#}},
year = {2006}
}
@article{Holzmann2008,
author = {Holzmann, Gerard J. and Joshi, Rajeev and Groce, Alex},
doi = {10.1007/s10515-008-0033-9},
issn = {0928-8910},
journal = {Automated Software Engineering},
month = {sep},
number = {3-4},
pages = {283--297},
title = {{Model driven code checking}},
url = {http://link.springer.com/10.1007/s10515-008-0033-9},
volume = {15},
year = {2008}
}
@incollection{Bifet2006,
author = {Bifet, Albert and Gavald{\`{a}}, Ricard},
booktitle = {Discovery Science},
doi = {10.1007/11893318_7},
pages = {29--40},
publisher = {Springer Berlin Heidelberg},
title = {{Kalman Filters and Adaptive Windows for Learning in Data Streams}},
url = {http://link.springer.com/10.1007/11893318{\_}7},
year = {2006}
}
@article{SREPT,
author = {Ramani, S and Gokhale, S S and Trivedi, K S},
journal = {Performance Evaluation},
number = {1-4},
pages = {37--60},
title = {{SREPT: Software Reliability Estimation and Prediction Tool}},
volume = {39},
year = {2000}
}
@inproceedings{4267573,
abstract = {To maintain effective security situational awareness, administrators require tools that present up-to-date information on the state of the network in the form of 'at-a-glance' displays, and that enable rapid assessment and investigation of relevant security concerns through drill-down analysis capability. In this paper, we present a passive network monitoring tool we have developed to address these important requirements, known as Panemoto (Passive Network Monitoring Tool). We show how Panemoto enumerates, describes, and characterizes all network components, including devices and connected networks, and delivers an accurate representation of the function of devices and logical connectivity of networks. We provide examples of Panemoto's output in which the network information is presented in two distinct but related formats: as a clickable network diagram (through the use of NetViz, a commercially available graphical display environment) and as statically-linked HTML pages, viewable in any standard web browser. Together, these presentation techniques enable a more complete understanding of the security situation of the network than each does individually.},
author = {Streilein, W and Kratkiewicz, K and Sikorski, M and Piwowarski, K and Webster, S},
booktitle = {Information Assurance and Security Workshop, 2007. IAW '07. IEEE SMC},
doi = {10.1109/IAW.2007.381945},
keywords = {Panemoto,local area networks,netwo,network diagram},
month = {jun},
pages = {284--290},
title = {{PANEMOTO: Network Visualization of Security Situational Awareness Through Passive Analysis}},
year = {2007}
}
@article{Erdogmus2005,
abstract = {Test-Driven Development (TDD) is based on formalizing a piece of functionality as a test, implementing the functionality such that the test passes, and iterating the process. This paper describes a controlled experiment for evaluating an important aspect of TDD: In TDD, programmers write functional tests before the corresponding implementation code. The experiment was conducted with undergraduate students. While the experiment group applied a test-first strategy, the control group applied a more conventional development technique, writing tests after the implementation. Both groups followed an incremental process, adding new features one at a time and regression testing them. We found that test-first students on average wrote more tests and, in turn, students who wrote more tests tended to be more productive. We also observed that the minimum quality increased linearly with the number of programmer tests, independent of the development strategy employed. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {On the effectiveness of the test-first approach to programming},
author = {Erdogmus, H and Morisio, M and Torchiano, M},
journal = {IEEE Transactions on Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {226--237},
title = {{On the effectiveness of the test-first approach to programming}},
url = {citeulike-article-id:3934600 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-21244432456{\&}{\#}38 partnerID=40},
volume = {31},
year = {2005}
}
@inproceedings{4373487,
abstract = {The secure perimeter awareness network (SPAN) at JFK is an integrated system of networked sensors for intrusion detection, assessment, reporting, and video recording. The system includes our airport security display processor (ASDP) system, pan-tilt-zoom (PTZ) cameras, an ASDP graphical user interface (GUI), and a common operational picture (COP). The ASDP system takes raw radar data from the ASDE-3 ground surveillance radar and monitors the airport property and perimeter for any security breaches. The system detects and tracks objects of interest beyond and at the perimeter, and on the air operations area. If a tracked object, such as a car, a boat (JFK borders Jamaica Bay), or a person, breaches the user-defined perimeter, the ASDP system issues an alarm, displays the intruder's position on the GUI, and slews a camera to the location of the intruder and tracks the intruder. The video is displayed on the COP, an integrated geo-registered, situational awareness display of all sensor data. The COP shows not only the video, but also a layered display and interface for radar data and other sensor inputs. Finally, SPAN records all video for review and possible future prosecution.},
author = {Barry, Ann S. and Mazel, David S.},
booktitle = {2007 41st Annual IEEE International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2007.4373487},
isbn = {978-1-4244-1129-0},
keywords = {ASDP system,COP display,GUI,airport security displ},
month = {oct},
pages = {183--188},
publisher = {Ieee},
title = {{The Secure Perimeter Awareness Network (SPAN) at John F. Kennedy International Airport}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4373487},
year = {2007}
}
@inproceedings{Malmi2014,
abstract = {We analyze the Computing Education Research (CER) literature to discover what theories, conceptual models and frameworks recent CER builds on. This gives rise to a broad understanding of the theoretical basis of CER that is useful for researchers working in that area, and has the potential to help CER develop its own identity as an independent field of study. Our analysis takes in seven years of publications (2005-2011, 308 papers) in three venues that publish long research papers in computing education: The journals ACM Transactions of Computing Education (TOCE) and Computer Science Education (CSEd), and the conference International Computing Education Research Workshop (ICER). We looked at the theoretical background works that are used or extended in the papers, not just referred to when describing related work. These background works include theories, conceptual models and frameworks. For each background work we tried to identify the discipline from which it originates, to gain an understanding of how CER relates to its neighboring fields. We also identified theoretical works originating within CER itself, showing that the field is building on its own theoretical works. Our main findings are that there is a great richness of work on which recent CER papers build; there are no prevailing theoretical or technical works that are broadly applied across CER; about half the analyzed papers build on no previous theoretical work, but a considerable share of these are building their own theoretical constructions. We discuss the significance of these findings for the whole field and conclude with some recommendations. Copyright {\textcopyright} 2014 ACM.},
address = {glasgow, Scotland},
author = {Malmi, Lauri and Sheard, Judy and Simon and Bednarik, Roman and Helminen, Juha and Kinnunen, P{\"{a}}ivi and Korhonen, Ari and Myller, Niko and Sorva, Juha and Taherkhani, Ahmad},
booktitle = {ICER 2014 - Proceedings of the 10th Annual International Conference on International Computing Education Research},
doi = {10.1145/2632320.2632358},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malmi et al. - 2014 - Theoretical underpinnings of computing education research - What is the evidence.pdf:pdf},
isbn = {9781450327558},
keywords = {Classifying publications,Computing education,Research methods},
pages = {27--34},
publisher = {ACM Press},
title = {{Theoretical underpinnings of computing education research - What is the evidence?}},
url = {http://dl.acm.org/citation.cfm?doid=2632320.2632358},
year = {2014}
}
@article{sampson1990deviant,
author = {Sampson, Robert J. and Lauritsen, Janet L.},
journal = {Journal of Research in Crime and Delinquency},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {110--139},
publisher = {Sage Publications},
title = {{Deviant lifestyles, proximity to crime, and the offender-victim link in personal violence}},
volume = {27},
year = {1990}
}
@article{Layman2006,
author = {Layman, Lucas and Williams, Laurie and Cunningham, Lynn},
journal = {Journal of Systems Architecture},
keywords = {mypubs},
mendeley-tags = {mypubs},
number = {11},
pages = {654--667},
title = {{Motivations and Measurements in an Agile Case Study}},
volume = {52},
year = {2006}
}
@inproceedings{Schwaber2002,
address = {Essen, Germany},
annote = {About why agile works and how RE is out-dated and can{\&}{\#}039;t say it doesn{\&}{\#}039;t.  Position paper that doesn{\&}{\#}039;t really contribute anything.},
author = {Schwaber, K},
keywords = {XP,requirements},
title = {{The Impact of Agile Processes on Requirements Engineering}},
year = {2002}
}
@inproceedings{shirai14,
address = {New York, NY, USA},
author = {Shirai, Yasutaka and Nichols, William and Kasunic, Mark},
booktitle = {Proceedings of the 2014 International Conference on Software and System Process},
doi = {10.1145/2600821.2600841},
isbn = {978-1-4503-2754-1},
keywords = {Data Quality,Database,TSP,Team Software Process},
pages = {25--29},
publisher = {ACM},
series = {ICSSP 2014},
title = {{Initial Evaluation of Data Quality in a TSP Software Engineering Project Data Repository}},
url = {http://doi.acm.org/10.1145/2600821.2600841},
year = {2014}
}
@article{Go2006,
abstract = {The test-driven software development method will make specification change easy compared with the conventional water fall model. The software release of the former will be done shortly compared with the latter. However, the quality assurance is difficult, because it is necessary to extract the test case without the source code. Thus, an algorithm is proposed to extract the test case from the specification, and the test case extraction tool using this algorithm is also proposed. It is shown that this tool can extract test cases for the state-less model appropriately. This paper considers whether the proposed method can be applied to the state transition model to extract test cases of actual SIP media server. The result shows that the test cases can be extracted appropriately by considering two or more possibilities for one item to estimate the output for each input. (author abst.)},
annote = {Test Item Extraction Tool for Test-Driven Software Development of SIP Services},
author = {Go, H M O and Tanaka, Y},
journal = {IEIC Technical Report (Institute of Electronics, Information and Communication Engineers)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {627},
pages = {33--36},
title = {{Test Item Extraction Tool for Test-Driven Software Development of SIP Services}},
url = {citeulike-article-id:3934626 http://sciencelinks.jp/j-east/article/200610/000020061006A0307897.php},
volume = {105},
year = {2006}
}
@inproceedings{Ferebee2011c,
abstract = {Efficient visualization of cyber incidents is the key in securing increasingly complex information infrastructure. Extrapolating security-related information from data from multiple sources can be a daunting task for organizations to maintain safe and secure operating environment. However, meaningful visualizations can significantly improve decision-making quality and help security administrators in taking rapid response. The purpose of this work is to explore this possibility by building on previously gained knowledge and understanding of weather maps used in meteorology, assessing the gaps, and applying various techniques and matrices to quantify the impacts of cyber incidences in an efficient way.},
author = {Ferebee, Denise and Dasgupta, Dipankar and Schmidt, Michael and Wu, Qishi},
booktitle = {2011 IEEE Symposium on Computational Intelligence in Cyber Security (CICS)},
doi = {10.1109/CICYBS.2011.5949412},
isbn = {978-1-4244-9905-2},
month = {apr},
pages = {171--178},
publisher = {IEEE},
title = {{Security visualization: Cyber security storm map and event correlation}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5949412},
year = {2011}
}
@inproceedings{5375533,
abstract = {It seems obvious: networks, software, authentication, and people have important and often complicated relationships and interactions. There's far too much going on to keep track of all of it, but we know there are important devils down in the details. We know they are there. Though many have been chasing this dream of security visualization for a couple of decades, we don't have that much to show for our efforts. We use NOCs and tools widely for managing large networks, but they get complicated fast. And most of the anomalous activity is weird but benign, leaving us awash in a sea of false positives. And those people in the NOCs seem totally resistant to 3D displays, data gloves, and other cool tools of our trade. What can we do? How can we help, really?},
author = {Cheswick, Bill},
booktitle = {2009 6th International Workshop on Visualization for Cyber Security},
doi = {10.1109/VIZSEC.2009.5375533},
isbn = {978-1-4244-5413-6},
keywords = {3D displays,data gloves,security visualization,vis},
pages = {viii--viii},
publisher = {Ieee},
title = {{Keynote address: Visual tools for security: Is there a there there?}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5375533},
year = {2009}
}
@inproceedings{Ho2005,
address = {New York, New York, USA},
author = {Ho, Joyce and Intille, Stephen S.},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '05},
doi = {10.1145/1054972.1055100},
isbn = {1581139985},
keywords = {context-aware computing,human-computer interface,interruption,mobile computing},
month = {apr},
pages = {909},
publisher = {ACM Press},
title = {{Using context-aware computing to reduce the perceived burden of interruptions from mobile devices}},
url = {http://dl.acm.org/citation.cfm?id=1054972.1055100},
year = {2005}
}
@inproceedings{Karamat2006,
abstract = {In the fast pace business world of today where competition and technology are at their zenith, software development companies need to improve their quality standards in addition to cost reduction in operations. To achieve these challenging objectives various developments are on the verge. In recent past agile methodologies have emerged as one of the most efficient implementations in the world of software development arena. Especially eXtreme programming (XP) which is integrated by Test first approach recent research proves the emergence of Test Driven Development (TDD) from this concept which is based on formalizing the requirement as a test and secondly to write such a code that can pass the test. This attempt of our research provides a mechanism to reduce the cost of testing, mainly due to troublesome test which fails again and again. We used TDD, analyzed the problem and proposed a workable solution. Test Driven Development is a technique which encourages less documentation resulting in a lot of difficulties for developers in contrast to traditional methods. In order to reduce the burden on developers we have proposed some steps in documentation. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Reducing test cost and improving documentation in TDD (Test Driven Development)},
author = {Karamat, T and Jamil, A N},
booktitle = {Proc. - Seventh ACIS Int. Conf. on Software Eng., Artific. Intelligence, Netw., and Parallel/Distributed Comput., SNPD 2006, including Second ACIS Int. Worshop on SAWN 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {73--76},
title = {{Reducing test cost and improving documentation in TDD (Test Driven Development)}},
url = {citeulike-article-id:3934668 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33845563622{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@article{Mugridge2008,
abstract = {Agile project teams aim to include both business and development personnel, emphasizing direct communication over written requirements documents. Rather than trying to understand all of a system's detailed requirements before development, they carry out high-level release planning and then drive small development increments in cycles of one or two weeks. Doing so avoids many of the potential problems in traditional, phased software development approaches and accepts that changes are inevitable. Storytest-driven development brings requirements and automated testing ideas and practices together to support this agile process. The author describes this development approach and how its concrete examples can clarify and communicate business rules, aid agile team discussions, and facilitate team members' understanding of the concepts at the heart of the business needs. Such examples are executable, serving a secondary role as automated tests. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Managing agile project requirements with storytest-driven development},
author = {Mugridge, R},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {68--75},
title = {{Managing agile project requirements with storytest-driven development}},
url = {citeulike-article-id:3934733 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-39449104187{\&}{\#}38 partnerID=40},
volume = {25},
year = {2008}
}
@inproceedings{Saff2005a,
address = {St. Louis, MO},
author = {Saff, David and Ernst, Michael D.},
booktitle = {Proceedings of the 27th international conference on Software engineering},
keywords = {continuous testing},
pages = {668--669},
publisher = {ACM},
title = {{Continuous testing in Eclipse}},
url = {http://portal.acm.org/citation.cfm?id=1062455.1062600},
year = {2005}
}
@techreport{NIST2010,
address = {NIST Special Publication 800-122},
author = {NIST},
institution = {NIST},
keywords = {mypubs},
mendeley-tags = {mypubs},
title = {{Guide to Protecting the Confidentiality of Personally Identifiable Information (PII)}},
year = {2010}
}
@inproceedings{4123769,
abstract = {This paper discusses the security toolbox in a simple visual manner. Watermarking and fingerprinting of all kinds are tools that apply themselves well to all content be it analogue or digital in representation. In contrast, cryptography and key exchange are purely digital tools. All four have their place in the security toolbox and all can be subjected to verification tools which are used as a signature to ensure that what has been delivered has not been subject to tampering},
author = {Wilkinson, J},
booktitle = {Crime and Security, 2006. The Institution of Engineering and Technology Conference on},
keywords = {cryptography,fingerprinting,key exchange,security},
month = {jun},
pages = {265--275},
title = {{The Security Toolbox}},
year = {2006}
}
@inproceedings{5066603,
abstract = {This paper focuses on establishing the need for new architectures on which to build visualisation systems that enhance computer forensic investigation of digital evidence. The issues surrounding processing of large quantities of digital evidence are established. In addition, the current state of visualisation and data analysis techniques for computer forensics are highlighted. This paper suggests need for new visualisation techniques in order to display data in familiar visual forms that facilitate efficient insight gaining into digital evidence. Visualisations techniques also require a source of processed data that contains context relevant information to present to an investigator. To this end this paper introduces the notion of data exploitation as a way to describe techniques that provide opportunistic data analysis across multiple sources of digital evidence. Data exploitation techniques provide normalisation techniques, event correlation, relationship extraction and investigative domain knowledge processing to occur across a set of evidence. This enables a visual representation of digital evidence to highlight relationships and events across many data sources, support an investigator throughout the entire data analysis process and enable an investigator to focus on the context of the current crime.},
author = {Osborne, G and Turnbull, B},
booktitle = {Availability, Reliability and Security, 2009. ARES '09. International Conference on},
doi = {10.1109/ARES.2009.120},
keywords = {computer forensics,data analysis techniques,data e},
month = {mar},
pages = {1012--1017},
title = {{Enhancing Computer Forensics Investigation through Visualisation and Data Exploitation}},
year = {2009}
}
@book{Cohen1988,
address = {Hillsdale, NJ},
author = {Cohen, Jacob Jack},
edition = {2nd},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Lawrence Erlbaum Associates},
title = {{Statistical Power Analysis for the Behavioral Sciences}},
year = {1988}
}
@article{Kitchenham1995,
annote = {{\textless}m:note{\textgreater}How to evaluate a new method or tool for your organization.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Classifications of case studies, formal experiments, and surveys based on Basili's classifications. {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Formal experiments: "research-in-the-small"{\textless}m:linebreak/{\textgreater}Case studies: "research-in-the-typical"{\textless}m:linebreak/{\textgreater}Surveys: "research-in-the-large"{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Guidelines for when to do case studies, when to do formal experiments, and when to do surveys.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}A "How-to" for planning and doing case studies.{\textless}/m:note{\textgreater}},
author = {Kitchenham, B and Pickard, L and Pfleeger, S L},
keywords = {case studies,case study,evaluation,method},
number = {4},
pages = {52--62},
title = {{Case Studies for Method and Tool Evaluation}},
volume = {12},
year = {1995}
}
@article{Smith2005a,
abstract = {Test-driven development (TDD) is an important topic to consider about agile software development. TDD involves running tests upfront in order to specify target requirements and to describe a target design. This contrasts to test-last development (TLD), which is a more traditional procedure. TLD involves requirement gathering and project design prior to the process of running software tests. This series of articles shows how to use TDD to develop software for an embedded system. A simple engineering project for high school students as the target for TDD investigation is chosen. This article shows how to use a TDD environment to develop a video game with the analog devices ADSP-BF533 Blackfin microcontroller. The thermal arm wrestling project was perfect because it was easy to set up, was a little flashy, wasn't too overpowering, and was just corny enough to interest high school students},
annote = {Practical application for TDD. Part 1. Write software in a test-driven development environment},
author = {Smith, M and Bariffi, M and Flaman, W and Geras, A and Huang, L and Kwan, A and Martin, A and Miller, J},
isbn = {0896-8985},
journal = {Circuit Cellar},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {176},
pages = {34--39},
title = {{Practical application for TDD. Part 1. Write software in a test-driven development environment}},
url = {citeulike-article-id:3934800 {\#}},
year = {2005}
}
@inproceedings{Tomayko2002,
address = {Essen, Germany},
annote = {Short paper, no real contribution. Based on a university study, but not a research paper by any means.},
author = {Tomayko, J E},
keywords = {agile,requirements},
title = {{Engineering of Unstable Requirements Using Agile Methods}},
year = {2002}
}
@book{Putnam1992,
address = {Englewood Cliffs, NJ},
author = {Putnam, L H and Myers, W},
publisher = {Yourdon Press},
title = {{Measures for Excellence:  Reliable Software on Time, Within Budget}},
year = {1992}
}
@inproceedings{Anvik2006,
address = {New York, New York, USA},
author = {Anvik, John and Hiew, Lyndon and Murphy, Gail C.},
booktitle = {Proceeding of the 28th international conference on Software engineering - ICSE '06},
doi = {10.1145/1134285.1134336},
isbn = {1595933751},
keywords = {bug report assignment,bug triage,issue tracking,machine learning,problem tracking},
month = {may},
pages = {361--370},
publisher = {ACM Press},
title = {{Who should fix this bug?}},
url = {http://dl.acm.org/citation.cfm?id=1134285.1134336},
year = {2006}
}
@inproceedings{4244868,
abstract = {With the development of computer networks, the spread of malicious network activities poses great risks to the operational integrity of many organizations and imposes heavy economic burdens on life and health. Therefore, risk assessment is very important in network security management and analysis. Network security situation analysis not only can describe the current state but also project the next behavior of the network. Alerts coming from IDS, Firewall, and other security tools are currently growing at a rapid pace. Large organizations are having trouble keeping on top of the current state of their networks. In this paper, we described cyberspace situational awareness from formal and visual methods. Next, to make security administrator comprehend security situation and project the next behaviors of the whole network, we present using parallel axes view to give expression clearly of security events correlations. At last, we concluded that visualization is an important research of risk evaluation and situation analysis of network.},
author = {Mixia, Liu and Dongmei, Yu and Qiuyu, Zhang and Honglei, Zhu},
booktitle = {Anti-counterfeiting, Security, Identification, 2007 IEEE International Workshop on},
doi = {10.1109/IWASID.2007.373676},
keywords = {Firewall,IDS,computer networks,cyberspace situatio},
month = {apr},
pages = {448--452},
title = {{Network Security Risk Assessment and Situation Analysis}},
year = {2007}
}
@book{Margolis2002,
address = {Cambridge, Massachusetts},
author = {Margolis, J and Fisher, A},
publisher = {The MIT Press},
title = {{Unlocking the Clubhouse:  Women in Computing}},
year = {2002}
}
@article{Crispin2006,
abstract = {Recently, software development teams using agile processes have started widely adopting test-driven development. But does TDD really improve software quality? {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Driving software quality: How test-driven development impacts software quality},
author = {Crispin, L},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {70--71},
title = {{Driving software quality: How test-driven development impacts software quality}},
url = {citeulike-article-id:3934578 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33846907119{\&}{\#}38 partnerID=40},
volume = {23},
year = {2006}
}
@article{Reifer,
author = {Reifer, Donald J.},
doi = {10.1109/TR.1979.5220578},
issn = {0018-9529},
journal = {IEEE Transactions on Reliability},
keywords = {-software reliability,analysis,art,effects,fault tolerant software,none,purpose,self checking software,software failure modes and,special math needed,widen state of the},
number = {3},
pages = {247--249},
title = {{Software Failure Modes and Effects Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5220578},
volume = {R-28}
}
@inproceedings{5284049,
abstract = {Intention recognition is the ability to predict an opposing forcepsilas high level goals. Knowing an attackerpsilas intention can support the decision-making of the network security administrators. Furthermore intent analysis plays an import role in the calculation of the inherent threat value. So how to recognize attack intention has become a research hot in network security domain recently.In this paper taxonomy of attack intention characterized by consequences of attack and targets of attack is introduced at first. Then a graphical model based on network security state is presented and used to recognize attack intention. D-S evidence theory is also introduced to deal with the uncertainty in the process of intent inference. Next algorithms of intention recognition and threat assessment are given in detail in order to offer a way to assess the network security situation. Finally several experiments are done in a local network. The results of the experiments prove the feasibility and validity of this method.},
author = {Wu, Peng and Zhigang, Wang and Junhua, Chen},
booktitle = {Information Assurance and Security, 2009. IAS '09. Fifth International Conference on},
doi = {10.1109/IAS.2009.158},
keywords = {Dempster-Shafer evidence theory,attack intention r},
pages = {360--363},
title = {{Research on Attack Intention Recognition Based on Graphical Model}},
volume = {1},
year = {2009}
}
@article{Oldham:2005:HAP:1060405.1060408,
address = {USA},
author = {Oldham, Joseph D},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oldham - 2005 - What Happens After Python in CS1.pdf:pdf},
issn = {1937-4771},
journal = {J. Comput. Sci. Coll.},
keywords = {CS2},
mendeley-tags = {CS2},
month = {jun},
number = {6},
pages = {7--13},
publisher = {Consortium for Computing Sciences in Colleges},
title = {{What Happens After Python in CS1?}},
url = {http://dl.acm.org/citation.cfm?id=1060405.1060408},
volume = {20},
year = {2005}
}
@misc{Ko2004,
address = {Vienna, Austria},
author = {Ko, Andrew J and Myers, Brad A},
pages = {151--158},
title = {{Designing the Whyline: A Debugging Interface for Asking Questions about Program Behavior}},
year = {2004}
}
@inproceedings{1021823,
abstract = {We introduce a complexity-theoretic model for studying computational security of binary image watermarking systems. Our model restricts algorithms used by the sender and the attacker to the class H of hiding functions. These are efficiently computable functions that preserve visual fidelity of the input image. Security of watermarking systems is to be established with complexity results about hiding functions. We also survey current theories of vision and propose an automata-theoretic model for visual fidelity called c-similarity. Finally we propose a candidate for H based on c-similarity and show that it is robust and contains infinitely many functions computable in polynomial time.},
author = {Tran, N},
booktitle = {Computer Security Foundations Workshop, 2002. Proceedings. 15th IEEE},
doi = {10.1109/CSFW.2002.1021823},
issn = {1063-6900},
keywords = {automata-theoretic model,c-similarity,complexity},
pages = {295--303},
title = {{Hiding functions and computational security of image watermarking systems}},
year = {2002}
}
@misc{OWASP2013,
author = {OWASP},
institution = {OWASP},
pages = {http://goo.gl/umM0Br},
title = {{Top 10 2013, http://goo.gl/umM0Br}},
url = {http://goo.gl/umM0Br},
year = {2013}
}
@article{McCarthy2002,
author = {McCarthy, Bill},
journal = {Annual Review of Sociology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {417--442},
title = {{New Economics of Sociolgical Criminology}},
volume = {28},
year = {2002}
}
@inproceedings{Li1998,
abstract = {The number of defects is an important measure of software quality which is widely used in industry. Unfortunately, accurate estimation of defect density can be a difficult task. Sampling techniques generally assume that the faults found are a representative sample of the all existing faults, which results in inaccurate estimates. Other existing techniques provide little information in addition to the number of faults already found. Software test coverage tools can easily and accurately measure the extent to which the software has been exercised. Both testing time and test coverage can be used as measures to model the defect finding process. However test coverage is a more direct measure of test effectiveness and can be expected to correlate better with the number of defects found. Here we describe a simple and intuitive procedure which can be used to estimate the total number of residual defects, once a suitable coverage level has been achieved. The technique is consistent with commo...},
author = {Li, Mn and Malaiya, Yk and Denton, Jason},
booktitle = {Proc. 7th Int'l Symposium on Software Reliability Engineering (ISSRE)},
pages = {307--315},
title = {{Estimating the number of defects: a simple and intuitive approach}},
url = {http://www.cs.colostate.edu/pubserv/pubs/Li-malaiya-p-li98.pdf},
year = {1998}
}
@article{Maier2008,
abstract = {The constructive collaboration of engineers and doctors is the basis for successful medical products for acceptable clinical surgery. The surgical micro-manipulator "MicroMan" was developed jointly by the Technical University of Munich and the Leipzig (Hals Nasen Ohren, HNO) Ears Nose and Throat Clinic. The rapid progress of its development has been spurred by the interplay of ideas between doctors and engineers. The discussion covers introduction; overcoming communication hurdles; engineers in the operating room; practical test: First use in the operating room (OP); using know-how to reduce the complexity; and summary. The "MicroMan" micro-manipulator was recently used at the HNO-Klinik Leipzig at a so-called tympanoplastic Type III Operation. Damaged eardrum was supplemented with a cartilage connective tissue transplant such that the closure of the middle ear Space (drum) was remade. Also, a titanium prosthesis of 2.5 mm dia was placed in the middle ear by telemanipulation. {\^{A}}{\textcopyright} Carl Hanser Verlag.},
annote = {Forging a link between the doctor and the engineer. Interdisciplinary development of medical products},
author = {Maier, T},
journal = {F und M Mechatronik},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1-2},
pages = {22--24},
title = {{Forging a link between the doctor and the engineer. Interdisciplinary development of medical products}},
url = {citeulike-article-id:3934702 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-47749132271{\&}{\#}38 partnerID=40},
volume = {116},
year = {2008}
}
@article{Jakopec03,
author = {Jakopec, M and y Baena, F Rodriguez and Harris, S J and Gomes, P and Cobb, J and Davies, B L},
journal = {IEEE Trans. on Robotics and Automation},
month = {oct},
number = {5},
pages = {902--911},
title = {{The Hands-On Orthopaedic Robot "Acrobot": Early Clinical Trials of Total Knee Replacement Surgery}},
volume = {19},
year = {2003}
}
@inproceedings{Nagappan2008,
abstract = {Often software systems are developed by organizations consisting of many teams of individuals working together. Brooks states in the Mythical Man Month book that product quality is strongly affected by organization structure. Unfortunately there has been little empirical evidence to date to substantiate this assertion. In this paper we present a metric scheme to quantify organizational complexity, in relation to the product development process to identify if the metrics impact failure-proneness. In our case study, the organizational metrics when applied to data from Windows Vista were statistically significant predictors of failure-proneness. The precision and recall measures for identifying failure-prone binaries, using the organizational metrics, was significantly higher than using traditional metrics like churn, complexity, coverage, dependencies, and pre-release bug measures that have been used to date to predict failure-proneness. Our results provide empirical evidence that the organizational metrics are related to, and are effective predictors of failure-proneness. Copyright 2008 ACM.},
address = {Leipzig, Germany},
author = {Nagappan, Nachiappan and Murphy, Brendan and Basili, Victor},
booktitle = {Proceedings of the 13th international conference on Software engineering - ICSE '08},
doi = {10.1145/1368088.1368160},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagappan, Murphy, Basili - 2008 - The influence of organizational structure on software quality.pdf:pdf},
isbn = {9781605580791},
pages = {521--530},
title = {{The influence of organizational structure on software quality}},
url = {http://portal.acm.org/citation.cfm?doid=1368088.1368160},
year = {2008}
}
@inproceedings{Schroeder2006,
author = {Schroeder, Bianca and Gibson, Garth A},
booktitle = {Proc.$\backslash$ International Conference on Dependable Systems and Networks},
isbn = {0-7695-2607-1},
pages = {249--258},
title = {{A large-scale study of failures in high-performance computing systems}},
year = {2006}
}
@inproceedings{Carrow2007a,
address = {New York, New York, USA},
author = {Carrow, Erwin Louis},
booktitle = {Proceedings of the 4th annual conference on Information security curriculum development - InfoSecCD '07},
doi = {10.1145/1409908.1409923},
isbn = {9781595939098},
keywords = {black holes,botnets,honeymoles,honeynets,honeypots,puppetnets,security threat gateway (STG),user space},
month = {sep},
pages = {1},
publisher = {ACM Press},
title = {{Puppetnets and botnets}},
url = {http://dl.acm.org/citation.cfm?id=1409908.1409923},
year = {2007}
}
@article{ComputingResearchAssociation2007,
author = {{Computing Research Association}},
number = {3},
pages = {7--22},
title = {{2005-2006 Taulbee Survey}},
volume = {19},
year = {2007}
}
@inproceedings{Lin2012,
author = {Lin, Jialiu and Sadeh, Norman and Amini, Shahriyar and Lindqvist, Janne and Hong, Jason I. and Zhang, Joy},
booktitle = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing - UbiComp '12},
doi = {10.1145/2370216.2370290},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin et al. - 2012 - Expectation and purpose.pdf:pdf},
isbn = {9781450312240},
keywords = {Android permissions,agile,crowdsourcing,mental model,mobile app,nsf,privacy as expectations,privacy summary},
mendeley-tags = {agile,nsf},
month = {sep},
pages = {501},
publisher = {ACM Press},
title = {{Expectation and purpose}},
url = {http://dl.acm.org/citation.cfm?id=2370216.2370290},
year = {2012}
}
@article{Grenning2007,
abstract = {The Test Driven Development (TDD) has been increasing in information technology applications and product development. The TDD is applied as unit tests and as acceptance tests. The former provides feedback to the developer about the code written while the latter, operate on integrated groups of modules in showing that the software meets its requirements. A number of challenges facing the embedded software developers such as hardware bottleneck, development system and target compiler compatibility, testable design, testing with the hardware, testing in limited-memory environments, and test cycle time and tool chain. Thus, TDD provides benefits to the developers and the development team, including enhanced predictability, repeatability, and reduced debugging time. The TDD help embedded developers deliver higher quality products, as well as help take hardware availability off the software critical path, enabling steady progress with or without hardware. Additionally, the use of TDD improves test coverage and find defects earlier in the development life cycle.},
annote = {Applying test driven development to embedded software},
author = {Grenning, J},
journal = {IEEE Instrumentation and Measurement Magazine},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {20--25},
title = {{Applying test driven development to embedded software}},
url = {citeulike-article-id:3934628 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-39649123512{\&}{\#}38 partnerID=40},
volume = {10},
year = {2007}
}
@incollection{Pietraszek2004,
address = {Berlin, Heidelberg},
author = {Pietraszek, Tadeusz},
booktitle = {Recent Advances in Instrusion Detection},
doi = {10.1007/b100714},
editor = {Jonsson, Erland and Valdes, Alfonso and Almgren, Magnus},
isbn = {978-3-540-23123-3},
pages = {102--124},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Using Adaptive Alert Classification to Reduce False Positives in Intrusion Detection}},
volume = {3224},
year = {2004}
}
@book{Stake1995,
address = {Thousand Oaks, CA},
author = {Stake, Robert E},
publisher = {Sage Publications, Inc.},
title = {{The Art of Case Study Research}},
year = {1995}
}
@article{Chittaranjan2011,
author = {Chittaranjan, Gokul and Blom, Jan and Gatica-Perez, Daniel},
doi = {10.1007/s00779-011-0490-1},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chittaranjan, Blom, Gatica-Perez - 2011 - Mining large-scale smartphone data for personality studies.pdf:pdf},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {Big-Five,Lausanne data collection campaign,Personality,Smartphones,agile,nsf},
mendeley-tags = {agile,nsf},
month = {dec},
number = {3},
pages = {433--450},
publisher = {Springer-Verlag},
title = {{Mining large-scale smartphone data for personality studies}},
url = {http://dl.acm.org/citation.cfm?id=2444516.2444522},
volume = {17},
year = {2011}
}
@inproceedings{1532071,
abstract = { While efficient graph-based representations have been developed for modeling combinations of low-level network attacks, relatively little attention has been paid to effective techniques for visualizing such attack graphs. This paper describes a number of new attack graph visualization techniques, each having certain desirable properties and offering different perspectives for solving different kinds of problems. Moreover, the techniques we describe can be applied not only separately, but can also be combined into coordinated attack graph views. We apply improved visual clustering to previously described network protection domains (attack graph cliques), which reduces graph complexity and makes the overall attack flow easier to understand. We also visualize the attack graph adjacency matrix, which shows patterns of network attack while avoiding the clutter usually associated with drawing large graphs. We show how the attack graph adjacency matrix concisely conveys the impact of network configuration changes on attack graphs. We also describe a novel attack graph filtering technique based on the interactive navigation of a hierarchy of attack graph constraints. Overall, our techniques scale quadratically with the number of machines in the attack graph.},
author = {Noel, S and Jacobs, M and Kalapa, Pramod and Jajodia, Sushil},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532071},
keywords = {coordinated attack graph views,graph adjacency ma},
pages = {99--106},
title = {{Multiple coordinated views for network attack graphs}},
year = {2005}
}
@inproceedings{Atkinson2005,
abstract = {Component-based development has yet to make a big impact in software engineering as it has in other engineering disciplines because the components available for reuse are relatively much more primitive. This means that reused components usually account for a much smaller proportion of the overall intellectual effort invested in new software products than they do in other kinds of products. To rectify this situation more advanced development methodologies are required that encourage and enable software engineers to exploit richer software components - sometimes called "business" components - in the development of new applications. This, in turn, requires more advanced techniques for specifying and retrieving prefabricated components. In this paper we present some initial steps towards such a methodology based on the integration of two independent but complementary technologies - model-based component modeling and test-driven component harvesting. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {Towards a methodology for component-driven design},
author = {Atkinson, C and Hummel, O},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {23--33},
title = {{Towards a methodology for component-driven design}},
url = {citeulike-article-id:3934544 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-24944533147{\&}{\#}38 partnerID=40},
volume = {3475},
year = {2005}
}
@inproceedings{Wang2004b,
address = {New York, New York, USA},
author = {Wang, Weichao and Bhargava, Bharat},
booktitle = {Proceedings of the 2004 ACM workshop on Wireless security - WiSe '04},
doi = {10.1145/1023646.1023657},
isbn = {158113925X},
keywords = {multi-dimensional scaling,sensor networks,visualization,wormhole attacks},
month = {oct},
pages = {51},
publisher = {ACM Press},
title = {{Visualization of wormholes in sensor networks}},
url = {http://dl.acm.org/citation.cfm?id=1023646.1023657},
year = {2004}
}
@inproceedings{766714,
abstract = {In recent years, packet filtering firewalls have seen some impressive technological advances (e.g., stateful inspection, transparency, performance, etc.) and widespread deployment. In contrast, firewall and security management technology is lacking. We present Firmato, a firewall management toolkit, with the following distinguishing properties and components: (1) an entity relationship model containing, in a unified form, global knowledge of the security policy and of the network topology; (2) a model definition language, which we use as an interface to define an instance of the entity relationship model; (3) a model compiler translating the global knowledge of the model into firewall-specific configuration files; and (4) a graphical firewall rule illustrator. We demonstrate Firmato's capabilities on a realistic example, thus showing that firewall management can be done successfully at an appropriate level of abstraction. We implemented our toolkit to work with a commercially available firewall product. We believe that our approach is an important step towards streamlining the process of configuring and managing firewalls, especially in complex, multi firewall installations},
author = {Bartal, Y. and Mayer, a. and Nissim, K. and Wool, a.},
booktitle = {Proceedings of the 1999 IEEE Symposium on Security and Privacy (Cat. No.99CB36344)},
doi = {10.1109/SECPRI.1999.766714},
isbn = {0-7695-0176-1},
keywords = {Firmato,abstraction,commercially available firewal},
pages = {17--31},
publisher = {IEEE Comput. Soc},
title = {{Firmato: a novel firewall management toolkit}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=766714},
year = {1999}
}
@article{shepperd12a,
author = {Shepperd, Martin J and MacDonell, Steven G},
journal = {Information {\{}{\&}{\}} Software Technology},
number = {8},
pages = {820--827},
title = {{Evaluating prediction systems in software project estimation}},
volume = {54},
year = {2012}
}
@article{1607919,
abstract = { IDGraphs is an interactive visualization system, supporting intrusion detection over massive network traffic streams. It features a novel time-versus-failed-connections mapping that aids in discovery of attack patterns. The number of failed connections (SYN-SYN/ACK) is a strong indicator of suspicious network flows. IDGraphs offers several flow aggregation methods that help reveal different attack patterns. The system also offers high visual scalability through the use of Histographs. The IDGraphs intrusion detection system detects and analyzes a variety of attacks and anomalies, including port scanning, worm outbreaks, stealthy TCP SYN flooding, and some distributed attacks. In this article, we demonstrate IDGraphs using a single day of NetFlow network traffic traces collected at edge routers at Northwestern University which has several OC-3 links.},
author = {Ren, Pin and Gao, Yan and Li, Zhichun and Chen, Yan and Watson, B},
doi = {10.1109/MCG.2006.36},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {Computer-Assisted,IDGraphs,NetFlow network traffic,SYN-SYN/ACK,Software,TC,User-Computer Interfac},
number = {2},
pages = {28--39},
title = {{IDGraphs: intrusion detection and analysis using stream compositing}},
volume = {26},
year = {2006}
}
@inproceedings{5318898,
abstract = {A grid system is a dynamic and distributed environment which contains mass entities and resources, and is involved in large-scale data exchanges and resource sharing. Thus in grid system, information security is a prime concern. Trust mechanism is a main part of grid security. Taking the fuzziness of behavior trust into consideration, this paper introduces fuzzy theory to grid trust and proposes a novel trust model in order to better manage trust relationships between grid entities, presenting the calculation of direct and recommended trust based on fuzzy comprehensive evaluation. Especially, the paper adds the definition mechanism of trust class based on fuzzy cluster analysis to the trust model, achieving dynamic and better visualized definition of trust class. Case study shows that the proposed model can efficiently evaluate entity trust and give clear, accurate judgment upon entities' trust class, which contributes to the establishment of access control policies.},
author = {Zhang, Shaomin and Zou, Yue and Wang, Baoyi},
booktitle = {Network and System Security, 2009. NSS '09. Third International Conference on},
doi = {10.1109/NSS.2009.10},
keywords = {access control,dynamic-distributed environment,fuz},
pages = {203--207},
title = {{A Novel Grid Trust Model Based on Fuzzy Theory}},
year = {2009}
}
@inproceedings{6092312,
abstract = {Cyber analysts are faced with the daunting challenge of identifying exploits and threats within potentially billions of daily records of network traffic. Enterprise-wide cyber traffic involves hundreds of millions of distinct IP addresses and results in data sets ranging from terabytes to petabytes of raw data. Creating behavioral models and identifying trends based on those models requires data intensive architectures and techniques that can scale as data volume increases. Analysts need scalable visualization methods that foster interactive exploration of data and enable identification of behavioral anomalies. Developers must carefully consider application design, storage, processing, and display to provide usability and interactivity with large-scale data. We present an application that highlights atypical behavior in enterprise network flow records. This is accomplished by utilizing data intensive architectures to store the data, aggregation techniques to optimize data access, statistical techniques to characterize behavior, and a visual analytic environment to render the behavioral trends, highlight atypical activity, and allow for exploration.},
author = {Best, D M and Hafen, R P and Olsen, B K and Pike, W A},
booktitle = {Large Data Analysis and Visualization (LDAV), 2011 IEEE Symposium on},
doi = {10.1109/LDAV.2011.6092312},
keywords = {IP address,behavior identification,behavioral anom},
pages = {15--22},
title = {{Atypical behavior identification in large-scale network traffic}},
year = {2011}
}
@inproceedings{Morrison2011a,
address = {Vancouver, BC},
author = {Morrison, Alistair and Brown, Owain and McMillan, Donald and Chalmers, Matthew},
booktitle = {Proceedings of the 29th Annual Conference on Human Factors in Computing Systems (Extended Abstracts) (CHI EA '11)},
doi = {10.1145/1979742.1979798},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Morrison et al. - 2011 - Informed consent and users' attitudes to logging in large scale trials.pdf:pdf},
isbn = {9781450302685},
keywords = {agile,app store,ethics,mass participation,nsf,user trials},
mendeley-tags = {agile,nsf},
month = {may},
pages = {1501--1506},
publisher = {ACM Press},
title = {{Informed consent and users' attitudes to logging in large scale trials}},
url = {http://dl.acm.org/citation.cfm?id=1979742.1979798},
year = {2011}
}
@article{Maimon2012,
author = {Maimon, David and Antonaccio, Olena and French, Michael T.},
doi = {10.1111/j.1745-9125.2011.00268.x},
issn = {00111384},
journal = {Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {may},
number = {2},
pages = {495--524},
title = {{Severe Sanctions, Easy Choice? Investigating the Role of School Sanctions in Preventing Adolescent Violent Offending}},
url = {http://doi.wiley.com/10.1111/j.1745-9125.2011.00268.x},
volume = {50},
year = {2012}
}
@inproceedings{Gupta2007,
abstract = {Test Driven Development (TDD) is an approach for developing programs incrementally by first writing tests and then writing enough code to satisfy them. Though there have been some experiments for evaluating TDD on smaller scope, its impact on a larger scope of program development activities needs to be investigated. In this work, we evaluate the impact of TDD on various program development activities like designing, coding, and testing, through a controlled experiment where we compare it with the conventional way of developing the code. In a single-factor block design, two groups of students developed two moderately sized programs following the two development-styles under study. Our results suggest that TDD helps in reducing overall development effort and improving developer's productivity whereas the code quality seems to be affected by the actual testing efforts applied during a development-style. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {An experimental evaluation of the effectiveness and efficiency of the test driven development},
author = {Gupta, A and Jalote, P},
booktitle = {Proceedings - 1st International Symposium on Empirical Software Engineering and Measurement, ESEM 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {285--294},
title = {{An experimental evaluation of the effectiveness and efficiency of the test driven development}},
url = {citeulike-article-id:3934631 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-47949108690{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Miller2004,
abstract = {Test driven development (also known as test-first development) is a technique associated with Extreme Programming and Agile Programming methods.[1] Most advocates of test driven development use automated testing tools to facilitate bookkeeping and to encourage frequent and thorough regression testing throughout development. These tools (such as JUnit, NUnit and XUnit) are increasingly popular, and deservedly so. However, in this paper, we suggest a simplified approach that may sometimes be preferable when introducing the idea of test driven programming: using a textfile and a straightforward testing harness. This technique will probably be of limited use to experienced test driven developers, but novices (both students and faculty) may find it a simpler way to understand how test-driven development works without having to install and learn an automated testing system.},
annote = {Test driven development on the cheap: text files and explicit scaffolding},
author = {Miller, K W},
journal = {Journal of Computing Sciences in Colleges},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {181--189},
title = {{Test driven development on the cheap: text files and explicit scaffolding}},
url = {citeulike-article-id:3934720 http://portal.acm.org/citation.cfm?id=1040172},
volume = {20},
year = {2004}
}
@inproceedings{Grottke2010,
author = {Grottke, Michael and Nikora, Allen P and Trivedi, Kishor S},
booktitle = {Proc.$\backslash$ 40th Annual IEEE/IFIP International Conference on Dependable Systems and Networks},
isbn = {978-1-4244-7500-1},
pages = {447--456},
title = {{An empirical investigation of fault types in space mission system software}},
year = {2010}
}
@book{Larman2004,
address = {Boston},
author = {Larman, Craig},
publisher = {Addison Wesley},
title = {{Agile and Iterative Development:  A Manager's Guide}},
year = {2004}
}
@inproceedings{Fainekos2009,
abstract = {This paper proposes a framework for determining the correctness and robustness of simulations of hybrid systems. The focus is on simulations generated from model-based design environments and, in particular, Simulink. The correctness and robustness of the simulation is guaranteed against floating-point rounding errors and system modeling uncertainties. Toward that goal, self-validated arithmetics, such as interval and affine arithmetic, are employed for guaranteed simulation of discrete-time hybrid systems. In the case of continuous-time hybrid systems, self-validated arithmetics are utilized for over-approximations of reachability computations.},
author = {Fainekos, Georgios and Sankaranarayanan, Sriram and Ivancic, Franjo and Gupta, Aarti},
booktitle = {2009 30th IEEE Real-Time Systems Symposium},
keywords = {Safety},
mendeley-tags = {Safety},
month = {dec},
pages = {345--354},
publisher = {IEEE},
title = {{Robustness of Model-Based Simulations}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5368184},
year = {2009}
}
@book{Jacobs1961,
address = {New York, NY},
author = {Jacobs, Jane},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Vintage Books},
title = {{The Death and Life of Great American Cities}},
year = {1962}
}
@inproceedings{Ceravolo2003,
address = {Chiangmai, Thailand},
author = {Ceravolo, P. and Damiani, E. and Marchesi, M. and Pinna, S. and Zavatarelli, F.},
booktitle = {Tenth Asia-Pacific Software Engineering Conference, 2003.},
doi = {10.1109/APSEC.2003.1254376},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ceravolo et al. - 2003 - A ontology-based process modelling for XP.pdf:pdf},
isbn = {0-7695-2011-1},
keywords = {agile,agile methodologies,am,and extreme programming,answer to this problem,but there is,claim to pro-,extreme pro-,gies,gramming methodology,ontology,software engineering,vide a first general,xp},
mendeley-tags = {agile},
pages = {236--242},
publisher = {IEEE Computer Society},
title = {{A ontology-based process modelling for XP}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1254376},
year = {2003}
}
@inproceedings{1532069,
abstract = { This paper explores the application of visualization techniques to aid in the analysis of malicious and non-malicious binary objects. These objects may include any logically distinct chunks of binary data such as image files, word processing documents and network packets. To facilitate this analysis, we present a novel visualization technique for comparing and navigating among 600-1000+ such objects at one time. While the visualization technique alone has powerful application for both directed and undirected exploration of many classes of binary objects, we chose to study network packets. To increase effectiveness, we strengthened the visualization technique with novel, domain-specific semantic zooming, interactive encoding and dynamic querying capabilities. We present results and lessons learned from implementing these techniques and from studying both malicious and non-malicious network packets. Our results indicate that the information visualization system we present is an efficient and effective way to compare large numbers of network packets, visually examine their payloads and navigate to areas of interest within large network datasets.},
author = {Conti, G and Grizzard, J and Ahamad, Mustaque and Owen, H},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532069},
keywords = {binary objects,domain-specific semantic zooming},
pages = {83--90},
title = {{Visual exploration of malicious network objects using semantic zoom, interactive encoding and dynamic queries}},
year = {2005}
}
@inproceedings{Layman2007c,
address = {Covington, KY},
author = {Layman, Lucas and Williams, Laurie and Slaten, Kelli},
booktitle = {Proceedings of the 28th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/1227504.1227466},
isbn = {1-59593-361-1},
issn = {00978418},
keywords = {CS1,mypubs,programming assignments,software engineering education},
mendeley-tags = {mypubs},
month = {mar},
pages = {459--463},
publisher = {ACM},
title = {{Note to self: Make Assignments Meaningful}},
year = {2007}
}
@article{Butt2008,
author = {Butt, Sarah and Phillips, James G.},
doi = {10.1016/j.chb.2007.01.019},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Agreeableness,Extraversion,Mobile phones,Personality,SMS,agile,nsf},
mendeley-tags = {agile,nsf},
month = {mar},
number = {2},
pages = {346--360},
publisher = {Elsevier Science Publishers B. V.},
title = {{Personality and self reported mobile phone use}},
url = {http://dl.acm.org/citation.cfm?id=1332141.1332400},
volume = {24},
year = {2008}
}
@inproceedings{Nawrocki2001,
address = {Warsaw, Poland},
author = {Nawrocki, Jerzy and Walter, Bartosz and Wojciechowski, Adam},
booktitle = {Euromicro Conference, 2001. Proceedings. 27th},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nawrocki, Walter, Wojciechowski - 2001 - Toward maturity model for extreme programming.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
pages = {233--239},
publisher = {IEEE},
title = {{Toward maturity model for extreme programming}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Toward+MAturity+Model+for+eXtreme+Programming{\#}0},
year = {2001}
}
@inproceedings{Yu2005,
abstract = {Mobile applications are increasingly location-based; i.e. their functionality is becoming both interactive and context-aware. Combined with an overall increase in the complexity of the devices delivering such services, and a growth in the number of possible networks that they can participate in, these systems require more than just the average approach to testing. The principles and practices of agile testing may serve development teams well here; since the systems ultimately end up being developed and deployed in an iterative and evolutionary manner. In this paper, we explore a testing framework for location-based services that can be employed test-first and yet also offers the full range of non-functional tests that these applications require. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {Agile testing of location based services},
author = {Yu, J and Tappenden, A and Geras, A and Smith, M and Miller, J},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {239--242},
title = {{Agile testing of location based services}},
url = {citeulike-article-id:3934849 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444591046{\&}{\#}38 partnerID=40},
volume = {3556},
year = {2005}
}
@inproceedings{Tao2012,
address = {Cary, NC},
author = {Tao, Yida and Dang, Yingnong and Xie, Tao and Zhang, Dongmei and Kim, Sunghun},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering - FSE '12},
doi = {10.1145/2393596.2393656},
isbn = {9781450316149},
pages = {Article No. 51},
title = {{How do software engineers understand code changes?}},
url = {http://dl.acm.org/citation.cfm?doid=2393596.2393656},
year = {2012}
}
@article{Botaschanjan2004a,
abstract = {This paper discusses a model-based approach to validate software requirements in agile development processes by simulation and in particular automated testing. The use of models as central development artifact needs to be added to the portfolio of software engineering techniques, to further increase efficiency and flexibility of the development beginning already early in the requirements definition phase. Testing requirements are some of the most important techniques to give feedback and to increase the quality of the result. Therefore testing of artifacts should be introduced as early as possible, even in the requirements definition phase.},
author = {Botaschanjan, Jewgenij and Pister, Markus and Rumpe, Bernhard},
issn = {1009-3095},
journal = {Journal of Zhejiang University. Science},
keywords = {model-based testing,requirements,requirements evolution,uml},
month = {may},
number = {5},
pages = {587--93},
pmid = {15083546},
title = {{Testing agile requirements models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15083546},
volume = {5},
year = {2004}
}
@inproceedings{6128387,
abstract = {Visualization modeling based on DAQ (Data Acquisition) of SHM (Structural Health Monitoring) system helps nondestructive and Intelligent evaluation on structure. It makes up the shortage of traditional visual methods and speeds up mastering the state of structural health. It promotes the security of structure greatly. Using simulation data from the Key Laboratory of Structural Engineering and Earthquake Resistance, the DynaTree method completes dynamic data visualization on SHM with Open Inventor toolkit.},
author = {Sun, Peng and Guo, Xin},
booktitle = {Computational Intelligence and Security (CIS), 2011 Seventh International Conference on},
doi = {10.1109/CIS.2011.346},
keywords = {DynaTree method,Openlnventor toolkit,data acquisit},
pages = {1546--1548},
title = {{Structural Health Monitoring via Dynamic Visualization}},
year = {2011}
}
@article{Rinderknecht2014,
abstract = {We survey the literature about the teaching and learning of recursive programming. After a short history of the advent of recursion in programming languages and its adoption by programmers, we present curricular approaches to recursion, including a review of textbooks and some programming methodology, as well as the functional and imperative paradigms and the distinction between control flow vs. data flow. We follow the researchers in stating the problem with base cases, noting the similarity with induction in mathematics, making concrete analogies for recursion, using games, visualizations, animations, multimedia environments, intelligent tutoring systems and visual programming. We cover the usage in schools of the Logo programming language and the associated theoretical didactics, including a brief overview of the constructivist and constructionist theories of learning; we also sketch the learners' mental models which have been identified so far, and non-classical remedial strategies, such as kinesthesis and syntonicity. We append an extensive and carefully collated bibliography, which we hope will facilitate new research. {\textcopyright} 2014 Vilnius University.},
author = {Rinderknecht, Christian},
doi = {10.15388/infedu.2014.06},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rinderknecht - 2014 - A Survey on Teaching and Learning Recursive Programming.pdf:pdf},
issn = {16485831},
journal = {Informatics in Education},
keywords = {Computer science education,Didactics of programming,Embedded recursion,Iteration,Loop,Mental models,Recursion,Tail recursion},
number = {1},
pages = {87--119},
title = {{A Survey on Teaching and Learning Recursive Programming}},
volume = {13},
year = {2014}
}
@article{Maimon2015,
abstract = {We employ knowledge regarding the early phases of system trespassing events and develop a context-related, theoretically driven study that explores computer networks' social vulnerabilities to remote system trespassing events. Drawing on the routine activities perspective, we raise hypotheses regarding the role of victim client computers in determining the geographical origins and temporal trends of (1) successful password cracking attempts and (2) system trespassing incidents. We test our hypotheses by analyzing data collected from large sets of target computers, built for the sole purpose of being attacked, that were deployed in two independent research sites (China and Israel). Our findings have significant implications for cyber-criminological theory and research.},
author = {Maimon, David and Wilson, Theodore and Ren, Wuling and Berenblum, Tamar},
doi = {10.1093/bjc/azu104},
issn = {0007-0955},
journal = {British Journal of Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {may},
number = {3},
pages = {615--634},
title = {{On the Relevance of Spatial and Temporal Dimensions in Assessing Computer Susceptibility to System Trespassing Incidents}},
url = {http://bjc.oxfordjournals.org/content/early/2015/01/05/bjc.azu104.short},
volume = {55},
year = {2015}
}
@inproceedings{Gutwin2004,
address = {New York, New York, USA},
author = {Gutwin, Carl and Penner, Reagan and Schneider, Kevin},
booktitle = {Proceedings of the 2004 ACM conference on Computer supported cooperative work - CSCW '04},
doi = {10.1145/1031607.1031621},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gutwin, Penner, Schneider - 2004 - Group awareness in distributed software development.pdf:pdf},
isbn = {1581138105},
keywords = {collaboration,collaborative software development,group awareness,oss},
mendeley-tags = {collaboration},
pages = {72--81},
publisher = {ACM Press},
title = {{Group awareness in distributed software development}},
url = {http://portal.acm.org/citation.cfm?doid=1031607.1031621},
year = {2004}
}
@inproceedings{Kontio1998,
address = {New York, New York, USA},
author = {Kontio, Jyrki and Getto, Gerhard and Landes, Dieter},
booktitle = {Proceedings of the 6th ACM SIGSOFT international symposium on Foundations of software engineering - SIGSOFT '98/FSE-6},
doi = {10.1145/288195.288301},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kontio, Getto, Landes - 1998 - Experiences in improving risk management processes using the concepts of the Riskit method.pdf:pdf},
isbn = {1581131089},
issn = {0163-5948},
keywords = {empirical study,project management,risk management},
month = {nov},
number = {6},
pages = {163--174},
publisher = {ACM Press},
title = {{Experiences in improving risk management processes using the concepts of the Riskit method}},
url = {http://dl.acm.org/citation.cfm?id=288195.288301},
volume = {23},
year = {1998}
}
@techreport{Commission2012,
address = {Washington, DC},
author = {Commission, Federal Trade},
institution = {US Federal Trade Commission},
title = {{Mobile Apps for Kids: Disclosures Still Not Making the Grade}},
year = {2012}
}
@article{Rahman2005,
abstract = {Web applications (WebApps) are fetching more widespread and increasingly more sophisticated, and as such they are vital to almost all major online businesses. WebApps are generally complex and highly dynamic as stated in R. S. Pressman (2003), authors believe that traditional software development methods are not suitable in many WebApps. In this paper, we have discussed the suitability of test-driven development (TDD) for WebApps. We describe the factors involved of adopting TDD and explain why a manager should adopt TDD. We consider different factors e.g. change of requirements, market pressure and competition, testing issues, learning curve, stakeholders' involvements, re-usability, nature of problem domain, etc. We have made recommendations, which guide a manager to choose a software development method in his/her next project},
annote = {Adopting test-driven development in Web applications developments},
author = {Rahman, S M and Salah, A},
journal = {Proceedings of the ISCA 20th International Conference Computers and their Applications},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {361--366},
title = {{Adopting test-driven development in Web applications developments}},
url = {citeulike-article-id:3934761 {\#}},
year = {2005}
}
@inproceedings{Sahami2016,
abstract = {In recent years, enrollments in undergraduate computer science programs have seen tremendous growth nationally. Often accompanying such growth is a concern from faculty that the additional students choosing to pursue computing may not have the same aptitude for the subject as was seen in prior student populations. Thus such students may exhibit weaker performance in computing courses. To help address this question, we present a statistical analysis using mixture modeling of students' performance in an introductory programming class at Stanford University over an eight year period, during which enrollments in the course more than doubled. Importantly, in this setting many variables that would normally confound such a study are directly controlled for. We find that the distribution of student performance during this period, as reflected in their programming assignment scores, remains remarkably stable despite the large growth in enrollment. We then explain how the notion of having “more weak students” and the fact that the distribution of student ability is unchanged can readily co-exist and lead to misperceptions about the quality of incoming students during an enrollment boom.},
address = {Memphis, TN, USA},
author = {Sahami, Mehran and Piech, Chris},
booktitle = {SIGCSE 2016 - Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
doi = {10.1145/2839509.2844621},
isbn = {9781450338561},
keywords = {CS2,Enrollment growth,Introductory programming,Mixture modeling,Student performance},
mendeley-tags = {CS2},
pages = {54--59},
publisher = {ACM Press},
title = {{As CS enrollments grow, are we attracting weaker students? A statistical analysis of student performance in introductory programming courses over time}},
url = {http://dl.acm.org/citation.cfm?doid=2839509.2844621},
year = {2016}
}
@misc{Meyer2007,
abstract = {In spite of cultural difference between the corresponding scientific communities, recognition is growing that test-based and specification-based approaches to software development actually complement each other. The revival of interest in testing tools and techniques follows in particular from the popularity of "Test-Driven Development"; rigorous specification and proofs have, for their part, also made considerable progress. There remains, however, a fundamental superiority of specifications over test: you can derive tests from a specification, but not the other way around. Contract-Driven Development is a new approach to systematic software construction combining ideas from Design by Contract, from TestDriven Development, from work on formal methods, and from advances in automatic testing as illustrated for example in our AutoTest tool. Like TDD it gives tests a central role in the development process, but these tests are deduced from possibly partial specifications (contracts) and directly supported by the development environment. This talk will explain the concepts and demonstrate their application. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Contract-driven development},
author = {Meyer, B and Leitner, A},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {11},
title = {{Contract-driven development}},
url = {citeulike-article-id:3934718 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-37149039638{\&}{\#}38 partnerID=40},
volume = {4422 LNCS},
year = {2007}
}
@inproceedings{Carlsson2006,
abstract = {In recent years, agile development methods have become increasingly popular, and although few people seem willing to go all the way with Extreme Programming, it seems that there is a general consensus today that test-driven development with unit testing is a Good Thing. However, this requires that tests are easy to write, since programmers are generally lazy, and that running the tests is easy and quick and the test results are presented in a concise manner, to make the feedback loop as short as possible. Failing this, it is likely that testing will not be extensively used during development. The concept of a lightweight unit testing framework that fulfils these goals, tailored to a particular programming language, was popularized by the JUnit framework for Java, written by Kent Beck and Erich Gamma. This was based on an earlier framework for Smalltalk called SUnit, by Kent Beck. The ideas in JUnit are easily transferred to any other object-oriented language, and today variants have been written for many different programming languages. We will here present our adaptation of these ideas to Erlang. The EUnit framework provides the usual features of such frameworks: writing tests is very easy indeed, and so is running them. Like most other similar frameworks, we rely on program introspection and naming conventions to reduce the amount of coding necessary to write tests. However, there are also some unusual aspects. Since Erlang is functional, rather than object oriented, it is not possible to use inheritance to provide basic test functionality, nor to use object instantiation to handle things like setup/teardown of contexts for tests. Instead, we base our system on a "language" for describing sets of tests, using mainly lists, tuples, and lambda expressions. We also make rather heavy use of preprocessor macros to allow more compact and readable notation. Because test descriptions are data, they can be easily combined, abstracted over, or even be generated on the fly. Lambda expressions allow subtests to be instantiated with setup/teardown contexts. Furthermore, the parallel and distributed nature of the Erlang language on one hand provides a challenge, but on the other hand gives us enormous power and flexibility. For instance, it is trivial to express in our test description language that a set of tests should be executed by a separate process, or on a specific machine, or all in parallel, or even as a number of subsets of parallel tests, with each subset running on a separate machine. Apart from providing easy distributed job control, this allows us to write unit tests that test the behaviour of parallel and distributed programs, something which otherwise tends to require so much coding for each test as to be impractical. Although EUnit is still under development, we feel that it has great potential, and that it uses some novel ideas that could be used to implement similar frameworks in other functional languages. EUnit is free software under the GNU Lesser General Public License. It has not yet been publicly released as of this writing.},
annote = {EUnit - A lightweight unit testing framework for erlang},
author = {Carlsson, R and R{\'{e}}mond, M},
booktitle = {Erlang'06 - Proceedings of the ACM SIGPLAN 2006 Erlang Workshop},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1},
title = {{EUnit - A lightweight unit testing framework for erlang}},
url = {citeulike-article-id:3934568 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33750979019{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@inproceedings{Snooke2004,
address = {Carcassonne, France},
author = {Snooke, Neal},
booktitle = {Proceedings of 15th International Workshop on the Principles of Diagnosis (DX04)},
pages = {221--226},
title = {{Model-based failure modes and effects analysis of software}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.1253{\&}rep=rep1{\&}type=pdf},
year = {2004}
}
@inproceedings{5670979,
abstract = {The bit-plane complexity segmentation (BPCS) steganography has good visual imperceptibility and high data embedding capacity, but the original BPCS method is vulnerable to the statistical analysis of complexity histogram. A modified BPCS method was proposed to enhance the security, but the deficiency still exists. Basing on the analysis of the original and modified BPCS method, an improved BPCS algorithm is proposed. While preserving the superiority of modified BPCS method, the improved method enhances the capability of histogram statistical analysis immunity, and increases the PSNR in some degree.},
author = {Shi, Peipei and Li, Zhaohui},
booktitle = {2010 International Conference on Multimedia Information Networking and Security},
doi = {10.1109/MINES.2010.87},
isbn = {978-1-4244-8626-7},
keywords = {BPCS steganography,bit-plane complexity segmentati},
pages = {388--391},
publisher = {Ieee},
title = {{An Improved BPCS Steganography Based on Dynamic Threshold}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5670979},
year = {2010}
}
@article{Briand2006,
address = {New York, New York, USA},
author = {Briand, Lionel C. and Labiche, Yvan and S{\'{o}}wka, Michal M.},
doi = {10.1145/1134285.1134300},
isbn = {1595933751},
journal = {Proceeding of the 28th international conference on Software engineering - ICSE '06},
keywords = {adequacy criteria,component,cots,uml},
pages = {92},
publisher = {ACM Press},
title = {{Automated, contract-based user testing of commercial-off-the-shelf components}},
url = {http://portal.acm.org/citation.cfm?doid=1134285.1134300},
year = {2006}
}
@book{Witten2005,
address = {San Francisco, CA},
author = {Witten, Ian H. and Frank, Eibe},
edition = {2nd},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Elsevier},
title = {{Data Mining: Practical Machine Learning Tools and Techniques}},
year = {2005}
}
@book{Gamma1995,
address = {Reading, Mass},
author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
publisher = {Addison-Wesley},
title = {{Design Patterns: Elements of Reusable Object-Oriented Software}},
year = {1995}
}
@inproceedings{Sharp2003,
address = {Keele, United Kingdom},
author = {Sharp, H and H Robinson},
title = {{An Ethnography of XP Practice}},
year = {2003}
}
@article{VonMayrhauser1999,
author = {von Mayrhauser, A and Vans, A M},
journal = {International Journal of Human-Computer Studies},
number = {1},
pages = {31--70},
title = {{Program understanding behavior during corrective maintenance of large-scale software}},
volume = {51},
year = {1999}
}
@article{Akaike,
author = {Akaike, H},
journal = {IEEE Transactions on Automatic Control},
number = {6},
pages = {716--723},
title = {{A new look at the statistical model identification}},
volume = {19},
year = {1974}
}
@article{Wirfs-Brock2008,
abstract = {Over the past 25 years, we've made great advances in tooling, technologies, and techniques that make software design more concrete. But design still requires careful thought. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Designing then and now},
author = {Wirfs-Brock, R J},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {29--31},
title = {{Designing then and now}},
url = {citeulike-article-id:3934841 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57049146452{\&}{\#}38 partnerID=40},
volume = {25},
year = {2008}
}
@book{Strauss1987,
address = {New York},
author = {Strauss, A.L.},
doi = {https://doi.org/10.1017/CBO9780511557842},
publisher = {Cambridge University Press},
title = {{Qualitative analysis for social scientists}},
year = {1987}
}
@inproceedings{sunshine2009crying,
author = {Sunshine, Joshua and Egelman, Serge and Almuhimedi, Hazim and Atri, Neha and Cranor, Lorrie Faith},
booktitle = {Proceedings of the 18th conference on USENIX security symposium},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
organization = {USENIX Association},
pages = {399--416},
title = {{Crying wolf: An empirical study of SSL warning effectiveness}},
year = {2009}
}
@incollection{Chi2000,
address = {Mahwah, New Jersey},
author = {Chi, Michelene T H and Glaser, Robert},
pages = {161--238},
publisher = {Lawrence Erlbaum Associates},
title = {{Self-Explaining:  The Dual Process of Generating Inference and Repairing Mental Models}},
year = {2000}
}
@inproceedings{Hummel2004,
abstract = {The reuse of software components is the key to improving productivity and quality levels in software engineering. However, although the technologies for plugging together components have evolved dramatically over the last few years (e.g. EJB, .NET, Web Services) the technologies for actually finding them in the first place are still relatively immature. In this paper we present a simple but effective approach for harvesting software components from the Internet. The initial discovery of components is achieved using a standard web search engine such as Google, and the evaluation of "fitness for purpose" is performed by automated testing. Since test-driven evaluation of software is the hallmark of Extreme Programming, and the approach naturally complements the extreme approach to software engineering, we refer to it as "Extreme Harvesting". The paper first explains the principles behind Extreme Harvesting and then describes a prototype implementation. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {Extreme harvesting: Test driven discovery and reuse of software components},
author = {Hummel, O and Atkinson, C},
booktitle = {Proceedings of the 2004 IEEE International Conference on Information Reuse and Integration, IRI-2004},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {66--72},
title = {{Extreme harvesting: Test driven discovery and reuse of software components}},
url = {citeulike-article-id:3934647 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-16244403981{\&}{\#}38 partnerID=40},
year = {2004}
}
@inproceedings{1565220,
abstract = {The following topics are dealt with: computer security applications; software security; network intrusion detection; security designs; protocol analysis; vulnerability assessment; automation; security analysis; operating system security mechanisms; data integrity; malware; distributed system security; access control; passwords and applied crypto; defense in depth/database security; privacy; security management; secure access; managing the enterprise; Internet security visualization; and security in healthcare},
booktitle = {Computer Security Applications Conference, 21st Annual},
doi = {10.1109/CSAC.2005.2},
issn = {1063-9527},
keywords = {Internet security visualization,access control,com},
pages = {c1},
title = {{Proceedings. 21st Annual Computer Security Applications Conference}},
year = {2005}
}
@inproceedings{Zazworka2010,
address = {Bolzano, Italy},
author = {Zazworka, Nico and Ackermann, Christopher},
booktitle = {Proceedings of the 2010 ACM-IEEE International Symposium on Empirical Software Engineering and Measurement - ESEM '10},
doi = {10.1145/1852786.1852865},
isbn = {9781450300391},
month = {sep},
pages = {Article 63},
title = {{CodeVizard}},
year = {2010}
}
@inproceedings{Gegick2009,
address = {Denver, CO},
author = {Gegick, Michael and Rotella, Pete and Williams, Laurie},
booktitle = {2009 International Conference on Software Testing Verification and Validation},
doi = {10.1109/ICST.2009.36},
isbn = {978-1-4244-3775-7},
month = {apr},
pages = {181--190},
title = {{Predicting Attack-prone Components}},
url = {http://ieeexplore.ieee.org/document/4815350/},
year = {2009}
}
@book{shneiderman1998designing,
author = {Shneiderman, Ben},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Pearson Education India},
title = {{Designing the user interface}},
year = {1998}
}
@article{Andersson2003,
abstract = {In his recent book "test-driven development" (Beck, 2003), K. Beck describes briefly the concept of "acceptance-test driven development", and is broadly sceptical to whether it will work. After a successful project that used this technique, we wish to argue in favour of it and the TextTest tool that we have built up around it. We have found that a working XP process can be built based around using only automated acceptance tests, and not doing any unit testing. In this paper we explain and analyse our XP process, its strengths and limitations, and by doing so we hope to inspire others to try and make it work for their projects too},
annote = {XP with acceptance-test driven development: a rewrite project for a resource optimization system},
author = {Andersson, J and Bache, G and Sutton, P},
isbn = {3540402152},
journal = {Extreme Programming and Agile Processes in Software Engineering 4th International Conference, XP 2003 Proceedings Lecture Notes in Computer Science Vol 2675},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {180--188},
title = {{XP with acceptance-test driven development: a rewrite project for a resource optimization system}},
url = {citeulike-article-id:3934541 {\#}},
volume = {2675},
year = {2003}
}
@inproceedings{4641277,
abstract = {Event logs or log files form an essential part of any network management and administration setup. While log files are invaluable to a network administrator, the vast amount of data they sometimes contain can be overwhelming and can sometimes hinder rather than facilitate the tasks of a network administrator. For this reason several event clustering algorithms for log files have been proposed, one of which is the event clustering algorithm proposed by Risto Vaarandi, on which his simple log file clustering tool (SLCT) is based. The aim of this work is to develop a visualization tool that can be used to view log files based on the clusters produced by SLCT. The proposed visualization tool, which is called LogView, utilizes treemaps to visualize the hierarchical structure of the clusters produced by SLCT. Our results based on different application log files show that LogView can ease the summarization of vast amount of data contained in the log files. This in turn can help to speed up the analysis of event data in order to detect any security issues on a given application.},
author = {Makanju, A and Brooks, S and Zincir-Heywood, A N and Milios, E E},
booktitle = {Privacy, Security and Trust, 2008. PST '08. Sixth Annual Conference on},
doi = {10.1109/PST.2008.17},
keywords = {LogView,administration setup,event clustering algo},
pages = {99--108},
title = {{LogView: Visualizing Event Log Clusters}},
year = {2008}
}
@article{Eugster2003,
author = {Eugster, Patrick T H and Felber, Pascal A and Kermarrec, Anne-marie},
journal = {ACM Computing Surveys},
number = {2},
pages = {114--131},
title = {{The Many Faces of Publish / Subscribe}},
volume = {35},
year = {2003}
}
@inproceedings{Karlesky2006,
abstract = {Methodologies for effectively managing software development risk and producing quality software are taking hold in the IT industry. However, similar practices for embedded systems, particularly in resource constrained systems, have not yet become prevalent. Today, quality in embedded software is generally tied to platform-specific testing tools geared towards debugging. We present here an integrated collection of concrete concepts and practices that are decoupled from platform-specific tools. In fact, our approach drives the actual design of embedded software. These strategies yield good design, systems that are testable under automation, and a significant reduction in software flaws. Examples from an 8 bit system with 16k of program memory and 255 bytes of RAM illustrate these ideas.},
annote = {Effective test driven development for embedded software},
author = {Karlesky, M J and Bereza, W I and Erickson, C B},
booktitle = {2006 IEEE International Conference on Electro Information Technology},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {382--387},
title = {{Effective test driven development for embedded software}},
url = {citeulike-article-id:3934669 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34250857338{\&}{\#}38 partnerID=40},
year = {2006}
}
@inproceedings{797889,
abstract = {This paper describes a barrier topping designed to be less visually aggressive, but equally effective, to orthodox security barriers. These generally employ one or more components which have the potential to physically injure a person contained within the compound, should they attempt to breach the barrier. The concept on which the barrier is based is one of passive resistance rather than visual deterrence, with particular focus on the defeat of an outside rescue attempt or the use of ropes and bringing aids from within. The design is based on a 120 deg; sector of a 1000 mm diameter section fitted to the top of an existing wall structure. The free edge of our 120 deg; sector is fitted with a series of stiff, steel pickets, approximately 900 mm long, 72 mm wide and spaced at 154 mm centres. These pickets project downwards from the 120 deg; sector at approximately 45 deg;. The effect, when standing at the base of a wall fitted with our shroud, is similar to standing under a small verandah or roof. The steel pickets, in combination with the 120 deg; sector, make it effectively impossible to climb unaided over the barrier. The design removes the threats of height and sharp deterrent barriers, which are often unsuitable for use, particularly in juvenile institutions and psychiatric hospitals},
author = {Buckley, N},
booktitle = {Security Technology, 1999. Proceedings. IEEE 33rd Annual 1999 International Carnahan Conference on},
doi = {10.1109/CCST.1999.797889},
keywords = {barrier topping,juvenile institutions,nonthreateni},
pages = {38--41},
title = {{Design and implementation of a nonthreatening barrier for deployment in special usage holding areas}},
year = {1999}
}
@inproceedings{Falaki2010,
address = {New York, New York, USA},
author = {Falaki, Hossein and Mahajan, Ratul and Kandula, Srikanth and Lymberopoulos, Dimitrios and Govindan, Ramesh and Estrin, Deborah},
booktitle = {Proceedings of the 8th international conference on Mobile systems, applications, and services - MobiSys '10},
doi = {10.1145/1814433.1814453},
isbn = {9781605589855},
keywords = {agile,nsf,smartphone usage,user behavior},
mendeley-tags = {agile,nsf},
month = {jun},
pages = {179--194},
publisher = {ACM Press},
title = {{Diversity in smartphone usage}},
url = {http://dl.acm.org/citation.cfm?id=1814433.1814453},
year = {2010}
}
@article{Larman,
author = {Larman, Craig and Basili, Victor},
number = {6},
pages = {47--56},
title = {{A History of Iterative and Incremental Development}},
volume = {36}
}
@inproceedings{Mattu2007,
abstract = {For modern systems there is growing proof that serial/traditional approaches, such as the traditional waterfall model and model driven architecture, are ineffective and development lifecycles need to be iterative and incremental. In this presentation, we discuss the iterative and incremental approach for software design methodology called Test Driven Design (TDD). The TDD Development Cycle starts with the requirement specification and therefore captures defects much earlier in the development cycle. TDD requires that no production code be written until first a unit test is written. We compare TDD with the traditional methods and describe in detail the TDD method. We cover continuous integration, acceptance testing, system wide testing for each iteration, test frameworks, cost of change, ROI, benefits and limitations of the new Test Driven Design and provide evidence from industry that TDD leads to higher programmer productivity with higher code quality. The future work investigations will extend the reach and effectiveness of TDD by using latest technologies to generate tests from message sequence charts and generating code thru use of a Model Compiler leading to an Advanced Test Driven Design methodology. Further investigations will also look at the concurrency issues by use of LTSA (Labelled Transition Analyzer) technology. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Test driven design methodology for component-based system},
author = {Mattu, B S and Shankar, R},
booktitle = {Proceedings of the 1st Annual 2007 IEEE Systems Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {25--31},
title = {{Test driven design methodology for component-based system}},
url = {citeulike-article-id:3934707 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34748876587{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{5375543,
abstract = {User testing is an integral component of user-centered design, but has only rarely been applied to visualization for cyber security applications. This paper describes a comparative evaluation of a visualization application and a traditional interface for analyzing network packet captures, that was conducted as part of the user-centered design process. Structured, well-defined tasks and exploratory, open-ended tasks were completed with both tools. Accuracy and efficiency were measured for the well-defined tasks, number of insights was measured for exploratory tasks and user perceptions were recorded for each tool. The results of this evaluation demonstrated that users performed significantly more accurately in the well-defined tasks, discovered a higher number of insights and demonstrated a clear preference for the visualization tool. The study presented here may be useful for future visualization for network security visualization evaluation designers. Some of the challenges and lessons learned are described.},
author = {Goodall, J R},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375543},
keywords = {cyber security application,data visualization,netw},
pages = {57--68},
title = {{Visualization is better! A comparative evaluation}},
year = {2009}
}
@misc{Peel2013,
author = {{Region of Peel Government}},
booktitle = {http://www.peelregion.ca/health/great-beginnings/breach-of-information-faq.htm},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{Region of Peel Announces Breach}},
url = {http://www.peelregion.ca/health/great-beginnings/breach-of-information-faq.htm},
urldate = {2015-01-01},
year = {2013}
}
@inproceedings{4721574,
abstract = {Bluetooth, a protocol designed to replace peripheral cables, has grown steadily over the last five years and includes a variety of applications. The Bluetooth protocol operates on a wide variety of mobile and wireless devices and is nearly ubiquitous. Several attacks exist that successfully target and exploit Bluetooth enabled devices. This paper describes the implementation of a network intrusion detection system for discovering malicious Bluetooth traffic. The work improves upon existing techniques, which only detect a limited set of attacks (based on measuring anomalies in the power levels of the Bluetooth device). The new method identifies reconnaissance, denial of service, and information theft attacks on Bluetooth enabled devices, using signatures of the attacks. Furthermore, this system includes an intrusion response component to detect attacks in progress, based on the attack classification. This paper presents the implementation of the Bluetooth intrusion detection system and demonstrates its detection, analysis, and response capabilities. The tool includes a visualization interface to facilitate the understanding of Bluetooth enabled attacks. The experimental results show that the system can significantly improve the overall security of an organization by identifying and responding to threats posed to the Bluetooth protocol.},
author = {OConnor, Terrence and Reeves, Douglas},
booktitle = {2008 Annual Computer Security Applications Conference (ACSAC)},
doi = {10.1109/ACSAC.2008.39},
isbn = {978-0-7695-3447-3},
issn = {1063-9527},
keywords = {Bluetooth,Bluetooth network-based misuse detection},
month = {dec},
pages = {377--391},
publisher = {Ieee},
title = {{Bluetooth Network-Based Misuse Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4721574},
year = {2008}
}
@misc{Kapoor2006a,
abstract = {Robots are increasingly being used in computer integrated surgery (CIS) systems, yet to our knowledge, there is no open source software that is specifically targeted at this application domain. In this paper, we derive unique requirements for medical robot controllers that are based on our experiences in the field. We present an overview of the second-generation open source software package (the cisst package), currently under development, that would include a family of application frameworks with dynamically loaded software components. We then focus on some key design details, which include extensive application of the command pattern as well as a dynamic interface query mechanism, and present a simple example to illustrate the concepts},
author = {Kapoor, A and Deguet, A and Kazanzides, P},
booktitle = {Proceedings 2006 IEEE International Conference on Robotics and Automation 2006 ICRA 2006},
doi = {10.1109/ROBOT.2006.1642285},
isbn = {0780395050},
issn = {10504729},
number = {May},
pages = {3813--3818},
publisher = {Ieee},
title = {{Software components and frameworks for medical robot control}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1642285},
year = {2006}
}
@inproceedings{5375546,
abstract = {Security configuration files are created and edited as text files. These files are the essential definition and control of the behavior of security devices. Despite their significant size, complexity, and the possibility of interaction between entries, no visually sophisticated tools exist that explicitly capture and visualize problematic interactions between rules to aid in the comprehension and modification of configuration files. Our initial work on the direct visualization of firewall configurations showed the limitations of visualizing just the range of packets that can be accepted. To visually capture the interactions between rules, we introduce the concept of a "created void." Created voids capture the information about destructive interactions between rules in a firewall ruleset, where an overlap between a deny rule prevents that packet from reaching an accept rule later in the ruleset. We present a lossless five-dimensional visualization of the convex solid decomposition of the set of acceptable packets from a firewall configuration, augmented with visual representations of created voids. This interactive visualization is embedded in a simple firewall ruleset editor, allowing the user to investigate the effect of changes in the ruleset.},
author = {Morrissey, S P and Grinstein, G},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375546},
keywords = {convex solid decomposition,created void,firewall c},
pages = {75--79},
title = {{Visualizing firewall configurations using created voids}},
year = {2009}
}
@inproceedings{Jørgensen2000,
address = {Zurich, Switzerland},
author = {J{\o}rgensen, Magne and Sj{\o}berg, Dag I K and Kirkeb{\o}en, Geir},
pages = {93--99},
title = {{The Prediction Ability of Experienced Software Maintainers}},
year = {2000}
}
@inproceedings{Gallagher2003,
abstract = {When computing program slices on all variables in a system, we observed that many of these slices are the same. This leads to the question: Are we looking at software clones? We discuss the genesis of this phenomena and present some of the data observations that led to the question. The answer to our query is not immediately clear. We end by presenting arguments both pro and con. Supporting the affirmative, we observed that some slice-clones are evidently the result of the usual genesis of software clones: failure to note appropriate abstractions. Also, slice-clones assist in program comprehension by coalescing into one program fragment the computations on many different variables. Opposing the proposition, we note that slice-clones do not arise due to programmer intent or the copying of existing idioms.},
author = {Gallagher, K. and Layman, L.},
booktitle = {Proceedings of the 11th International Workshop on Program Comprehension (IWPC '03)},
doi = {10.1109/WPC.2003.1199209},
isbn = {0-7695-1883-4},
issn = {1092-8138},
keywords = {Accidents,Automatic control,Cloning,Computer science,Data analysis,Displays,Educational institutions,Programming profession,Reverse engineering,Software maintenance,mypubs,program comprehension,program fragment,program slices,program slicing,software clones},
mendeley-tags = {mypubs},
pages = {251--256},
publisher = {IEEE Comput. Soc},
shorttitle = {Program Comprehension, 2003. 11th IEEE Internation},
title = {{Are decomposition slices clones?}},
year = {2003}
}
@article{sharit2003effects,
author = {Sharit, Joseph and Czaja, Sara J and Nair, Sankaran and Lee, Chin Chin},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {234--251},
publisher = {SAGE Publications},
title = {{Effects of age, speech rate, and environmental support in using telephone voice menu systems}},
volume = {45},
year = {2003}
}
@inproceedings{Kaczmarczyk2010,
abstract = {Computing educators are often baffled by the misconceptions that their CS1 students hold. We need to understand these misconceptions more clearly in order to help students form correct conceptions. This paper describes one stage in the development of a concept inventory for Computing Fundamentals: investigation of student misconceptions in a series of core CS1 topics previously identified as both important and difficult. Formal interviews with students revealed four distinct themes, each containing many interesting misconceptions. Three of those misconceptions are detailed in this paper: two misconceptions about memory models, and data assignment when primitives are declared. Individual misconceptions are related, but vary widely, thus providing excellent material to use in the development of the CI. In addition, CS1 instructors are provided immediate usable material for helping their students understand some difficult introductory concepts. Copyright 2010 ACM.},
address = {Milwaukee, WI, USA},
author = {Kaczmarczyk, Lisa C. and Petrick, Elizabeth R. and East, J. Philip and Herman, Geoffrey L.},
booktitle = {SIGCSE'10 - Proceedings of the 41st ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1734263.1734299},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kaczmarczyk et al. - 2010 - Identifying student misconceptions of programming.pdf:pdf},
isbn = {9781605588858},
keywords = {CS1,Concept inventory,Curriculum,Misconceptions,Pedagogy,Programming},
pages = {107--111},
publisher = {ACM Press},
title = {{Identifying student misconceptions of programming}},
url = {http://portal.acm.org/citation.cfm?doid=1734263.1734299},
year = {2010}
}
@misc{Madeyski2006,
abstract = {Background: Test-driven development (TDD) and pair programming are software development practices popularized by eXtreme Programming methodology. The aim of the practices is to improve software quality. Objective: Provide an empirical evidence of the Impact of both practices on package dependencies playing a role of package level design quality indicators. Method: An experiment with a hundred and eighty eight MSc students from Wroclaw University of Technology, who developed finance-accounting system in different ways (CS - classic solo, TS - TDD solo, CP - classic pairs, TP - TDD pairs). Results: It appeared that package level design quality indicators (namely package dependencies in an object-oriented design) were not significantly affected by development method. Limitations: Generalization of the results is limited due to the fact that MSc students participated in the study. Conclusions: Previous research revealed that using test-driven development instead of classic (test-last) testing approach had statistically significant positive Impact on some class level software quality indicators (namely CBO and RFC metrics) in case of solo programmers as well as pairs. Combined results suggest that the positive Impact of test-driven development on software quality may be limited to class level. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {The impact of pair programming and test-driven development on package dependencies in object-oriented design - An experiment},
author = {Madeyski, L},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {278--289},
title = {{The impact of pair programming and test-driven development on package dependencies in object-oriented design - An experiment}},
url = {citeulike-article-id:3934699 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746265818{\&}{\#}38 partnerID=40},
volume = {4034 LNCS},
year = {2006}
}
@inproceedings{Koike2004a,
address = {New York, New York, USA},
author = {Koike, Hideki and Ohno, Kazuhiro},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029232},
isbn = {1581139748},
keywords = {intrusion detection,visualization},
month = {oct},
pages = {143},
publisher = {ACM Press},
title = {{SnortView}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029232},
year = {2004}
}
@article{Downey2003,
abstract = {We apply four of the 12 XP techniques to parser development. Tools such as lex and yacc have been in programmers' parser-development tool-chests for a long time. SableCC's syntax is easy to learn and close to Extended Backus-Naur Format (EBNF), the standard grammar notation used in major specifications. To satisfy the continuous integration and test-driven development principles, we need a one-touch build script. Since SableCC comes with an Ant task, we can integrate the creation of the lexer/parser source code as part of our build. We write a SableCC grammar that can handle a fragment of IDL that represents just a handful of tokens and productions from the CORBA (common object request broker architecture) 3.0 specification. CORBA is a mature distributed object technology, and IDL is its language-neutral interface definition language},
annote = {Extreme parsing},
author = {Downey, K F},
isbn = {1044-789X},
journal = {Dr Dobb's Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {8},
pages = {33--37},
title = {{Extreme parsing}},
url = {citeulike-article-id:3934591 {\#}},
volume = {28},
year = {2003}
}
@inproceedings{Knodel2005,
abstract = {The software architecture is one of the most crucial artifacts within the lifecycle of a software system. Decisions made at the architectural level directly enable, facilitate, hamper, or interfere with the achievement of business goals as well as meeting functional and quality requirements. Software architectures are also essential for the success of product line engineering. In this work, we summarize how, from our practical experience, static architecture evaluation contributes to architecture development. We also describe the different purposes of architectural evaluations.},
address = {Pittsburgh, PA},
author = {Knodel, Jens and Lindvall, Mikael and Muthig, Dirk},
booktitle = {Conference on Software Architecture, WICSA 5th Working IEEE/IFIP},
keywords = {Software architecture},
pages = {237 -- 238},
series = {5th IEEE/IFIP Working Conference on Software Architecture (WICSA)},
title = {{Static Evaluation of Software Architectures - A Short Summary}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1620126},
year = {2005}
}
@article{Pretschner2005a,
author = {Pretschner, A and Wagner, S and K, C and Sostawa, B and Z, R and Ag, B M W},
isbn = {1581139632},
keywords = {abstraction,age,automotive software,case,ch,corresponding author,cover-,ethz,inf,model-based development,pretscha,test case generation},
number = {3},
pages = {392--401},
title = {{One Evaluation of Model-Based Testing and its Automation}},
year = {2005}
}
@inproceedings{Alzahrani2018,
abstract = {Many teachers of CS 1 (introductory programming) have switched to Python rather than C, C++, or Java. One reason is the belief that Python's interpreted nature plus simpler syntax and semantics ease a student's learning, but data supporting that belief is scarce. This paper addresses the question: Do Python learners struggle less than C++ learners? We analyzed student submissions on small coding exercises in CS 1 courses at 20 different universities, 10 courses using Python, and 11 using C++. Each course used either the Python or C++ version of an online textbook from one publisher, each book having 100+ small coding exercises, expected to take 2-5 minutes each. We considered 11 exercises whose Python and C++ versions were nearly identical and that appeared in various chapters. We defined struggle rate for exercises, where struggle means a student spent excessive time or attempts on an exercise. Based on that rate, we found the learning for Python was not eased; in fact, Python students had significantly higher struggle rates than C++ students (26{\%} vs. 13{\%}). Higher rates were seen even when considering only classes with no prerequisites, classes for majors only, or classes for non-majors only. We encourage the community to do further analyses, to help guide teachers when choosing a CS 1 language.},
address = {Baltimore, MD, USA},
author = {Alzahrani, Nabeel and Vahid, Frank and Edgcomb, Alex and Nguyen, Kevin and Lysecky, Roman},
booktitle = {SIGCSE 2018 - Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3159450.3160586},
isbn = {9781450351034},
month = {feb},
pages = {86--91},
publisher = {Association for Computing Machinery, Inc},
title = {{Python versus C++: An analysis of student struggle on small coding exercises in introductory programming courses}},
year = {2018}
}
@inproceedings{Patil2005,
address = {New York, New York, USA},
author = {Patil, Sameer and Lai, Jennifer},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems - CHI '05},
doi = {10.1145/1054972.1054987},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Patil, Lai - 2005 - Who gets to know what when.pdf:pdf},
isbn = {1581139985},
keywords = {agile,awareness,context-aware computing,contextual communication,information disclosure,nsf,permission structures,privacy},
mendeley-tags = {agile,nsf},
month = {apr},
pages = {101},
publisher = {ACM Press},
title = {{Who gets to know what when}},
url = {http://dl.acm.org/citation.cfm?id=1054972.1054987},
year = {2005}
}
@inproceedings{Kaufmann2003,
abstract = {A Spring 2003 experiment examines the claims that test-driven development or test-first programming improves software quality and programmer confidence. The results indicate support for these claims and inform larger future experiments.},
annote = {Implications of test-driven development: a pilot study},
author = {Kaufmann, R and Janzen, D},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {298--299},
publisher = {ACM New York, NY, USA},
title = {{Implications of test-driven development: a pilot study}},
url = {citeulike-article-id:3934670 http://portal.acm.org/citation.cfm?id=949344.949421},
year = {2003}
}
@inproceedings{Wu2013,
author = {Wu, Ji and Ali, Shaukat and Yue, Tao and Tian, Jie},
booktitle = {2013 IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)},
doi = {10.1109/ISSRE.2013.6698921},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2013 - Experience report Assessing the reliability of an industrial avionics software Results, insights and recommendations.pdf:pdf},
isbn = {978-1-4799-2366-3},
keywords = {Aerospace electronics,Complexity theory,Correlation,DO-178B,Reliability assessment,Software,Software reliability,Testing,aerospace computing,avionics,avionics software,complexity measures,data analysis,experience report,industrial RTOS4A,industrial avionics software,industrial case study,operating systems (computers),real time operating system-for-avionics,real time operating systems,reliability assessment,reliability requirements,safety requirements,safety-critical software,software metrics,statistical testing,statistical tests,system recovery,testing data analysis,testing effectiveness measures,testing effort measures},
language = {English},
month = {nov},
pages = {218--227},
publisher = {IEEE},
title = {{Experience report: Assessing the reliability of an industrial avionics software: Results, insights and recommendations}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6698921},
year = {2013}
}
@article{Graves2000,
author = {Graves, T.L. and Karr, A.F. and Marron, J.S. and Siy, H.},
doi = {10.1109/32.859533},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
month = {jul},
number = {7},
pages = {653--661},
title = {{Predicting fault incidence using software change history}},
url = {http://ieeexplore.ieee.org/document/859533/},
volume = {26},
year = {2000}
}
@inproceedings{5633664,
abstract = {Botnets are emerging as the most significant threat facing online ecosystems and computing assets due to their enormous volume and sheer power. It is a major challenge for cyber-security research community to combat the emerging threat of botnets. Most of useful approaches for botnet traffic detection are based on passive network traffic monitoring and analysis. Nevertheless, typical network traffic generates a huge amount of data for analysis. In addition, the poor user interfaces of the existing tools lead to the insufficient utilization of the captured data, and do not consider utilization of human intellectual capability. The proposed visual network monitoring system tackles these issues by adopting proper visualization techniques. The proposed visualization techniques enhance the visibility of network traffic related to invariant bot behaviors, and provide notification of bot existence without distracting the user with huge volumes of data. The visual illustration of typical bot behavior improves the botnet traffic detection process by engaging human perception capabilities. This approach assists security personnel with a visual security tool to mitigate botnet threats by discovering invariant botnet behaviors during the benign state of a botnet in small to medium size networks. Moreover, the user friendly interface of this system is interactive, flexible, and easy to use.},
author = {Shahrestani, A and Feily, M and Ahmad, R and Ramadass, S},
booktitle = {Emerging Security Information Systems and Technologies (SECURWARE), 2010 Fourth International Conference on},
doi = {10.1109/SECURWARE.2010.37},
keywords = {botnet traffic detection,cyber-security research c},
month = {jul},
pages = {182--188},
title = {{Discovery of Invariant Bot Behavior through Visual Network Monitoring System}},
year = {2010}
}
@misc{Verizon2010,
author = {Verizon},
howpublished = {$\backslash$url{\{}http://goo.gl/28pPGM{\}}},
institution = {Verizon},
title = {{2010 Data Breach Investigations Report, http://goo.gl/28pPGM}},
url = {http://goo.gl/28pPGM},
urldate = {2019-01-09},
year = {2010}
}
@inproceedings{4388994,
abstract = {Using mobile devices for visualization provides a ubiquitous environment for accessing information and effective decision making. These visualizations are critical in satisfying the knowledge needs of operators in areas as diverse as education, business, law enforcement, protective services, medical services, scientific discovery, and homeland security. In this paper, we present an efficient and interactive mobile visual analytic system for increased situational awareness and decision making in emergency response and training situations. Our system provides visual analytics with locational scene data within a simple interface tailored to mobile device capabilities. In particular, we focus on processing and displaying sensor network data for first responders. To verify our system, we have used simulated data of The Station nightclub fire evacuation.},
author = {Kim, SungYe and Jang, Yun and Mellema, A and Ebert, D S and Collins, T},
booktitle = {Visual Analytics Science and Technology, 2007. VAST 2007. IEEE Symposium on},
doi = {10.1109/VAST.2007.4388994},
keywords = {data visualization,decision making,emergency respo},
pages = {35--42},
title = {{Visual Analytics on Mobile Devices for Emergency Response}},
year = {2007}
}
@inproceedings{George2003,
abstract = {Test Driven Development (TDD) is a software development practice in which unit test cases are incrementally written prior to code implementation. In our research, we ran a set of structured experiments with 24 professional pair programmers. One group developed code using TDD while the other a waterfall-like approach. Both groups developed a small Java program. We found that the TDD developers produced higher quality code, which passed 18{\%} more functional black box test cases. However, TDD developer pairs took 16{\%} more time for development. A moderate correlation between time spent and the resulting quality was established upon analysis. It is conjectured that the resulting high quality of code written using the TDD practice may be due to the granularity of TDD, which may encourage more frequent and tighter verification and validation. Lastly, the programmers which followed a waterfall-like process often did not write the required automated test cases after completing their code, which might be indicative of the tendency among practitioners toward inadequate testing. This observation supports that TDD has the potential of increasing the level of testing in the industry as testing as an integral part of code development.},
annote = {An initial investigation of test driven development in industry},
author = {George, B and Williams, L},
booktitle = {Proceedings of the ACM Symposium on Applied Computing},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1135--1139},
title = {{An initial investigation of test driven development in industry}},
url = {citeulike-article-id:3934619 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0038310314{\&}{\#}38 partnerID=40},
year = {2003}
}
@misc{Cook2014,
author = {Cook, Kris and Grinstein, Georges and Whiting, Mark},
booktitle = {http://www.vacommunity.org/VAST+Challenge+2014},
title = {{VAST 2014 Challenge: The Kronos Incident}},
url = {http://www.vacommunity.org/VAST+Challenge+2014},
urldate = {2019-07-07},
year = {2014}
}
@book{Dorner1996,
address = {Cambridge},
author = {D{\"{o}}rner, D},
publisher = {Perseus Books},
title = {{The Logic of Failure: Recognizing and Avoiding Error in Complex Situations}},
year = {1997}
}
@inproceedings{5168086,
abstract = {The Department of Homeland Security law enforcement faces the continual challenge of analyzing their custom data sources in a geospatial context. From a strategic perspective law enforcement has certain requirements to first broadly characterize a given situation using their custom data sources and then once it is summarily understood, to geospatially analyze their data in detail. For the past three years the generalized data-driven analysis and integration (GDDAI) project has operated through DHS Science and Technology on behalf of Immigration and Customs Enforcement (ICE) to develop and deploy hybrid data analysis methods that improve ICE analysts' ability to answer time-critical queries for their agents. GDDAI provides this hybrid environment through seamless customized interfaces built with commercial off the shelf (COTS) and government off the shelf (GOTS) tools. At its core, to help characterize user data, GDDAI relies upon multidimensional relational data cubes which are based on online analytical processing (OLAP) technologies that provide intuitive and graphical access to the massively complex set of summary views available in large relational (SQL) repositories. As an extension to OLAP we provide a proximity-based search engine, a rich map feature database which allows users to add their customized reports, and a geocoder that derives coordinates from addressable information in the user database. While the design of data cubes can be technically challenging, our primary contributions are in the ability to effectively "drill-through" from a summary analysis to a detailed analysis when a population of interest is identified. In addition to this data transformation capability, our geospatial analysis infrastructure can perform proximity searches on the detailed "drill-through" records. Iteratively users can conduct increasingly more fine-tuned geospatial searches by combining or excluding previous geospatial results with their current drill-through results.},
author = {Stephan, E and Burke, J and Carlson, C and Gillen, D and Joslyn, C and Olsen, B and Critchlow, T},
booktitle = {Technologies for Homeland Security, 2009. HST '09. IEEE Conference on},
doi = {10.1109/THS.2009.5168086},
keywords = {DHS programs,Department of Homeland Security law e},
month = {may},
pages = {553--560},
title = {{Geospatial data analysis for DHS programs}},
year = {2009}
}
@inproceedings{4271660,
abstract = {This paper presents a novel technique for analyzing security protocols based on an abstraction of the program semantics. This technique is based on a novel structure called causal graph which captures the causality among program events within a finite graph. A core property of causal graphs is that they abstract away from the multiplicity of protocol sessions, hence constituting a concise tool for reasoning about an even infinite number of concurrent protocol sessions; deciding security only requires a traversal of the causal graph, thus yielding a decidable, and typically very efficient, approach for security protocol analysis. Additionally, causal graphs allow for dealing with different security properties such as secrecy and authenticity in a uniform manner. Both the construction of the causal graph from a given protocol description and the analysis have been fully automated and tested on several example protocols from the literature.},
author = {Backes, M and Cortesi, A and Maffei, M},
booktitle = {Computer Security Foundations Symposium, 2007. CSF '07. 20th IEEE},
doi = {10.1109/CSF.2007.11},
issn = {1063-6900},
keywords = {causal graph;causality-based abstraction;concurren},
month = {jul},
pages = {355--369},
title = {{Causality-based Abstraction of Multiplicity in Security Protocols}},
year = {2007}
}
@inproceedings{Janzen2007,
abstract = {Test-driven learning (TDL) is an approach to teaching computer programming that involves introducing and exploring new concepts through automated unit tests. TDL offers the potential of teaching testing for free, of improving programmer comprehension and ability, and of improving software quality both in terms of design quality and reduced defect density.This paper introduces test-driven learning as a pedagogical tool. It will provide examples of how TDL can be incorporated at multiple levels in computer science and software engineering curriculum for beginning through professional programmers. In addition, the relationships between TDL and test-driven development will be explored.Initial evidence indicates that TDL can improve student comprehension of new concepts while improving their testing skills with no additional instruction time. In addition, by learning to construct programs in a test-driven manner, students are expected to be more likely to develop their own code with a test-driven approach, likely resulting in improved software designs and quality. Copyright 2006 ACM.},
annote = {Test-driven learning: Intrinsic integration of testing into the CS/SE curriculum},
author = {Janzen, D S and Saiedian, H},
booktitle = {Proceedings of the Thirty-Seventh SIGCSE Technical Symposium on Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {254--258},
title = {{Test-driven learning: Intrinsic integration of testing into the CS/SE curriculum}},
url = {citeulike-article-id:3934657 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-37849018803{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Kemerer1993,
author = {Kemerer, C F},
keywords = {backfiring,function points},
number = {2},
pages = {85--97},
title = {{Reliability of Function Point Measurement: A Field Experiment}},
volume = {36},
year = {1993}
}
@inproceedings{4724727,
abstract = {This paper proposed a novel blind image watermark scheme in discrete cosine transform (DCT) domain. The original image is divided into small image blocks with size 8*8 first, and then the binary watermark bits are embedded into DCT domain of image blocks adaptively through a novel watermark embedding algorithm. The original image is not needed in the extraction algorithm. For gaining good fidelity and robustness, support vector machine (SVM) is adopted to simulate human visual system (HVS) and class image blocks into several types. Different embedding strengths are determined for different image types as human has different sensitivity to them. There characteristics of image blocks are used for classification. The embedding locations are selected randomly ranging from 10 to 24 of AC coefficients by a key to enhance the security of watermark. The experimental results show that our scheme has both good fidelity and strong robustness.},
author = {Meng, Fanman and Peng, Hong and Pei, Zheng and Wang, Jun},
booktitle = {2008 International Conference on Computational Intelligence and Security},
doi = {10.1109/CIS.2008.20},
isbn = {978-0-7695-3508-1},
keywords = {DCT,blind image watermarking,discrete cosine trans},
month = {dec},
pages = {16--20},
publisher = {Ieee},
title = {{A Novel Blind Image Watermarking Scheme Based on Support Vector Machine in DCT Domain}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4724727},
volume = {2},
year = {2008}
}
@inproceedings{5484737,
abstract = {The Asymmetric Threat Response and Analysis Program (ATRAP) is a software system for intelligence fusion, visualization, reasoning, and prediction. ATRAP consists of a set of tools for annotating and automatically extracting entities and relationships from documents, visualizing this information in relational, geographic, and temporal dimensions, and determining future courses of action of adversaries by creating situational threat templates and applying customized prediction algorithms. In this paper, we first describe the task of analyzing data in intelligence reports, and then provide an overview of major system components: the Text Highlighter tool, the ThoughtSpace {\#}x2122; visualization environment, and the Template Builder and prediction tool. Subsequently, we describe linguistic characteristics of intelligence reports, and describe ATRAP's named entity recognition system.},
author = {Chan, Erwin and Ginsburg, Jason and {Ten Eyck}, Brian and Rozenblit, Jerzy and Dameron, Mike},
booktitle = {Intelligence and Security Informatics (ISI), 2010 IEEE International Conference on},
doi = {10.1109/ISI.2010.5484737},
month = {may},
pages = {202--207},
title = {{Text analysis and entity extraction in asymmetric threat response and prediction}},
year = {2010}
}
@inproceedings{Wiese2013,
address = {New York, New York, USA},
author = {Wiese, Jason},
booktitle = {Proceedings of the adjunct publication of the 26th annual ACM symposium on User interface software and technology - UIST '13 Adjunct},
doi = {10.1145/2508468.2508472},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiese - 2013 - Enabling an ecosystem of personal behavioral data.pdf:pdf},
isbn = {9781450324069},
keywords = {agile,contextual inferences,nsf,personal behavioral data},
mendeley-tags = {agile,nsf},
month = {oct},
pages = {41--44},
publisher = {ACM Press},
title = {{Enabling an ecosystem of personal behavioral data}},
url = {http://dl.acm.org/citation.cfm?id=2508468.2508472},
year = {2013}
}
@misc{Rust2006,
abstract = {It is widely accepted that the absence of a structured approach to spreadsheet engineering is a key factor in the high level of spreadsheet errors. In this paper we propose and investigate the application of Test-Driven Development to the creation of spreadsheets. Through a pair of case studies we demonstrate that Test-Driven Development can be applied to the development of spreadsheets. A supporting tool under development by the authors is also documented along with proposed research to determine the effectiveness of the methodology and the associated tool. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {Test-driven development: Can it work for spreadsheet engineering?},
author = {Rust, A and Bishop, B and Mcdaid, K},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {209--210},
title = {{Test-driven development: Can it work for spreadsheet engineering?}},
url = {citeulike-article-id:3934772 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746199983{\&}{\#}38 partnerID=40},
volume = {4044 LNCS},
year = {2006}
}
@inproceedings{5375530,
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375530},
pages = {iii},
title = {{Contents}},
year = {2009}
}
@article{Avizienis2004,
abstract = {This paper gives the main definitions relating to dependability, a generic concept including a special case of such attributes as reliability, availability, safety, integrity, maintainability, etc. Security brings in concerns for confidentiality, in addition to availability and integrity. Basic definitions are given first. They are then commented upon, and supplemented by additional definitions, which address the threats to dependability and security (faults, errors, failures), their attributes, and the means for their achievement (fault prevention, fault tolerance, fault removal, fault forecasting). The aim is to explicate a set of general concepts, of relevance across a wide range of situations and, therefore, helping communication and cooperation among a number of scientific and technical communities, including ones that are concentrating on particular types of system, of system failures, or of causes of system failures.},
author = {Avizienis, A. and Laprie, J.-C. and Randell, B. and Landwehr, C.},
doi = {10.1109/TDSC.2004.2},
issn = {1545-5971},
journal = {IEEE Transactions on Dependable and Secure Computing},
month = {jan},
number = {1},
pages = {11--33},
shorttitle = {Dependable and Secure Computing, IEEE Transactions},
title = {{Basic concepts and taxonomy of dependable and secure computing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1335465},
volume = {1},
year = {2004}
}
@inproceedings{Danielsiek2012,
abstract = {We describe the first results of our work towards a concept inventory for Algorithms and Data Structures. Based on expert interviews and the analysis of 400 exams we were able to identify several core topics which are prone to error. In a pilot study, we verified misconceptions known from the literature and identified previously unknown misconceptions related to Algorithms and Data Structures. In addition to this, we report on methodological issues and point out the importance of a two-pronged approach to data collection. {\textcopyright} 2012 ACM.},
address = {Raleigh, NC, USA},
author = {Danielsiek, Holger and Paul, Wolfgang and Vahrenhold, Jan},
booktitle = {SIGCSE'12 - Proceedings of the 43rd ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2157136.2157148},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Danielsiek, Paul, Vahrenhold - 2012 - Detecting and understanding students' misconceptions related to algorithms and data structures(2).pdf:pdf},
isbn = {9781450310987},
keywords = {CS1/2,concept inventories,misconceptions},
pages = {21--26},
publisher = {ACM Press},
title = {{Detecting and understanding students' misconceptions related to algorithms and data structures}},
url = {http://dl.acm.org/citation.cfm?doid=2157136.2157148},
year = {2012}
}
@inproceedings{Kagdi2006,
address = {Shanghai, China},
author = {Kagdi, Huzefa and Yusuf, Shehnaaz and Maletic, Jonathan I.},
booktitle = {Proceedings of the 2006 international workshop on Mining software repositories  - MSR '06},
doi = {10.1145/1137983.1137996},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kagdi, Yusuf, Maletic - 2006 - Mining sequences of changed-files from version histories.pdf:pdf},
isbn = {1595933972},
pages = {47--53},
title = {{Mining sequences of changed-files from version histories}},
url = {http://portal.acm.org/citation.cfm?doid=1137983.1137996},
year = {2006}
}
@book{Glaser1967,
address = {Chicago, IL},
author = {Glaser, Barney G and Strauss, Anselm L},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Aldine de Gruyter},
title = {{The Discovery of Grounded Theory: Strategies for Qualitative Research}},
year = {1967}
}
@article{Zhao2015,
abstract = {BACKGROUND: Topic modelling is an active research field in machine learning. While mainly used to build models from unstructured textual data, it offers an effective means of data mining where samples represent documents, and different biological endpoints or omics data represent words. Latent Dirichlet Allocation (LDA) is the most commonly used topic modelling method across a wide number of technical fields. However, model development can be arduous and tedious, and requires burdensome and systematic sensitivity studies in order to find the best set of model parameters. Often, time-consuming subjective evaluations are needed to compare models. Currently, research has yielded no easy way to choose the proper number of topics in a model beyond a major iterative approach.

METHODS AND RESULTS: Based on analysis of variation of statistical perplexity during topic modelling, a heuristic approach is proposed in this study to estimate the most appropriate number of topics. Specifically, the rate of perplexity change (RPC) as a function of numbers of topics is proposed as a suitable selector. We test the stability and effectiveness of the proposed method for three markedly different types of grounded-truth datasets: Salmonella next generation sequencing, pharmacological side effects, and textual abstracts on computational biology and bioinformatics (TCBB) from PubMed.

CONCLUSION: The proposed RPC-based method is demonstrated to choose the best number of topics in three numerical experiments of widely different data types, and for databases of very different sizes. The work required was markedly less arduous than if full systematic sensitivity studies had been carried out with number of topics as a parameter. We understand that additional investigation is needed to substantiate the method's theoretical basis, and to establish its generalizability in terms of dataset characteristics.},
author = {Zhao, Weizhong and Chen, James J and Perkins, Roger and Liu, Zhichao and Ge, Weigong and Ding, Yijun and Zou, Wen},
doi = {10.1186/1471-2105-16-S13-S8},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao et al. - 2015 - A heuristic approach to determine an appropriate number of topics in topic modeling.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
month = {jan},
pages = {S8},
pmid = {26424364},
title = {{A heuristic approach to determine an appropriate number of topics in topic modeling.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=4597325{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {16 Suppl 1},
year = {2015}
}
@article{Janzen2005a,
abstract = {The test-driven development strategy requires writing automated tests prior to developing functional code in small, rapid iterations. XP is an agile method that develops object-oriented software in very short iterations with little upfront design. Although not originally given this name, TDD was described as an integral XP practice necessary for analysis, design, and testing that also enables design through refactoring, collective ownership, continuous integration, and programmer courage. Along with pair programming and refactoring, TDD has received considerable individual attention since XP's introduction. Developers have created tools specifically to support TDD across a range of languages and have written numerous books explaining how to apply TDD concepts. Researchers have begun to examine TDD's effects on defect reduction and quality improvements in academic and professional practitioner environments, and educators have started to examine how to integrate TDD into computer science and software engineering pedagogy.},
annote = {Test-driven development: Concepts, taxonomy, and future direction},
author = {Janzen, D and Saiedian, H},
journal = {Computer},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {9},
pages = {43--50},
title = {{Test-driven development: Concepts, taxonomy, and future direction}},
url = {citeulike-article-id:3934653 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-25844466601{\&}{\#}38 partnerID=40},
volume = {38},
year = {2005}
}
@misc{Fowler,
author = {Fowler, Martin and Highsmith, Jim},
pages = {28--32},
title = {{The Agile Manifesto}}
}
@inproceedings{Girard2006,
abstract = {Test-Driven Development is a design strategy where a set of tests over a class is defined prior to the implementation of that class. The goal is to use the tests to exercise the class being developed, to provide immediate feedback of the overall quality of the code, and to identify and correct bugs as they are written. This paper presents a method to teaching CS1 using a "Test-First" approach that will be tried in fall 2006. The test-first approach requires students to think about the problem, identify corner cases, analyze ways their code may fail, and evaluate their design. Further, a proposed evaluation is presented for determining the success of the approach. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Work in progress: A test-first approach to teaching CS1},
author = {Girard, C D and Wellington, C},
booktitle = {Proceedings - Frontiers in Education Conference, FIE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Work in progress: A test-first approach to teaching CS1}},
url = {citeulike-article-id:3934624 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-48749089148{\&}{\#}38 partnerID=40},
year = {2006}
}
@misc{Pipka2003a,
abstract = {Today, software availability as well as adaptability has a strong impact on application development. Together with the success of the Internet technology, it is often necessary to offer a web-based software solution. For Java development, J2EE builds a bridge from traditional towards web application development. Java Server Pages and Servlets allow the implementation of web sites with dynamic content. Anyway, it is still possible to apply well-tried programming paradigms, e.g. the Model-View-Controller (MVC) paradigm. With regard to software quality as well as adaptability, application development is affected more and more by advanced testing techniques. Especially unit testing provides a powerful technique to develop new functionality as well as to extend and reuse existing parts. This is also strengthened by the success of agile processes that force test-driven development. From there, application code grows up with unit tests from scratch on. For Java, JUnit has become the defacto standard unit test environment. Nevertheless, test-driven web application development requires broadening the unit test approach. Considering the special requirements of web application development, both the scope of the unit tests as well as the JUnit framework itself have to be extended to enable a test-driven development. In this paper, we present an approach to customize the unit test cycle as well as the JUnit framework towards a test-driven web application development. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
annote = {Test-driven web application development in Java},
author = {Pipka, J U},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {378--393},
title = {{Test-driven web application development in Java}},
url = {citeulike-article-id:3934754 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35248886635{\&}{\#}38 partnerID=40},
volume = {2591},
year = {2003}
}
@article{Pipka2003,
abstract = {Unit tests are widely accepted nowadays in object oriented programming to reduce the error rate in application code. Furthermore, refactoring serves a standard technique to clean up code and achieve a better code quality [1]. During the last years, both techniques prove their worth in many software projects. This is also strengthened by the development of powerful refactoring tools and flexible frameworks for unit testing. It became obvious that both techniques complement each other well: On the one hand, unit tests serve as a safety net for refactoring - to preserve the behavior unchanged. On the other hand, refactoring facilitates further development of test and application code. Nevertheless, it turned out that developing test and application code in parallel is not enough for a couple of reasons. For example, there could remain application code that is untested. Furthermore, the definition of unit tests also sharpens the interfaces and therefore the software design is improved. These issues are addressed by Test Driven Development (TDD) that becomes an accepted concept during the development of central software components that have to provide high quality. In TDD, new code is only written when an automated unit test has failed. In detail, you have to follow a clear procedure to add new application code: First, you write a little test that follows the intention and does not work in the first place. Next, you make the test work as quick as possible. Finally you eliminate all of the duplication that is created to get the test run. Again, refactoring is an important technique to bring the TDD approach to life: It is a common procedure to change the application code under the assumption that the {\^{a}}€{\oe}observational equivalence{\^{a}}€ is still given, i.e. all tests are still passing [2]. And here is where a new concept, Test Driven Refactoring, comes to play: In many cases, refactoring the application code also affects the test code. Thus, it is also necessary to adapt your unit tests. But how can your unit tests prove that the {\^{a}}€{\oe}observational equivalence{\^{a}}€ before and after the refactoring is still the same when they have to be changed themselves? The solution for this issue is to refactor application code driven by unit tests: First, you start to extend and adapt the existing unit tests itself with respect to the target refactoring. At this moment, you already can examine if the target refactoring really reflects your intention. Next, you apply the target refactoring to your application code. Finally, you run the unit tests and verify if the refactored application code behaves in the way you have expected it in your unit tests before. Test Driven Refactoring is not only applicable when implementing code in TDD style. It is also an important approach when developing and refactoring in a more classic style. This is based on the fact that Test Driven Refactoring sorts out if a refactoring is applicable without changing the behavior of your application code. Furthermore, this approach defines a refactoring route and thus backs you up to find a specific way applying the target refactoring from the beginning to the end. This practitioners report presents how unit tests and refactoring strengthen software quality. Beyond it, this report focuses how these concepts can be complemented by Test Driven Refactoring, which extends the {\^{a}}€{\oe}Test First, by Intention{\^{a}}€ approach towards refactoring [3][4]. It shows how the refactoring purpose as well as its system wide effects gets easier to grasp and application code could better be extended, maintained and reused.},
annote = {Development Upside Down: Following the Test First Trail},
author = {Pipka, J U},
journal = {Practitioners Report at ECOOP},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Development Upside Down: Following the Test First Trail}},
url = {citeulike-article-id:3934753 http://www.axtelsoft.com/delphi/xp/development{\_}upside{\_}down.pdf},
volume = {3},
year = {2003}
}
@inproceedings{Ackermann2009,
address = {Washington, DC},
author = {Ackermann, Christopher and Lindvall, Mikael and Dennis, Greg},
booktitle = {Proceedings of the 2009 European Conference on Software Maintenance and Reengineering},
pages = {259--262},
title = {{Redesign for flexibility and maintainability: A case study}},
year = {2009}
}
@inproceedings{Rode2006b,
address = {New York, New York, USA},
author = {Rode, Jennifer and Johansson, Carolina and DiGioia, Paul and Filho, Roberto Silva and Nies, Kari and Nguyen, David H. and Ren, Jie and Dourish, Paul and Redmiles, David},
booktitle = {Proceedings of the second symposium on Usable privacy and security - SOUPS '06},
doi = {10.1145/1143120.1143138},
isbn = {1595934480},
keywords = {configuration in action,dynamic visualizations,effective security,history,peer-to-peer file sharing,theoretical security,usable security,user and media characterization,user study},
month = {jul},
pages = {145},
publisher = {ACM Press},
title = {{Seeing further}},
url = {http://dl.acm.org/citation.cfm?id=1143120.1143138},
year = {2006}
}
@incollection{Srikant1996,
abstract = {The problem of mining sequential patterns was recently in-troduced in We are given a database of sequences, where each se-quence is a list of transactions ordered by transaction-time, and each transaction is a set of items. The problem is to discover all sequential patterns with a user-speciied minimum support, where the support of a pattern is the number of data-sequences that contain the pattern. An example of a sequential pattern is of customers bough F oundation' anRingworld' in one transaction, followed bSecond Foundation' in a later transaction". We generalize the problem as follows. First, we add time constraints that specify a minimum anddor maximum time period between adjacent elements in a pattern. Second, we relax the restric-tion that the items in an element of a sequential pattern must come from the same transaction, instead allowing the items to be present i n a set of transactions whose transaction-times are within a user-speciied time window. Third, given a user-deened taxonomy is-a hierarchy on items, we allow sequential patterns to include items across all levels of the taxonomy. We present GSP, a new algorithm that discovers these generalized se-quential patterns. Empirical evaluation using synthetic and real-life data indicates that GSP is much faster than the AprioriAll algorithm pre-sented in GSP scales linearly with the number of data-sequences, and has very good scale-up properties with respect to the average data-sequence size.},
address = {Berlind},
author = {Srikant, Ramakrishnan and Agrawal, Rakesh},
booktitle = {Advances in Database Technology --EDBT '96},
doi = {10.1007/bfb0014140},
editor = {Apers, P and Bouzeghoub, M and Gardarin, G},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Srikant, Agrawal - 1996 - Mining sequential patterns Generalizations and performance improvements.pdf:pdf},
pages = {1--17},
publisher = {Springer},
title = {{Mining sequential patterns: Generalizations and performance improvements}},
url = {http://link.springer.com/10.1007/BFb0014140},
year = {1996}
}
@inproceedings{Catanese2010,
address = {New York, New York, USA},
author = {Catanese, Salvatore Amato and Fiumara, Giacomo},
booktitle = {Proceedings of the 2nd ACM workshop on Multimedia in forensics, security and intelligence - MiFor '10},
doi = {10.1145/1877972.1877992},
isbn = {9781450301572},
keywords = {LogAnalysis,cellular telephone network analysis,forensic investigation,forensic statistical analysis,visualization},
month = {oct},
pages = {71},
publisher = {ACM Press},
title = {{A visual tool for forensic analysis of mobile phone traffic}},
url = {http://dl.acm.org/citation.cfm?id=1877972.1877992},
year = {2010}
}
@article{Schreck1999,
abstract = {In this paper I theorize that low self-control is a reason why offenders are at high risk of being victims of crime. I reformulate self-control theory into a theory of vulnerability and test several of its hypotheses, using data from a survey administered to a sample of college students. This research investigates how well self-control explains different forms of victimization, and the extent to which self-control mediates the effects of gender and family income on victimization. Low self-control significantly increases the odds of both personal and property victimization and substantially reduces the effects of gender and income. When criminal behavior is controlled, the self-control measure still has a significant direct effect on victimization. These results have many implications for victimization research.},
author = {Schreck, Christopher J.},
doi = {10.1080/07418829900094291},
issn = {0741-8825},
journal = {Justice Quarterly},
keywords = {agile,nsf},
language = {en},
mendeley-tags = {agile,nsf},
month = {sep},
number = {3},
pages = {633--654},
publisher = {Taylor {\&} Francis Group},
title = {{Criminal victimization and low self-control: An extension and test of a general theory of crime}},
url = {http://www.tandfonline.com/doi/abs/10.1080/07418829900094291},
volume = {16},
year = {1999}
}
@article{Ruiz2007,
abstract = {Regardless of test-driven development's well-known benefits, it has suffered slow adoption in GUI development. Because GUIs are one of applications' most important components, testing them is essential for improving the entire system's safety and robustness. Testing GUIs is difficult, but several recommendations and practices can simplify test-driven GUI development for Java Swing applications.This article is part of a special issue on test-driven development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Test-driven GUI development with testNG and abbot},
author = {Ruiz, A and Price, Y W},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {51--57},
title = {{Test-driven GUI development with testNG and abbot}},
url = {citeulike-article-id:3934771 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248396855{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@inproceedings{1254332,
abstract = { The IPsec protocol provides a mechanism to enforce a range of security services for both confidentiality and integrity, enabling secure transmission of information across networks. Dynamic parameterization of IPsec, via the KeyNote trust management system, further enables security mechanisms to adjust the level of security service "on-the-fly" to respond to changing network and operational conditions. However KeyNote requires that an IPsec policy be defined in the KeyNote specification syntax. Defining such a dynamic security policy in the KeyNote policy specification language is complicated and can lead to incorrect specification of the desired policy, thus degrading the security of the network. We present an alternative XML representation of this language and a graphical user interface to create and manage a consistent and correct security policy. The interface has the simplicity of a simple menu-driven editor that not only provides KeyNote with a policy in the specified syntax but also integrates techniques to support administrative policy verification.},
author = {Mohan, R. and Levin, T.E. and Irvine, C.E.},
booktitle = {19th Annual Computer Security Applications Conference, 2003. Proceedings.},
doi = {10.1109/CSAC.2003.1254332},
isbn = {0-7695-2041-3},
keywords = {IPsec protocol,KeyNote trust management system,X},
pages = {276--285},
publisher = {Ieee},
title = {{An editor for adaptive XML-based policy management of IPsec}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1254332},
year = {2003}
}
@article{Adhikari2002,
abstract = {The abysmal quality of software has led to extreme programming (XP), a discipline for developing software that requires test-driven design, continuous testing and acceptance of constant change. As Evant Solutions, an application services provider (ASP) delivering complex demand chain management solutions has found, XP has enabled rapid development of quality software. The XP movement emphasizes testing that helps to bring new developers up to speed rapidly. It also aids developers when they refactor, or change, code. Every programmer writing code has to refactor existing code to streamline it, and continuous testing gives developers confidence that their refactoring works. Because developers spend more time on testing than they do on coding, writing the tests is a skill in itself Tests have to be designed carefully and the results must be scrutinized closely. The test code has to be changed if necessary so the tests do not consume too much time},
annote = {Testing focus boosts XP},
author = {Adhikari, R},
isbn = {1073-9564},
journal = {Application Development Trends},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {27--29},
title = {{Testing focus boosts XP}},
url = {citeulike-article-id:3934531 {\#}},
volume = {9},
year = {2002}
}
@inproceedings{5276940,
abstract = {Addressing recent online banking threats, the banking industry offers us several solutions for our safety online banking experience, however those solutions may not finally secure the users under the rising threats. The main challenges are how to enable safe online banking on a compromised host, and solving the general ignorance of security warning. CAPTCHA is primarily used to anti bot automated login, also, CAPTCHA base application can further provides secure PIN input against keylogger and mouse-logger for bank's customer. Assuming users are always unconscious of security warning in our model, we have designed a series of attacks and defenses under this interesting condition. In this work, we started by formalizing a security defense utilizing CAPCTCHA, its limitations are analyzed; Then, we attacked a local bank employing CAPTCHA solution, which we show how its can be bypassed from its vulnerability in its implementation. We further introduce control-relaying man-in-the-middle (CR-MITM) attack, a remote attack just like a remote terminal service that can capture and relay user inputs without local Trojan assistant, which is possible to defeat CAPTCHA phishing protection in the future. Under our model, we conclude, visual security defense alone is feeble for anti-phishing.},
author = {Leung, Chun-Ming},
booktitle = {Anti-counterfeiting, Security, and Identification in Communication, 2009. ASID 2009. 3rd International Conference on},
doi = {10.1109/ICASID.2009.5276940},
keywords = {CAPTCHA,anti bot automated login,anti-phishing,ban},
pages = {118--123},
title = {{Visual security is feeble for anti-phishing}},
year = {2009}
}
@article{Schneidewind1992,
author = {Schneidewind, N F},
number = {5},
pages = {410--422},
title = {{Methodology for Validating Software Metrics}},
volume = {18},
year = {1992}
}
@article{Yoshida2005,
abstract = {In this paper, we describe a learning support system STAND for computer programming based on test driven development. To check programming assignments involves a lot of work of instructor. The basics of our approach is that the learner checks it instead of the instructor. We propose Triple Checked Testing which is a method for checking whether code by learner fills specification. We developed STAND system that implements the scheme that enables to make self training materials with check function. Materials contain reference code and test case made by instructor, and we support code reading in our system. Our system helps learner to improve not only coding skill but also a variety of programming skills. (author abst.)},
annote = {A Learning Support System for Programming Based on Test Driven Development: Self Training Architecture for Novice Developers},
author = {Yoshida, E and Kakugawa, H},
journal = {IEIC Technical Report (Institute of Electronics, Information and Communication Engineers)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {331},
pages = {27--32},
title = {{A Learning Support System for Programming Based on Test Driven Development: Self Training Architecture for Novice Developers}},
url = {citeulike-article-id:3934848 http://sciencelinks.jp/j-east/article/200524/000020052405A1000720.php},
volume = {105},
year = {2005}
}
@inproceedings{Ning2003b,
address = {New York, New York, USA},
author = {Ning, Peng and Xu, Dingbang},
booktitle = {Proceedings of the 10th ACM conference on Computer and communication security - CCS '03},
doi = {10.1145/948109.948137},
isbn = {1581137389},
keywords = {alert correlation,intrusion detection,profiling attack strategies},
month = {oct},
pages = {200},
publisher = {ACM Press},
title = {{Learning attack strategies from intrusion alerts}},
url = {http://dl.acm.org/citation.cfm?id=948109.948137},
year = {2003}
}
@article{Men2003,
author = {Men, E-},
isbn = {1581136242},
pages = {5--12},
title = {{A 7ran51t10n-6a5ed 5trate9y f0r 06ject-0r1ented 50ftware 7e5t1n9}},
year = {2003}
}
@book{Fowler1999,
address = {Boston, MA},
author = {Fowler, M and Beck, K and Brant, J and Opdyke, W and Roberts, D},
keywords = {refactoring},
pages = {431},
publisher = {Addison-Wesley},
title = {{Refactoring: Improving the Design of Existing Code}},
year = {1999}
}
@inproceedings{5655060,
abstract = {Effective planning, response, and recovery (PRR) involving terrorist attacks or natural disasters come with a vast array of information needs. Much of the required information originates from disparate sources in widely differing formats. However, one common attribute the information often possesses is physical location. The organization and visualization of this information can be critical to the success of the PRR mission. Organizing information geospatially is often the most intuitive for the user. In the course of developing a field tool for the U.S. Department of Homeland Security (DHS) Office for Bombing Prevention, a geospatial integrated problem solving environment software framework was developed by Oak Ridge National Laboratory. This framework has proven useful as well in a number of other DHS, Department of Defense, and Department of Energy projects. An overview of the software architecture along with application examples are presented.},
author = {Koch, D B},
booktitle = {Technologies for Homeland Security (HST), 2010 IEEE International Conference on},
doi = {10.1109/THS.2010.5655060},
keywords = {DHS,PRR,US Department of Homeland Security,bombing},
pages = {211--215},
title = {{A geospatial integrated problem solving environment for Homeland Security applications}},
year = {2010}
}
@book{Wood1995,
address = {New York, NY},
author = {Wood, J and Silver, D},
publisher = {Wiley},
title = {{Joint Application Development}},
year = {1995}
}
@article{ChandraSingpurwallaStephens,
author = {Chandra, M and Singpurwalla, N D and Stephens, M A},
journal = {Journal of the American Statistical Association},
number = {375},
pages = {729--731},
title = {{Kolmogorov Statistics for Tests of Fit for the Extreme-Value and {\{}W{\}}eibull Distributions}},
volume = {76},
year = {1981}
}
@inproceedings{Wilson1997,
address = {New York, New York, USA},
author = {Wilson, William M. and Rosenberg, Linda H. and Hyatt, Lawrence E.},
booktitle = {Proceedings of the 19th international conference on Software engineering - ICSE '97},
doi = {10.1145/253228.253258},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson, Rosenberg, Hyatt - 1997 - Automated analysis of requirement specifications.pdf:pdf},
isbn = {0897919149},
month = {may},
pages = {161--171},
publisher = {ACM Press},
title = {{Automated analysis of requirement specifications}},
url = {http://dl.acm.org/citation.cfm?id=253228.253258},
year = {1997}
}
@article{Grottke2005b,
author = {Grottke, Michael and Trivedi, Kishor S},
journal = {Journal of the Reliability Engineering Association of Japan},
number = {7},
pages = {425--438},
title = {{Software Faults, Software Aging and Software Rejuvenation}},
volume = {27},
year = {2005}
}
@misc{McGibbon2007,
author = {McGibbon, Thomas},
title = {{Return on Investment from Software Process Improvement}},
url = {https://goo.gl/MJIJZ3},
year = {2007}
}
@article{Savoia2007,
abstract = {The views of Alberto, cofounder and CTO of Agitar Software, about the future prospects of software developer and software testing, are presented. Developer testing play key role in testing of various codes using various methodologies such as JUnit (the unit-testing framework), eXtreme Progamming and other Agile methodologies. Developer testing are mainly performed by test-infected developers and organizations as most of the software developer consider software testing a method for quality audit (QA). Significant changes to the developer testing practices should be done to efficient software development. Software developers with T3 gene efficiently conduct software testing compared to software developers with T1 and T2 gene. Software developer should use Test Driven Development (TDD) method to efficiently test software.},
annote = {Are you immune to test infection?},
author = {Savoia, A},
journal = {Dr. Dobb's Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {7},
pages = {10},
title = {{Are you immune to test infection?}},
url = {citeulike-article-id:3934782 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34547243218{\&}{\#}38 partnerID=40},
volume = {32},
year = {2007}
}
@article{Teasley2002,
author = {Teasley, Stephanie D. and Covi, Lisa A. and Krishnan, M.S. and Olson, Judith S.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Teasley et al. - 2002 - Rapid software development through team collocation.pdf:pdf},
journal = {IEEE Transactions on Software Engineering},
keywords = {collaboration},
mendeley-tags = {collaboration},
number = {7},
pages = {671--683},
publisher = {Published by the IEEE Computer Society},
title = {{Rapid software development through team collocation}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/TSE.2002.1019481},
volume = {28},
year = {2002}
}
@inproceedings{Min2013,
address = {San Antonio, TX},
author = {Min, Jun-Ki and Wiese, Jason and Hong, Jason I. and Zimmerman, John},
booktitle = {Proceedings of the 2013 Conference on Computer Supported Cooperative Work (CSCW '13)},
doi = {10.1145/2441776.2441810},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Min et al. - 2013 - Mining smartphone data to classify life-facets of social relationships.pdf:pdf},
isbn = {9781450313315},
keywords = {agile,interpersonal relationships mining,life-facets,mobile social network,nsf,smartphone},
mendeley-tags = {agile,nsf},
month = {feb},
pages = {285},
publisher = {ACM Press},
title = {{Mining smartphone data to classify life-facets of social relationships}},
year = {2013}
}
@techreport{IEEEComputerSociety1986,
address = {New York, NY},
author = {{IEEE Computer Society}},
institution = {The Institute of Electrical and Electronics Engineergs},
title = {{IEEE Standard for Software Unit Testing}},
year = {1986}
}
@inproceedings{Janzen2007b,
abstract = {This paper reports on a pilot project that incorporated small empirical studies in three industry short courses. These laboratory experiments were one component of a larger leveled study on the effects of test-driven development (TDD) on internal software quality. The approach is proposed to have pedagogical value to student-developers by improving their understanding and appreciation for empirical evidence, to instructors by providing feedback through surveys and exercises, and to the community at large by reporting results of the studies. Pre-experiment surveys in the three pilot experiments revealed large differences in programmer opinions of TDD. Possible correlations to development environment and programmer experience will be proposed. Post-experiment surveys revealed improvements in programmer opinions of TDD following the experiment exercises. Crafting sufficiently small but interesting assignments proved to be challenging. Few complete solutions were submitted and some developers were unwilling to submit their partial solutions. Positive observations will be made regarding the use of experiments in short courses. For instance, participating in the study encourages analytical thinking, prompts developers to evaluate alternative approaches, and instills the value of empirical evidence. Ethical concerns regarding threats to validity are raised and addressed. The authors find that ethical considerations not only support performing such studies, but encourage it as the duty of software professionals. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Empirical software engineering in industry short courses},
author = {Janzen, D S and Turner, C S and Saiedian, H},
booktitle = {Software Engineering Education Conference, Proceedings},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {89--96},
title = {{Empirical software engineering in industry short courses}},
url = {citeulike-article-id:3934660 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34948876396{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Zhang2007,
abstract = {According to the two core values of Test-Driven Development, firstly this paper uses test cases to analyze the defect of present software protection products, and secondly recreates the core library of protection products according to the defect list. And the security model in the recreated core library introduces a new concept: security tunnel, so that it can prevent hacker to crack the core library. Adding mutex code to key code of the core library improves its robust. This kind of design ensures the quality of the products, as well as shortens the development period of the core library.},
annote = {Development of core library on software protection products},
author = {Zhang, R},
journal = {Beijing Jiaotong Daxue Xuebao/Journal of Beijing Jiaotong University},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {128--130},
title = {{Development of core library on software protection products}},
url = {citeulike-article-id:3934851 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-41349093209{\&}{\#}38 partnerID=40},
volume = {31},
year = {2007}
}
@article{Gupta2010,
author = {Gupta, A and Cruzes, Daniela and Shull, Forrest and Conradi, Reidar and Ronneberg, H and Landre, E},
journal = {Journal of Software Maintenance and Evolution: research and Practice},
number = {5},
pages = {359--380},
title = {{An examination of change profiles in reusable and non-reusable software systems}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/smr.459/abstract},
volume = {22},
year = {2010}
}
@inproceedings{Briand1998b,
address = {Kyoto, Japan},
author = {Briand, Lionel C and Emam, Khaled El and Bomarius, Frank},
pages = {390--399},
title = {{COBRA: A Hybrid Method for Software Cost Estimation, Benchmarking, and Risk Assessment}},
year = {1998}
}
@inproceedings{posnett11,
author = {Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
booktitle = {Proceedings of ASE'11},
title = {{Ecological Inference in Empirical Software Engineering}},
year = {2011}
}
@inproceedings{Horn2011a,
address = {New York, New York, USA},
author = {Horn, Chris and D'Amico, Anita},
booktitle = {Proceedings of the 8th International Symposium on Visualization for Cyber Security - VizSec '11},
doi = {10.1145/2016904.2016909},
isbn = {9781450306799},
keywords = {Protovis,computer network defense (CND),decision aid,decision model,information security,visual analytics,visualization,web-based tools},
month = {jul},
pages = {1--6},
publisher = {ACM Press},
title = {{Visual analysis of goal-directed network defense decisions}},
url = {http://dl.acm.org/citation.cfm?id=2016904.2016909},
year = {2011}
}
@article{McCaulley1990,
author = {McCaulley, M H},
keywords = {MBTI,myers-briggs},
pages = {537--542},
title = {{The MBTI and Individual Pathways in Engineering Design}},
volume = {80},
year = {1990}
}
@inproceedings{Csallner2005,
address = {St. Louis, MO},
author = {Csallner, Christoph and Smaragdakis, Yannis},
pages = {422--431},
title = {{Check 'n' Crash: Combining Static Checking and Testing}},
year = {2005}
}
@misc{Raines2002,
author = {Raines, Claire},
title = {{Managing Millennials}},
url = {http://www.generationsatwork.com/articles/millenials.htm},
year = {2002}
}
@article{Aharony2011643,
annote = {The Ninth Annual {\{}IEEE{\}} International Conference on Pervasive Computing and Communications (PerCom 2011)},
author = {Aharony, Nadav and Pan, Wei and Ip, Cory and Khayal, Inas and Pentland, Alex},
doi = {10.1016/j.pmcj.2011.09.004},
issn = {1574-1192},
journal = {Pervasive and Mobile Computing},
keywords = {Mobile health,Mobile sensing,Rich data,Social computing,Social health,agile,nsf},
mendeley-tags = {agile,nsf},
number = {6},
pages = {643--659},
title = {{Social fMRI: Investigating and shaping social mechanisms in the real world}},
url = {http://www.sciencedirect.com/science/article/pii/S1574119211001246},
volume = {7},
year = {2011}
}
@article{Mugridge2003,
abstract = {We identify two main challenges in the teaching of test driven development (TDD) over the last two years. The first challenge is to get students to rethink learning and design, and to really engage with this new approach. The second challenge is to explicitly develop their skills in testing, design and refactoring, given that they have little experience in these areas. This requires that fast and effective feedback be provided},
annote = {Challenges in teaching test driven development},
author = {Mugridge, R},
isbn = {3540402152},
journal = {Extreme Programming and Agile Processes in Software Engineering 4th International Conference, XP 2003 Proceedings Lecture Notes in Computer Science Vol 2675},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {410--413},
title = {{Challenges in teaching test driven development}},
url = {citeulike-article-id:3934732 {\#}},
volume = {2675},
year = {2003}
}
@article{Koss2002,
annote = {Test driven development in C/C++},
author = {Koss, R and Langr, J},
journal = {C/C++ Users Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {10},
pages = {6},
title = {{Test driven development in C/C++}},
url = {citeulike-article-id:3934682 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0036787963{\&}{\#}38 partnerID=40},
volume = {20},
year = {2002}
}
@article{Ma2004a,
author = {Ma, Kwan-Liu},
doi = {10.1145/1039140.1039146},
issn = {00978930},
journal = {ACM SIGGRAPH Computer Graphics},
month = {nov},
number = {4},
pages = {4},
title = {{Visualization for security}},
url = {http://dl.acm.org/citation.cfm?id=1039140.1039146},
volume = {38},
year = {2004}
}
@inproceedings{5276965,
abstract = {A consumer friendly and consumer's self identification universal anti-counterfeiting system that enables consumers using their mobile phone to decode an invisible optical security safety product code concealed in a MOID Tag for instant visual identification and sends the information back to the anti-counterfeit control center via the global cellular network for instant online identification. The control center will also provide additional information of the product to the mobile phone and when the center detects a fake signal, it will request the consumer to provide information of the merchant and alert local law enforcement agents to follow up.},
author = {Lo, A K W and Ho, S S O},
booktitle = {Anti-counterfeiting, Security, and Identification in Communication, 2009. ASID 2009. 3rd International Conference on},
doi = {10.1109/ICASID.2009.5276965},
keywords = {MOID,anticounterfeit control,global cellular netwo},
pages = {429--431},
title = {{MOID mobile phone micro optic identification}},
year = {2009}
}
@misc{Mosemann2001,
address = {Toronto, ON},
author = {Mosemann, Russell and Wiedenbeck, Susan},
pages = {79--88},
title = {{Navigation and Comprehension of Programs by Novices Programmers}},
year = {2001}
}
@inproceedings{Bailey2000,
address = {Nashville, TN},
author = {Bailey, Brian P and Konstan, Josephy A and Carlis, John V},
keywords = {agile,hci,interruption,nsf},
mendeley-tags = {agile,nsf},
pages = {757--762},
title = {{Measuring the Effects of Interruptions on Task Performance in the User Interface}},
volume = {2},
year = {2000}
}
@inproceedings{Brechner2005,
abstract = {Like many software companies, Microsoft has been doing distributed application development for many years. However, recent changes in the market have altered the rules, both in terms of customer expectations and programming models for ubiquitous interconnected smart devices. These changes have provoked two dramatic shifts in the way we develop software. The first is the creation and use of the .NET Framework as a simple, secure, and robust platform for device-independent software development, data manipulation, and communications. The second is an agile yet highly disciplined approach to designing, testing, implementing, and verifying our software which presumes all bugs are unacceptable and must be found and fixed early before they impact internal groups, external partners, and eventually our customers. This paper discusses the nature and impact of these two dramatic shifts to the development practices at Microsoft. Copyright 2005 ACM.},
annote = {Journey of enlightenment: The evolution of development at Microsoft},
author = {Brechner, E},
booktitle = {Proceedings - 27th International Conference on Software Engineering, ICSE05},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {39--42},
title = {{Journey of enlightenment: The evolution of development at Microsoft}},
url = {citeulike-article-id:3934560 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33244487794{\&}{\#}38 partnerID=40},
year = {2005}
}
@article{4302579,
abstract = {Computer network defense (CND) requires analysts to detect both known and novel forms of attacks in massive volumes of network data. It's through discovering the unexpected that CND analysts detect new versions of mal ware (such as viruses and Trojan horses) that have passed through their antivirus products, new methods of intrusion that have breached their firewalls and intrusion detection systems (IDSs), and new groups of cyber-criminals pressing the attack. This paper presents visual assistant for information assurance analysis. VIAssist is a visualization framework based on a comprehensive cognitive task analysis of CND analysts, and so fits their work practices and operational environment.},
author = {D'Amico, A D and Goodall, J R and Tesone, D R and Kopylec, J K},
doi = {10.1109/MCG.2007.137},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {CND analyst,VIAssist,cognitive task analysis,compu},
number = {5},
pages = {20--27},
title = {{Visual Discovery in Computer Network Defense}},
volume = {27},
year = {2007}
}
@article{KitchenhamA.vonMayrhauserF.NiessinkN.SchneidewindJ.SingerG.H.TravassosS.TakadaR.VehilainenandH.Yang1999,
annote = {Ideas for the XP-EF are here. Provides an ontology of software maintenance.},
author = {{Kitchenham  A. von Mayrhauser, F. Niessink, N. Schneidewind, J. Singer, G. H. Travassos, S. Takada, R. Vehilainen, and H. Yang}, B A},
isbn = {TR99-03},
keywords = {maintenance,ontology},
pages = {365--389},
title = {{Towards an Ontology of Software Maintenance}},
volume = {11},
year = {1999}
}
@inproceedings{Hu2010a,
address = {New York, New York, USA},
author = {Hu, Hongxin and Ahn, Gail-Joon and Kulkarni, Ketan},
booktitle = {Proceedings of the 3rd ACM workshop on Assurable and usable security configuration - SafeConfig '10},
doi = {10.1145/1866898.1866902},
isbn = {9781450300933},
keywords = {anomaly management,firewall policies,visualization tool},
month = {oct},
pages = {17},
publisher = {ACM Press},
title = {{FAME}},
url = {http://dl.acm.org/citation.cfm?id=1866898.1866902},
year = {2010}
}
@article{Mannila2006,
abstract = {In this paper, we present the results from a two-part study. We analyze 60 programs written by novice programmers aged 16 – 19 after their first programming course, in either Java or Python. The aim is to find difficulties independent of the language used, and such originating from the language. Second, we analyze the transition from a “simple” language to a more “advanced” one, by following up on eight students, who learned programming in Python before moving on to Java. Our results suggest that a simple language gives rise to fewer syntax errors as well as logic errors. The qualitative part of our study did not reveal any disadvantages from having learned to program in a simple language when moving on to a more complex one. This suggests that not only can a simple language be used when introducing programming as a general skill, but also when providing basic skills to future professionals in the field. {\textcopyright} 2006 Taylor {\&} Francis.},
author = {Mannila, Linda and Peltom{\"{a}}ki, Mia and Salakoski, Tapio},
doi = {10.1080/08993400600912384},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mannila, Peltom{\"{a}}ki, Salakoski - 2006 - What about a simple language Analyzing the difficulties in learning to program.pdf:pdf},
issn = {17445175},
journal = {Computer Science Education},
number = {3},
pages = {211--227},
publisher = {Routledge},
title = {{What about a simple language? Analyzing the difficulties in learning to program}},
url = {https://www.tandfonline.com/doi/abs/10.1080/08993400600912384},
volume = {16},
year = {2006}
}
@inproceedings{Jones2007,
abstract = {One recent change in software development is developers starting to take responsibility for the quality of their work by writing and executing automated tests. As with any new activity, there is a wide range of ways to perform this task. DevCreek collects, aggregates, and displays testing activity for individuals, teams, and the developer community as a whole. The goal is to provide individuals with global norms with which they can compare their testing. This demonstration will show the data collection, real-time feedback, and aggregate reporting facilities within DevCreek. We will exercise our Eclipse plug-in during a short, live development session.},
annote = {Improving quality together},
author = {Jones, D G and Cameron, G R},
booktitle = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {878--879},
title = {{Improving quality together}},
url = {citeulike-article-id:3934665 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42149146827{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Casey2006,
author = {Casey, Eoghan},
doi = {10.1145/1113034.1113068},
issn = {00010782},
journal = {Communications of the ACM},
month = {feb},
number = {2},
pages = {48},
publisher = {ACM},
title = {{Investigating sophisticated security breaches}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=1113068{\&}type=html},
volume = {49},
year = {2006}
}
@article{Barnett2004,
author = {Barnett, Mike and Grieskamp, Wolfgang and Nachmanson, Lev and Schulte, Wolfram and Tillmann, Nikolai and Veanes, Margus},
pages = {252--266},
title = {{Towards a Tool Environment for Model-Based Testing with AsmL}},
year = {2004}
}
@inproceedings{Best2010a,
address = {New York, New York, USA},
author = {Best, Daniel M. and Bohn, Shawn and Love, Douglas and Wynne, Adam and Pike, William A.},
booktitle = {Proceedings of the Seventh International Symposium on Visualization for Cyber Security - VizSec '10},
doi = {10.1145/1850795.1850805},
isbn = {9781450300131},
keywords = {behavior modeling,high-throughput,network flow,real-time,symbolic aggregate approximation,visualization},
month = {sep},
pages = {79--90},
publisher = {ACM Press},
title = {{Real-time visualization of network behaviors for situational awareness}},
url = {http://dl.acm.org/citation.cfm?id=1850795.1850805},
year = {2010}
}
@inproceedings{Ravi2005,
author = {Ravi, Nishkam and Dandekar, Nikhil and Mysore, Preetham and Littman, ML},
booktitle = {Proceedings of the 17th Conference on Innovative Applications of Artificial Intelligence (AAAI '05)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ravi et al. - 2005 - Activity recognition from accelerometer data.pdf:pdf},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {1541--1546},
title = {{Activity recognition from accelerometer data}},
url = {http://www.aaai.org/Papers/IAAI/2005/IAAI05-013},
year = {2005}
}
@inproceedings{Seaman2008,
address = {Kaiserslautern, Germany},
author = {Seaman, Carolyn B. and Shull, Forrest and Regardie, Myrna and Elbert, D and Feldmann, Raimund L. and Guo, Y and Godfrey, Sally},
booktitle = {International Symposium on Empirical Software Engineering},
title = {{Defect Categorization: Making Use of a Decade of Widely Varying Historical Data}},
year = {2008}
}
@article{Kervinen2006,
author = {Kervinen, Antti and Maunumaa, Mika and P{\"{a}}{\"{a}}kk{\"{o}}nen, Tuula and Katara, Mika},
pages = {16--31},
title = {{Model-Based Testing Through a GUI}},
year = {2006}
}
@article{Loui2008,
abstract = {The academic programming language community continues to reject the change in programming practices brought about by scripting. We need a programming language pragmatics to go past the analysis of syntax and semantics in the same way that linguistics studies perlocution and illocution. Pragmatic questions are not the easiest for mathematically inclined computer scientists to address. They refer by nature to people and their habits, sociology, and the day's technological demands. An industrial psychology literature, apart from computing, has sometimes addressed questions of this kind. But this kind of study must become part of programming language theory within computing. It's the importance of just these kinds of questions that makes programmers choose scripting languages. The author recommends that scripting, not Java, be taught first, asserting that students should learn to love their own possibilities before they learn to loathe other people's restrictions.},
author = {Loui, Ronald P.},
doi = {10.1109/MC.2008.228},
issn = {00189162},
journal = {Computer},
keywords = {Computing curricula,Programming languages,Scripting,Software engineering},
month = {jul},
number = {7},
pages = {22--26},
title = {{In praise of scripting: Real programming pragmatism}},
url = {http://ieeexplore.ieee.org/document/4563874/},
volume = {41},
year = {2008}
}
@incollection{Miyata1986,
address = {Hillsdale, NJ},
author = {Miyata, Yoshiro and Norman, Donald A and Draper, Stephen W},
keywords = {hci,interruption},
pages = {267--284},
publisher = {Lawrence Erlbaum Associates},
title = {{Psychological Issues in Support of Multiple Activities}},
year = {1986}
}
@article{1333625,
abstract = {The task of sifting through large amounts of data to find useful information spawned the field of data mining. Most data mining approaches are based on machine-learning techniques, numerical analysis, or statistical modeling. They use human interaction and visualization only minimally. Such automatic methods can miss some important features of the data. Incorporating human perception into the data mining process through interactive visualization can help us better understand the complex behaviors of computer network systems. This article describes visual-analytics-based solutions and outlines a visual exploration process for log analysis. Three log-file analysis applications demonstrate our approach's effectiveness in discovering flaws and intruders in network systems.},
author = {Teoh, Soon Tee and Ma, Kwan-Liu and Wu, S F and Jankun-Kelly, T.J. J},
doi = {10.1109/MCG.2004.26},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {Application software,Computer Communication Networks,Computer Graphics,Computer Security,Computer networks,Data analysis,Data mining,Data security,Data visualization,Database Management Systems,Databases,Factual,Humans,Information Storage and Retrieval,Internet routing stability,Online,Online Systems,Performance analysis,Software,User-Computer Interface,Visual analytics,data mining,data visualisation,flaws detection,human interaction,information visualization,intr,intruders,intrusion detection,log-file analysis,machine-learning,network visualization,security of data,visual data analysis,visual data mining},
month = {sep},
number = {5},
pages = {27--35},
shorttitle = {Computer Graphics and Applications, IEEE},
title = {{Detecting flaws and intruders with visual data analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1333625},
volume = {24},
year = {2004}
}
@article{Briggs2007,
abstract = {Test-Driven Development is a design strategy where a set of tests over a class is defined prior to the implementation of that class. The goal is to use the tests to exercise the class being developed, to provide immediate feedback of the overall quality of the code, and to identify and correct bugs as they are written. Testing is usually performed with automated testing tools, such as JUnit, which give clear feedback about the status of the tests 1 . The test-first approach requires students to think about the problem, identify corner cases, analyze ways their code may fail, and evaluate the goodness of their design. This paper presents a tool for teaching CS1 using a "Test-First" approach that will allow students to construct their tests using a simple GUI interface. The goal is to give CS1 students the ability to do test-driven development independently. AD  -, USA},
annote = {Tools and techniques for test-driven learning in CS1},
author = {Briggs, Tom and Girard, Dudley},
isbn = {1937-4771},
journal = {J. Comput. Small Coll.},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {37--43},
title = {{Tools and techniques for test-driven learning in CS1}},
url = {citeulike-article-id:3934561 http://portal.acm.org/citation.cfm?id=1181849.1181854},
volume = {22},
year = {2007}
}
@article{Schreck2004,
abstract = {The fact that crime and victimization share similar correlates suggests that family and peer contexts are potentially useful for explaining individual differences in violent victimization. In this research, we used routine activities and lifestyles frameworks to reveal how strong bonds of family attachment can promote more effective guardianship while simultaneously making children less attractive as targets and limiting their exposure to motivated offenders. Conversely, the routine activities perspective suggests that exposure to delinquent peers will enhance risk. Using data from the National Longitudinal Study of Adolescent Health (Add Health), we found that family and peer context variables do correspond with a higher risk of violent victimization among teenagers, net controls for unstructured and unsupervised activities and demographic characteristics. The role of family and peer group characteristics in predicting victimization risk suggests new theoretical directions for victimization research.},
author = {Schreck, Christopher J. and Fisher, Bonnie S.},
doi = {10.1177/0886260504268002},
issn = {0886-2605},
journal = {Journal of Interpersonal Violence},
keywords = {Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adolescent Psychology,Crime Victims,Crime Victims: psychology,Crime Victims: statistics {\&} numerical data,Family Relations,Female,Follow-Up Studies,Humans,Interpersonal Relations,Juvenile Delinquency,Juvenile Delinquency: psychology,Juvenile Delinquency: statistics {\&} numerical data,Life Style,Male,Peer Group,Questionnaires,Risk Factors,Risk-Taking,United States,agile,nsf},
mendeley-tags = {agile,nsf},
month = {sep},
number = {9},
pages = {1021--41},
pmid = {15296615},
title = {{Specifying the Influence of Family and Peers on Violent Victimization: Extending Routine Activities and Lifestyles Theories}},
url = {http://jiv.sagepub.com/content/19/9/1021.short},
volume = {19},
year = {2004}
}
@article{Ropponen2000,
abstract = {Software risk management can be defined as an attempt to formalize risk oriented correlates of development success into a readily applicable set of principles and practices. By using a survey instrument we investigate this claim further. The investigation addresses the following questions: 1) What are the components of software development risk? 2) how does risk management mitigate risk components, and 3) what environmental factors if any influence them? Using principal component analysis we identify six software risk components: 1) scheduling and timing risks, 2) functionality risks, 3) subcontracting risks, 4) requirements management, 5) resource usage and performance risks, and 6) personnel management risks. By using one-way ANOVA with multiple comparisons we examine how risk management (or the lack of it) and environmental factors (such as development methods, manager's experience) influence each risk component. The analysis shows that awareness of the importance of risk management and systematic practices to manage risks have an effect on scheduling risks, requirements management risks, and personnel management risks. Environmental contingencies were observed to affect all risk components. This suggests that software risks can be best managed by combining specific risk management considerations with a detailed understanding of the environmental context and with sound managerial practices, such as relying on experienced and well-educated project managers and launching correctly sized projects},
author = {Ropponen, J. and Lyytinen, K.},
doi = {10.1109/32.841112},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
number = {2},
pages = {98--112},
title = {{Components of software development risk: how to address them? A project manager survey}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=841112},
volume = {26},
year = {2000}
}
@article{ElEmam2001,
author = {{El Emam}, Khaled and Benlarbi, Saida and Goel, Nishith Nisith and Rai, Shesh N.},
doi = {10.1109/32.935855},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
number = {7},
pages = {630--650},
title = {{The confounding effect of class size on the validity of object-oriented metrics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=935855},
volume = {27},
year = {2001}
}
@article{Stott2003,
abstract = {Established software engineering practices which are applied to the demands of developing today's software is discussed. It starts by describing the sort of broken process which is pandemic in the industry and then explains the nature of XP in terms of how it can address this problem. This paper discusses some of the more controversial XP practices such as eschewing a formal requirements document and test-driven development.},
annote = {Extreme programming: Turning the world upside down},
author = {Stott, W},
journal = {IEE Computing and Control Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {18--23},
title = {{Extreme programming: Turning the world upside down}},
url = {citeulike-article-id:3934808 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0038819448{\&}{\#}38 partnerID=40},
volume = {14},
year = {2003}
}
@article{Kazanzides2009a,
abstract = {The use of robots in medicine is increasing, leading to the call for specific safety standards. This is a challenging endeavor, however, because the patient must usually be placed in the robot's workspace and the medical staff must frequently interact with the robot. Although specific safety standards for medical robots do not yet exist, there are several medical device standards and well-established principles of risk analysis and safety design that can and should be applied. This paper presents a tutorial overview of safety design for medical robots, starting with a discussion of high-level safety requirements, followed by methods for risk assessment (or hazard analysis) and a brief discussion of some sample safety strategies.},
author = {Kazanzides, Peter},
institution = {Dept. of Computer Science, Johns Hopkins University, Baltimore, MD, USA. pkaz@jhu.edu},
journal = {Conference Proceedings of the International Conference of IEEE Engineering in Medicine and Biology Society},
keywords = {biomedical engineering,equipment design,equipment safety,humans,risk assessment,robotics,robotics instrumentation,robotics standards},
pages = {7208--7211},
pmid = {19965279},
title = {{Safety design for medical robots.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19965279},
volume = {2009},
year = {2009}
}
@inproceedings{Wellington2005,
abstract = {Shippensburg University offers an upper division project course in which the students use a variant of Extreme Programming (XP) including: the Planning Game, the Iteration Planning Game, test driven development, stand-up meetings and pair programming. We start the course with two weeks of controlled lab exercises designed to teach the students about test driven development in JUnit/Eclipse and designing for testability (with the humble dialog box design pattern) while practicing pair programming. The rest of our semester is spent in three four-week iterations developing a product for a customer. Our teams are generally large (14-16 students) so that the projects can be large enough to motivate the use of configuration management and defect tracking tools. The requirement of pair programming limits the amount of project work the students can do outside of class, so class time is spent on the projects and teaching is on-demand individual mentoring with lectures/labs inserted as necessary. One significant challenge in managing this course is tracking individual responsibilities and activities to ensure that all of the students are fully engaged in the project. To accomplish this, we have modified the story and task cards from XP to provide feedback to the students and track individual performance against goals as part of the students' grades. The resulting course has been well received by the students. This paper will describe this course in more detail and assess its effect on students' software engineering background through students' feedback and code metrics. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {Managing a project course using extreme programming},
author = {Wellington, C A},
booktitle = {Proceedings - Frontiers in Education Conference, FIE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Managing a project course using extreme programming}},
url = {citeulike-article-id:3934834 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33947248135{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@misc{Melnik2007,
abstract = {This descriptive case study is about the dynamics of a software engineering team using executable acceptance test-driven development in a real world project. The experiences of a customer, a developer, and a tester were discussed. The observed consensus among multiple stakeholders speaks of the effectiveness of the practice in the given context. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Multiple perspectives on executable acceptance test-driven development},
author = {Melnik, G and Maurer, F},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {245--249},
title = {{Multiple perspectives on executable acceptance test-driven development}},
url = {citeulike-article-id:3934716 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38149029225{\&}{\#}38 partnerID=40},
volume = {4536 LNCS},
year = {2007}
}
@article{Williams2002,
author = {Williams, Laurie and Wiebe, Eric and Yang, Kai and Ferzli, Miriam and Miller, Carol},
number = {3},
pages = {197--212},
title = {{In Support of Pair Programming in the Introductory Computer Science Course}},
volume = {12},
year = {2002}
}
@article{Yadla2005,
author = {Yadla, Suresh and Hayes, Jane Huffman and Dekhtyar, Alex},
doi = {10.1007/s11334-005-0011-3},
issn = {1614-5046},
journal = {Innovations in Systems and Software Engineering},
month = {jul},
number = {2},
pages = {116--124},
title = {{Tracing requirements to defect reports: an application of information retrieval techniques}},
url = {http://link.springer.com/10.1007/s11334-005-0011-3},
volume = {1},
year = {2005}
}
@inproceedings{Rumpe2002,
address = {Alghero, Italy},
author = {Rumpe, Bernhard and Schr{\"{o}}der, Astrid},
booktitle = {Proceedings of the 3rd Int'l. Conference on Extreme Programming and Agile Processes in Software Engineering (XP2002)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rumpe, Schr{\"{o}}der - 2002 - Quantitative survey on extreme programming projects.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
pages = {26--30},
publisher = {Citeseer},
title = {{Quantitative survey on extreme programming projects}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.15.7477{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@inproceedings{Mishali2008,
abstract = {A tool is presented for guiding Test-Driven Development (TDD), called TDD-Guide. The tool is integrated into an existing development environment and guides the developer during the development by providing notifications that encourage use of TDD. The TDD practice is defined through rules that can easily be changed and are used to generate code incorporated to a development environment using an aspect-based framework, so that the development of the tool has agile characteristics. Feedback from user experiments both validates the rules and suggests refinements to improve TDD-Guide, as is shown in descriptions of two user experiments.},
annote = {The TDD-Guide Training and Guidance Tool for Test-Driven Development},
author = {Mishali, O and Dubinsky, Y and Katz, S},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {10--14},
publisher = {Springer},
title = {{The TDD-Guide Training and Guidance Tool for Test-Driven Development}},
url = {citeulike-article-id:3934721 http://www.springerlink.com/index/r0243v85r38t7v31.pdf},
year = {2008}
}
@article{Begel2007,
author = {Begel, Andrew and Nagappan, Nachiappan},
doi = {10.1109/ESEM.2007.12},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Begel, Nagappan - 2007 - Usage and Perceptions of Agile Software Development in an Industrial Context An Exploratory Study.pdf:pdf},
issn = {1938-6451},
journal = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
keywords = {agile},
mendeley-tags = {agile},
month = {sep},
pages = {255--264},
publisher = {Ieee},
title = {{Usage and Perceptions of Agile Software Development in an Industrial Context: An Exploratory Study}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4343753},
year = {2007}
}
@article{Shull2005a,
abstract = {While it is clear that there are many sources of variation from one development context to another, it is not clear a priori what specific variables will influence the effectiveness of a process in a given context. For this reason, we argue that knowledge about software process must be built from families of studies, in which related studies are run within similar contexts as well as very different ones. Previous papers have discussed how to design related studies so as to document as precisely as possible the values of likely context variables and be able to compare with those observed in new studies. While such a planned approach is important, we argue that an opportunistic approach is also practical. The approach would combine results from multiple individual studies after the fact, enabling recommendations to be made about process effectiveness in context. In this paper, we describe two processes with which we have been working to build empirical knowledge about software development processes: one is a manual and informal approach, which relies on identifying common beliefs or ‘folklore' to identify useful hypotheses and a manual analysis of the information in papers to investigate whether there is support for those hypotheses; the other is a formal approach based around encoding the information in papers into a structured hypothesis base that can then be searched to organize hypotheses and their associated support. We test these processes by applying them to build knowledge in the area of defect folklore (i.e. commonly accepted heuristics about software defects and their behavior). We show that the formal methodology can produce useful and feasible results, especially when it is compared to the results output from the more manual, expert-based approach. The formalized approach, by relying on a reusable hypothesis base, is repeatable and also capable of producing a more thorough basis of support for hypotheses, including results from papers or articles that may have been overlooked or not considered by the experts.},
author = {Shull, Forrest and Cruzes, Daniela and Basili, Victor R and Mendonca, M},
journal = {Information and Software Technology},
keywords = {Journal},
month = {dec},
number = {15},
pages = {1019--1032},
title = {{Simulating Families of Studies to Build Confidence in Defect Hypotheses}},
url = {http://www.sciencedirect.com/science?{\_}ob=ArticleURL{\&}{\_}udi=B6V0B-4HKCYTK-2{\&}{\_}user=961305{\&}{\_}coverDate=12/31/2005{\&}{\_}rdoc=1{\&}{\_}fmt=high{\&}{\_}orig=gateway{\&}{\_}origin=gateway{\&}{\_}sort=d{\&}{\_}docanchor={\&}view=c{\&}{\_}searchStrId=1734428957{\&}{\_}rerunOrigin=scholar.google{\&}{\_}acct=C000049425{\&}{\_}version=1{\&}{\_}urlVersion=0{\&}{\_}userid=961305{\&}md5=9820f85f2bb54ca0bf4e0cdd95be4e63{\&}searchtype=a},
volume = {47},
year = {2005}
}
@inproceedings{5438042,
abstract = {Despite a long standing need to incorporate human factors into security risk analysis, taking a balanced approach to analysing security and usability concerns remains a challenge. Balancing security and usability is difficult due to human biases in security perception, and managing the sheer volume of data arising from risk and task analysis. This paper presents an approach for qualitatively and quantitively analysing and visualising the results of risk and task analysis. We demonstrate this approach using a realistic example, and we discuss how these techniques fit within the larger context of secure systems design.},
author = {Faily, S and Flechais, I},
booktitle = {Availability, Reliability, and Security, 2010. ARES '10 International Conference on},
doi = {10.1109/ARES.2010.28},
keywords = {human factors,integrating requirements and informa},
pages = {543--548},
title = {{Analysing and Visualising Security and Usability in IRIS}},
year = {2010}
}
@inproceedings{Stephano2006b,
address = {New York, New York, USA},
author = {Stephano, Amanda L. and Groth, Dennis P.},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179598},
isbn = {1595935495},
keywords = {evaluation,security,usability,visual interfaces},
month = {nov},
pages = {109},
publisher = {ACM Press},
title = {{USEable security}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179598},
year = {2006}
}
@article{Sweeney2002,
abstract = {Consider a data holder, such as a hospital or a bank, that has a privately held collection of person-specific, field structured data. Suppose the data holder wants to share a version of the data with researchers. How can a data holder release a version of its private data with scientific guarantees that the individuals who are the subjects of the data cannot be re-identified while the data remain practically useful? The solution provided in this paper includes a formal protection model named k-anonymity and a set of accompanying policies for deployment. A release provides k-anonymity protection if the information for each person contained in the release cannot be distinguished from at least k-1 individuals whose information also appears in the release. This paper also examines re-identification attacks that can be realized on releases that adhere to k-anonymity unless accompanying policies are respected. The k-anonymity protection model is important because it forms the basis on which the real-world syste...},
author = {Sweeney, Latanya},
doi = {10.1142/S0218488502001648},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sweeney - 2002 - k-Anonymity A Model for Protecting Privacy.pdf:pdf},
issn = {0218-4885},
journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
keywords = {Data anonymity,agile,data fusion,data privacy,nsf,privacy,re-identification},
language = {en},
mendeley-tags = {agile,nsf},
month = {oct},
number = {5},
pages = {557--570},
publisher = {World Scientific Publishing Company},
title = {{k-Anonymity: A Model for Protecting Privacy}},
url = {http://www.worldscientific.com/doi/abs/10.1142/S0218488502001648},
volume = {10},
year = {2002}
}
@article{Afrati2006,
abstract = {We consider the problem of answering queries using views, where queries and views are conjunctive queries with arithmetic comparisons over dense orders. Previous work only considered limited variants of this problem, without giving a complete solution. We first show that obtaining equivalent rewritings for conjunctive queries with arithmetic comparisons is decidable. Then, we consider the problem of finding maximally contained rewritings (MCRs) where the decidability proof does not carry over. We investigate two special cases of this problem where the query uses only semi-interval comparisons. In both cases decidability of finding MCRs depends on the query containment test. First, we address the case where the homomorphism property holds in testing query containment. In this case decidability is easy to prove but developing an efficient algorithm is not trivial. We develop such an algorithm and prove that it is sound and complete. This algorithm applies in many cases where the query uses only left (or right) semi-interval comparisons. Then, we develop a new query containment test for the case where the containing query uses both left and right semi-interval comparisons but with only one left (or right) semi-interval subgoal. Based on this test, we show how to produce an MCR which is a Datalog query with arithmetic comparisons. The containment test that we develop obtains a result of independent interest. It finds another special case where query containment in the presence of arithmetic comparisons can be tested in nondeterministic polynomial time. {\^{A}}{\textcopyright} 2006 Elsevier B.V. All rights reserved.},
annote = {Rewriting queries using views in the presence of arithmetic comparisons},
author = {Afrati, F and Li, C and Mitra, P},
journal = {Theoretical Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1-2},
pages = {88--123},
title = {{Rewriting queries using views in the presence of arithmetic comparisons}},
url = {citeulike-article-id:3934532 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33750731959{\&}{\#}38 partnerID=40},
volume = {368},
year = {2006}
}
@inproceedings{chin2012measuring,
author = {Chin, Erika and Felt, Adrienne Porter and Sekar, Vyas and Wagner, David},
booktitle = {Proceedings of the Eighth Symposium on Usable Privacy and Security (SOUPS '12)},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
organization = {ACM},
pages = {1:1--1:16},
title = {{Measuring user confidence in smartphone security and privacy}},
year = {2012}
}
@misc{Weiser1981,
address = {San Diego, CA},
author = {Weiser, Mark},
pages = {439--449},
title = {{Program Slicing}},
year = {1981}
}
@inproceedings{Wang2006,
address = {New York, New York, USA},
author = {Wang, Weichao and Lu, Aidong},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179585},
isbn = {1595935495},
keywords = {interactive detection,sybil attacks,topology visualization,visualization on network security,wireless networks},
month = {nov},
pages = {51},
publisher = {ACM Press},
title = {{Visualization assisted detection of sybil attacks in wireless networks}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179585},
year = {2006}
}
@inproceedings{Smith2005c,
address = {Chicago, IL},
author = {Smith, Sarah E and Williams, Laurie and Xu, Jun},
title = {{Expediting Programmer AWAREness of Anomalous Code}},
year = {2005}
}
@article{Hall2000,
author = {Hall, Gregory A. and Munson, John C.},
doi = {10.1016/S0164-1212(00)00031-5},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {churn},
mendeley-tags = {churn},
month = {oct},
number = {2},
pages = {111--118},
title = {{Software evolution: code delta and code churn}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0164121200000315},
volume = {54},
year = {2000}
}
@misc{Heckman2007,
author = {Heckman, Sarah},
number = {January 11},
title = {{AWARE Research Home Page}},
url = {http://agile.csc.ncsu.edu/aware},
volume = {2007},
year = {2007}
}
@inproceedings{5137305,
abstract = {The underpinning of situational awareness in computer networks is to identify adversaries, estimate impact of attacks, evaluate risks, understand situations and make sound decisions on how to protect valued assets swiftly and accurately. SA also underscores situation assessment in order to make accurate forecast in dynamic and complex environments. In this paper, situational awareness in computer network security is investigated. Functional attributes of situational awareness in computer network security are discussed: dynamism and complexity, automation, realtime processing, multisource data fusion, heterogeneity, security visualisation, decision control, risk assessment, resolution, forecasting and prediction.},
author = {Onwubiko, C},
booktitle = {Intelligence and Security Informatics, 2009. ISI '09. IEEE International Conference on},
doi = {10.1109/ISI.2009.5137305},
keywords = {computer network security,decision control,multiso},
month = {jun},
pages = {209--213},
title = {{Functional requirements of situational awareness in computer network security}},
year = {2009}
}
@article{Legeard2004a,
author = {Legeard, Bruno and Peureux, Fabien and Utting, Mark},
doi = {10.1002/stvr.287},
issn = {0960-0833},
journal = {Software Testing, Verification and Reliability},
keywords = {b notation,boundary values,model-based testing,set constraint solving},
month = {jun},
number = {2},
pages = {81--103},
title = {{Controlling test case explosion in test generation from B formal models}},
url = {http://doi.wiley.com/10.1002/stvr.287},
volume = {14},
year = {2004}
}
@inproceedings{Dhamija2005,
address = {New York, New York, USA},
author = {Dhamija, Rachna and Tygar, J. D.},
booktitle = {Proceedings of the 2005 symposium on Usable privacy and security - SOUPS '05},
doi = {10.1145/1073001.1073009},
isbn = {1595931783},
month = {jul},
pages = {77--88},
publisher = {ACM Press},
title = {{The battle against phishing}},
url = {http://dl.acm.org/citation.cfm?id=1073001.1073009},
year = {2005}
}
@article{Felder1988,
author = {Felder, R M and Silverman, L K},
keywords = {education,learning,learning styles,teaching styles},
number = {7},
pages = {674--681},
title = {{Learning and Teaching Styles in Engineering Education}},
volume = {78},
year = {1988}
}
@article{Holzmann2006,
author = {Holzmann, Gerard J},
doi = {10.1109/MC.2006.212},
issn = {0018-9162},
journal = {Computer},
keywords = {Data encapsulation,Guidelines,Job shop scheduling,Laboratories,NASA,Performance evaluation,Software safety,Statistical analysis,Testing,Upper bound,coding rules,program verification,safety-critical code development rules,safety-critical software,safety-critical software component analysis,software development,software reliability,software technologies,software verifiability},
language = {English},
month = {jun},
number = {6},
pages = {95--97},
publisher = {IEEE},
title = {{The Power of 10: Rules for Developing Safety-Critical Code}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1642624},
volume = {39},
year = {2006}
}
@inproceedings{Kazanzides10,
address = {Midas Journal},
author = {Kazanzides, Peter and DiMaio, Simon and Deguet, Anton and Vagvolgyi, Balazs and Balicki, Marcin and Schneider, Caitlin and Kumar, Rajesh and Jog, Amod and Itkowitz, Brandon and Hasser, Christopher and Taylor, Russell},
booktitle = {MICCAI Workshop on Systems and Arch. for Computer Assisted Interventions},
month = {jun},
title = {{The Surgical Assistant Workstation (SAW) in Minimally-Invasive Surgery and Microsurgery}},
url = {http://hdl.handle.net/10380/3179},
year = {2010}
}
@book{Tuckman1999,
author = {Tuckman, B W},
title = {{Conducting Educational Research}},
year = {1999}
}
@inproceedings{Kinnunen2010,
abstract = {In this paper, we introduce a new way to categorise existing educational research making it possible to find new previously overlooked research topics. This novel categorisation system is based on the didactic foci of the research papers. Our categorisation scheme is not data driven as in previously published categorisation systems but is derived from the didactic triangle, which is a theoretical model describing the elements of a teaching-studying-learning processes. The didactic-focus-based categorisation system can be used to promote discussion about missing types of research foci within the computing education research (CER) community. In addition, the new categorisation system supports meta-level analysis of published research papers and thus contributes to the discussion of the goals and the present state of CER. We analyse previously existing categorisation systems and describe how our system differs. Finally, we give two examples how to apply the new theoretical categorisation system. First, we use research papers published in ICER conferences 2005-2009 as our source material to illustrate how to apply the new theoretical categorisation system for revealing a number of areas for novel research such that seem to have received little attention from the CER community. The second example highlights how the categorisation system can be used to find overlooked research topics on some specific research area (in our example students' success in CS1).},
address = {Aarhus, Denmark},
author = {Kinnunen, P{\"{a}}ivi and Meisalo, Veijo and Malmi, Lauri},
booktitle = {ICER'10 - Proceedings of the International Computing Education Research Workshop},
doi = {10.1145/1839594.1839598},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kinnunen, Meisalo, Malmi - 2010 - Have we missed something Identifying missing types of research in computing education.pdf:pdf},
isbn = {9781450301466},
keywords = {CS2,Classification criteria,Computing education research,Didactic focus,Didactic triangle,Literature},
mendeley-tags = {CS2},
pages = {13--21},
publisher = {ACM Press},
title = {{Have we missed something? Identifying missing types of research in computing education}},
url = {http://portal.acm.org/citation.cfm?doid=1839594.1839598},
year = {2010}
}
@article{Hale1999,
author = {Hale, JE and Sharpe, Shane},
journal = {Journal of Software},
keywords = {adaptive learning,comprehension models,expertise,models,problem solving,software maintenance,structural learning theory,verbal protocol analysis},
number = {July 1998},
pages = {73--91},
title = {{An evaluation of the cognitive processes of programmers engaged in software debugging}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1096-908X(199903/04)11:2{\%}3C73::AID-SMR187{\%}3E3.0.CO;2-A/abstract},
volume = {91},
year = {1999}
}
@article{Cohen1960,
abstract = {A coefficient of interjudge agreement for nominal scales, formula-omitted, is presented. It is directly interpretable as the pro-portion of joint judgments in which there is agreement, after chance agreement is excluded. Its upper limit is +1.00, and its lower limit falls between zero and -1.00, depending on the distribution of judgments by the two judges. The maximum value which x can take for any given problem is given, and the implications of this value to the question of agreement discussed. An interesting characteristic of x is its identity with 0 in the dichotomous case when the judges give the same marginal distributions. Finally, its standard error and techniques for estimation and hypothesis testing are presented. {\textcopyright} 1960, Sage Publications. All rights reserved.},
author = {Cohen, Jacob},
doi = {10.1177/001316446002000104},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen - 1960 - A Coefficient of Agreement for Nominal Scales.pdf:pdf},
issn = {15523888},
journal = {Educational and Psychological Measurement},
number = {1},
pages = {37--46},
title = {{A Coefficient of Agreement for Nominal Scales}},
url = {/record/1960-06759-001},
volume = {20},
year = {1960}
}
@inproceedings{Nagappan2003,
abstract = {Programmers who use the test-driven development practice of the Extreme Programming methodology write extensive automated unit and acceptance tests. This paper describes an Eclipse plug-in that utilizes the results of this automated testing, and in combination with a suite of internal product metrics, provides an early assessment of product reliability. We discuss the correlation between the metrics we use and the reliability of the developed software, as well as the general functional characteristics of the alpha version of our Eclipse plug-in.},
address = {New York, NY, USA},
annote = {"Good enough" software reliability estimation plug-in for Eclipse},
author = {Nagappan, Nachiappan and Williams, Laurie and Vouk, Mladen},
booktitle = {eclipse '03: Proceedings of the 2003 OOPSLA workshop on eclipse technology eXchange},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {30--34},
publisher = {ACM},
title = {{"Good enough" software reliability estimation plug-in for Eclipse}},
url = {citeulike-article-id:3934736 http://dx.doi.org/10.1145/965660.965667},
year = {2003}
}
@inproceedings{5168087,
abstract = {Today's safety and security missions are exceedingly complex, and are becoming more so every day. The operational complexity arises from the growing enormity of available information as well as the participation of multiple interdependent agencies and organizations. Increasingly, decision-makers need an integrated, intelligent system that can seamlessly acquire, fuse, reason about, distribute, and protect information to provide enhanced, individualized decision support and situational understanding as well as foster effective collaboration. To meet this need, Raytheon is developing an intelligent system called Confluencetrade, which consists of an ontological framework, domain independent knowledge generation, integration, and reasoning agents, as well as various visualization components. Confluencetrade can be applied to any mission by creating a mission ontology that extends the common framework, and which explicitly defines a semantic model of the physical, information, cognitive, and social domains for the mission. This paper will describe the Confluencetrade system and demonstrate its applicability to the mission of maritime and port security.},
author = {De, P and Jennings, D and Sperry, D},
booktitle = {Technologies for Homeland Security, 2009. HST '09. IEEE Conference on},
doi = {10.1109/THS.2009.5168087},
keywords = {Confluence system,complex missions,domain independ},
month = {may},
pages = {561--568},
title = {{Confluence {\#}x2122;: Knowledge-based decision support in complex missions}},
year = {2009}
}
@article{Sauser2009,
abstract = {When important projects fail, the investigation is often focused on the engineering and technical reasons for the failure. That was the case in NASA's Mars Climate Orbiter (MCO) that was lost in space after completing its nine-month journey to Mars. Yet, in many cases the root cause of the failure is not technical, but managerial. Often the problem is rooted in management's failure to select the right approach to the specific project. The objective of this paper is to enrich our understanding of project failure due to managerial reasons by utilizing different contingency theory frameworks for a retrospective look at unsuccessful projects and perhaps more important, potential prevention of future failures. The evolving field of project management contingency theory provides an opportunity at this time to re-examine the concept of fit between project characteristics and project management, and offer deeper insights on why projects fail. After outlining several existing contingency studies, we use three distinct frameworks for analyzing the MCO project. These frameworks include Henderson and Clark's categorization of change and innovation, Shenhar and Dvir's NTCP diamond framework, and Pich, Loch, and De Meyer's strategies for managing uncertainty. While each framework provides a different perspective, collectively, they demonstrate that in the MCO program, the choices made by managers, or more accurately, the constraints imposed on them under the policy of ‘better, faster, cheaper', led the program to its inevitable failure. This paper shows that project management contingency theory can indeed provide new insights for a deeper understanding of project failure. Furthermore, it suggests implications for a richer upfront analysis of a project's unique characteristics of uncertainty and risk, as well as additional directions of research. Such research may help establish new and different conceptions on project success and failure beyond the traditional success factors, and subsequently develop more refined contingency frameworks. The results of such research may enable future project managers to rely less on heuristics and possibly lead to a new application of “project management design.”},
author = {Sauser, Brian J. and Reilly, Richard R. and Shenhar, Aaron J.},
doi = {10.1016/j.ijproman.2009.01.004},
issn = {02637863},
journal = {International Journal of Project Management},
keywords = {Contingency theory,Project failure,Project management},
month = {oct},
number = {7},
pages = {665--679},
title = {{Why projects fail? How contingency theory can provide new insights - A comparative analysis of NASA's Mars Climate Orbiter loss}},
url = {http://www.sciencedirect.com/science/article/pii/S0263786309000052},
volume = {27},
year = {2009}
}
@inproceedings{Carlson2008,
abstract = {Teaching the Agile practices of Test-Driven Development and Refactoring to entry level students can be a challenge. This paper outlines an approach to doing so with simplicity at its core. It has been an effective way to communicate not only the mechanics of the practices, but also the reasons behind why the practices are important to a professional software developer. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {An Agile classroom experience: Teaching TDD and refactoring},
author = {Carlson, B},
booktitle = {Proceedings - Agile 2008 Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {465--469},
title = {{An Agile classroom experience: Teaching TDD and refactoring}},
url = {citeulike-article-id:3934567 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-52949101403{\&}{\#}38 partnerID=40},
year = {2008}
}
@article{Yuan2010c,
abstract = {Using animated visualization tools has been an important teaching approach in computer science education. We have developed three visualization and animation tools that demonstrate various information security concepts and actively engage learners. The information security concepts illustrated include: packet sniffer and related computer network concepts, the Kerberos authentication architecture, and wireless network attacks. These tools are implemented using Macromedia Flash MX Professional Edition. The animations can run from a Web page as Flash Applets or as standalone applications. These visualization tools are intended to be used in undergraduate level computer network and security courses. They can be used as classroom instructor demos, student exercises, or Web-based student learning resources. These tools have been used in various computer network and information security courses at North Carolina A{\&}T State University, and have received positive feedback from the students.},
author = {Yuan, Xiaohong and Vega, Percy and Qadah, Yaseen and Archer, Ricky and Yu, Huiming and Xu, Jinsheng},
doi = {10.1145/1656255.1656258},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
number = {4},
pages = {1--28},
title = {{Visualization Tools for Teaching Computer Security}},
url = {http://portal.acm.org/citation.cfm?doid=1656255.1656258},
volume = {9},
year = {2010}
}
@inproceedings{Robinson2003,
address = {Salt Lake City, UT},
author = {Robinson, H and Sharp, H},
pages = {12--21},
title = {{XP Culture: Why the twelve practices both are and are not the most significant thing}},
year = {2003}
}
@inproceedings{Nagappan95,
address = {St. Louis, MO},
author = {Nagappan, N. and Ball, T.},
booktitle = {Proceedings of the 27th International Conference on Software Engineering (ICSE '05)},
doi = {10.1109/ICSE.2005.1553571},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagappan, Ball - 2005 - Use of relative code churn measures to predict system defect density.pdf:pdf},
isbn = {1-59593-963-2},
keywords = {churn},
language = {English},
mendeley-tags = {churn},
pages = {284--292},
title = {{Use of relative code churn measures to predict system defect density}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1553571},
year = {2005}
}
@article{Norman1975,
author = {Norman, Donald A and Bobrow, Daniel G},
journal = {Cognitive Psychology},
keywords = {hci,psychology},
number = {1},
pages = {44--64},
title = {{On Data-limited and Resource-limited Processes}},
volume = {7},
year = {1975}
}
@misc{Sugita2010a,
abstract = {The generation and optimization of tool paths are considered to be challenging problems for the use of a milling robot in minimally invasive orthopedic surgery. The objective of this study was to minimize the collision of the cutting tool with soft tissue, and we propose a novel approach to tool path generation and optimization. Starting with the physical requirements, we modeled some important components, and on the basis of this model, we propose a geometric optimization approach to improve the tool path. Case studies show the validity of this approach. We developed software for the application, and then evaluated the effectiveness of the application.},
author = {Sugita, N and Nakano, T and Kato, T and Nakajima, Y and Mitsuishi, M},
booktitle = {IEEEASME Transactions on Mechatronics},
doi = {10.1109/TMECH.2009.2030184},
issn = {10834435},
keywords = {biomedical equipment,bones,cutting,machine tool control,manufacturing automation},
number = {3},
pages = {471--479},
title = {{Tool Path Generator for Bone Machining in Minimally Invasive Orthopedic Surgery}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5247031},
volume = {15},
year = {2010}
}
@article{Boloix1995,
author = {Boloix, G and P N Robillard},
keywords = {evaluation,ontologies,ontology},
number = {12},
pages = {17--26},
title = {{A Software System Evaluation Framework}},
volume = {28},
year = {1995}
}
@incollection{Diep2015,
address = {Boston, MA},
author = {Diep, Madeline and Esker, Linda and Falessi, Davide and Layman, Lucas and Shaw, Michele and Shull, Forrest},
booktitle = {The Art and Science of Analyzing Software Data},
edition = {1st},
editor = {Bird, Christian and Menzies, Tim and Zimmerman, Thomas},
keywords = {agile,mypubs,nsf},
mendeley-tags = {agile,mypubs,nsf},
pages = {327--348},
publisher = {Morgan Kaufmann},
title = {{Applying Software Data Analysis in Industry Contexts: When Research Meets reality}},
year = {2014}
}
@article{Damm2005,
abstract = {This paper identifies and presents an approach to software component-level testing that in a cost effective way can move defect detection earlier in the development process. A department at Ericsson AB introduced a test automation tool for component-level testing in two projects together with the concept test-driven development (TDD), a practice where the test code is written before the product code. The implemented approach differs from how TDD is used in Extreme Programming (XP) in that the tests are written for components exchanging XMLs instead of writing tests for every method in every class. This paper describes the implemented test automation tool, how test-driven development was implemented with the tool, and experiences from the implementation. Preliminary results indicate that the concept decreases the development lead-time significantly. {\^{A}}{\textcopyright} 2004 Elsevier B.V. All rights reserved.},
annote = {Introducing test automation and test-driven development: An experience report},
author = {Damm, L O and Lundberg, L and Olsson, D},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {SPEC.ISS.},
pages = {3--15},
title = {{Introducing test automation and test-driven development: An experience report}},
url = {citeulike-article-id:3934583 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-15744390072{\&}{\#}38 partnerID=40},
volume = {116},
year = {2005}
}
@inproceedings{Gaines1989,
address = {San Mateo, CA},
author = {Gaines, BR and Shaw, MLG},
booktitle = {Proceedings of the 11th International Joint Conference on Artificial Intelligence},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gaines, Shaw - 1989 - Comparing the Conceptual Systems of Experts.pdf:pdf},
pages = {633--638},
title = {{Comparing the Conceptual Systems of Experts}},
url = {http://ijcai.org/Past Proceedings/IJCAI-89-VOL1/PDF/101.pdf},
year = {1989}
}
@inproceedings{Zelkowitz1998g,
author = {Zelkowitz, M V and Wallace, D R},
title = {{Culture Conflicts in Software Engineering Technology Transfer}},
year = {1998}
}
@inproceedings{Sauve2008,
abstract = {In this paper, we report our experience in teaching software development to Computer Science undergraduate students using acceptance test-driven development (ATDD) and the acceptance testing tool Easy Accept. A typical software design course in a Computer Science curriculum has been modified to include project assignments with executable analysis, which gives students more focus on meeting requirements, boosts confidence in the code being written and results in increased software correctness. We evaluate the benefits of the approach and give suggestions on how to cope with its limitations.},
annote = {Teaching software development with ATDD and EasyAccept},
author = {Sauve, J P and Neto, O L A},
booktitle = {SIGCSE'08 - Proceedings of the 39th ACM Technical Symposium on Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {542--546},
title = {{Teaching software development with ATDD and EasyAccept}},
url = {citeulike-article-id:3934781 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57449093775{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Cosentino2016,
address = {Austin, TX},
author = {Cosentino, Valerio and Luis, Javier and Cabot, Jordi},
booktitle = {Proceedings of the 13th International Workshop on Mining Software Repositories - MSR '16},
doi = {10.1145/2901739.2901776},
isbn = {9781450341868},
pages = {137--141},
title = {{Findings from GitHub}},
url = {http://dl.acm.org/citation.cfm?doid=2901739.2901776},
year = {2016}
}
@inproceedings{Dasireddy2010a,
address = {New York, New York, USA},
author = {Dasireddy, Swetha and Gasior, Wade and Cui, Xiaohui and Yang, Li},
booktitle = {Proceedings of the Sixth Annual Workshop on Cyber Security and Information Intelligence Research - CSIIRW '10},
doi = {10.1145/1852666.1852712},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dasireddy et al. - 2010 - Alerts visualization and clustering in network-based intrusion detection.pdf:pdf},
isbn = {9781450300179},
keywords = {flocking model,intrusion detection,visualization},
month = {apr},
pages = {1},
publisher = {ACM Press},
title = {{Alerts visualization and clustering in network-based intrusion detection}},
url = {http://dl.acm.org/citation.cfm?id=1852666.1852712},
year = {2010}
}
@article{Cherf1992,
author = {Cherf, G. S.},
doi = {10.1007/BF01720922},
issn = {0963-9314},
journal = {Software Quality Journal},
month = {sep},
number = {3},
pages = {147--158},
publisher = {Kluwer Academic Publishers},
title = {{An investigation of the maintenance and support characteristics of commercial software}},
url = {http://link.springer.com/10.1007/BF01720922},
volume = {1},
year = {1992}
}
@article{Williams2005,
author = {Williams, Laurie and Layman, Lucas and Abrahamsson, Pekka},
doi = {10.1145/1082983.1083179},
isbn = {1-59593-121-X},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {mypubs},
mendeley-tags = {mypubs},
month = {jul},
number = {4},
pages = {1},
publisher = {ACM},
title = {{On establishing the essential components of a technology-dependent framework}},
volume = {30},
year = {2005}
}
@article{Liao2019,
abstract = {As enrollments and class sizes in postsecondary institutions have increased, instructors have sought auto- mated and lightweight means to identify students who are at risk of performing poorly in a course. This identification must be performed early enough in the term to allow instructors to assist those students before they fall irreparably behind. This study describes a modeling methodology that predicts student final exam scores in the third week of the term by using the clicker data that is automatically collected for instructors when they employ the Peer Instruction pedagogy. The modeling technique uses a support vector machine binary classifier, trained on one term of a course, to predict outcomes in the subsequent term. We applied this modeling technique to five different courses across the computer science curriculum, taught by three different instructors at two different institutions. Our modeling approach includes a set of strengths not seen wholesale in prior work, while maintaining competitive levels of accuracy with that work. These strengths include using a lightweight source of student data, affording early detection of struggling students, and pre- dicting outcomes across terms in a natural setting (different final exams, minor changes to course content), across multiple courses in a curriculum, and across multiple institutions},
author = {Liao, Soohyun Nam and Zingaro, Daniel and Thai, Kevin and Alvarado, Christine and Griswold, William G. and Porter, Leo},
doi = {10.1145/3277569},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liao et al. - 2019 - A robust machine learning technique to predict low-performing students.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {At-risk students,CS2,Clicker data,Cross-term,Machine learning,Multiinstitution,Peer instruction,Prediction},
mendeley-tags = {CS2},
month = {jan},
number = {3},
pages = {Article 18},
publisher = {Association for Computing Machinery},
title = {{A robust machine learning technique to predict low-performing students}},
volume = {19},
year = {2019}
}
@article{4202564,
abstract = {This article describes a novel visual information concealment technique, referred to as optical watermarking, for the authentication of original printed documents. An optical watermark is a two-dimensional binary image. It can be of any shape and can be printed on any part of a document. The optical watermark is constructed by the superposition of multiple two-dimensional binary images (referred to as layers), each with different carrier structural patterns embedding various hidden information. The hidden information is embedded into each layer using phase modulation. Based on properties of the human visual system and modulation principle, the hidden information becomes visible to the human eyes only when a right "key" is positioned on top of the optical watermark with the right alignment. Here, "keys" play the similar role as keys in encryption, that is, to decode hidden information. Thus, with such a "lock and key" approach, it greatly improves the security level of the optical watermark. In addition, the multiple layer structure of the optical watermark makes it extremely robust against reverse engineering attacks. Due to its high security and tight link with electronic document systems, which requires documents to be finally printed on paper, the optical watermark has been applied to various electronic document systems. These are online ticketing, online bill of lading, and remote signing and printing of documents, where critical and unique information are embedded in watermarks and printed together with individual documents for future authentication. It has also been used in offline and traditional antiforgery applications, such as brand protection, preprinted high-value tickets, and identification documents},
author = {Huang, Sheng and Wu, Jian Kang},
doi = {10.1109/TIFS.2007.897255},
issn = {1556-6013},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {antiforgery applications,carrier structural patter},
month = {jun},
number = {2},
pages = {164--173},
title = {{Optical Watermarking for Printed Document Authentication}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4202564},
volume = {2},
year = {2007}
}
@article{Damm2006,
abstract = {For many software development organizations it is of crucial importance to reduce development costs while still maintaining high product quality. Since testing commonly constitutes a significant part of the development time, one way to increase efficiency is to find more faults early when they are cheaper to pinpoint and remove. This paper presents empirical results from introducing a concept for early fault detection. That is, an alternative approach to Test-Driven Development which was applied on a component level instead of on a class/method level. The selected method for evaluating the result of introducing the concept was based on an existing method for fault-based process assessment and was proven practically useful for evaluating fault reducing improvements. The evaluation was made on two industrial projects and on different features within a project that only implemented the concept partly. The evaluation result demonstrated improvements regarding decreased fault rates and Return On Investment (ROI), e.g. the total project cost became about 5-6{\%} less already in the first two studied projects. {\^{A}}{\textcopyright} 2005 Elsevier Inc. All rights reserved.},
annote = {Results from introducing component-level test automation and Test-Driven Development},
author = {Damm, L O and Lundberg, L},
journal = {Journal of Systems and Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {7},
pages = {1001--1014},
title = {{Results from introducing component-level test automation and Test-Driven Development}},
url = {citeulike-article-id:3934581 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33646759617{\&}{\#}38 partnerID=40},
volume = {79},
year = {2006}
}
@inproceedings{1410469,
abstract = {Notice of Violation of IEEE Publication Principles

"Dynamic Removal Algorithm for Constrained Delaunay Triangulations"
by Dong-Yu Xu, Guang-Rong Tang, and Yu-Pin Luo
in the Proceedings of the Third International Conference on Image and Graphics, 2004,

After careful and considered review of the content and authorship of this paper by a duly constituted expert committee, this paper has been found to be in violation of IEEE's Publication Principles.

This paper contains significant portions of original text from the paper cited below. The original text was copied without attribution (including appropriate references to the original author(s) and/or paper title) and without permission.

Due to the nature of this violation, reasonable effort should be made to remove all past references to this paper, and future references should be made to the following article:

"Fully Dynamic Constrained Delaunay Triangulations"
by Marcelo Kallman, Hanspeter Bieri, and Daniel Thalmann
in Geometric Modelling for Scientific Visualization, G. Brunnett, B. Hamann, H.Mueller, L. Linsen (Eds.), Springer-Verlag, 2003, pp. 241-257Delaunay triangulations and constrained Delaunay triangulations are very popular tool used to analyze planar domain. An efficient removal algorithm is required when constructing the approximate skeleton of complicated dynamic planar domains. We present a fully dynamic algorithm for CDT. It inserts and removes constraints that maybe points or any kind of polygonal lines. Furthermore, the algorithm automatically detects and deals with the overlapping, self-intersections and duplicate of constraints.},
author = {Dong-Yu, Xu and Guang-Rong, Tang and Yu-Pin, Luo},
booktitle = {Multi-Agent Security and Survivability, 2004 IEEE First Symposium on},
doi = {10.1109/ICIG.2004.59},
keywords = {complicated dynamic planar domain,constrained Dela},
pages = {406--409},
title = {{Notice of Violation of IEEE Publication Principles Dynamic removal algorithm for constrained Delaunay triangulations}},
year = {2004}
}
@phdthesis{Orlando2007,
author = {Orlando, S},
school = {Universit{\`{a}} degli Studi di Napoli Federico II},
title = {{Software Aging Analysis of off the Shelf Software Items}},
year = {2007}
}
@inproceedings{downs2006decision,
author = {Downs, Julie S and Holbrook, Mandy B and Cranor, Lorrie Faith},
booktitle = {Proceedings of the second symposium on Usable privacy and security},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
organization = {ACM},
pages = {79--90},
title = {{Decision strategies and susceptibility to phishing}},
year = {2006}
}
@inproceedings{Menzies2003,
address = {San Francisco, CA},
author = {Menzies, Tim and Lutz, Robyn and Mikulski, Carmen},
booktitle = {Proceedings of Fifteenth International Conference on Software Engineering and Knowledge Engineering},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Menzies, Lutz, Mikulski - 2003 - Better analysis of defect data at NASA.pdf:pdf},
keywords = {anomaly,artificial intelligence,association,debugging,dent,learning,metrics,nasa mis-,reports on deep space,rule learning,software engineering,surprise,testing and,treatment learning},
title = {{Better analysis of defect data at NASA}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.197.5917{\&}rep=rep1{\&}type=pdf},
year = {2003}
}
@techreport{NationalAeronauticsandSpaceAdministration2004,
address = {NASA-STD-8719.13B},
author = {{National Aeronautics and Space Administration}},
institution = {National Aeronautics and Space Administration},
title = {{NASA Software Safety Standard}},
year = {2004}
}
@book{Lave1991,
address = {New York, NY},
author = {Lave, Jean and Wenger, Etienne},
publisher = {Cambridge University Press},
title = {{Situated Learning:  Legitimate peripheral participation}},
year = {1991}
}
@book{Humphrey2000,
address = {Reading, MA},
author = {Humphrey, Watts S},
pages = {463},
publisher = {Addison-Wesley},
title = {{Introduction to the Team Software Process}},
year = {2000}
}
@inproceedings{Hata2005,
abstract = {This paper describes a design method of automated medical diagnosis system (AMDS), which provides a normal degree for a disease. In this paper, we consider blood test data of human. Suppose a disease which can be diagnosed by the test, first, we do statistical analysis. Second, we determine the reference range of the test. Third, we design a fuzzy inference system consisting of membership functions based on the reference range. The inference system plays primary role in the AMDS. Finally, we show the design of AMDS for diabetes and the experimental results. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {Design of automated medical diagnosis system with normal degree},
author = {Hata, Y and Ishikawa, O and Kondo, K and Kobashi, S},
booktitle = {Annual Conference of the North American Fuzzy Information Processing Society - NAFIPS},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {633--637},
title = {{Design of automated medical diagnosis system with normal degree}},
url = {citeulike-article-id:3934634 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33744975034{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@inproceedings{6046041,
abstract = {Since children cannot live in a safe cyberspace environment for ever, setting up filtering or using educational materials are commonly used. A more effective approach to helping children assess website risk is to provide an environment in which children can better understand a website's features and determine the risks of accessing the website for themselves. We have constructed a prototype visualization system for helping children understand website features and use them to identify high-risk websites. We applied a graphical search interface named 'Concentric Ring View' which we proposed to support flexible retrieval for multi-attribute metadata. It was tested using actual and dummy websites featuring five attributes: action, color, atmosphere, number of images, and number of links. The risk level of each actual website was estimated by the features of the portal sites through which it was accessed. A dummy website was used when the actual one was deemed to be too risky. The testing revealed several distinguishing characteristics of high-risk websites. Adding other search attributes such as structure and text appearance should make it possible to characterize risky websites more completely.},
author = {Kajiyama, Tomoko},
booktitle = {2011 Sixth International Conference on Availability, Reliability and Security},
doi = {10.1109/ARES.2011.102},
isbn = {978-1-4577-0979-1},
keywords = {Web sites,children,educational materials,prototype},
month = {aug},
pages = {656--660},
publisher = {Ieee},
title = {{A Visualization Method for Helping Children Assess the Risk of Websites}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6046041},
year = {2011}
}
@book{Kitchenham1996,
address = {Cambridge, MA},
author = {Kitchenham, B},
keywords = {measurement,metrics},
publisher = {Blackwell},
title = {{Software Metrics: Measurement for Software Process Improvement}},
year = {1996}
}
@inproceedings{Sliwerski2005,
address = {St. Louis, MO},
author = {{\'{S}}liwerski, Jacek and Zimmermann, Thomas and Zeller, Andreas},
booktitle = {Proceedings of the 2005 international workshop on Mining software repositories - MSR '05},
doi = {10.1145/1083142.1083147},
isbn = {1595931236},
pages = {1--5},
publisher = {ACM Press},
title = {{When do changes induce fixes?}},
url = {http://portal.acm.org/citation.cfm?doid=1083142.1083147},
year = {2005}
}
@inproceedings{Janzen2005,
abstract = {This research involves empirical software engineering studies applied in academic and professional settings to assess the influence of test-driven development on software quality. Particular focus is given to internal software design quality. Pedagogical implications are also examined. Initial results and the study protocol and plans will be presented.},
annote = {Software architecture improvement through test-driven development},
author = {Janzen, D S},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {240--241},
publisher = {ACM New York, NY, USA},
title = {{Software architecture improvement through test-driven development}},
url = {citeulike-article-id:3934655 http://portal.acm.org/citation.cfm?id=1094855.1094954},
year = {2005}
}
@inproceedings{1532065,
abstract = { When performing packet-level analysis in intrusion detection, analysts often lose sight of the big picture while examining these low-level details. In order to prevent this loss of context and augment the available tools for intrusion detection analysis tasks, we developed an information visualization tool, the time-based network traffic visualizer (TNV). TNV is grounded in an understanding of the work practices of intrusion detection analysts, particularly foregrounding the overarching importance of context and time in the process of intrusion detection analysis. The main visual component of TNV is a matrix showing network activity of hosts over time, with connections between hosts superimposed on the matrix, complemented by multiple, linked views showing port activity and the details of the raw packets. Providing low-level textual data in the context of a high-level, aggregated graphical display enables analysts to examine packet-level details within the larger context of activity. This combination has the potential to facilitate the intrusion detection analysis tasks and help novice analysts learn what constitutes normal on a particular network.},
author = {Goodall, J R and Lutters, W G and Rheingans, P and Komlodi, A},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532065},
keywords = {information visualization,intrusion detection ana},
pages = {47--54},
title = {{Preserving the big picture: visual network traffic analysis with TNV}},
year = {2005}
}
@article{AbdelGhaly1986,
author = {Abdel-Ghaly, A A and Chan, P Y and Littlewood, B},
journal = {IEEE Transactions on Software Engineering},
number = {9},
pages = {950--967},
title = {{Evaluation of Competing Software Reliability Predictions}},
volume = {12},
year = {1986}
}
@article{Wirfs-Brock2007,
abstract = {Software development approaches all emphasize a core set of values and principles around which practices, techniques, and tools have emerged. A thoughtful designer should be able to pick and choose among practices without losing their essence. But not all practices are congruent. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Driven to ... Discovering your design values},
author = {Wirfs-Brock, R J},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {9--11},
title = {{Driven to ... Discovering your design values}},
url = {citeulike-article-id:3934840 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33846935997{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@article{Hyndman2006,
author = {Hyndman, R J and Koehler, A B},
journal = {International Journal of Forecasting},
number = {4},
pages = {679--688},
title = {{Another Look at Measures of Forecast Accuracy}},
volume = {22},
year = {2006}
}
@inproceedings{Trætteberg2006,
abstract = {Programming exercises are an important part of an introductory course in programming. To improve the focus on encapsulation, requirements-based testing and give better feedback given to the students during their work, we have created an Eclipse-based plugin called JExercise. Based on a model of an exercise, it presents the structure of requirements to the student and allows her to test the code by running accompanying JUnit tests. {\^{A}}{\textcopyright} 2006 ACM.},
annote = {JExercise: A specification-based and test-driven exercise support plugin for Eclipse},
author = {Tr{\ae}tteberg, H and Aalberg, T},
booktitle = {Proceedings of the 2006 OOPSLA Workshop on Eclipse Technology eXchange, ETX 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {70--74},
title = {{JExercise: A specification-based and test-driven exercise support plugin for Eclipse}},
url = {citeulike-article-id:3934820 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34548246409{\&}{\#}38 partnerID=40},
year = {2006}
}
@inproceedings{Singer,
address = {Bethesda, MD},
author = {Singer, Janice},
booktitle = {14th IEEE International Conference on Software Maintenance},
doi = {10.1109/ICSM.1998.738502},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singer - Unknown - Practices of software maintenance.pdf:pdf},
isbn = {0-8186-8779-7},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {139--145},
publisher = {IEEE Comput. Soc},
title = {{Practices of software maintenance}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=738502}
}
@inproceedings{Herley2009a,
address = {Oxford, UK},
author = {Herley, Cormac},
booktitle = {Proceedings of the 2009 New Security Paradigms Workshop (NSPW '09)},
doi = {10.1145/1719030.1719050},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herley - 2009 - So Long, and No Thanks for the Externalities.pdf:pdf},
isbn = {9781605588452},
keywords = {agile,nsf,security,security advice,user education},
mendeley-tags = {agile,nsf,security},
month = {sep},
pages = {133--144},
publisher = {ACM Press},
title = {{So Long, and No Thanks for the Externalities}},
url = {http://dl.acm.org/citation.cfm?id=1719030.1719050},
year = {2009}
}
@inproceedings{Layman2016,
address = {Austin, TX},
author = {Layman, Lucas and Nikora, Allen P. and Meek, Joshua and Menzies, Tim},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories (MSR '16)},
file = {:C$\backslash$:/Users/laymanl/Desktop/layman pubs/Layman2016.pdf:pdf},
pages = {303--314},
title = {{Topic Modeling of NASA Space System Problem Reports}},
year = {2016}
}
@book{cockburn2001writing,
author = {Cockburn, Alistair},
publisher = {Addison-Wesley Reading},
title = {{Writing effective use cases}},
volume = {1},
year = {2001}
}
@article{Blazejewski2004,
author = {Blazejewski, Adam and Coggins, Richard},
keywords = {equities,high-frequency financial data,self-organizing map,trade clustering,trade direction},
month = {jan},
pages = {85--90},
title = {{Application of self-organizing maps to clustering of high-frequency financial data}},
url = {http://dl.acm.org/citation.cfm?id=976440.976452},
year = {2004}
}
@article{Ciupa2008a,
author = {Ciupa, Ilinca and Meyer, Bertrand and Oriol, Manuel and Pretschner, Alexander},
doi = {10.1109/ISSRE.2008.18},
journal = {2008 19th International Symposium on Software Reliability Engineering (ISSRE)},
month = {nov},
pages = {157--166},
publisher = {Ieee},
title = {{Finding Faults: Manual Testing vs. Random+ Testing vs. User Reports}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4700320},
year = {2008}
}
@inproceedings{4272059,
abstract = {The task of exploring and analysing large quantities of communication network security data is difficult. Visualisation of the data should help the analyses and make data exploration faster and easier. This paper describes prototype software that visualises the alerts effectively and provides a simple presentation. The needs analysis of this prototype is based on the suggested needs of network security analyst's tasks as seen in the literature. The prototype software incorporates various projections of the alert data in 3-dimensional displays. Filtering, drill-down and playback of alerts at variable speed are incorporated to strengthen the analysis. We integrate a false alert classifier using classification tree algorithm to classify alerts into false and true alerts. Real-time visual observation is also included. We describe some example analyses to prove the usefulness of our prototype.},
author = {Musa, S and Parish, D J},
booktitle = {Information Visualization, 2007. IV '07. 11th International Conference},
doi = {10.1109/IV.2007.149},
issn = {1550-6037},
keywords = {3D displays,alert,alert drill-down,alert filtering},
month = {jul},
pages = {726--733},
title = {{Visualising Communication Network Security Attacks}},
year = {2007}
}
@inproceedings{Kalliamvakou2014,
address = {Hyderabad, India},
author = {Kalliamvakou, Eirini and Gousios, Georgios and Blincoe, Kelly and Singer, Leif and German, Daniel M. and Damian, Daniela},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories - MSR 2014},
doi = {10.1145/2597073.2597074},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kalliamvakou et al. - 2014 - The promises and perils of mining GitHub.pdf:pdf},
isbn = {9781450328630},
keywords = {Mining software repositories,bias,code reviews,git,github},
pages = {92--101},
publisher = {ACM Press},
title = {{The promises and perils of mining GitHub}},
url = {http://dl.acm.org/citation.cfm?doid=2597073.2597074},
year = {2014}
}
@inproceedings{Hill1992,
address = {Monterey, CA},
author = {Hill, W C and Hollan, J D and Wroblewski, D and McCandless, T},
pages = {3--9},
title = {{Edit Wear and Read Wear}},
year = {1992}
}
@techreport{Malaiya1998,
author = {Malaiya, Yashwant K. and Denton, Jason},
booktitle = {Computer Science Technical Report CS-98-104},
institution = {Colorado State University},
title = {{Estimating defect density using test coverage}},
url = {http://www.cs.colostate.edu/testing/robust/research/defect.pdf},
year = {1998}
}
@inproceedings{Bettenberg2012,
address = {Zurich, Switzerland},
author = {Bettenberg, Nicolas and Nagappan, Meiyappan and Hassan, Ahmed E.},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
isbn = {9781467317610},
pages = {60--69},
title = {{Think locally, act globally: improving defect and effort prediction models}},
url = {http://dl.acm.org/citation.cfm?id=2664455{\&}CFID=910003172{\&}CFTOKEN=41235350},
year = {2012}
}
@inproceedings{1532064,
abstract = { Traffic anomalies and attacks are commonplace in today's networks and identifying them rapidly and accurately is critical for large network operators. For a statistical intrusion detection system (IDS), it is crucial to detect at the flow-level for accurate detection and mitigation. However, existing IDS systems offer only limited support for: 1) interactively examining detected intrusions and anomalies; 2) analyzing worm propagation patterns; 3) and discovering correlated attacks. These problems are becoming even more acute as the traffic on today's high-speed routers continues to grow. IDGraphs is an interactive visualization system for intrusion detection that addresses these challenges. The central visualization in the system is a flow-level trace plotted with time on the horizontal axis and aggregated number of unsuccessful connections on the vertical axis. We then summarize a stack of tens or hundreds of thousands of these traces using the histographs (Pin Ren and Watson, 2005) technique, which maps data frequency at each pixel to brightness. Users may then interactively query the summary view, performing analysis by highlighting subsets of the traces. For example, brushing a linked correlation matrix view highlights traces with similar patterns, revealing distributed attacks that are difficult to detect using standard statistical analysis. We apply IDGraphs system to a real network router data-set with 179M flow-level records representing a total traffic of 1.16TB. The system successfully detects and analyzes a variety of attacks and anomalies, including port scanning, worm outbreaks, stealthy TCP SYN floodings, and some distributed attacks.},
author = {Watson, B.},
booktitle = {IEEE Workshop on Visualization for Computer Security, 2005. (VizSEC 05).},
doi = {10.1109/VIZSEC.2005.1532064},
isbn = {0-7803-9477-1},
keywords = {IDGraphs,correlated attack discovery,distributed},
pages = {39--46},
publisher = {Ieee},
title = {{IDGraphs: intrusion detection and analysis using histographs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1532064},
year = {2005}
}
@article{Corritore2001,
author = {Corritore, Cynthia L. and Wiedenbeck, Susan},
doi = {10.1006/ijhc.2000.0423},
issn = {10715819},
journal = {International Journal of Human-Computer Studies},
keywords = {object-orientated programmers,procedural programmers,program comprehension,software maintenance},
month = {jan},
number = {1},
pages = {1--23},
title = {{An exploratory study of program comprehension strategies of procedural and object-oriented programmers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1071581900904233},
volume = {54},
year = {2001}
}
@article{Basili1996j,
author = {Basili, V R and Briand, L C and Melo, W L},
keywords = {CK metrics},
number = {10},
pages = {751--761},
title = {{A Validation of Object-Oriented Design Metrics as Quality Indicators}},
volume = {22},
year = {1996}
}
@misc{Terveen1993,
address = {Amsterdam, The Netherlands},
author = {Terveen, Loren G and Selfridge, Peter G and Long, M David},
pages = {15--22},
title = {{From 'Folklore' to 'Living Design Memory'}},
year = {1993}
}
@inproceedings{Thornton2008,
abstract = {Tools like JUnit and its relatives are making software testing reachable even for introductory students. At the same time, however, many introductory computer sciences courses use graphical interfaces as an "attention grabber" for students and as a metaphor for teaching object-oriented programming. Unfortunately, developing software tests for programs that have significant graphical user interfaces is beyond the abilities of typical students (and, for that matter, many educators). This paper describes a framework for combining readily available tools to create an infrastructure for writing tests for Java programs that have graphical user interfaces. These tests are level-appropriate for introductory students and fit in with current approaches in computer science education that incorporate testing in programming assignments. An analysis of data collected during actual student use of the framework in a CS1 course is presented. Copyright 2008 ACM.},
annote = {Supporting student-written tests of GUI programs},
author = {Thornton, M and Edwards, S H and Tan, R P and P{\'{e}}rez-Qui{\~{n}}ones, M},
booktitle = {SIGCSE'08 - Proceedings of the 39th ACM Technical Symposium on Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {537--541},
title = {{Supporting student-written tests of GUI programs}},
url = {citeulike-article-id:3934816 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-57349155450{\&}{\#}38 partnerID=40},
year = {2008}
}
@article{Cadar2008,
author = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
journal = {conference on Operating systems},
title = {{KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs}},
url = {http://static.usenix.org/events/osdi08/tech/full{\_}papers/cadar/cadar{\_}html/paper.html},
year = {2008}
}
@incollection{Kanoun1996,
author = {Kanoun, Karama and Laprie, Jean-Claude},
booktitle = {Handbook of Software Reliability Engineering},
chapter = {10},
editor = {Lyu, Michael},
publisher = {McGraw-Hill},
title = {{Trend Analysis}},
year = {1996}
}
@article{Laplante2005,
abstract = {Potemkin village is "something that appears elaborate and impressive but in actual fact lacks substance". The software analogies for the Potemkin village are ripe for exploration. Software antipatterns address problem-solution pairs in which the typical solution is more harmful than the problem it solves. In essence, an antipattern presents an identifiable problem, the accompanying harmful effects of the typical solution, and a more appropriate solution called a refactoring. In this sense, the software version of a Potemkin village is an antipattern: Problem: deliver software with an impressive interface quickly. Solution: employ a ready-made architecture that provides an impressive interface quickly; spend as little time as possible on the back-end processing. Refactoring: do it right the first time. The author doesn't want to make the case too strongly that the building of Potemkin villages is a deliberate strategy of fraud that companies perpetrate. Is a weak piece of software covered by an elaborate GUI a deliberate fraud or simply poor design? You must assume the latter in the absence of proof. When faced with a Potemkin village or an emperor's new clothes situation, you must expose it immediately. Doing so is not easy when a high-quality GUI masks the shortcomings. You can, however, detect the situation through design reviews, and code inspections, reviews, or walkthroughs. Therefore, managers who oversee software projects (and customers who buy software) should require these reviews. Testing can sometimes uncover the situation, but it might be too late at this point. Test-driven design, on the other hand, can help avoid a Potemkin village},
annote = {The Potemkin village and the art of deception},
author = {Laplante, P A},
isbn = {1520-9202},
journal = {IT Professional},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {62--64},
title = {{The Potemkin village and the art of deception}},
url = {citeulike-article-id:3934687 {\#}},
volume = {7},
year = {2005}
}
@inproceedings{5730252,
abstract = {A dazzler, sometimes called an optical distracter, transmits a visually intense light, usually in a narrow beam, in order to 1) attract the attention of a person and to make them alert to other visual or audible warnings against approaching a protected asset in a threatening manner, or 2) to suppress the actions of noncompliant persons (continued advance or aiming of weapons) by obscuring their vision with disability glare. Dazzlers feature in land operations, at military check points in non-combat land operations, to momentarily distract an assailant, to alert drivers in vehicles approaching a check point, and to alert civilian traffic to approaching forces. Dazzlers are being considered for maritime security; for port protection, counter piracy, and critical infrastructure protection to warn the occupants of small boats and for suppressing the vision of noncompliant persons who may be intent on attacking. The distances of initial detection and warning for maritime operations are generally much longer (on the order of 300 to 3000 m) than the distances in land operations (50 to 300 m). Dazzler aiming is furthermore likely to be under automatic control rather than handheld as in land operations, in order to more control dwell time against a distant contact, and to provide a large operator-dazzler separation for operator safety in counter piracy. The requirements for dazzler effectiveness therefore change when going from land operations to maritime. Models of disability glare are used here to model and study the effectiveness of dazzlers for maritime security across changing ambient light conditions, with aiming uncertainties, and for different types of sources (coherent lasers versus incoherent spotlights). Key requirements for specifying a dazzler system are considered.},
author = {Kessel, R},
booktitle = {Waterside Security Conference (WSS), 2010 International},
doi = {10.1109/WSSC.2010.5730252},
keywords = {automatic control,dazzler effectiveness,disability},
pages = {1--6},
title = {{The effectiveness of dazzlers for maritime security}},
year = {2010}
}
@book{Shadish2002,
address = {Boston},
author = {Shadish, William R and Cook, Thomas D and Campbell, Donald T},
publisher = {Houghton Mifflin},
title = {{Experimental and Quasi-experimental Designs for Generalized Causal Inference}},
year = {2002}
}
@inproceedings{Canfora2006a,
abstract = {Test driven development (TDD) is gaining interest among practitioners and researchers: it promises to increase the quality of the code. Even if TDD is considered a development practice, it relies on the use of unit testing. For this reason, it could be an alternative to the testing after coding (TAC), which is the usual approach to run and execute unit tests after having written the code. We wondered which are the differences between the two practices, from the standpoint of quality and productivity. In order to answer our research question, we carried out an experiment in a Spanish Software House. The results suggest that TDD improves the unit testing but slows down the overall process. Copyright 2006 ACM.},
annote = {Evaluating advantages of test driven development: A controlled experiment with professionals},
author = {Canfora, G and Cimitile, A and Garcia, F and Piattini, M and Visaggio, C A},
booktitle = {ISCE'06 - Proceedings of the 5th ACM-IEEE International Symposium on Empirical Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {364--371},
title = {{Evaluating advantages of test driven development: A controlled experiment with professionals}},
url = {citeulike-article-id:3934564 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34247325175{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@article{Buhrmester2011a,
author = {Buhrmester, M. and Kwang, T. and Gosling, S. D.},
doi = {10.1177/1745691610393980},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Buhrmester, Kwang, Gosling - 2011 - Amazon's Mechanical Turk A New Source of Inexpensive, Yet High-Quality, Data.pdf:pdf},
issn = {1745-6916},
journal = {Perspectives on Psychological Science},
language = {en},
month = {feb},
number = {1},
pages = {3--5},
publisher = {SAGE Publications},
title = {{Amazon's Mechanical Turk: A New Source of Inexpensive, Yet High-Quality, Data?}},
url = {http://pps.sagepub.com/content/6/1/3.full},
volume = {6},
year = {2011}
}
@article{Law2006,
abstract = {The tests needed to prove, verify, and validate a software application are determined before the software application is developed. This is the essence of test driven development, an agile practice built upon sound software engineering principles. When applied effectively, this practice can have many benefits. The question becomes how to effectively adopt test driven development. This paper describes the experiences and lessons learned by two teams who adopted test driven development methodology for software systems developed at TransCanada. The overall success of test driven methodology is contingent upon the following key factors: experienced team champion, well-defined test scope, supportive database environment, repeatable software design pattern, and complementary manual testing. All of these factors and the appropriate test regime will lead to a better chance of success in a test driven development project},
annote = {Learning effective test driven development: software development projects in an energy company},
author = {Law, W K A},
journal = {ICSOFT 2006 Proceedings of the First International Conference on Software and Data Technologies},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {p},
title = {{Learning effective test driven development: software development projects in an energy company}},
url = {citeulike-article-id:3934688 {\#}},
year = {2006}
}
@inproceedings{Wu2006,
address = {New York, New York, USA},
author = {Wu, Min and Miller, Robert C. and Garfinkel, Simson L.},
booktitle = {Proceedings of the SIGCHI conference on Human Factors in computing systems - CHI '06},
doi = {10.1145/1124772.1124863},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Miller, Garfinkel - 2006 - Do security toolbars actually prevent phishing attacks.pdf:pdf},
isbn = {1595933727},
keywords = {agile,e-commerce,nsf,security,user interface design,user study,world wide web and hypermedia},
mendeley-tags = {agile,nsf,security},
month = {apr},
pages = {601},
publisher = {ACM Press},
title = {{Do security toolbars actually prevent phishing attacks?}},
url = {http://dl.acm.org/citation.cfm?id=1124772.1124863},
year = {2006}
}
@inproceedings{Landre2007,
abstract = {In this paper we present the experience gained and lessons learned when the IT department at Statoil ASA, a large Oil and Gas company in Norway, applied Domain-Driven design techniques in combination with agile software development practices to assess the software architecture of our next generation oil trading and supply chain application. Our hypothesis was that the use of object oriented techniques, Domain-Driven design and a proper object-relational mapping tool would significantly improve the performance and reduce the code base compared with current legacy systems. The legacy system is based on several Oracle databases serving a variety of clients written in Java, Gupta Centura Team Developer and HTML. The databases have a layer of business logic written in PL/SQL offering various system services to the clients. To validate our new object oriented software architecture, we re-implemented one of the most computationally heavy and data intensive services using Test First and DomainDriven design techniques. The resulting software was then tested on a set of servers with a representative subset of data from the production environment. We found that using these techniques improved our software architecture with respect to performance as well as code quality when running on top of our Oracle databases. We also tested the switch to an object database from Versant and achieved additional performance gains.},
annote = {Agile enterprise software development using domain-driven design and test first},
author = {Landre, E and Wesenberg, H and {\O}lmheim, J},
booktitle = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {983--993},
title = {{Agile enterprise software development using domain-driven design and test first}},
url = {citeulike-article-id:3934685 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42149155540{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Teh2006,
abstract = {We consider problems involving groups of data where each observation within a group is a draw from a mixture model and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes ...},
author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
doi = {10.1198/016214506000000302},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {Clustering,Hierarchical model,Markov chain Monte Carlo,Mixture model,Nonparametric Bayesian statistics},
language = {en},
month = {dec},
number = {476},
pages = {1566--1581},
publisher = {Taylor {\&} Francis},
title = {{Hierarchical Dirichlet Processes}},
url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214506000000302},
volume = {101},
year = {2006}
}
@inproceedings{Layman2004c,
address = {New York, New York, USA},
author = {Layman, Lucas},
booktitle = {Companion to the 19th annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications - OOPSLA '04},
doi = {10.1145/1028664.1028787},
isbn = {1581138334},
keywords = {agile software development,case studies,extreme programming,mypubs},
mendeley-tags = {mypubs},
month = {oct},
pages = {328},
publisher = {ACM Press},
title = {{Empirical investigation of the impact of extreme programming practices on software projects}},
year = {2004}
}
@inproceedings{Colombe2004a,
address = {New York, New York, USA},
author = {Colombe, Jeffrey B. and Stephens, Gregory},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029231},
isbn = {1581139748},
keywords = {anomaly detection,cognitive load,human-computer interaction,information visualization},
month = {oct},
pages = {138},
publisher = {ACM Press},
title = {{Statistical profiling and visualization for detection of malicious insider attacks on computer networks}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029231},
year = {2004}
}
@inproceedings{Wang2008,
address = {Leipzig, Germany},
author = {Wang, Xiaoyin and Zhang, Lu and Xie, Tao and Anvik, John and Sun, Jiasu},
booktitle = {Proceedings of the 13th international conference on Software engineering - ICSE '08},
doi = {10.1145/1368088.1368151},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2008 - An approach to detecting duplicate bug reports using natural language and execution information.pdf:pdf},
isbn = {9781605580791},
keywords = {bug reports,duplicate,duplicate bug report,execution information,information retrieval,nlp},
mendeley-tags = {bug reports,duplicate,nlp},
month = {may},
pages = {461},
publisher = {ACM Press},
title = {{An approach to detecting duplicate bug reports using natural language and execution information}},
url = {http://dl.acm.org/citation.cfm?id=1368088.1368151},
year = {2008}
}
@inproceedings{Porter2019,
abstract = {A Concept Inventory (CI) is a validated assessment to measure student conceptual understanding of a particular topic. This work presents a CI for Basic Data Structures (BDSI) and the process by which the CI was designed and validated. We discuss: 1) the collection of faculty opinions from diverse institutions on what belongs on the instrument, 2) a series of interviews with students to identify their conceptions and misconceptions of the content, 3) an iterative design process of developing draft questions, conducting interviews with students to ensure the questions on the instrument are interpreted properly, and collecting faculty feedback on the questions themselves, and 4) a statistical evaluation of final versions of the instrument to ensure its internal validity. We also provide initial results from pilot runs of the CI.},
address = {Toronto, ON, Canada},
author = {Porter, Leo and Zingaro, Daniel and Liao, Soohyun Nam and Taylor, Cynthia and Webb, Kevin C. and Lee, Cynthia and Clancy, Michael},
booktitle = {Proceedings of the 2019 ACM Conference on International Computing Education Research - ICER '19},
doi = {10.1145/3291279.3339404},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Porter et al. - 2019 - BDSI A Validated Concept Inventory for Basic Data Structures.pdf:pdf},
isbn = {9781450361859},
keywords = {CS2,assessment,concept inventory,data structures},
mendeley-tags = {CS2},
pages = {111--119},
publisher = {ACM Press},
title = {{BDSI: A Validated Concept Inventory for Basic Data Structures}},
url = {http://dl.acm.org/citation.cfm?doid=3291279.3339404},
year = {2019}
}
@inproceedings{5168067,
abstract = {Recent research and development efforts within BAE Systems have focused on applying geospatial technologies within emergency management policies and procedures. Challenges within the existing literature can be categorized into four broad concepts - gathering, fusion, dissemination and visualization. A unique combination focused on providing communication; information fusion and management; and collaboration solutions through the application of data collection, manipulation and brokering; process workflow technology, and advanced geospatial capabilities and functionality within an open source software environment have resulted in the creation of two integrated applications - Salina and incidentOS. A real world implementation of these proof of concept applications has shown that a possible solution exists to answer some of the outstanding issues currently highlighted in the literature. The underlying open source technology utilized in the data platform can be implemented in numerous combinations and therefore has broader application to a wide range of domains including emergency management.},
author = {Intorelli, A and Braig, D and Moquin, R},
booktitle = {Technologies for Homeland Security, 2009. HST '09. IEEE Conference on},
doi = {10.1109/THS.2009.5168067},
keywords = {Salina,data fusion,e,emergency management policies},
month = {may},
pages = {417--424},
title = {{Real-time data fusion and visualization in support of emergency response operations}},
year = {2009}
}
@inproceedings{Krebs2002,
address = {Chicago, IL},
author = {Krebs, W and Wells, D and Williams, L},
pages = {60--69},
publisher = {Springer},
title = {{Turning the Knobs:  A Coaching Pattern for XP through Agile Metrics}},
year = {2002}
}
@inproceedings{Egelman2008a,
address = {Florence, Italy},
author = {Egelman, Serge and Cranor, Lorrie Faith and Hong, Jason},
booktitle = {Proceeding of the twenty-sixth annual CHI Conference on Human Factors in Computing Systems - CHI '08},
doi = {10.1145/1357054.1357219},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Egelman, Cranor, Hong - 2008 - You've been warned.pdf:pdf},
isbn = {9781605580111},
keywords = {agile,mental models,nsf,phishing,security,usable privacy and security,warning messages},
mendeley-tags = {agile,nsf,security},
month = {apr},
pages = {1065--1074},
publisher = {ACM Press},
title = {{You've been warned}},
url = {http://dl.acm.org/citation.cfm?id=1357054.1357219},
year = {2008}
}
@book{cranor2008security,
author = {Cranor, Lorrie Faith and Garfinkel, Simson},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {O'Reilly Media},
title = {{Security and usability: Designing secure systems that people can use}},
year = {2008}
}
@book{Strauss1990,
address = {Newbury Park, CA},
author = {Strauss, Anselm L and Corbin, Juliet M},
keywords = {grounded theory},
publisher = {Sage},
title = {{Basics of qualitative research: Grounded theory procedures and techniques}},
year = {1990}
}
@inproceedings{bruckman2002,
author = {Bruckman, Amy and Bandlow, Alisa},
booktitle = {The Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies and Emerging Applications},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
organization = {L. Erlbaum Associates Inc.},
pages = {428--440},
title = {{Human-computer interaction for kids}},
year = {2002}
}
@article{Kulinskaya,
author = {Kulinskaya, Elena},
journal = {Journal of Nonparametric Statistics},
number = {1},
pages = {43--60},
title = {{Coefficients of the asymptotic distribution of the {\{}K{\}}olmogorov-{\{}S{\}}mirnov statistic when parameters are estimated}},
volume = {5},
year = {1995}
}
@inproceedings{Oppenheimer2003,
author = {Oppenheimer, David and Ganapathi, Archana and Patterson, David A},
booktitle = {Proc.$\backslash$ 4th Conference on USENIX Symposium on Internet Technologies and Systems},
pages = {1--16},
title = {{Why do internet services fail, and what can be done about it?}},
volume = {4},
year = {2003}
}
@inproceedings{Spacco2006,
abstract = {Testing is an important part of the software development cycle that should be covered throughout the computer science curriculum. However, for students to truly learn the value of testing, they need to benefit from writing test cases for their own software.We report on our initial experiences teaching students to write test cases and evaluating student-written test suites, with an emphasis on our observation that, without proper incentive to write test cases early, many students will complete the programming assignment first and then add the build of their test cases afterwards. Based on these experiences, we propose new mechanisms to provide better incentives for students to write their test cases early.We also report on some of the limitations of code coverage as a tool for evaluating test suites, and finally conclude with a survey of related work on introducing testing into the undergraduate curriculum.},
annote = {Helping students appreciate test-driven development (TDD)},
author = {Spacco, J and Pugh, W},
booktitle = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {907--913},
title = {{Helping students appreciate test-driven development (TDD)}},
url = {citeulike-article-id:3934805 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248396787{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@inproceedings{Degerstedt2006,
abstract = {In this paper we present a development tool for testing dialogue systems. Testing software through the specification is important for software development in general and should be as automated as possible. For dialogue systems, the corpus can be seen as one part of the specification and the dialogue system should be tested on available corpora on each new build. The testing tool is inspired from work on agile software development methods, test driven development and unit testing, and can be used in two modes and during various phases of development.},
annote = {LINTest, A development tool for testing dialogue systems},
author = {Degerstedt, L and J{\"{o}}nsson, A},
booktitle = {INTERSPEECH 2006 and 9th International Conference on Spoken Language Processing, INTERSPEECH 2006 - ICSLP},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {489--492},
title = {{LINTest, A development tool for testing dialogue systems}},
url = {citeulike-article-id:3934585 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-44949099726{\&}{\#}38 partnerID=40},
volume = {1},
year = {2006}
}
@article{Fagan1976,
author = {Fagan, Michael E},
keywords = {inspection,inspections},
pages = {182--211},
title = {{Advances in software inspections to reduce errors in program development}},
volume = {15},
year = {1976}
}
@inproceedings{Kersten2006,
address = {Portland, OR},
author = {Kersten, Mik and Murphy, Gail C},
booktitle = {Foundations of Software Engineering (SIGSOFT/FSE '06)},
keywords = {doi,ide,mylar,program comprehension,task context},
mendeley-tags = {program comprehension,task context},
pages = {1--11},
title = {{Using Task Context to Improve Programmer Productivity}},
year = {2006}
}
@inproceedings{Janzen2007a,
abstract = {Test-driven development (TDD) has garnered considerable attention in professional settings and has made some inroads into software engineering and computer science education. A series of leveled experiments were conducted with students in beginning undergraduate programming courses through upper-level undergraduate, graduate, and professional training courses. This paper reports that mature programmers who try TDD are more likely to choose TDD over a similar test-last approach. Additionally this research reveals differences in programmer acceptance of TDD between beginning programmers who were reluctant to adopt TDD and more mature programmers who were more willing to adopt TDD. Attention is given to confounding factors, and future studies aimed at resolving these factors are identified. Finally proposals are made to improve early programmer acceptance of TDD. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {A leveled examination of test-driven development acceptance},
author = {Janzen, D S and Saiedian, H},
booktitle = {Proceedings - International Conference on Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {719--722},
title = {{A leveled examination of test-driven development acceptance}},
url = {citeulike-article-id:3934656 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34548772886{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{4041182,
abstract = {Signature-matching intrusion detection systems can experience significant decreases in performance when the load on the IDS-host increases. We propose a solution that off-loads some of the computation performed by the IDS to the graphics processing unit (GPU). Modern GPUs are programmable, stream-processors capable of high-performance computing that in recent years have been used in non-graphical computing tasks. The major operation in a signature-matching IDS is matching values seen operation to known black-listed values, as such, our solution implements the string-matching on the GPU. The results show that as the CPU load on the IDS host system increases, PixelSnort's performance is significantly more robust and is able to outperform conventional Snort by up to 40{\%}},
author = {Jacob, Nigel and Brodley, Carla},
booktitle = {Computer Security Applications Conference, 2006. ACSAC '06. 22nd Annual},
doi = {10.1109/ACSAC.2006.35},
issn = {1063-9527},
keywords = {GPU,graphics processing unit,signature-matching in},
pages = {371--380},
title = {{Offloading IDS Computation to the GPU}},
year = {2006}
}
@incollection{Curtis1990,
address = {New York, NY},
annote = {Ego-less programming{\textless}m:linebreak{\textgreater}{\textless}/m:linebreak{\textgreater}Communication boundaries},
author = {Curtis, B and Walz, D and Hoc, J.-M. and Green, T R G and Samurcay, R and Gilmore, D J and Gaines, B R and Monk, A},
keywords = {communications,egoless,psychology},
pages = {253--270},
publisher = {Harcourt Brace Jovanovich},
title = {{The Psychology of Programming in the Large: Team and Organizational Behaviour}},
year = {1990}
}
@misc{Freeman1999,
address = {Washington, DC},
author = {Freeman, P and Aspray, W},
publisher = {Computing Research Association},
title = {{The Supply of Information Technology Workers in the United States}},
url = {http://www.cra.org/reports/wits/cra.wits.html},
year = {1999}
}
@article{Tversky,
abstract = {Reports that people have erroneous intuitions about the laws of chance. In particular, they regard a sample randomly drawn from a population as highly representative, I.e., similar to the population in all essential characteristics. The prevalence of the belief and its unfortunate consequences for psychological research are illustrated by the responses of 84 professional psychologists to a questionnaire concerning research decisions.},
author = {Tversky, Amos and Kahneman, Daniel},
journal = {Psychological Bulletin1},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {105--110},
title = {{Belief in the law of small numbers.}},
volume = {76},
year = {1971}
}
@book{Highsmith1999,
author = {Highsmith, J},
publisher = {Dorset House},
title = {{Adaptive Software Development}},
year = {1999}
}
@inproceedings{Bache2002,
abstract = {Extreme Programming (XP) as written [1] prescribes doing and automating both unit and functional testing. Our experiences lead us to believe that these two sorts of testing lie at two ends of a more or less continuous scale, and that it can be desirable to instead run a XP project with just one test suite, occupying the middle ground between unit and functional. We believe that this testing approach offers most of the advantages of a standard XP testing approach, in a simpler way. This report explains what we have done, and our theory as to why it works.},
annote = {One suite of automated tests: examining the unit/functional divide},
author = {Bache, G and Bache, E},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{One suite of automated tests: examining the unit/functional divide}},
url = {citeulike-article-id:3934547 {\#}},
year = {2002}
}
@article{Socha2006,
abstract = {This paper explores the question of whether designing software is different from designing other things (we believe it is). We discuss several key distinctions that are largely missing from the discourse on software design yet which are vital to the success of software designs. These distinctions are increasingly important as software becomes prevalent in the design tools and products of other engineering disciplines. By considering what is similar and what is different we help reveal how the lessons of software design may help other disciplines, and vice versa. This in turn illuminates a core meta-question of how educators in academia and industry can help evolve our understanding of what we do so that we can be more effective at software design. But first, we need to understand what is different, and what is not different, about this discipline called software design. {\^{A}}{\textcopyright} 2066 TEMPUS Publications.},
annote = {Is designing software different from designing other things?},
author = {Socha, D and Walter, S},
journal = {International Journal of Engineering Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {540--550},
title = {{Is designing software different from designing other things?}},
url = {citeulike-article-id:3934804 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33745289065{\&}{\#}38 partnerID=40},
volume = {22},
year = {2006}
}
@incollection{Ryan2000,
address = {Thousand Oaks, CA},
author = {Ryan, Gery W and Bernard, H Russel},
booktitle = {Handbook of Qualitative Research},
chapter = {29},
edition = {2nd},
editor = {Denzin, Norman K and Lincoln, Yvonna S},
keywords = {coding,qualitative},
pages = {769--802},
publisher = {Sage},
title = {{Data management and analysis methods}},
year = {2000}
}
@book{McGarry2002,
address = {Boston, MA},
author = {McGarry, F and Card, D and Jones, C and Layman, B and Clark, E and Dead, H and Hall, F},
isbn = {0201715163},
keywords = {framework,measurement,mypubs},
mendeley-tags = {mypubs},
publisher = {Addison wesley},
title = {{Practical Software Measurement: Objective Information for Decision Makers}},
year = {2002}
}
@inproceedings{Olbrich2009,
abstract = {Code smells are design flaws in object-oriented designs that may lead to maintainability issues in the further evolution of the software system. This study focuses on the evolution of code smells within a system and their impact on the change behavior (change frequency and size). The study investigates two code smells, God Class and Shotgun Surgery, by analyzing the historical data over several years of development of two large scale open source systems. The detection of code smells in the evolution of those systems was performed by the application of an automated approach using detection strategies. The results show that we can identify different phases in the evolution of code smells during the system development and that code smell infected components exhibit a different change behavior. This information is useful for the identification of risk areas within a software system that need refactoring to assure a future positive evolution.},
address = {Lake Buena Vista, FL},
author = {Olbrich, Steffen and Cruzes, Daniela S. and Basili, Victor and Zazworka, Nico},
booktitle = {2009 3rd International Symposium on Empirical Software Engineering and Measurement},
doi = {10.1109/ESEM.2009.5314231},
isbn = {978-1-4244-4842-5},
month = {oct},
pages = {390--400},
publisher = {IEEE},
title = {{The evolution and impact of code smells: A case study of two open source systems}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/ESEM.2009.5314231},
year = {2009}
}
@inproceedings{Singh2011a,
address = {New York, New York, USA},
author = {Singh, Ankit and Bradel, Lauren and Endert, Alex and Kincaid, Robert and Andrews, Christopher and North, Chris},
booktitle = {Proceedings of the 8th International Symposium on Visualization for Cyber Security - VizSec '11},
doi = {10.1145/2016904.2016907},
isbn = {9781450306799},
keywords = {interaction styles,large high-resolution displays,prototyping,screen design,user-centered design},
month = {jul},
pages = {1--8},
publisher = {ACM Press},
title = {{Supporting the cyber analytic process using visual history on large displays}},
url = {http://dl.acm.org/citation.cfm?id=2016904.2016907},
year = {2011}
}
@inproceedings{386794,
abstract = {A tool for verifying security properties of data is described, based on colored Petri nets. A conceptual data model for complex applications and a mandatory security model based on the principles of Bell-LaPadula are presented. The tool operates on such models, using the Petri net analysis techniques, based on the reachability graph. The absence of interference of sensitive associations, the reachability of information, and the temporal relationships among data access properties are studied, and conditions on the reachability graph are defined, to guarantee that such properties hold in the system},
author = {Castano, S and Samarati, P and Villa, C},
booktitle = {Security Technology, 1993. Security Technology, Proceedings. Institute of Electrical and Electronics Engineers 1993 International Carnahan Conference on},
doi = {10.1109/CCST.1993.386794},
keywords = {colored Petri nets; data; data access; reachabilit},
month = {oct},
pages = {244--250},
title = {{Verifying system security using Petri Nets}},
year = {1993}
}
@article{Gosling2003,
abstract = {When time is limited, researchers may be faced with the choice of using an extremely brief measure of the Big-Five personality dimensions or using no measure at all. To meet the need for a very brief measure, 5 and 10-item inventories were developed and evaluated. Although somewhat inferior to standard multi-item instruments, the instruments reached adequate levels in terms of: (a) convergence with widely used Big-Five measures in self, observer, and peer reports, (b) test-retest reliability, (c) patterns of predicted external correlates, and (d) convergence between self and observer ratings. On the basis of these tests, a 10-item measure of the Big-Five dimensions is offered for situations where very short measures are needed, personality is not the primary topic of interest, or researchers can tolerate the somewhat diminished psychometric properties associated with very brief measures. {\textcopyright} 2003 Elsevier Science (USA). All rights reserved.},
author = {Gosling, Samuel D. and Rentfrow, Peter J. and Swann, William B.},
doi = {10.1016/S0092-6566(03)00046-1},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gosling, Rentfrow, Swann - 2003 - A very brief measure of the Big-Five personality domains.pdf:pdf},
issn = {00926566},
journal = {Journal of Research in Personality},
number = {6},
pages = {504--528},
publisher = {Academic Press Inc.},
title = {{A very brief measure of the Big-Five personality domains}},
volume = {37},
year = {2003}
}
@inproceedings{1532060,
abstract = {Not available},
author = {Abdullah, K. and Lee, C.P. and Conti, G. and Copeland, J.a. and Stasko, J.},
booktitle = {IEEE Workshop on Visualization for Computer Security, 2005. (VizSEC 05).},
doi = {10.1109/VIZSEC.2005.1532060},
isbn = {0-7803-9477-1},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {1--10},
publisher = {Ieee},
title = {{IDS rainStorm: visualizing IDS alarms}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1532060},
year = {2005}
}
@techreport{Glasser2002a,
author = {Gl{\"{a}}sser, Uwe and Gurevich, Yuri and Veanes, Margus},
institution = {Microsoft Research},
title = {{An Abstract Communication Model}},
year = {2002}
}
@book{Hartmanis1939,
author = {Hartmanis, J and Leeuwen, J Van},
isbn = {3540203036},
title = {{Lecture Notes in Computer Science}},
year = {1939}
}
@inproceedings{Parnin2011,
address = {New York, New York, USA},
author = {Parnin, Chris and Orso, Alessandro},
booktitle = {Proceedings of the 2011 International Symposium on Software Testing and Analysis - ISSTA '11},
doi = {10.1145/2001420.2001445},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parnin, Orso - 2011 - Are automated debugging techniques actually helping programmers.pdf:pdf},
isbn = {9781450305624},
keywords = {statistical debugging,user studies},
month = {jul},
pages = {199},
publisher = {ACM Press},
title = {{Are automated debugging techniques actually helping programmers?}},
url = {http://dl.acm.org/citation.cfm?id=2001420.2001445},
year = {2011}
}
@article{Marshall1992,
author = {Marshall, E},
journal = {Science},
number = {5050},
pages = {1347},
title = {{Fatal Error: How {\{}P{\}}atriot Overlooked a {\{}S{\}}cud}},
volume = {255},
year = {1992}
}
@inproceedings{Melnik2002,
address = {Chicago, IL},
author = {Melnik, Grigori and Maurer, Frank and Wells, Don and Williams, Laurie},
pages = {241--250},
publisher = {Springer Verlag Lecture Notes in Computer Science 2418},
title = {{Perceptions of Agile Practices: A Student Survey}},
year = {2002}
}
@article{Nebut2006,
author = {Nebut, C. and Fleurey, F. and {Le Traon}, Y. and Jezequel, J.-M.},
doi = {10.1109/TSE.2006.22},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {mar},
number = {3},
pages = {140--155},
title = {{Automatic test generation: a use case driven approach}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1610607},
volume = {32},
year = {2006}
}
@inproceedings{Oberheide2008,
address = {Breckenridge, CO},
author = {Oberheide, Jon and Veeraraghavan, Kaushik and Cooke, Evan and Flinn, Jason and Jahanian, Farnam},
booktitle = {Proceedings of the First Workshop on Virtualization in Mobile Computing (MobiVirt '08)},
doi = {10.1145/1622103.1629656},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oberheide et al. - 2008 - Virtualized in-cloud security services for mobile devices.pdf:pdf},
isbn = {9781605583280},
keywords = {agile,antivirus,malware detection,mobile devices,nsf,security},
mendeley-tags = {agile,nsf,security},
month = {jun},
pages = {31},
publisher = {ACM Press},
title = {{Virtualized in-cloud security services for mobile devices}},
url = {http://dl.acm.org/citation.cfm?id=1622103.1629656},
year = {2008}
}
@inproceedings{Grinter2003,
address = {New York, New York, USA},
author = {Grinter, Rebecca and Eldridge, Margery},
booktitle = {Proceedings of the conference on Human factors in computing systems - CHI '03},
doi = {10.1145/642611.642688},
isbn = {1581136307},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {apr},
pages = {441},
publisher = {ACM Press},
title = {{Wan2tlk?}},
url = {http://dl.acm.org/citation.cfm?id=642611.642688},
year = {2003}
}
@inproceedings{Vasilescu2014,
address = {Baltimore, MD},
author = {Vasilescu, Bogdan and Serebrenik, Alexander and Devanbu, Prem and Filkov, Vladimir},
booktitle = {Proceedings of the 17th ACM conference on Computer supported cooperative work {\&} social computing - CSCW '14},
doi = {10.1145/2531602.2531659},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vasilescu et al. - 2014 - How social Q{\&}ampA sites are changing knowledge sharing in open source software communities.pdf:pdf},
isbn = {9781450325400},
keywords = {sentiment},
mendeley-tags = {sentiment},
pages = {342--354},
publisher = {ACM Press},
title = {{How social Q{\&}A sites are changing knowledge sharing in open source software communities}},
url = {http://dl.acm.org/citation.cfm?doid=2531602.2531659},
year = {2014}
}
@inproceedings{Goodall2010b,
address = {New York, New York, USA},
author = {Goodall, John R. and Radwan, Hassan and Halseth, Lenny},
booktitle = {Proceedings of the Seventh International Symposium on Visualization for Cyber Security - VizSec '10},
doi = {10.1145/1850795.1850800},
isbn = {9781450300131},
keywords = {data fusion,security visualization,software analysis,software assurance,software visualization},
month = {sep},
pages = {46--51},
publisher = {ACM Press},
title = {{Visual analysis of code security}},
url = {http://dl.acm.org/citation.cfm?id=1850795.1850800},
year = {2010}
}
@inproceedings{4373477,
abstract = {Around the world, there is great concern about the movement of threat materials using seaport shipping containers. The benefit of early detection of weapons of mass destruction is obvious. However, the inspection process needs to be conducted in such a way as to not unreasonably impede normal commerce. Prior to actual deployment of new detection systems, policies, or procedures, it is useful to construct an operational and cost model of the port facility and to run simulations to gage the impact. Using a simulation model beforehand aids decision makers in evaluating tradeoffs. PortSim was developed at ORNL by the author to allow a user to investigate a number of parameters in order to see the impact on port operations and cost. It consolidates a conceptual operations model, cost information, policy and procedures database, a real-time data acquisition capability, and information flow tracking and provides a visualization of port operations in a geospatial environment. This paper describes the use of PortSim to simulate and visualize a typical port.},
author = {Koch, D B},
booktitle = {Security Technology, 2007 41st Annual IEEE International Carnahan Conference on},
doi = {10.1109/CCST.2007.4373477},
keywords = {Portsim,cost model,data acquisition,decision makin},
pages = {109--116},
title = {{PortSim-A Port Security Simulation and Visualization Tool}},
year = {2007}
}
@inproceedings{Dehlinger2004,
abstract = {The current development of high-integrity product lines threatens to outstrip existing tools for product-line verification. Software Fault Tree Analysis (SFTA) is a technique that has been used successfully to investigate contributing causes to potential hazards in safety-critical applications. This paper adapts SFTA to product lines of systems. The contribution is to define: (1) the technique to construct a product-line SFTA; and (2) the pruning technique required to reuse the SFTA for the analysis of a new system in the product line. The paper describes how product-line SFTA integrates with forward-analysis techniques such as Software Failure Modes, Effects, and Criticality Analysis (SFMECA), supports requirements evolution, and helps identify previously unforeseen constraints on the systems to be built. Applications to two small examples are used to illustrate the technique.},
author = {Dehlinger, J. and Lutz, R.R.},
booktitle = {Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.},
doi = {10.1109/HASE.2004.1281726},
isbn = {0-7695-2094-4},
pages = {12--21},
publisher = {IEEE},
title = {{Software fault tree analysis for product lines}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1281726},
year = {2004}
}
@book{Shue1999,
address = {Bloomington, IN},
author = {Shue, Sylvia and Vest, Jennifer Lisa and Villarreal, Joseph},
publisher = {Indiana University Press},
title = {{Philanthropy in Communities of Color}},
year = {1999}
}
@inproceedings{Pinheiro2002,
address = {Essen, Germany},
annote = {Excellent paper.  Discusses RE principles philosophically, and how XP violates some of these principles.  It{\&}{\#}039;s more of a position paper, but is well thought out.  Not against agile, but is there as a starting point for possible improvements.},
author = {Pinheiro, F A C},
keywords = {agile,requirements,xp},
title = {{Requirements Honesty}},
year = {2002}
}
@inproceedings{Biggio2014,
address = {New York, New York, USA},
author = {Biggio, Battista and Rieck, Konrad and Ariu, Davide and Wressnegger, Christian and Corona, Igino and Giacinto, Giorgio and Roli, Fabio},
booktitle = {Proceedings of the 2014 Workshop on Artificial Intelligent and Security Workshop - AISec '14},
doi = {10.1145/2666652.2666666},
isbn = {9781450331531},
keywords = {adversarial machine learning,clustering,computer security,malware detection,security evaluation,unsupervised learning},
month = {nov},
pages = {27--36},
publisher = {ACM Press},
title = {{Poisoning behavioral malware clustering}},
url = {http://dl.acm.org/citation.cfm?id=2666652.2666666},
year = {2014}
}
@article{Poole2001,
author = {Poole, Charles and Huisman, Jan Willem},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Poole, Huisman - 2001 - Using Extreme Programming in a Maintenance Environment.pdf:pdf},
journal = {IEEE Software},
keywords = {XP,agile,maintenance},
mendeley-tags = {agile},
number = {6},
pages = {42--50},
title = {{Using Extreme Programming in a Maintenance Environment}},
volume = {18},
year = {2001}
}
@misc{Hayashi2004,
abstract = {We are developing a methodology for Test-Driven Development of Models (TDDM) based on an experimental UML 2.0 modeling tool SMART. Our experience shows that TDDM is quite useful for agile model developments. SMART provides guidance on how to build models based on compiler errors of testcases, something similar to what Quick Fix of Eclipse does. It also provides such guidance from failures of testcases, which seems difficult in the case of TDD of programs. {\^{A}}{\textcopyright} Springer-Verlag 2004.},
annote = {Test driven development of UML models with SMART modeling system},
author = {Hayashi, S and Yibing, P and Sato, M and Mori, K and Sejeon, S and Haruna, S},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {395--409},
title = {{Test driven development of UML models with SMART modeling system}},
url = {citeulike-article-id:3934635 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-29144467884{\&}{\#}38 partnerID=40},
volume = {3273},
year = {2004}
}
@inproceedings{Orso2003a,
address = {New York, New York, USA},
author = {Orso, Alessandro and Apiwattanapong, Taweesup and Harrold, Mary Jean},
booktitle = {Proceedings of the 9th European software engineering conference held jointly with 10th ACM SIGSOFT international symposium on Foundations of software engineering - ESEC/FSE '03},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Orso, Apiwattanapong, Harrold - 2003 - Leveraging field data for impact analysis and regression testing.pdf:pdf},
issn = {0163-5948},
keywords = {gamma technology,impact analysis,regression testing,software engineering},
month = {sep},
number = {5},
pages = {128},
publisher = {ACM Press},
title = {{Leveraging field data for impact analysis and regression testing}},
url = {http://dl.acm.org/citation.cfm?id=940071.940089},
volume = {28},
year = {2003}
}
@inproceedings{5604041,
abstract = {Intrusion Detection Systems (IDS) have been used widely to detect malicious behavior in network communication and hosts. IDS management is an important capability for distributed IDS solutions, which makes it possible to integrate and handle different types of sensors or collect and synthesize alerts generated from multiple hosts located in the distributed environment. Sophisticated attacks are difficult to detect and make it necessary to integrate multiple data sources for detection and correlation. Attack graph (AG) is used as an effective method to model, analyze, and evaluate the security of complicated computer systems or networks. The attack graph workflow consists of three parts: information gathering, attack graph construction, and visualization. This paper proposes the integration of the AG workflow with an IDS management system to improve alert and correlation quality. The vulnerability and system information is used to prioritize and tag the incoming IDS alerts. The AG is used during the correlation process to filter and optimize correlation results. A prototype is implemented using automatic vulnerability extraction and AG creation based on unified data models.},
author = {Roschke, S and Cheng, Feng and Meinel, C},
booktitle = {Information Assurance and Security (IAS), 2010 Sixth International Conference on},
doi = {10.1109/ISIAS.2010.5604041},
keywords = {IDS management system,at,attack graph construction},
pages = {68--73},
title = {{Using vulnerability information and attack graphs for intrusion detection}},
year = {2010}
}
@article{Gino2009,
abstract = {In a world where encounters with dishonesty are frequent, it is important to know if exposure to other people's unethical behavior can increase or decrease an individual's dishonesty. In Experiment 1, our confederate cheated ostentatiously by finishing a task impossibly quickly and leaving the room with the maximum reward. In line with social-norms theory, participants' level of unethical behavior increased when the confederate was an in-group member, but decreased when the confederate was an out-group member. In Experiment 2, our confederate instead asked a question about cheating, which merely strengthened the saliency of this possibility. This manipulation decreased the level of unethical behavior among the other group members. These results suggest that individuals' unethicality does not depend on the simple calculations of cost-benefit analysis, but rather depends on the social norms implied by the dishonesty of others and also on the saliency of dishonesty.},
author = {Gino, Francesca and Ayal, Shahar and Ariely, Dan},
doi = {10.1111/j.1467-9280.2009.02306.x},
issn = {1467-9280},
journal = {Psychological science},
keywords = {Deception,Ethics,Humans,Imitative Behavior,Male,Social Behavior,Young Adult,agile,nsf},
mendeley-tags = {agile,nsf},
month = {mar},
number = {3},
pages = {393--8},
pmid = {19254236},
title = {{Contagion and differentiation in unethical behavior: the effect of one bad apple on the barrel.}},
url = {http://pss.sagepub.com/content/20/3/393.short},
volume = {20},
year = {2009}
}
@article{Daday2005,
abstract = {The criminological literature presents substantial evidence that victims and offenders in violent crimes share demographic characteristics, engage in similar lifestyles and activities, and reside in socially disorganized neighborhoods. However, research has examined these relationships separately using either victimization or offending data, and prior studies have not examined these relationships by comparing victims and offenders within the same incidents. This limits the effect of examining whether these factors are associated with victimization and offending in similar or distinct ways. Using a law enforcement database of victims (n = 1,248) and offenders (n = 1,735) involved within the same aggravated battery incidents (n = 1,015) in Bernalillo County, New Mexico, this research explores whether victims and offenders involved in non‐lethal violence share certain individual, neighborhood and situational characteristics. Results suggest that victims and offenders live in socially disorganized neighborhoo...},
author = {Daday, Jerry K. and Broidy, Lisa M. and Crandall, Cameron S. and Sklar, David P.},
doi = {10.1080/14786010500287347},
issn = {1478-601X},
journal = {Criminal Justice Studies},
keywords = {Offending,Risky lifestyles,Routine Activities Theory,Victimization,Violent crime,agile,nsf},
language = {en},
mendeley-tags = {agile,nsf},
month = {jan},
number = {3},
pages = {215--235},
publisher = {Routledge},
title = {{Individual, Neighborhood, and Situational Factors Associated with Violent Victimization and Offending}},
url = {http://www.tandfonline.com/doi/abs/10.1080/14786010500287347},
volume = {18},
year = {2005}
}
@article{Goodall2019,
abstract = {Despite the best efforts of cyber security analysts, networked computing assets are routinely compromised, resulting in the loss of intellectual property, the disclosure of state secrets, and major financial damages. Anomaly detection methods are beneficial for detecting new types of attacks and abnormal network activity, but such algorithms can be difficult to understand and trust. Network operators and cyber analysts need fast and scalable tools to help identify suspicious behavior that bypasses automated security systems, but operators do not want another automated tool with algorithms they do not trust. Experts need tools to augment their own domain expertise and to provide a contextual understanding of suspicious behavior to help them make decisions. In this paper we present Situ, a visual analytics system for discovering suspicious behavior in streaming network data. Situ provides a scalable solution that combines anomaly detection with information visualization. The system's visualizations enable operators to identify and investigate the most anomalous events and IP addresses, and the tool provides context to help operators understand why they are anomalous. Finally, operators need tools that can be integrated into their workflow and with their existing tools. This paper describes the Situ platform and its deployment in an operational network setting. We discuss how operators are currently using the tool in a large organization's security operations center and present the results of expert reviews with professionals.},
author = {Goodall, John R. and Ragan, Eric D. and Steed, Chad A. and Reed, Joel W. and Richardson, G. David and Huffer, Kelly M.T. and Bridges, Robert A. and Laska, Jason A.},
doi = {10.1109/TVCG.2018.2865029},
issn = {19410506},
journal = {IEEE Transactions on Visualization and Computer Graphics},
title = {{Situ: Identifying and explaining suspicious behavior in networks}},
year = {2019}
}
@inproceedings{WilliamsW.KrebsL.LaymanandA.Anton2003,
abstract = {Software organizations are progressively adopting the development practices associated with the extreme programming (XP) methodology. Most reports on the efficacy of these practices are anecdotal. This paper provides a benchmark measurement framework for researchers and practitioners to express concretely the XP practices the organization has selected to adopt and/or modify, and the outcome thereof. The framework enables the necessary meta-analysis for combining families of case studies. The results of running framework-based case studies in various contexts will eventually constitute a body of knowledge of systematic, empirical evaluations of XP and its practices. Additionally, this benchmark provides a baseline framework that can be adapted for industrial case studies of other technologies and processes. To provide a foundation on the use of the framework, we present the initial validation of our XP evaluation framework based upon a year-long study of an IBM team that adopted a subset of XP practices.},
address = {Edinburgh, Scotland},
annote = {From Duplicate 1 (Toward a Framework for Evaluating Extreme Programming - Williams, Laurie; Krebs, William; Layman, Lucas; Anton, Annie I; Abrahamsson, Pekka)

rejected icse paper},
author = {Williams, Laurie and Krebs, William and Layman, Lucas and Anton, Annie I and Abrahamsson, Pekka},
booktitle = {Proceedings of the 8th International Conference on Evaluation and Assessment in Software Engineering (EASE '04)},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {11--20},
publisher = {IET Digital Library},
title = {{Toward a Framework for Evaluating Extreme Programming}},
url = {http://digital-library.theiet.org/content/conferences/10.1049/ic{\_}20040394},
year = {2004}
}
@article{Groce2007a,
author = {Groce, Alex and Holzmann, Gerard and Joshi, Rajeev},
doi = {10.1109/ICSE.2007.68},
isbn = {0-7695-2828-7},
issn = {0270-5257},
journal = {Software Engineering, 29th International Conference on Software Engineering ICSE07},
month = {may},
pages = {621--631},
publisher = {Ieee},
title = {{Randomized differential testing as a prelude to formal verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4222623 http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4222623},
year = {2007}
}
@inproceedings{Sanders2012,
abstract = {Threshold concepts can be used to both organize disciplinary knowledge and explain why students have difficulties at certain points in the curriculum. Threshold concepts transform a student's view of the discipline; before being learned, they can block a student's progress. In this paper, we propose that in computing, skills, in addition to concepts, can sometimes be thresholds. Some students report finding skills more difficult than concepts. We discuss some computing skills that may be thresholds and compare threshold skills and threshold concepts. Copyright 2012 ACM.},
address = {Auckland, New Zealand},
author = {Sanders, Kate and Boustedt, Jonas and Eckerdal, Anna and Mccartney, Robert and Mostr{\"{o}}m, Jan Erik and Thomas, Lynda and Zander, Carol},
booktitle = {ICER'12 - Proceedings of the 9th Annual International Conference on International Computing Education Research},
doi = {10.1145/2361276.2361283},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanders et al. - 2012 - Threshold concepts and threshold skills in computing.pdf:pdf},
isbn = {9781450316040},
keywords = {CS2,Threshold concepts,Threshold skills},
mendeley-tags = {CS2},
pages = {23--30},
publisher = {ACM Press},
title = {{Threshold concepts and threshold skills in computing}},
url = {http://dl.acm.org/citation.cfm?doid=2361276.2361283},
year = {2012}
}
@inproceedings{190215,
abstract = {The following topics were dealt with: image processing; computer graphics; image reconstruction; facial recognition; signature recognition; security surveillance; vehicle identification; movement control; biometric measurements},
booktitle = {Electronic Images and Image Processing in Security and Forensic Science, IEE Colloquium on},
keywords = {access control,biometric measurements,computer g},
month = {may},
title = {{IEE Colloquium on `Electronic Images and Image Processing in Security and Forensic Science' (Digest No.087)}},
year = {1990}
}
@inproceedings{4529375,
abstract = {A security policy constitutes one of the major actors in the protection of communication networks. For this, and in order to manage the access grants in accordance with the security constraints, a security policy has to be validated before its deployment. Unfortunately, in the literature, there is no well established validation mechanisms ensuring the well founded of such security policies. This paper proposes a validation framework for security policies where: (1) executable specifications are used to build an 'Executable Security Policy', (2) a validation model is proposed to support the validation activity, and (3) a validation of the executable security policy is performed. The main contributions provided by this paper concerns the adaptation of some concepts and mechanisms traditionally used in software engineering for validation aims, such as specification, executable specification or reachability graph. All the definitions made in this paper have been proposed in accordance with the firewall case.},
author = {Abassi, R and {El Fatmi}, S G},
booktitle = {Availability, Reliability and Security, 2008. ARES 08. Third International Conference on},
doi = {10.1109/ARES.2008.124},
keywords = {agile,communication networks;executable specification;fi,nsf},
mendeley-tags = {agile,nsf},
month = {mar},
pages = {467--472},
title = {{A Model for Specification and Validation of Security Policies in Communication Networks: The Firewall Case}},
year = {2008}
}
@article{McCauley2015,
abstract = {Hundreds of articles have been published on the topics of teaching and learning recursion, yet fewer than 50 of them have published research results. This article surveys the computing education research literature and presents findings on challenges students encounter in learning recursion, mental models students develop as they learn recursion, and best practices in introducing recursion. Effective strategies for introducing the topic include using different contexts such as recurrence relations, programming examples, fractal images, and a description of how recursive methods are processed using a call stack. Several studies compared the efficacy of introducing iteration before recursion and vice versa. The paper concludes with suggestions for future research into how students learn and understand recursion, including a look at the possible impact of instructor attitude and newer pedagogies.},
author = {McCauley, Ren{\'{e}}e and Grissom, Scott and Fitzgerald, Sue and Murphy, Laurie},
doi = {10.1080/08993408.2015.1033205},
issn = {17445175},
journal = {Computer Science Education},
keywords = {learning,mental models,pedagogy,programming,recursion,research,student misconceptions,teaching},
month = {jan},
number = {1},
pages = {37--66},
publisher = {Routledge},
title = {{Teaching and learning recursive programming: a review of the research literature}},
url = {https://www.tandfonline.com/doi/abs/10.1080/08993408.2015.1033205},
volume = {25},
year = {2015}
}
@inproceedings{Lutz2000,
address = {New York, New York, USA},
author = {Lutz, Robyn R.},
booktitle = {Proceedings of the conference on The future of Software engineering - ICSE '00},
keywords = {Safety,future directions,software engineering,software safety},
mendeley-tags = {Safety},
month = {may},
pages = {213--226},
publisher = {ACM Press},
title = {{Software engineering for safety}},
url = {http://dl.acm.org/citation.cfm?id=336512.336556},
year = {2000}
}
@inproceedings{4804450,
abstract = {Protecting our nation's cyber infrastructure and securing sensitive information are critical challenges for homeland security and require the research, development and deployment of new technologies that can be transitioned into the field for combating cyber security risks. Particular areas of concern are the deliberate and intended actions associated with malicious exploitation, theft or destruction of data, or the compromise of networks, communications or other IT resources, of which the most harmful and difficult to detect threats are those propagated by an insider. However, current efforts to identify unauthorized access to information, such as what is found in document control and management systems, are limited in scope and capabilities. In order to address this issue, this effort involves performing further research and development on the existing graph-based anomaly detection (GBAD) system. GBAD discovers anomalous instances of structural patterns in data that represent entities, relationships and actions. Input to GBAD is a labeled graph in which entities are represented by labeled vertices and relationships or actions are represented by labeled edges between entities. Using the minimum description length (MDL) principle to identify the normative pattern that minimizes the number of bits needed to describe the input graph after being compressed by the pattern, GBAD implements algorithms for identifying the three possible changes to a graph: modifications, insertions and deletions. Each algorithm discovers those substructures that match the closest to the normative pattern without matching exactly.},
author = {Eberle, W and Holder, L},
booktitle = {Conference For Homeland Security, 2009. CATCH '09. Cybersecurity Applications Technology},
doi = {10.1109/CATCH.2009.7},
keywords = {insider threat detection;labeled graph-based anoma},
month = {mar},
pages = {237--241},
title = {{Insider Threat Detection Using Graph-Based Approaches}},
year = {2009}
}
@misc{Baumeister2004a,
abstract = {In the context of test driven development, tests specify the behavior of a program before the code that implements it, is actually written. In addition, they are used as main source of documentation in XP projects, together with the program code. However, tests alone describe the properties of a program only in terms of examples and thus are not sufficient to completely describe the behavior of a program. In contrast, formal specifications allow to generalize these example properties to more general properties, which leads to a more complete description of the behavior of a program. Specifications add another main artifact to XP in addition to the already existent ones, i.e. code and tests. The interaction between these three artifacts further improves the quality of both software and documentation. The goal of this paper is to show that it is possible, with appropriate tool support, to combine formal specifications with test driven development without loosing the agility of test driven development. {\^{A}}{\textcopyright} Springer-Verlag 2004.},
annote = {Combining formal specifications with test driven development},
author = {Baumeister, H},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1--12},
title = {{Combining formal specifications with test driven development}},
url = {citeulike-article-id:3934550 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35048883508{\&}{\#}38 partnerID=40},
volume = {3134},
year = {2004}
}
@article{duncan1989boundary,
author = {Duncan, John and Others},
journal = {Perception},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {4},
pages = {457--469},
title = {{Boundary conditions on parallel processing in human vision}},
volume = {18},
year = {1989}
}
@inproceedings{Conti2005,
address = {New York, New York, USA},
author = {Conti, Gregory and Ahamad, Mustaque and Stasko, John},
booktitle = {Proceedings of the 2005 symposium on Usable privacy and security - SOUPS '05},
doi = {10.1145/1073001.1073010},
isbn = {1595931783},
keywords = {denial of information,information visualization,malicious visualizations,secure visualization,usability attacks},
month = {jul},
pages = {89--100},
publisher = {ACM Press},
title = {{Attacking information visualization system usability overloading and deceiving the human}},
url = {http://dl.acm.org/citation.cfm?id=1073001.1073010},
year = {2005}
}
@inproceedings{Moser2008,
address = {Leipzig, Germany},
author = {Moser, Raimund and Pedrycz, Witold and Succi, Giancarlo},
booktitle = {Proceedings of the 13th international conference on Software engineering - ICSE '08},
doi = {10.1145/1368088.1368114},
isbn = {9781605580791},
pages = {181--190},
publisher = {ACM Press},
title = {{A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction}},
url = {http://portal.acm.org/citation.cfm?doid=1368088.1368114},
year = {2008}
}
@article{Andersson2007,
author = {Andersson, Carina and Runeson, Per},
doi = {10.1109/TSE.2007.1005},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andersson, Runeson - 2007 - A Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Computer languages,Conducting materials,Empirical research,Helium,Pareto optimisation,Pareto principle,Quality management,Software engineering,Software systems,System testing,Telecommunication switching,complex software system,replication,software fault distribution,software fault distributions.,software fault tolerance},
language = {English},
month = {may},
number = {5},
pages = {273--286},
publisher = {IEEE},
title = {{A Replicated Quantitative Analysis of Fault Distributions in Complex Software Systems}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4160967},
volume = {33},
year = {2007}
}
@misc{Watt2004,
abstract = {The experience of XP planning for many is not a successful one. We have found that by making acceptance tests not only central to the definition of a story but central to our process itself, they can be used to drive the entire development. This paper describes an adaptation, or evolution to XP style planning based around acceptance testing which takes the existing planning practices (with some additions) and organises them in a way that we believe can lead to better planning and more predictable results. {\^{A}}{\textcopyright} Springer-Verlag 2004.},
annote = {Acceptance test driven planning},
author = {Watt, R J and Leigh-Fellows, D},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {43--49},
title = {{Acceptance test driven planning}},
url = {citeulike-article-id:3934832 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35048841700{\&}{\#}38 partnerID=40},
volume = {3134},
year = {2004}
}
@inproceedings{Shimozawa2005,
abstract = {While the use of test-driven development as a debugging, pedagogic, and analytical methodology for object-oriented and procedural systems is well documented, it is a relatively unexplored and informal practice within the paradigm of source transformation. This paper describes a test-driven approach to the specification and evaluation of source transformation programs through rule-by-rule and type-by-type unit testing. We introduce the Transformation Engineering Toolkit for Eclipse (TETE), a test-driven framework centered around a simple yet flexible infrastructure for automatically and non-invasively unit testing subtransformations, application strategies, and grammar types specified in the TXL source transformation language. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {TETE: A non-invasive unit testing framework for source transformation},
author = {Shimozawa, D M and Cordy, J R},
booktitle = {Proceedings - 13th IEEE International Workshop on Software Technology and Engineering Practice, STEP 2005},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {137--146},
title = {{TETE: A non-invasive unit testing framework for source transformation}},
url = {citeulike-article-id:3934792 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33947695163{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@inproceedings{Sandusky1995,
address = {Sanibel Island, FL},
author = {Sandusky, Robert J. and Gasser, Les},
booktitle = {Proceedings of the 2005 international ACM SIGGROUP conference on Supporting group work - GROUP '05},
doi = {10.1145/1099203.1099238},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sandusky, Gasser - 1995 - Negotiation and the coordination of information and activity in distributed software problem management.pdf:pdf},
isbn = {1595932232},
keywords = {collaboration,coordination,coordination theory,distributed collective practices,mechanisms,negotiation,software problem management},
mendeley-tags = {collaboration},
pages = {187--196},
publisher = {ACM Press},
title = {{Negotiation and the coordination of information and activity in distributed software problem management}},
url = {http://portal.acm.org/citation.cfm?doid=1099203.1099238},
year = {1995}
}
@inproceedings{4072167,
abstract = {The detection of counterfeit in printed documents is currently based mainly on built-in security features or on human expertise. We propose a classification system that supports non-expert users to distinguish original documents from PC-made forgeries by analyzing the printing technique used. Each letter in a document is classified using a support vector machine that has been trained to distinguish laser from inkjet printouts. A color-coded visualization helps the user to interpret the per-letter classification results},
author = {Lampert, Christoph H and Mei, Lin and Breuel, Thomas M},
booktitle = {Computational Intelligence and Security, 2006 International Conference on},
doi = {10.1109/ICCIAS.2006.294214},
keywords = {PC-made forgeries,color-coded visualization,docume},
pages = {639--644},
title = {{Printing Technique Classification for Document Counterfeit Detection}},
volume = {1},
year = {2006}
}
@inproceedings{Jalbert2008,
address = {Anchorage, Alaska},
author = {Jalbert, Nicholas and Weimer, Westley},
booktitle = {2008 IEEE International Conference on Dependable Systems and Networks (DSN '08)},
doi = {10.1109/DSN.2008.4630070},
isbn = {978-1-4244-2397-2},
keywords = {Computer bugs,Costs,Filtering,Open source software,Operating systems,Software maintenance,Software quality,Software systems,Software tools,Spatial databases,automated duplicate bug report detection,graph clustering,graph theory,pattern classification,pattern clustering,program debugging,software bug tracking system,software development maintenance activity,software maintenance,software tools,surface feature,textual semantics,tracking},
language = {English},
pages = {52--61},
title = {{Automated duplicate detection for bug tracking systems}},
year = {2008}
}
@article{Pirolli1999,
author = {Pirolli, Peter and Card, Stuart},
journal = {Psychological Review},
number = {4},
pages = {643--675},
title = {{Information Foraging}},
volume = {106},
year = {1999}
}
@article{browning2013,
author = {Browning, Christopher R and Jackson, Aubrey L},
journal = {Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {4},
pages = {1009--1043},
publisher = {Wiley Online Library},
title = {{The social ecology of public space: Active streets and violent crime in urban neighborhoods}},
volume = {51},
year = {2013}
}
@inproceedings{Poole2005,
abstract = {Learn techniques for Test-Driven Development of user interfaces. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {Test-driven user interfaces},
author = {Poole, C},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {285--286},
title = {{Test-driven user interfaces}},
url = {citeulike-article-id:3934756 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444498633{\&}{\#}38 partnerID=40},
volume = {3556},
year = {2005}
}
@article{Spear2012,
abstract = {The common trace format overcomes deficiencies in traditional software tools to optimize modern multicore designs by providing open source formats to monitor the state and interaction of concurrent systems over time.},
author = {Spear, Aaron and Levy, Markus and Desnoyers, Mathieu},
doi = {10.1109/MC.2012.191},
issn = {0018-9162},
journal = {Computer},
month = {dec},
number = {12},
pages = {60--64},
title = {{Using Tracing to Solve the Multicore System Debug Problem}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6200244},
volume = {45},
year = {2012}
}
@article{Basili2008,
abstract = {Computational scientists developing software for HPC systems face unique software engineering issues. Attempts to transfer SE technologies to this domain must take these issues into account.},
author = {Basili, Victor R and Carver, J and Cruzes, Daniela and Hochstein, Lorin and Hollingsworth, J K and Shull, Forrest and Zelkowitz, Marvin V},
journal = {IEEE Software},
month = {jul},
number = {4},
pages = {29--36},
title = {{Understanding the High Performance Computing Community: A Software Engineer's Perspective}},
url = {http://www.cs.umd.edu/{~}basili/publications/journals/J111.pdf},
volume = {25},
year = {2008}
}
@article{Avakame1998,
author = {Avakame, Edem F.},
journal = {Violence and Victims},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {3},
pages = {301--316},
publisher = {Springer Publishing Company},
title = {{Intergenerational Transmission of Violence, Self-Control, and Conjugal Violence: A Comparative Analysis of Physical Violence and Psychological Aggression}},
url = {http://umd.library.ingentaconnect.com/content/springer/vav/1998/00000013/00000003/art00007},
volume = {13},
year = {1998}
}
@inproceedings{Cordy2006,
abstract = {The TXL transformation framework has been widely used in practical source transformation tasks in industry and academia for many years. At the core of the framework is the TXL language, a functional programming language specifically designed for expressing source transformation tasks. TXL programs are self-contained, specifying and implementing all aspects of parsing, pattern matching, transformation rules, application strategies and unparsing in a single uniform notation with no dependence on other tools or technologies. Programs are directly interpreted by the TXL processor without any compile or build step, making it particularly well suited to rapid turnaround, test-driven development. In this paper we provide a practical introduction to using TXL in rapidly developing source transformations from concrete examples, and review experience in applying TXL to a number of practical large scale applications in source code analysis, renovation and migration. Copyright {\^{A}}{\textcopyright} 2006 ACM.},
annote = {Source transformation, analysis and generation in TXL},
author = {Cordy, J R},
booktitle = {Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1--11},
title = {{Source transformation, analysis and generation in TXL}},
url = {citeulike-article-id:3934577 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34250686713{\&}{\#}38 partnerID=40},
year = {2006}
}
@inproceedings{6027534,
abstract = {The estimation of security risks in complex information and communication technology systems is an essential part of risk management processes. A proper computation of risks requires a good knowledge about the probability distributions of different upcoming events or behaviours. Usually, technical risk assessment in Information Technology (IT) systems is concerned with threats to specific assets. However, for many scenarios it can be useful to consider the risk of the violation of particular security properties. The set of suitable qualities comprises authenticity of messages or non-repudiability of actions within the system but also more general security properties like confidentiality of data. Furthermore, as current automatic security analysis tools are mostly confined to a technical point of view and thereby missing implications on an application or process level, it is of value to facilitate a broader view including the relation between actions within the IT system and their external influence. The property based approach aims to help assessing risks in a process-oriented or service level view of a system and also to derive a more detailed estimation on a technical level. Moreover, as systems' complexities are growing, it becomes less feasible to calculate the probability of all patterns of a system's behaviour. Thus, a model based simulation of the system is advantageous in combination with a focus on precisely defined security properties. This paper introduces the first results supporting a simulation based risk analysis tool that enables a security property oriented view of risk. The developed tool is based on an existing formal validation, verification and simulation tool, the Simple Homomorphism Verification Tool (SHVT). The new simulation software provides a graphical interface for a monitor automaton which facilitates the explicit definition of security properties to be investigated during the simulation cycles. Furthermore, in order to model different- - likelihoods of actions in a system, weighting factors can be used to sway the behaviour where the occurrence of events is not evenly distributed. These factors provide a scheme for weighting classes of transitions. Therefore, the tool facilitates probabilistic simulation, providing information about the probability distribution of satisfaction or violation of specified properties.},
author = {Winkelvos, T and Rudolph, C and Repp, J},
booktitle = {Information Security South Africa (ISSA), 2011},
doi = {10.1109/ISSA.2011.6027534},
keywords = {data confidentiality property,graphical interface},
pages = {1--8},
title = {{A property based security risk analysis through weighted simulation}},
year = {2011}
}
@inproceedings{VonMayrhauser1997,
address = {Alexandria, VA},
author = {von Mayrhauser, A and Vans, A M},
booktitle = {Seventh Workshop on Empirical Studies of Programmers},
keywords = {debugging,program comprehension},
mendeley-tags = {debugging,program comprehension},
pages = {157--179},
title = {{Program understanding behavior during debugging of large scale software}},
year = {1997}
}
@inproceedings{5168026,
abstract = {Analysis of voluminous computer network data has become a common practice for cyber defense, but few tools provide adequate support for cyber-infrastructure defenders' workflow, visual exploration, IP geo-location, scalability, collaboration, or reporting. The state-of-the-art in visual analysis tools for cyber defense is typically no more than spreadsheets and primitive charting. While familiar to users, this approach ignores the human perceptual ability to identify novel patterns and anomalies when data is presented graphically. This paper reports on a visual analytics systems, VIAssist, being developed for cyber-infrastructure protection that helps cyber defenders better understand the massive, multi-dimensional datasets to protect our nation's critical infrastructure.},
author = {Goodall, J R and Sowul, M},
booktitle = {Technologies for Homeland Security, 2009. HST '09. IEEE Conference on},
doi = {10.1109/THS.2009.5168026},
keywords = {IP geo-location,cyber defense,cyber-infrastructure},
month = {may},
pages = {143--150},
title = {{VIAssist: Visual analytics for cyber defense}},
year = {2009}
}
@misc{Ambler2007a,
author = {Ambler, Scott},
number = {7},
title = {{Survey Says...Agile Has Crossed the Chasm}},
volume = {32},
year = {2007}
}
@inproceedings{Yoo2004a,
address = {New York, New York, USA},
author = {Yoo, InSeon},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029222},
isbn = {1581139748},
keywords = {self-organizing maps,visualization,windows executable viruses},
month = {oct},
pages = {82},
publisher = {ACM Press},
title = {{Visualizing windows executable viruses using self-organizing maps}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029222},
year = {2004}
}
@book{Shaw1942,
address = {Chicago, IL},
author = {Shaw, Clifford and McKay, Henry},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {University of Chicago Press},
title = {{Juvenile Delinquency and Urban Areas}},
year = {1942}
}
@article{Go2005,
abstract = {Soft release in short term can be expected to be achieved by applying the test-driven development technique to communication software. However, it is necessary to extract test items in the situation before coding programs to be tested. The test items are extracted by three steps; covering extraction of input system, forecast of output, and generation of test program. Especially, the first step is difficult. To solve this problem, this paper proposes an algorithm that pulls out parameters from specifications and extracts the input system by paying attention within range where parameters can be taken. Applying this algorithm to the development of reverse proxy software, which has plural classes, 15 test items were extracted from class I and 37 test items were extracted from class II. (author abst.)},
annote = {Application Model and Evaluation of Test-Driven Development to Communication Software},
author = {Go, H M O and Shiraga, A and Abe, T and Tanaka, Y},
journal = {IEIC Technical Report (Institute of Electronics, Information and Communication Engineers)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {689},
pages = {201--204},
title = {{Application Model and Evaluation of Test-Driven Development to Communication Software}},
url = {citeulike-article-id:3934625 http://sciencelinks.jp/j-east/article/200513/000020051305A0395929.php},
volume = {104},
year = {2005}
}
@inproceedings{6120855,
abstract = {With tremendous attacks in the Internet, there is a high demand for network analysts to know about the situations of network security effectively. Traditional network security tools lack the capability of analyzing and assessing network security situations comprehensively. In this paper, we introduce a novel network situation awareness tool CNSSA (Comprehensive Network Security Situation Awareness) to perceive network security situations comprehensively. Based on the fusion of network information, CNSSA makes a quantitative assessment on the situations of network security. It visualizes the situations of network security in its multiple and various views, so that network analysts can know about the situations of network security easily and comprehensively. The case studies demonstrate how CNSSA can be deployed into a real network and how CNSSA can effectively comprehend the situation changes of network security in real time.},
author = {Xi, Rongrong and Jin, Shuyuan and Yun, Xiaochun and Zhang, Yongzheng},
booktitle = {Trust, Security and Privacy in Computing and Communications (TrustCom), 2011 IEEE 10th International Conference on},
doi = {10.1109/TrustCom.2011.62},
keywords = {CNSSA,Internet,comprehensive network security situ},
pages = {482--487},
title = {{CNSSA: A Comprehensive Network Security Situation Awareness System}},
year = {2011}
}
@article{Basili1988,
abstract = {Experience from a dozen years of analyzing software engineering processes and products is summarized as a set of software engineering and measurement principles that argue for software engineering process models that integrate sound planning and analysis into the construction process. In the TAME (Tailoring A Measurement Environment) project at the University of Maryland, such an improvement-oriented software engineering process model was developed that uses the goal/question/metric paradigm to integrate the constructive and analytic aspects of software development. The model provides a mechanism for formalizing the characterization and planning tasks, controlling and improving projects based on quantitative analysis, learning in a deeper and more systematic way about the software process and product, and feeding the appropriate experience back into the current and future projects. The TAME system is an instantiation of the TAME software engineering process model as an ISEE (integrated software engineering environment). The first in a series of TAME system prototypes has been developed. An assessment of experience with this first limited prototype is presented including a reassessment of its initial architecture},
author = {Basili, V.R. and Rombach, H.D.},
doi = {10.1109/32.6156},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
month = {jun},
number = {6},
pages = {758--773},
title = {{The TAME project: towards improvement-oriented software environments}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6156},
volume = {14},
year = {1988}
}
@inproceedings{Hoffman2004,
author = {Hoffman, A.{\~{}}R. and Green, N.{\~{}}H. and Garrett, H.{\~{}}B.},
booktitle = {Environmental Testing for Space Programmes},
editor = {Fletcher, K},
month = {aug},
pages = {43--50},
series = {ESA Special Publication},
title = {{Assessment of In-flight Anomalies of Long Life Outer Planet Missions}},
volume = {558},
year = {2004}
}
@inproceedings{Reese1997,
abstract = {Validation of software requirements is an important part of software engineering. This paper describes a new safety analysis technique called software deviation analysis to help identify weaknesses in how software handles an imperfect environment. The technique propagates deviations in software inputs to output deviations. A qualitative analysis is used to improve the search efficiency.},
address = {Boston, MA},
author = {Reese, Jon D and Leveson, Nancy G},
booktitle = {19th International Conference on Software Engineering (ICSE'97)},
doi = {10.1109/ICSE.1997.610264},
month = {may},
pages = {250--260},
title = {{Software Deviation Analysis}},
url = {http://www.computer.org/csdl/proceedings/icse/1997/2162/00/21620250-abs.html},
year = {1997}
}
@article{Cubranic2005,
abstract = {Abstract—Sociological and technical difficulties, such as a lack of informal encounters, can make it difficult for new members of noncollocated software development teams to learn from their more experienced colleagues. To address this situation, we have developed a tool, named Hipikat, that provides developers with efficient and effective access to the group memory for a software development project that is implicitly formed by all of the artifacts produced during the development. This project memory is built automatically with little or no change to existing work practices. After describing the Hipikat tool, we present two studies investigating Hipikat's usefulness in software modification tasks. One study evaluated the usefulness of Hipikat's recommendations on a sample of 20 modification tasks performed on the Eclipse Java IDE during the development of release 2.1 of the Eclipse software. We describe the study, present quantitative measures of Hipikat's performance, and describe in detail three cases that illustrate a range of issues that we have identified in the results. In the other study, we evaluated whether software developers who are new to a project can benefit from the artifacts that Hipikat recommends from the project memory. We describe the study, present qualitative observations, and suggest implications of using project memory as a learning aid for project newcomers.},
author = {{\v{C}}ubrani{\'{c}}, Davor and Murphy, Gail C and Singer, Janice and Booth, Kellogg S},
number = {5},
pages = {446--465},
title = {{Hipikat: A Project Memory for Software Devleopment}},
volume = {31},
year = {2005}
}
@article{Standish1994,
author = {Standish},
title = {{The Chaos Report}},
year = {1994}
}
@article{rombach11,
author = {Rombach, D},
journal = {Int. J. Software Informatics},
number = {3},
title = {{Empirical Software Engineering Models: Can They Become the Equivalent of Physical Laws in Traditional Engineering?}},
volume = {5},
year = {2011}
}
@inproceedings{VanDeursen2001,
address = {Stuttgart, Germany},
author = {van Deursen, Arie},
booktitle = {Proceedings of the 8th Working Conference on Software Maintenance and Reengineering (WCRE 2001)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van Deursen - 2001 - Program comprehension risks and opportunities in extreme programming.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
number = {R 0110},
pages = {1--10},
publisher = {IEEE Computer Society},
title = {{Program comprehension risks and opportunities in extreme programming}},
url = {http://www.narcis.info/publication/RecordID/oai:cwi.nl:4343},
year = {2001}
}
@inproceedings{Neuhaus2010,
address = {San Jose, CA},
author = {Neuhaus, Stephan and Zimmermann, Thomas},
booktitle = {2010 IEEE 21st International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.2010.53},
isbn = {978-1-4244-9056-1},
issn = {1071-9458},
keywords = {CVE topic model,Common Vulnerability and Exposures database,Databases,Forgery,Manuals,Mathematical model,NIST,PHP,Resource management,SQL,SQL injection,Security,XSS,application servers,buffer overflow,buffer storage,cross-site request forgery,data analysis,data mining,description text,format strings,hypermedia markup languages,learning (artificial intelligence),machine learning,security,security of data,security trend analysis,trends,vulnerability report,vulnerability type},
language = {English},
month = {nov},
pages = {111--120},
publisher = {IEEE},
title = {{Security Trend Analysis with CVE Topic Models}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=5635130},
year = {2010}
}
@article{Hummel2008,
abstract = {Accelerating the software development process by assembling new applications from existing software assets has been a goal of the IT industry for many years. However, most of today's systematic software reuse uses heavyweight approaches such as product-line engineering. Now, with the explosion in open source software repositories and the advent of a new generation of powerful software search engines, this is set to change. Code Conjurer is an Eclipse plug-in that extracts interface and test information from a developer's coding activities and uses this information to issue test-driven searches to a code-search engine. It presents components matching the developer's needs as reuse recommendations without disturbing the development work. Automated dependency resolution then allows selected components to be woven into the current project with minimal manual effort. {\^{A}}{\textcopyright} 2008 IEEE.},
annote = {Code conjurer: Pulling reusable software out of thin air},
author = {Hummel, O and Janjic, W and Atkinson, C},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {5},
pages = {45--52},
title = {{Code conjurer: Pulling reusable software out of thin air}},
url = {citeulike-article-id:3934650 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-50549088139{\&}{\#}38 partnerID=40},
volume = {25},
year = {2008}
}
@inproceedings{McPherson2004d,
address = {New York, New York, USA},
author = {McPherson, Jonathan and Ma, Kwan-Liu and Krystosk, Paul and Bartoletti, Tony and Christensen, Marvin},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029220},
isbn = {1581139748},
keywords = {information visualization,network security,user interfaces},
month = {oct},
pages = {73},
publisher = {ACM Press},
title = {{PortVis}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029220},
year = {2004}
}
@article{Underseth2007,
abstract = {The growing complexity of embedded software development requires a new unified, test-driven approach that can be adopted by early developer testing practices and implements automated software verification to prevent and detect more defects. The objective is to have all developers and integrators create tests that can be reused and automated throughout the development cycle. This strategy maximizes automated testing to produce a more effective, scalable software process that can handle the increasing complexities of embedded software efforts. Automation is the key to achieving these major objectives and the strategy is to unify all software developers and integrators in delivering reusable, automated tests for a common test framework. An embedded software verification platform also provides value at this stage by enabling developers to validate their tests before the availability of code.},
annote = {A test-driven approach to developing embedded software},
author = {Underseth, M},
journal = {EE: Evaluation Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {4},
pages = {44--50},
title = {{A test-driven approach to developing embedded software}},
url = {citeulike-article-id:3934823 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248340575{\&}{\#}38 partnerID=40},
volume = {46},
year = {2007}
}
@inproceedings{Banerjee1995,
address = {Seville, Spain},
author = {Banerjee, N.},
booktitle = {Proceedings of Conference on Software Quality Management},
pages = {219--230},
title = {{Utilization of FMEA concept in software lifecycle management}},
url = {http://library.witpress.com/pages/PaperInfo.asp?PaperID=10560},
volume = {11},
year = {1995}
}
@inproceedings{Grantner2005,
abstract = {The key concepts to develop a verification methodology for teaching and research in the digital system design area, are discussed. In the graduate course Advanced Microprocessor Intefacing (ECE 605) students also work with the same set of development tools while taking on more challenging projects that are comparable to similar ones carried out in industry. The given methodology of creating test bench emphasize the reuse of module and tests. first the architecture is given, then it is followed by the detailed description of each module.},
annote = {Development of a test bench for VHDL projects},
author = {Grantner, J L and Tamayo, P A and Gottipati, R and Florida, D},
booktitle = {ASEE Annual Conference and Exposition, Conference Proceedings},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {4397--4407},
title = {{Development of a test bench for VHDL projects}},
url = {citeulike-article-id:3934627 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-22544480818{\&}{\#}38 partnerID=40},
year = {2005}
}
@inproceedings{4126211,
abstract = {This paper presents various methods for visualization and analysis of email networks; visualization on the surface of a sphere to reveal communication patterns between different groups, a hierarchical drawing displaying the centrality analysis of nodes to emphasize important nodes, a 2.5D visualization for temporal email networks to analyze the evolution of email relationships changing over time, and an ambient display for finding social circles derived from the email network. Each method was evaluated with various data sets from a research organization. We also extended our method for visual analysis of an email virus network.},
author = {Fu, X and Hong, S.-H. and Nikolov, N S and Shen, X and Wu, Y and Xu, K},
booktitle = {Visualization, 2007. APVIS '07. 2007 6th International Asia-Pacific Symposium on},
doi = {10.1109/APVIS.2007.329302},
keywords = {email network,self-organizing map,vi,virus network},
pages = {1--8},
title = {{Visualization and analysis of email networks}},
year = {2007}
}
@article{Begel2008,
author = {Begel, Andrew and Simon, Beth},
doi = {10.1145/1352322.1352218},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Begel, Simon - 2008 - Struggles of new college graduates in their first software development job.pdf:pdf},
isbn = {978-1-59593-799-5},
issn = {00978418},
journal = {ACM SIGCSE Bulletin},
keywords = {computer science education,human aspects of software engineering,software development},
month = {feb},
number = {1},
pages = {226--230},
publisher = {ACM},
title = {{Struggles of new college graduates in their first software development job}},
url = {http://dl.acm.org/citation.cfm?id=1352322.1352218},
volume = {40},
year = {2008}
}
@article{Brooks1980,
annote = {{\textless}m:note{\textgreater}"The selection of subjects for a cognitive engineering experiment must meet two criteria: 1) the subjects must be representative of the entire population and 2) the subjects must have similar experiences and abilities.  These are contradictory though because the population of programmers is very heterogenuous.  Even students in programming classes show differences in ability.  There is no evidence to show that student findings can be generalized to expert findings.  {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}One way to remove variability is to subject all the subjects to each type of factor being analyzed (within-subject designs). {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}When choosing test matierals, they must actually TEST the construct understudy.  They must be representative of the problem (i.e. can scale up to a "real" problem if you are studying "real" problems).  {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}**Could possibly characterize the materials according to program/design complexity.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}**How to measure if a technique is better: does it reduce effort?  Comprehension?{\textless}/m:note{\textgreater}},
author = {Brooks, R E},
keywords = {experiment design,psychology},
number = {4},
pages = {207--213},
title = {{Studying Programmer Behavior Experimentally: The Problems of Proper Methodology}},
volume = {23},
year = {1980}
}
@inproceedings{Bettenburg2008,
address = {Leipzig, Germany},
author = {Bettenburg, Nicolas and Premraj, Rahul and Zimmermann, Thomas and Kim, Sunghun},
booktitle = {Proc. of the 2008 Int'l Wkshp. on Mining Software Repositories (MSR '08)},
doi = {10.1145/1370750.1370757},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bettenburg et al. - 2008 - Extracting structural information from bug reports.pdf:pdf},
isbn = {9781605580241},
keywords = {bug reports,patches,source code,stack traces,structural information},
month = {may},
pages = {27--30},
title = {{Extracting structural information from bug reports}},
year = {2008}
}
@article{Mischel1989,
author = {Mischel, Walter and Shoda, Yuichi and Rodriguez, Monica L.},
journal = {Science},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {4907},
pages = {933--938},
title = {{Delay of Gratification in Children}},
volume = {244},
year = {1989}
}
@inproceedings{Li2007,
abstract = {The k-anonymity privacy requirement for publishing microdata requires that each equivalence class (i.e., a set of records that are indistinguishable from each other with respect to certain "identifying" attributes) contains at least k records. Recently, several authors have recognized that k-anonymity cannot prevent attribute disclosure. The notion of l-diversity has been proposed to address this; l-diversity requires that each equivalence class has at least l well-represented values for each sensitive attribute. In this paper we show that l-diversity has a number of limitations. In particular, it is neither necessary nor sufficient to prevent attribute disclosure. We propose a novel privacy notion called t-closeness, which requires that the distribution of a sensitive attribute in any equivalence class is close to the distribution of the attribute in the overall table (i.e., the distance between the two distributions should be no more than a threshold t). We choose to use the earth mover distance measure for our t-closeness requirement. We discuss the rationale for t-closeness and illustrate its advantages through examples and experiments.},
author = {Li, Ninghui and Li, Tiancheng and Venkatasubramanian, Suresh},
booktitle = {23rd IEEE International Conference on Data Engineering},
doi = {10.1109/ICDE.2007.367856},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Li, Venkatasubramanian - 2007 - t-Closeness Privacy Beyond k-Anonymity and l-Diversity.pdf:pdf},
isbn = {1-4244-0802-4},
keywords = {Computer science,Data security,Databases,Diseases,Earth,Motion measurement,Privacy,Protection,Publishing,Remuneration,agile,attribute disclosure,data privacy,database theory,earth mover distance measure,equivalence class,identifying attributes,k-anonymity privacy requirement,l-diversity,microdata publishing,nsf,t-closeness},
mendeley-tags = {agile,nsf},
pages = {106--115},
publisher = {IEEE},
shorttitle = {Data Engineering, 2007. ICDE 2007. IEEE 23rd Inter},
title = {{t-Closeness: Privacy Beyond k-Anonymity and l-Diversity}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4221659},
year = {2007}
}
@article{Fullilove1990,
author = {Fullilove, R E and Treisman, P U},
number = {3},
pages = {463--478},
title = {{Mathematics achievement among African American undergraduates of the University of California, Berkeley: An evaluation of the Mathematics Workshop Program}},
volume = {59},
year = {1990}
}
@inproceedings{Zhou2012,
address = {Zurich, Switzerland},
author = {Zhou, Jian and Zhang, Hongyu and Lo, David},
booktitle = {2012 34th International Conference on Software Engineering (ICSE)},
doi = {10.1109/ICSE.2012.6227210},
isbn = {978-1-4673-1067-3},
issn = {0270-5257},
keywords = {BugLocator,Computational modeling,Computer bugs,Equations,Indexing,Information retrieval,Mathematical model,Vectors,bug localization,bug reports,feature location,information retrieval,information retrieval based method,information retrieval-based bug localization,open source projects,program debugging,public domain software,rVSM,revised vector space model,software quality,software system,source code files,textual similarity},
language = {English},
month = {jun},
pages = {14--24},
publisher = {IEEE},
title = {{Where should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6227210},
year = {2012}
}
@article{Xu2003b,
author = {Xu, Jennifer and Chen, Hsinchun},
isbn = {3-540-40189-X},
month = {jun},
pages = {232--248},
title = {{Untangling criminal networks: a case study}},
url = {http://dl.acm.org/citation.cfm?id=1792094.1792117},
year = {2003}
}
@inproceedings{Robillard2003,
address = {Montreal, Canada},
author = {Robillard, Martin P and Murphy, Gail C},
pages = {225--234},
title = {{Automatically Inferring Concern Code from Program Investigation Activities}},
year = {2003}
}
@inproceedings{Speier1997,
address = {Atlanta, GA},
author = {Speier, Cheri and Valacich, Joseph S and Vessey, Iris},
booktitle = {Proceedings of the 18th International Conference on Information Systems (ICIS '97)},
keywords = {hci,interruption},
pages = {21--36},
title = {{The Effects of Task Interruption and Information Presentation on Individual Decision Making}},
year = {1997}
}
@article{Machanavajjhala2007,
author = {Machanavajjhala, Ashwin and Kifer, Daniel and Gehrke, Johannes and Venkitasubramaniam, Muthuramakrishnan},
doi = {10.1145/1217299.1217302},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Machanavajjhala et al. - 2007 - L -diversity.pdf:pdf},
issn = {15564681},
journal = {ACM Transactions on Knowledge Discovery from Data},
keywords = {Data privacy,agile,k -anonymity,nsf,privacy-preserving data publishing,ℓ-diversity},
mendeley-tags = {agile,nsf},
month = {mar},
number = {1},
pages = {Article No. 3},
publisher = {ACM},
title = {{L -diversity}},
url = {http://dl.acm.org/citation.cfm?id=1217299.1217302},
volume = {1},
year = {2007}
}
@inproceedings{Elbaum2000,
address = {Portland, OR},
author = {Elbaum, Sebastian and Malishevsky, Alexey G. and Rothermel, Gregg},
booktitle = {Proceedings of the International Symposium on Software Testing and Analysis - ISSTA '00},
doi = {10.1145/347324.348910},
isbn = {1581132662},
pages = {102--112},
title = {{Prioritizing test cases for regression testing}},
url = {http://portal.acm.org/citation.cfm?doid=347324.348910},
year = {2000}
}
@inproceedings{5168084,
abstract = {To enhance support of emergency response decision making, we are investigating decision aids that use simulation models to predict the range of plausible consequences of each potential course of action. Due to the rapid pace of emergency operations, the user interface displaying the models' results needs to facilitate quick understanding of a changing landscape of possible futures. This paper describes a user test aimed at (1) determining whether a model-driven visualization was useful and (2) confirming the validity of a principled design approach for developing decision-making test situations. Two groups of participants received identical textual descriptions of situations and were asked to decide the number of emergency response vehicles to dispatch. One group also saw a visual depiction of model results. The decision-aided group's decisions were closer to the normatively correct decisions and this group's confidence was higher. Further, the decision-aided group rated the degree of decision support higher. The decision times for each of the four types of test situations differed significantly from each other, re-validating our method of developing test situations.},
author = {Drury, J L and Klein, G and Pfaff, M S and More, L D},
booktitle = {Technologies for Homeland Security, 2009. HST '09. IEEE Conference on},
doi = {10.1109/THS.2009.5168084},
keywords = {decision-making test situation,dynamic decision su},
month = {may},
pages = {537--544},
title = {{Dynamic Decision Support for Emergency}},
year = {2009}
}
@techreport{Siniaalto2006,
abstract = {This document contains an introduction to Testdriven development (TDD) and it aims at providing an extensive literature review by gathering all the empirical results related to TDD that have been published in the academic articles and experience reports in scientific forums. This work also acts as a basis for an empirical experiment that is arranged by Agile VTT during October December 2005. The results of this literature survey show indications that TDD may help to improve software quality significantly, in terms of decreased fault rates, when employed in an industrial context. The results did not show remarkable differences in the developer productivity in any of the studies included in this work.},
annote = {Test driven development: empirical body of evidence},
author = {Siniaalto, Maria},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {15},
publisher = {Information Technology for European Advancement},
title = {{Test driven development: empirical body of evidence}},
url = {citeulike-article-id:3934795 {\#}},
volume = {D.2.7},
year = {2006}
}
@inproceedings{Sherry2004,
abstract = {As avionics systems play an increasingly more pervasive role in airline cockpit operations, the ease-of-use of these systems to perform airline mission tasks, increasingly impacts the costs of pilot training, and the efficiency and safety margins of cockpit operations. Airworthiness regulations and regulatory certification processes for avionics equipment do not explicitly call for the design and verification testing of the pilot-avionics interaction. Without explicit design of the pilot-avionics interaction, avionics equipment is fielded with user-interfaces that require pilots to learn unnecessarily long sequences of memorized actions during training. These memorized action sequences then have to be recalled during line operations, even after not being used for several months. The number and complexity of the memorized action sequences directly contribute to airline training costs, and impact the efficiency and safety margins of airline cockpit operations. This paper proposes the inclusion of a Task Design Document (TDD) in the DO-178B avionics equipment certification process to explicitly design pilot-avionics interaction. The TDD specifies how the operator will interact with the automation to perform airline mission tasks. The structure and content of a Task Design Document (TDD) is described with example specifications. The implications of this proposal on the DO-178B process are also discussed. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {Task design and verification testing for certification of avionics equipment},
author = {Sherry, L and Feary, M},
booktitle = {AIAA/IEEE Digital Avionics Systems Conference - Proceedings},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Task design and verification testing for certification of avionics equipment}},
url = {citeulike-article-id:3934791 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-16244389952{\&}{\#}38 partnerID=40},
volume = {2},
year = {2004}
}
@inproceedings{6107873,
abstract = {Criterra is a software suite that automatically determines optimum locations and heights in seconds/minutes for security system sensors, and locations for infrastructure and response forces based on dominant mosaic, line-of-sight, time-and-space, Doppler, propagation and other algorithms, executed on a terabyte size 3D geospatial and object database. Inputs include specifications of sensor systems, barriers, and response forces. Criterra is based on a methodology that integrates intelligence and scientific processes to quickly and automatically determine optimum locations that are NOT trial-and error. Criterra Automatically calculates: 1) threat paths and predicted locations, 2) the optimal locations for the minimum number of minimum height sensor towers, mobile sensors, and/or unattended ground sensors, and communication repeater towers to meet detection, surveillance and reconnaissance requirements considering predicted threat locations/levels within area use constraints, and 3) placements for fences, infrastructure and response forces within constraints to accomplish protection and interception with high degrees of confidence. The results are displayed on a visually rich and geospatially accurate 3D map which the user can navigate or use for computational model modifications, redefinition of constraints, and further analysis and planning. Criterra can be used now to support land and maritime border security threat, sensor, infrastructure, and response analysis and geospatial planning; physical security analysis and geospatial planning for protection of airports, seaports, military bases and other critical infrastructure; attack and disaster preparation, recovery, and response planning; and force management and collaboration using 3D maps with results of Criterra analyses as embedded mission data. Criterra performs geospatial analysis to predict threat location at specific times, and provides area use data for CONOPs and risk analysis. Criterra is used in require- ents definitions, installations, training, operations, experimentation, and test and evaluation. Criterra can support analysis of small and extremely large areas and catastrophic events. Criterra is a predictive technique for pre-placing mobile or fixed resources; and locating and tasking first responders. It supports situational understanding, information management and data visualization on a 3D geospatially accurate laptop display. Users can use Criterra for information sharing, collaborative decision making and integrated/interoperable decision support where all users are referencing and plotting data on a shared 3D geospatial map. It can support decontamination and restoration strategies, approaches, and sensor placements following WMD events. Facilities, monuments, airports, seaports, transportation infrastructure, land and coastal borders, and ports of entry are all areas where Criterra computational models and Criterra analysis algorithms can be used to determine optimum locations. Criterra computational models and analysis algorithms are used today to plan the optimum locations for ground surveillance and communications repeaters for protection of critical infrastructure, borders and for ports. Criterra is used for planning security for people and facility assets. A Criterra database consists of two types of data: 1) 1 meter square slices of an area, (size limited only by the storage media), containing for each slice: latitude, longitude, height/altitude, terrain composition, slope, use (such as threat, allowed for sensor installation, road, asset ...), derived from GIS data (such as DEM and DTM), and object data (such as buildings, infrastructure, vehicles ...); and 2) radar, camera, UGS, radio, response force and barrier specifications. Criterra algorithms analyze the slices and add to the database for each slice, a) line-of-sight connectivity factors to all other slices, and b) traverse-ability factors to all other slices.},
author = {Cassenti, L and Leed, P E P},
booktitle = {Technologies for Homeland Security (HST), 2011 IEEE International Conference on},
doi = {10.1109/THS.2011.6107873},
keywords = {3D geospatial map,3D geospatially accurate laptop},
pages = {213--218},
title = {{Criterra automatic location planning}},
year = {2011}
}
@article{Stefik2013,
abstract = {Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax. Copyright {\textcopyright} 2013 by the Association for Computing Machinery (ACM).},
author = {Stefik, Andreas and Siebert, Susanna},
doi = {10.1145/2534973},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stefik, Siebert - 2013 - An empirical investigation into programming language syntax.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {Novice programmers,Programming languages,Syntax},
month = {nov},
number = {4},
pages = {1--40},
title = {{An empirical investigation into programming language syntax}},
url = {https://dl.acm.org/doi/10.1145/2534973},
volume = {13},
year = {2013}
}
@inproceedings{Bowyer2006,
abstract = {A number of agile practices are included in software engineering curricula, including test-driven development. Continuous integration often is not included, despite it becoming increasingly common in industry to code, test, and integrate at the same time. This paper describes a study whereby software engineering undergraduates were given a short intensive experience of test-driven development with continuous integration using an environment that imitated a typical industrial circumstance. Assessment was made of students' agile experience rather than of project deliverables, using a novel set of process measures that examined students' participation and performance in agile testing. Results showed good participation by student pairs, and clear understanding of agile processes and configuration management. Future work will investigate automation of the assessment of continuous integration and configuration management server data.},
annote = {Assessing undergraduate experience of continuous integration and test-driven development},
author = {Bowyer, J and Hughes, J},
booktitle = {Proceedings - International Conference on Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {691--694},
title = {{Assessing undergraduate experience of continuous integration and test-driven development}},
url = {citeulike-article-id:3934559 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34247157372{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@inproceedings{Gachechiladze2017,
abstract = {—Recent research has provided evidence that software developers experience a wide range of emotions. We argue that among those emotions anger deserves special attention as it can serve as an onset for tools supporting collaborative software development. This, however, requires a fine-grained model of the anger emotion, able to distinguish between anger directed towards self, others, and objects. Detecting anger towards self could be useful to support developers experiencing difficulties; detection of anger towards others might be helpful for community management; detecting anger towards objects might be helpful to recommend and prioritize improvements. As a first step towards automatic identification of anger direction, we built a classifier for anger direction, based on a manually annotated gold standard of 723 sentences that were obtained by mining comments in Apache issue reports.},
address = {Buenos Aires, Argentina},
author = {Gachechiladze, Daviti and Lanubile, Filippo and Novielli, Nicole and Serebrenik, Alexander and Nl, D Gachechiladze@alumnus Tue},
booktitle = {ICSE NIER},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gachechiladze et al. - 2017 - Anger and Its Direction in Collaborative Software Development.pdf:pdf},
keywords = {sentiment},
mendeley-tags = {sentiment},
title = {{Anger and Its Direction in Collaborative Software Development}},
url = {http://www.win.tue.nl/{~}aserebre/ICSE2017NIER.pdf},
year = {2017}
}
@article{Abreu2019,
abstract = {Low performance of nontechnical engineering students in programming courses is a problem that remains unsolved. Over the years, many authors have tried to identify the multiple causes for that failure, but there is unanimity on the fact that motivation is a key factor for the acquisition of knowledge by students. To better understand motivation, a new evaluation strategy has been adopted in a second programming course of a nontechnical degree, consisting of 91 students. The goals of the study were to identify if those students felt more motivated to answer multiple-choice questions in comparison to development questions, and what type of question better allows for testing student knowledge acquisition. Possibilities around the motivational qualities of multiple-choice questions in programming courses will be discussed in light of the results. In conclusion, it seems clear that student performance varies according to the type of question. Our study points out that multiple-choice questions can be seen as a motivational factor for engineering students and it might also be a good way to test acquired programming concepts. Therefore, this type of question could be further explored in the evaluation points.},
author = {Abreu, Pedro Henriques and Silva, Daniel Castro and Gomes, Anabela},
doi = {10.1145/3243137},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abreu, Silva, Gomes - 2019 - Multiple-choice questions in programming courses Can we use them and are students motivated by them.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {CS2,Evaluation methodologies,Pedagogical issues,Programming,Programming languages,Student's motivation},
mendeley-tags = {CS2},
month = {jan},
number = {1},
pages = {Article 6},
publisher = {Association for Computing Machinery},
title = {{Multiple-choice questions in programming courses: Can we use them and are students motivated by them?}},
volume = {19},
year = {2019}
}
@inproceedings{Layman2008a,
address = {Leipzig, Germany},
author = {Layman, Lucas M and Williams, Laurie A and {St. Amant}, Robert},
booktitle = {1st Workshop on Cooperative and Human Aspects of Software Engineering (CHASE '08)},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {73--76},
title = {{MimEc: Intelligent User Notification of Faults in the Eclipse IDE}},
year = {2008}
}
@inproceedings{Valdes2004a,
address = {New York, New York, USA},
author = {Valdes, Alfonso and Fong, Martin},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029228},
isbn = {1581139748},
keywords = {data mining,internet worms,intrusion detection,scalable visualization},
month = {oct},
pages = {124},
publisher = {ACM Press},
title = {{Scalable visualization of propagating internet phenomena}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029228},
year = {2004}
}
@article{Callison-Burch2009,
author = {Callison-Burch, Chris},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Callison-Burch - 2009 - Fast, cheap, and creative evaluating translation quality using Amazon's Mechanical Turk.pdf:pdf},
isbn = {978-1-932432-59-6},
journal = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
month = {aug},
pages = {286--295},
publisher = {Association for Computational Linguistics},
title = {{Fast, cheap, and creative: evaluating translation quality using Amazon's Mechanical Turk}},
url = {http://dl.acm.org/citation.cfm?id=1699510.1699548},
year = {2009}
}
@inproceedings{Alonso2012,
address = {Dallas, TX},
author = {Alonso, Javier and Grottke, Michael and Nikora, Allen P and Trivedi, Kishor S},
booktitle = {Proc. 23rd IEEE International Symposium on Software Reliability Engineering Workshops},
isbn = {978-1-4673-4638-2},
pages = {331--340},
title = {{The nature of the times to flight software failure during space missions}},
year = {2012}
}
@inproceedings{Rappa2005,
address = {San Jose, CA},
author = {Rappa, M and Smith, S E and Yacoub, A and Williams, L},
title = {{OpenSeminar: Web-based Collaboration Tool of Open Educational Resources}},
year = {2005}
}
@inproceedings{4534459,
abstract = {Many emergency response units are currently faced with restrictive budgets which prohibit their use of technology both in training and in real-world situations. Our work focuses on creating an affordable, mobile, state-of-the-art emergency response test-bed through the integration of low-cost, commercially available products. We have developed a command, control, communications, surveillance and reconnaissance system that will allow small-unit exercises to be tracked and recorded for evaluation purposes. Our system can be used for military and first responder training providing the nexus for decision making through the use of computational models, advanced technology, situational awareness and command and control. During a training session, data is streamed back to a central repository allowing commanders to evaluate their squads in a live action setting and assess their effectiveness in an after-action review. In order to effectively analyze this data, an interactive visualization system has been designed in which commanders can track personnel movement, view surveillance feeds, listen to radio traffic, and fast-forward/rewind event sequences. This system provides both 2-D and 3-D views of the environment while showing previously traveled paths, responder orientation and activity level. Both stationary and personnel-worn mobile camera video feeds may be displayed, as well as the associated radio traffic.},
author = {Maciejewski, R and Kim, Sung Ye and King-Smith, D and Ostmo, K and Klosterman, N and Mikkilineni, A K and Ebert, D S and Delp, E J and Collins, T F},
booktitle = {Technologies for Homeland Security, 2008 IEEE Conference on},
doi = {10.1109/THS.2008.4534459},
keywords = {command and control,decision making,emergency resp},
month = {may},
pages = {252--256},
title = {{Situational Awareness and Visual Analytics for Emergency Response and Training}},
year = {2008}
}
@inproceedings{Marrero2005,
abstract = {The complexity of languages like Java and C++ can make introductory programming classes in these languages extremely challenging for many students. Part of the complexity comes from the large number of concepts and language features that students are expected to learn while having little time for adequate practice or examples. A second source of difficulty is the emphasis that object-oriented programming places on abstraction. We believe that by placing a larger emphasis on testing in programming assignments in these introductory courses, students have an opportunity for extra practice with the language, and this affords them a gentler transition into the abstract thinking needed for programming. In this paper we describe how we emphasized testing in introductory programming assignments by requiring that students design and implement tests before starting on the program itself. We also provide some preliminary results and student reactions. Copyright 2005 ACM.},
annote = {Testing first: Emphasizing testing in early programming courses},
author = {Marrero, W and Settle, A},
booktitle = {Proceedings of the 10th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {4--8},
title = {{Testing first: Emphasizing testing in early programming courses}},
url = {citeulike-article-id:3934704 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-29844442321{\&}{\#}38 partnerID=40},
year = {2005}
}
@article{Charette1996,
abstract = {Because large-scale software projects increasingly affect the public good, the “normal science” paradigm is proving insufficient to model their complexity and potential consequences. The “postnormal science” paradigm offers a better fit, using a robust management approach predicated on a risk-taking ethic},
author = {Charette, R.N.},
doi = {10.1109/52.526838},
issn = {07407459},
journal = {IEEE Software},
month = {jul},
number = {4},
pages = {110--117},
title = {{Large-scale project management is risk management}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=526838},
volume = {13},
year = {1996}
}
@inproceedings{Eberlein2002,
address = {Essen, Germany},
annote = {{\textless}m:note{\textgreater}some flaws in agile requirements practices: poor stakeholder identification, assuming the customer knows everything, poor verification and validation, non-functional requirements, requirements traceability.  {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Offers some suggestions of how to address these issues.  None of them have been tested in this paper however.{\textless}/m:note{\textgreater}},
author = {Eberlein, A and Leite, J},
keywords = {XP,agile,requirements},
title = {{Agile Requirements Definition: A View from Requirements Engineering}},
year = {2002}
}
@inproceedings{4290950,
abstract = {Computer vision is afield which addresses many of the functional characteristics commonly associated with human vision. For example, identifying objects in a complex scene is a typical - and difficult - problem, but represents a task domain which well illustrates the way in which insights at the human-machine interface can be mutually beneficial, and is the area on which this paper focuses. Specifically, there is great current security interest in recognising human faces, and this task provides a very typical and important context for the system proposed though our system is also concerned with the study of less complex objects. The system seeks to develop working models of the operation of the human visual cognition system via a comparison between empirical experimentation on human subjects and the construction of an automated device to mimic the results of the human experimentation based on the operation of Multi- classifier systems (MCS).},
author = {Sirlantzis, Kostantinos and Howells, Gareth and Lloyd-Jones, Toby and Fairhurst, Michael},
booktitle = {2007 ECSIS Symposium on Bio-inspired, Learning, and Intelligent Systems for Security (BLISS 2007)},
doi = {10.1109/BLISS.2007.12},
isbn = {0-7695-2919-4},
keywords = {automatic visual cognition,computer vision,human c},
month = {aug},
pages = {111--114},
publisher = {Ieee},
title = {{A Multi-Classifier Approach to Modelling Human and Automatic Visual Cognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4290950},
year = {2007}
}
@article{ziefle2005older,
author = {Ziefle, M and Bay, S},
journal = {Behaviour {\&} Information Technology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {5},
pages = {375--389},
publisher = {Taylor {\&} Francis},
title = {{How older adults meet complexity: aging effects on the usability of different mobile phones}},
volume = {24},
year = {2005}
}
@incollection{year,
title = {{No Title}}
}
@inproceedings{4529468,
abstract = {There is increasing demand for running interacting applications in a secure and controllable way on mobile devices. Such demand is not fully supported by the Java/.NET security model based on trust domains nor by current security monitors or language-based security approaches. We propose an approach that allows security policies that are i) expressive enough to capture multiple sessions and interacting applications, ii) suitable for efficient monitoring, iii) convenient for a developer to specify them. Since getting all three at once is impossible, we advocate a logical language, 2D-LTL a bi-dimensional temporal logic fit for multiple sessions and for which efficient monitoring algorithms can be given, and a graphical language based on standard UML sequence diagrams with a tight correspondence between the two.},
author = {Massacci, F and Naliuka, K},
booktitle = {Availability, Reliability and Security, 2008. ARES 08. Third International Conference on},
doi = {10.1109/ARES.2008.191},
keywords = {Java/.NET security model,UML policies,bidimensiona},
month = {mar},
pages = {1112--1119},
title = {{Towards Practical Security Monitors of UML Policies for Mobile Applications}},
year = {2008}
}
@inproceedings{Kessler1986,
address = {Washington, DC},
author = {Kessler, Claudius M and Anderson, John R},
booktitle = {First Workshop on Empirical Studies of Programmers},
isbn = {0-89391-388-X},
pages = {198--212},
title = {{A model of novice debugging in LISP}},
url = {USA{\%}7D,},
year = {1986}
}
@inproceedings{Atsuta2004,
abstract = {eXtreme Programming (XP) is an agile software development process in which both short period iterative development and communication between project members are important. Software development aims at producing high quality programs within a promised period. To gain an improvement in product quality, XP proposes twelve practices such as Pair Programming, Test Driven Development, Refactoring, etc. Pair Programming makes it possible for all project members to deepen understanding of their programs, and also it becomes easy to find the error of them. Consequently, it improves quality of their programs. It will be required that the pair has to work together with sufficient communication. However, in realistic software development, it is necessary to carry out development in the distributed environment by time and spatial restrictions of developers, such as office environment, and telecommuting, an international project. In XP, since communication is important, we propose XP support environment on a network from the viewpoint of pair programming support so that the quality of a program can be improved also in distributed environment. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {EXtreme programming support tool in distributed environment},
author = {Atsuta, S and Matsuura, S},
booktitle = {Proceedings - International Computer Software and Applications Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {32--33},
title = {{EXtreme programming support tool in distributed environment}},
url = {citeulike-article-id:3934545 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-18844449938{\&}{\#}38 partnerID=40},
volume = {2},
year = {2004}
}
@article{Offutt2003,
author = {Offutt, Jeff and Liu, Shaoying and Abdurazik, Aynur and Ammann, Paul},
doi = {10.1002/stvr.264},
issn = {0960-0833},
journal = {Software Testing, Verification and Reliability},
keywords = {formal methods,software testing,specification-based testing},
month = {jan},
number = {1},
pages = {25--53},
title = {{Generating test data from state-based specifications}},
url = {http://doi.wiley.com/10.1002/stvr.264},
volume = {13},
year = {2003}
}
@inproceedings{1532061,
abstract = { Anomalous communication patterns are one of the leading indicators of computer system intrusions according to the system administrators we have interviewed. But a major problem is being able to correlate across the host/network boundary to see how network connections are related to running processes on a host. This paper introduces Portall, a visualization tool that gives system administrators a view of the communicating processes on the monitored machine correlated with the network activity in which the processes participate. Portall is a prototype of part of the Network Eye framework we have introduced in an earlier paper (Ball, et al., 2004). We discuss the Portall visualization, the supporting infrastructure it requires, and a formative usability study we conducted to obtain administrators' reactions to the tool.},
author = {Fink, G A and Muessig, P and North, C},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532061},
keywords = {Network Eye framework,Portall visualization,anom},
pages = {11--19},
title = {{Visual correlation of host processes and network traffic}},
year = {2005}
}
@phdthesis{Ispir2004,
abstract = {In this thesis, the Test Driven Development method (TDD) is studied for use in developing embedded software. The required framework is written for the development environment Rhapsody. Integration of TDD into a classical development cycle, without necessitating a transition to agile methodologies of software development and required unit test framework to apply TDD to an object oriented embedded software development project with a specific development environment and specific project conditions are done in this v thesis. A software tool for unit testing is developed specifically for this purpose, both to support the proposed approach and to illustrate its application. The results show that RhapUnit supplies the required testing functionality for developing embedded software in Rhapsody with TDD. Also, development of RhapUnit is a successful example of the application of TDD.},
annote = {Test Driven Development of Embedded Systems},
author = {Ispir, Mustafa},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Test Driven Development of Embedded Systems}},
url = {citeulike-article-id:3934651 {\#}},
year = {2004}
}
@article{Benisch2010,
author = {Benisch, Michael and Kelley, Patrick Gage and Sadeh, Norman and Cranor, Lorrie Faith},
doi = {10.1007/s00779-010-0346-0},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Benisch et al. - 2010 - Capturing location-privacy preferences quantifying accuracy and user-burden tradeoffs.pdf:pdf},
issn = {1617-4909},
journal = {Personal and Ubiquitous Computing},
keywords = {Location sharing,Mobile and pervasive computing,Privacy and security,Usability,agile,nsf},
mendeley-tags = {agile,nsf},
month = {dec},
number = {7},
pages = {679--694},
publisher = {Springer-Verlag},
title = {{Capturing location-privacy preferences: quantifying accuracy and user-burden tradeoffs}},
url = {http://dl.acm.org/citation.cfm?id=2039085.2039111},
volume = {15},
year = {2010}
}
@phdthesis{Melnik2007a,
abstract = {Grigori's thesis. You cannot copy/paste content from the PDF.},
annote = {Empirical Analyses of Executable Acceptance Test Driven Development},
author = {Melnik, G I},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
publisher = {UNIVERSITY OF CALGARY,},
title = {{Empirical Analyses of Executable Acceptance Test Driven Development}},
url = {citeulike-article-id:3934717 http://ase.cpsc.ucalgary.ca/uploads/Publications/MelnikPhD.pdf},
year = {2007}
}
@inproceedings{Glatz2010a,
address = {New York, New York, USA},
author = {Glatz, Eduard},
booktitle = {Proceedings of the Seventh International Symposium on Visualization for Cyber Security - VizSec '10},
doi = {10.1145/1850795.1850802},
isbn = {9781450300131},
keywords = {information visualization,network,security},
month = {sep},
pages = {58--63},
publisher = {ACM Press},
title = {{Visualizing host traffic through graphs}},
url = {http://dl.acm.org/citation.cfm?id=1850795.1850802},
year = {2010}
}
@article{Jones2018,
abstract = {Our cybersecurity workforce needs surpass our ability to meet them. These needs could be mitigated by developing relevant curricula that prioritize the knowledge, skills, and abilities (KSAs) most important to cybersecurity jobs. To identify the KSAs needed for performing cybersecurity jobs, we administered survey interviews to 44 cyber professionals at the premier hacker conferences Black Hat 2016 and DEF CON 24. Questions concerned 32 KSAs related to cyber defense. Participants rated how important each KSA was to their job and indicated where they had learned that KSA. Fifteen of these KSAs were rated as being of higher-than-neutral importance. Participants also answered open-ended questions meant to uncover additional KSAs that are important to cyber-defense work. Overall, the data suggest that KSAs related to networks, vulnerabilities, programming, and interpersonal communication should be prioritized in cybersecurity curricula.},
author = {Jones, Keith S. and Namin, Akbar Siami and Armstrong, Miriam E.},
doi = {10.1145/3152893},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jones, Namin, Armstrong - 2018 - The core cyber-defense knowledge, skills, and abilities that cybersecurity students should learn in sch.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {And Abilities,Cyber-Defense,Cybersecurity Curricula,Cybersecurity Education,Cybersecurity Workforce Framework,Knowledge,Skills},
month = {aug},
number = {3},
pages = {Article 11},
publisher = {Association for Computing Machinery},
title = {{The core cyber-defense knowledge, skills, and abilities that cybersecurity students should learn in school: Results from interviews with cybersecurity professionals}},
volume = {18},
year = {2018}
}
@inproceedings{Murphy2015,
abstract = {A goal-plan analysis was conducted to examine the variety of plans students use in writing a recursive method for an operation on a binary search tree. Students were asked to write a recursive method to count the nodes in a binary search tree with exactly one child. The problem incorporated two goals: traversing the tree and counting nodes with one child. Three traversal plans and four counting plans were observed in student solutions. Over half of the students used the arm's-length recursion plan, which involves testing for the base case before it is actually reached in order to avoid making recursive calls. This strategy creates complex and error prone code. Making students aware of arm's-length recursion may help them avoid introducing bugs into their recursive code. Although nearly all of the 18 participants demonstrated viable plans for solving the problem, their solutions contained a variety of errors: 55 total errors of 15 types. Students had particular difficulty with base cases, misplaced calculations, and missing method calls. Knowledge of these errors can be useful for instructors when developing lecture examples, identifying distractors for peer instruction multiple-choice questions and for designing homework exercises. Instructors can counteract these problems by providing a variety of recursive examples.},
address = {Kansas City, MO, USA},
author = {Murphy, Laurie and Fitzgerald, Sue and Grissom, Scott and McCauley, Ren{\'{e}}e},
booktitle = {SIGCSE 2015 - Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2676723.2677232},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy et al. - 2015 - Bug infestation! A goal-plan analysis of CS2 students' recursive binary tree solutions.pdf:pdf},
isbn = {9781450329668},
keywords = {Arm's-length recursion,BST,Binary search trees,CS2,Computer science education research,Data structures,Recursion},
month = {feb},
pages = {482--487},
publisher = {Association for Computing Machinery, Inc},
title = {{Bug infestation! A goal-plan analysis of CS2 students' recursive binary tree solutions}},
url = {http://dl.acm.org/citation.cfm?doid=2676723.2677232},
year = {2015}
}
@article{mchale02,
author = {McHale, James},
journal = {Crosstalk},
month = {sep},
title = {{TSP: Process Costs and Benefits}},
year = {2002}
}
@article{Beck2003,
author = {Beck, Kent and Boehm, Barry},
doi = {10.1109/MC.2003.1204374},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Beck, Boehm - 2003 - Agility through discipline a debate.pdf:pdf},
issn = {0018-9162},
journal = {Computer},
keywords = {agile},
mendeley-tags = {agile},
month = {jun},
number = {6},
pages = {44--46},
title = {{Agility through discipline: a debate}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1204374},
volume = {36},
year = {2003}
}
@article{Leblanc1987a,
abstract = {The debugging cycle is the most common methodology for finding and correcting errors in sequential programs. Cyclic debugging is effective because sequential programs are usually deterministic. Debugging parallel programs is considerably more difficult because successive executions of the same program often do not produce the same results. In this paper we present a general solution for reproducing the execution behavior of parallel programs, termed Instant Replay. During program execution we save the relative order of significant events as they occur, not the data associated with such events. As a result, our approach requires less time and space to save the information needed for program replay than other methods. Our technique is not dependent on any particular form of interprocess communication. It provides for replay of an entire program, rather than individual processes in isolation. No centralized bottlenecks are introduced and there is no need for synchronized clocks or a globally consistent logical time. We describe a prototype implementation of Instant Replay on the BBN Butterfly{\&}{\#}8482; Parallel Processor, and discuss how it can be incorporated into the debugging cycle for parallel programs.},
author = {Leblanc, T.J. and Mellor-Crummey, J.M.},
issn = {0018-9340},
journal = {IEEE Transactions on Computers},
month = {apr},
number = {4},
pages = {471--482},
title = {{Debugging parallel programs with instant replay}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1676929},
volume = {C-36},
year = {1987}
}
@inproceedings{Asuncion2010,
address = {Cape Town, South Africa},
author = {Asuncion, Hazeline U. and Asuncion, Arthur U. and Taylor, Richard N.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - ICSE '10},
doi = {10.1145/1806799.1806817},
isbn = {9781605587196},
keywords = {latent dirichlet allocation,software architecture,software traceability,topic model},
month = {may},
pages = {95--104},
publisher = {ACM Press},
title = {{Software traceability with topic modeling}},
url = {http://dl.acm.org/citation.cfm?id=1806799.1806817},
year = {2010}
}
@article{Hieatt2002,
abstract = {An overview is given on test-first design and the creation of testable code for Web applications. It is explained how testing has been critical to building Evant's application at speed while maintaining a high degree of quality.},
annote = {Going faster: Testing the Web application},
author = {Hieatt, E and Mee, R},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {60--65},
title = {{Going faster: Testing the Web application}},
url = {citeulike-article-id:3934641 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-0036496386{\&}{\#}38 partnerID=40},
volume = {19},
year = {2002}
}
@inproceedings{Rasmussen2010d,
address = {New York, New York, USA},
author = {Rasmussen, Jamie and Ehrlich, Kate and Ross, Steven and Kirk, Susanna and Gruen, Daniel and Patterson, John},
booktitle = {Proceedings of the Seventh International Symposium on Visualization for Cyber Security - VizSec '10},
doi = {10.1145/1850795.1850807},
isbn = {9781450300131},
keywords = {information visualization,managed security services,user studies},
month = {sep},
pages = {102--113},
publisher = {ACM Press},
title = {{Nimble cybersecurity incident management through visualization and defensible recommendations}},
url = {http://dl.acm.org/citation.cfm?id=1850795.1850807},
year = {2010}
}
@inproceedings{6107846,
abstract = {Maintaining situation awareness in the maritime domain is a challenging mandate. Task analysis activities were conducted to identify where visual analytics science and technology could improve maritime domain awareness and reduce information overload. Three promising opportunities were identified: the visualization of normal maritime behaviour, anomaly detection, and the collaborative analysis of a vessel of interest. In this paper, we describe the result of our user studies along with potential visual analytics solutions and features considered for a maritime analytics prototype.},
author = {Lavigne, Valerie and Gouin, Denis and Davenport, Michael},
booktitle = {2011 IEEE International Conference on Technologies for Homeland Security (HST)},
doi = {10.1109/THS.2011.6107846},
isbn = {978-1-4577-1376-7},
keywords = {anomaly detection,collaborative analysis,maritime},
month = {nov},
pages = {49--54},
publisher = {Ieee},
title = {{Visual analytics for maritime domain awareness}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6107846},
year = {2011}
}
@book{Kelly1955,
address = {New York, NY},
author = {Kelly, George},
keywords = {repertory grid},
publisher = {Norton},
title = {{The Psychology of Personal Constructs}},
year = {1955}
}
@techreport{Herbsleb1994,
address = {CMU/SEI-94-TR-13},
author = {Herbsleb, James and Carleton, Anita and Rozum, James and Siegel, Jane and Zubrow, Dave},
institution = {Carnegie-Mellon University Software Engineering Institute},
title = {{Benefits of CMM-Based Software Process Improvement: Initial Results}},
url = {http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2{\&}doc=GetTRDoc.pdf{\&}AD=ADA283848},
year = {1994}
}
@inproceedings{Tandon2004,
address = {New York, New York, USA},
author = {Tandon, Gaurav and Chan, Philip and Mitra, Debasis},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029212},
isbn = {1581139748},
keywords = {anomaly detection,data cleaning,motifs},
month = {oct},
pages = {16},
publisher = {ACM Press},
title = {{MORPHEUS}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029212},
year = {2004}
}
@book{Beck2001b,
address = {Boston, MA},
annote = {Insight on how to do the planning game, deal with user stories, perform estimations, etc.},
author = {Beck, K and Fowler, M},
keywords = {XP,extreme programming},
publisher = {Addison-Wesley},
title = {{Planning Extreme Programming}},
year = {2001}
}
@inproceedings{McDermott2005a,
address = {New York, New York, USA},
author = {McDermott, J.},
booktitle = {Proceedings of the 2005 workshop on New security paradigms - NSPW '05},
doi = {10.1145/1146269.1146293},
isbn = {1595933174},
keywords = {GSPML,security principal,security protocol},
month = {sep},
pages = {97},
publisher = {ACM Press},
title = {{Visual security protocol modeling}},
url = {http://dl.acm.org/citation.cfm?id=1146269.1146293},
year = {2005}
}
@misc{Williams2000,
author = {Williams, L and Kessler, R and Cunningham, W and Jeffries, R},
number = {4},
pages = {19--25},
title = {{Strengthening the Case for Pair-Programming}},
volume = {17},
year = {2000}
}
@article{Jain2010,
abstract = {Organizing data into sensible groupings is one of the most fundamental modes of understanding and learning. As an example, a common scheme of scientific classification puts organisms into a system of ranked taxa: domain, kingdom, phylum, class, etc. Cluster analysis is the formal study of methods and algorithms for grouping, or clustering, objects according to measured or perceived intrinsic characteristics or similarity. Cluster analysis does not use category labels that tag objects with prior identifiers, i.e., class labels. The absence of category information distinguishes data clustering (unsupervised learning) from classification or discriminant analysis (supervised learning). The aim of clustering is to find structure in data and is therefore exploratory in nature. Clustering has a long and rich history in a variety of scientific fields. One of the most popular and simple clustering algorithms, K-means, was first published in 1955. In spite of the fact that K-means was proposed over 50 years ago and thousands of clustering algorithms have been published since then, K-means is still widely used. This speaks to the difficulty in designing a general purpose clustering algorithm and the ill-posed problem of clustering. We provide a brief overview of clustering, summarize well known clustering methods, discuss the major challenges and key issues in designing clustering algorithms, and point out some of the emerging and useful research directions, including semi-supervised clustering, ensemble clustering, simultaneous feature selection during data clustering, and large scale data clustering.},
author = {Jain, Anil K.},
doi = {10.1016/j.patrec.2009.09.011},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Data clustering,Historical developments,King-Sun Fu prize,Perspectives on clustering,User's dilemma,agile,nsf},
mendeley-tags = {agile,nsf},
month = {jun},
number = {8},
pages = {651--666},
title = {{Data clustering: 50 years beyond K-means}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865509002323},
volume = {31},
year = {2010}
}
@article{Stucky2009,
author = {Stucky, Thomas D. and Ottensmann, John R.},
doi = {10.1111/j.1745-9125.2009.00174.x},
issn = {00111384},
journal = {Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {nov},
number = {4},
pages = {1223--1264},
title = {{Land Use and Violent Crime}},
url = {http://doi.wiley.com/10.1111/j.1745-9125.2009.00174.x},
volume = {47},
year = {2009}
}
@misc{Mu2006,
abstract = {Usage of test-driven development (TDD) is said to lead to better testable programs. However, no study answers either the question how this better testability can be measured nor whether the feature of better testability exists. To answer both questions we present the concept of the controllability of assignments. We studied this metric on various TDD and conventional projects. Assignment controllability seems to support the rules of thumb for testable code, e.g. small classes with low coupling are better testable than large classes with high coupling. And as opposed to the Chidamber and Kemerer metric suite for object-oriented design, controllability of assignments seems to be an indicator whether a project was developed with TDD or not. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {The effect of test-driven development on program code},
author = {Mu, M M},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {94--103},
title = {{The effect of test-driven development on program code}},
url = {citeulike-article-id:3934725 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746192565{\&}{\#}38 partnerID=40},
volume = {4044 LNCS},
year = {2006}
}
@article{Friedman2002,
author = {Friedman, G and City, Technion and Hartman, A and Nagin, K},
isbn = {1581135629},
keywords = {automated test generation,finite state machine modeling,state},
pages = {134--143},
title = {{Projected State Machine Coverage for Software Testing}},
year = {2002}
}
@inproceedings{5283830,
abstract = {Data hiding is a technique which embeds secret data into cover media. It is important to the multimedia security and has been widely studied. Recently, some researchers paid attention to reversible data hiding methods. These methods can reconstruct the original image from the stego-image when embedded data are extracted. In this paper, we propose a new reversible steganographic method for vector quantization (VQ) compressed images. The new method uses the hit pattern to achieve reversibility, and the hit pattern strategy successfully reduces the overhead. The codebook is partitioned by the extension method which makes the stego-image have good visual features. Also, a look-up table is used to speed up the partitioning operation. Compared to Chang et al.'s method, the experimental results show that the proposed method has higher capacity, better visual quality, and lower running time.},
author = {Yang, Cheng-Hsing and Huang, Cheng-Ta and Wang, Shiuh-Jeng},
booktitle = {2009 Fifth International Conference on Information Assurance and Security},
doi = {10.1109/IAS.2009.192},
isbn = {978-0-7695-3744-3},
keywords = {codebook,hit pattern,image compression,multimedia},
pages = {607--610},
publisher = {Ieee},
title = {{Reversible Steganography Based on Side Match and Hit Pattern for VQ-Compressed Images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5283830},
volume = {2},
year = {2009}
}
@article{Oblinger2003,
author = {Oblinger, Diana},
number = {4},
pages = {37--47},
title = {{Boomers, Gen-Xers, and Millennials:  Understanding the New Students}},
volume = {38},
year = {2003}
}
@techreport{U.S.DepartmentofHomelandSecurity2009,
address = {Washington, D. C.},
author = {{U.S. Department of Homeland Security}},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/U.S. Department of Homeland Security - 2009 - A Roadmap for Cybersecurity Research.pdf:pdf},
institution = {U.S. Department of Homeland Security},
keywords = {agile,nsf,security},
mendeley-tags = {agile,nsf},
number = {November},
title = {{A Roadmap for Cybersecurity Research}},
url = {http://www.cyber.st.dhs.gov/docs/DHS-Cybersecurity-Roadmap.pdf},
year = {2009}
}
@book{Boehm2003,
author = {Boehm, B and Turner, R},
isbn = {0321186125},
publisher = {Addison Wesley},
title = {{Balancing Agility and Discipline:  A Guide for the Perplexed}},
year = {2003}
}
@article{Melis2006,
abstract = {In this article, we present a simulation model that we developed for exploring the influence of two key Extreme Programming (XP) practices - Test-first Programming and Pair Programming - on the evolution of an XP software project. We present the results obtained simulating a typical XP project changing the usage levels of the two practices. We focused on output variables related to effort, size, quality and released functionalities. Copyright {\^{A}}{\textcopyright} 2006 John Wiley {\&} Sons, Ltd.},
annote = {Evaluating the impact of test-first programming and pair programming through software process simulation},
author = {Melis, M and Turnu, I and Cau, A and Concas, G},
journal = {Software Process Improvement and Practice},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {4},
pages = {345--360},
title = {{Evaluating the impact of test-first programming and pair programming through software process simulation}},
url = {citeulike-article-id:3934713 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33747129327{\&}{\#}38 partnerID=40},
volume = {11},
year = {2006}
}
@article{Youngs1974,
author = {Youngs, Edward A},
doi = {10.1016/S0020-7373(74)80027-1},
issn = {0020-7373},
journal = {International Journal of Man-Machine Studies},
number = {3},
pages = {361--376},
title = {{Human Errors in Programming}},
volume = {6},
year = {1974}
}
@article{Norman1981,
author = {Norman, Donald A},
journal = {Psychological Review},
keywords = {action slip,agile,interruption,nsf,psychology},
mendeley-tags = {agile,nsf},
number = {1},
pages = {1--15},
title = {{Categorization of Action Slips}},
volume = {88},
year = {1981}
}
@article{DeMarco2002,
author = {DeMarco, Tom and Boehm, Barry},
doi = {10.1109/MC.2002.1009175},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/DeMarco, Boehm - 2002 - The agile methods fray.pdf:pdf},
issn = {0018-9162},
journal = {Computer},
keywords = {agile},
mendeley-tags = {agile},
month = {jun},
number = {6},
pages = {90--92},
title = {{The agile methods fray}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1009175},
volume = {35},
year = {2002}
}
@inproceedings{Ihantola2015,
abstract = {{\textcopyright} 2015 ACM. Educational data mining and learning analytics promise better understanding of student behavior and knowledge, as well as new information on the tacit factors that contribute to student actions. This knowledge can be used to inform decisions related to course and tool design and pedagogy, and to further engage students and guide those at risk of failure. This working group report provides an overview of the body of knowledge regarding the use of educational data mining and learning analytics focused on the teaching and learning of programming. In a literature survey on mining students' programming processes for 2005-2015, we observe a significant increase in work related to the field. However, the majority of the studies focus on simplistic metric analysis and are conducted within a single institution and a single course. This indicates the existence of further avenues of research and a critical need for validation and replication to better understand the various contributing factors and the reasons why certain results occur. We introduce a novel taxonomy to analyse replicating studies and discuss the importance of replicating and reproducing previous work. We describe what is the state of the art in collecting and sharing programming data. To better understand the challenges involved in replicating or reproducing existing studies, we report our experiences from three case studies using programming data. Finally, we present a discussion of future directions for the education and research community.},
address = {Vilnius, Lithuania},
author = {Ihantola, Petri and Vihavainen, Arto and Ahadi, Alireza and Butler, Matthew and B{\"{o}}rstler, J{\"{u}}rgen and Edwards, Stephen H. and Isohanni, Essi and Korhonen, Ari and Petersen, Andrew and Rivers, Kelly and Rubio, Miguel {\'{A}}ngel and Sheard, Judy and Skupas, Bronius and Spacco, Jaime and Szabo, Claudia and Toll, Daniel},
booktitle = {ITiCSE-WGP 2015 - Proceedings of the 2015 ITiCSE Conference on Working Group Reports},
doi = {10.1145/2858796.2858798},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ihantola et al. - 2015 - Educational data mining and learning analytics in programming Literature review and case studies.pdf:pdf},
isbn = {9781450341462},
keywords = {CS2,Educational data mining,Learning analytics,Literature review,Programming,Replication},
mendeley-tags = {CS2},
pages = {41--63},
publisher = {ACM Press},
title = {{Educational data mining and learning analytics in programming: Literature review and case studies}},
url = {http://dl.acm.org/citation.cfm?doid=2858796.2858798},
year = {2015}
}
@inproceedings{Guo2010,
author = {Guo, P J and Zimmermann, T and Nagappan, N},
booktitle = {Proceedings of the 32nd International Conference on Software Engineering},
pages = {495--504},
title = {{Characterizing and predicting which bugs get fixed: An empirical study of microsoft windows}},
url = {http://portal.acm.org/citation.cfm?id=1806871},
year = {2010}
}
@book{Humphrey:2005,
author = {Humphrey, Watts S},
isbn = {0321349628},
publisher = {Addison-Wesley Professional},
title = {{TSP(SM)-Leading a Development Team (SEI Series in Software Engineering)}},
year = {2005}
}
@inproceedings{Bowles2001,
address = {Philadelphia, PA},
author = {Bowles, John B. and Wan, Chi},
booktitle = {Proccedings of the Annual Reliability and Maintainability Symposium (RAMS '01)},
doi = {10.1109/RAMS.2001.902433},
isbn = {0-7803-6615-8},
keywords = {conclusions,fmea,functional fmea,illustrating how software failure,interface fmea,modes and effects analysis,provides a comprehensive example,software failure modes,software fmea,software reliability,summary,the work shown here},
pages = {1--6},
publisher = {IEEE},
title = {{Software failure modes and effects analysis for a small embedded control system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=902433},
year = {2001}
}
@inproceedings{Gutzwiller2016b,
author = {Gutzwiller, Robert S. and Hunt, Sarah M. and Lange, Douglas S.},
booktitle = {2016 IEEE International Multi-Disciplinary Conference on Cognitive Methods in Situation Awareness and Decision Support (CogSIMA)},
doi = {10.1109/COGSIMA.2016.7497780},
isbn = {978-1-5090-0632-8},
month = {mar},
pages = {14--20},
publisher = {IEEE},
title = {{A task analysis toward characterizing cyber-cognitive situation awareness (CCSA) in cyber defense analysts}},
url = {http://ieeexplore.ieee.org/document/7497780/},
year = {2016}
}
@misc{Purviance2011,
author = {Purviance, Phil},
keywords = {security},
mendeley-tags = {security},
title = {{XSS in Skype for iOS}},
url = {https://superevr.com/blog/2011/xss-in-skype-for-ios/},
year = {2011}
}
@inproceedings{5958025,
abstract = {Popup blocking, form filling, and many other features of modern web browsers were first introduced as third-party extensions. New extensions continue to enrich browsers in unanticipated ways. However, powerful extensions require capabilities, such as cross-domain network access and local storage, which, if used improperly, pose a security risk. Several browsers try to limit extension capabilities, but an empirical survey we conducted shows that many extensions are over-privileged under existing mechanisms. This paper presents ibex, a new framework for authoring, analyzing, verifying, and deploying secure browser extensions. Our approach is based on using type-safe, high-level languages to program extensions against an API providing access to a variety of browser features. We propose using Data log to specify fine-grained access control and dataflow policies to limit the ways in which an extension can use this API, thus restricting its privilege over security-sensitive web content and browser resources. We formalize the semantics of policies in terms of a safety property on the execution of extensions and develop a verification methodology that allows us to statically check extensions for policy compliance. Additionally, we provide visualization tools to assist with policy analysis, and compilers to translate extension source code to either. NET byte code or JavaScript, facilitating cross-browser deployment of extensions. We evaluate our work by implementing and verifying{\~{}}NumExt extensions with a diverse set of features and security policies. We deploy our extensions in Internet Explorer, Chrome, Fire fox, and a new experimental HTML5 platform called C3. In so doing, we demonstrate the versatility and effectiveness of our approach.},
author = {Guha, A and Fredrikson, M and Livshits, B and Swamy, N},
booktitle = {Security and Privacy (SP), 2011 IEEE Symposium on},
doi = {10.1109/SP.2011.36},
issn = {1081-6011},
keywords = {API,access control,browser extensions,compilers,da},
month = {may},
pages = {115--130},
title = {{Verified Security for Browser Extensions}},
year = {2011}
}
@article{Mingsong2006,
author = {Mingsong, Chen and Xiaokang, Qiu and Xuandong, Li},
isbn = {159593085X},
journal = {{\ldots} on Automation of software test},
keywords = {instrumentation,random test case,uml activity diagram},
pages = {2--8},
title = {{Automatic test case generation for UML activity diagrams}},
url = {http://dl.acm.org/citation.cfm?id=1138931},
year = {2006}
}
@misc{Basili2011,
address = {Fraunhofer Center for Experimental Software Engineering TR 11-101 (http://goo.gl/fEKc2)},
author = {Basili, Victor R. and Layman, Lucas and Zelkowitz, Marvin V.},
keywords = {mypubs},
mendeley-tags = {mypubs},
publisher = {Fraunhofer Center for Experimental Software Engineering},
title = {{A Methodology for Exposing Software Development Risk in Emergent System Properties}},
year = {2011}
}
@misc{AmericanAssociationofUniversityWomenEducationFoundation2000,
author = {{American Association of University Women Education Foundation}},
title = {{Educating Girls in the New Computer Age}},
url = {http://www.aauw.org/member{\_}center/publications/TechSavvy/TechSavvy.pdf},
year = {2000}
}
@techreport{IEEE829,
author = {IEEE},
doi = {http://dx.doi.org/10.1109/IEEESTD.2008.4578383},
institution = {IEEE},
title = {{829-2008 - IEEE Standard for Software and System Test Documentation}}
}
@inproceedings{Jung2010,
author = {Jung, Min Yang and Deguet, Anton and Kazanzides, Peter},
booktitle = {IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS)},
pages = {6107--6112},
title = {{A Component-based Architecture for Flexible Integration of Robotic Systems}},
year = {2010}
}
@article{Utting,
author = {Utting, Mark and Torreborre, Eric},
number = {1},
pages = {223--230},
title = {{Model-Based Testing from UML Models}},
volume = {3}
}
@article{Cornish1987,
author = {Cornish, Derek B. and Clarke, Ronald V.},
doi = {10.1111/j.1745-9125.1987.tb00826.x},
issn = {0011-1384},
journal = {Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {nov},
number = {4},
pages = {933--948},
title = {{Understanding Crime Displacement: An Application of Rational Choice Theory}},
url = {http://doi.wiley.com/10.1111/j.1745-9125.1987.tb00826.x},
volume = {25},
year = {1987}
}
@inproceedings{Jakobsen2006,
address = {Montr{\'{e}}al, Qu{\'{e}}bec, Canada},
author = {Jakobsen, Mikkel R{\o}nne and Hornb{\ae}k, Kasper},
keywords = {attention,doi,fisheye},
pages = {377--386},
title = {{Evaluating a Fisheye View of Source Code}},
year = {2006}
}
@article{Becker1968,
author = {Becker, Gary},
journal = {The Journal of Political Economy},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {169--217},
title = {{Crime and Punishment: An Economic Approach}},
volume = {76},
year = {1968}
}
@inproceedings{Werlinger2007,
address = {New York, New York, USA},
author = {Werlinger, Rodrigo and Botta, David and Beznosov, Konstantin},
booktitle = {Proceedings of the 3rd symposium on Usable privacy and security - SOUPS '07},
doi = {10.1145/1280680.1280702},
isbn = {9781595938015},
month = {jul},
pages = {149},
publisher = {ACM Press},
title = {{Detecting, analyzing and responding to security incidents}},
url = {http://dl.acm.org/citation.cfm?id=1280680.1280702},
year = {2007}
}
@inproceedings{Lakkaraju2004a,
address = {New York, New York, USA},
author = {Lakkaraju, Kiran and Yurcik, William and Lee, Adam J.},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029219},
isbn = {1581139748},
keywords = {NetFlows,security system state,security visualization,situational awareness},
month = {oct},
pages = {65},
publisher = {ACM Press},
title = {{NVisionIP}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029219},
year = {2004}
}
@article{Huff2002,
author = {Huff, C},
keywords = {design,gender,psychology},
number = {2},
pages = {112--115},
title = {{Gender, Software Design, and Occupational Equity}},
volume = {34},
year = {2002}
}
@misc{Layman2005c,
address = {North Carolina State University, Department of Computer Science TR-2005-40 (http://goo.gl/3izvne)},
author = {Layman, Lucas and Cornwell, Travis and Williams, Laurie and Osborne, Jason},
keywords = {mypubs},
mendeley-tags = {mypubs},
title = {{Personality Profiles and Learning Styles of Advanced Undergraduate Computer Science Students}},
year = {2005}
}
@article{Smith2008,
abstract = {In the business world, the use of Agile methodologies has been demonstrated as providing a pro-active, rather than reactive, path for the developer to create defect-free products. Although similarities exist with business desktop and line-of-business systems, the closer connection of signal processing systems to the hardware side of a product, and associated physical constraints, makes the adaption of desktop Agile methodologies for the embedded world difficult; and the adoption of these methodologies by developers problematic. We focus on our experiences in developing test frameworks to support transforming a subset of extreme programming from the world of desktop applications into a suitable embedded domain production methodology. Details are provided of the issues surrounding an Embedded xUnit testing framework that will permit development of digital signal processing applications on a wide range of standalone and multi-processor systems in research, teaching and commercial development environments. {\^{A}}{\textcopyright} 2008 Springer Science+Business Media, LLC.},
annote = {A Test-oriented Embedded System Production Methodology},
author = {Smith, M and Miller, J and Daeninck, S},
journal = {Journal of Signal Processing Systems},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {1--21},
title = {{A Test-oriented Embedded System Production Methodology}},
url = {citeulike-article-id:3934802 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-52049087804{\&}{\#}38 partnerID=40},
year = {2008}
}
@inproceedings{Abrahamsson2003,
address = {Belek, Turkey},
author = {Abrahamsson, P},
pages = {259--266},
title = {{Extreme Programming: First Results from a Controlled Case Study}},
year = {2003}
}
@article{Setty2008,
abstract = {The choice to use static or load-time weaving techniques in the development cycle of large AspectJ programs is not clear. It is a common practice to iteratively remove errors from programs by making small changes, recompiling, and testing the change. Previous research has shown that incremental compilation of aspect-oriented programs using static weavers can take longer compared to objectoriented programs, which in turn increases the time spent in each iteration. It has been suggested that utilizing load-time weavers can potentially alleviate the problem. However, there is a trade-off involved which is the increased execution time due to the overhead involved in weaving while loading classes. In this paper, we report on a case study in which we examine the parameters that differentiate the two techniques during the edit-compile-test cycle and determine which technique is more favorable as these parameters vary. Our results show that the parameters that differentiate the techniques are the number of classes loaded, the size of the project and the number of join points executed including repetitions. We also find that load-time weaving does solve the problem of incremental compilation in aspect-oriented programming to some extent under some favorable values of the parameters mentioned. We find that the performance of static weaving with respect to load-time weaving is directly proportional to the number of classes loaded during test, and the performance of load-time weaving with respect to static weaving is directly proportional to the size of the project and the number of join points executed. Our results also show that the percentage of join points affected by aspects do not differentiate between the two techniques.},
annote = {Weave Now or Weave Later: A Test Driven Development Perspective on Aspect-oriented Deployment Models},
author = {Setty, R and Dyer, R and Rajan, H},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Weave Now or Weave Later: A Test Driven Development Perspective on Aspect-oriented Deployment Models}},
url = {citeulike-article-id:3934788 http://archives.cs.iastate.edu/documents/disk0/00/00/05/65/00000565-00/Setty-Dyer-Rajan-TR-08-02.pdf},
year = {2008}
}
@inproceedings{Betin-Can2005,
author = {Betin-Can, Aysu and Bultan, Tevfik and Lindvall, Mikael and Lux, Benjamin and Topp, Stefan},
booktitle = {ASE},
keywords = {Software Engineering},
pages = {14--23},
publisher = {ACM},
series = {20th IEEE/ACM International Conference on Automated Software Engineering},
title = {{Application of Design for Verification with Concurrency Controllers to Air Traffic Control Software}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.69.7886{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@inproceedings{5671141,
abstract = {Pixel-value-differencing (PVD) based steganographic methods, which take into account the characteristic of human visual system, usually lead to large embedding capacity and good visual imperceptibility. In this paper, a recently proposed PVD-based steganography is further studied. First, by deriving a sufficient condition for data embedding/extraction, we show that the method can be generalized, and we have in fact a much wider choice of embedding parameters. Then, the security of this PVD-based steganography is analyzed. A targeted steganalyzer for this method is designed, and the experiments show that the method can be easily detected. That is to say, like other PVD-based steganographic methods, this most recent one can not resist statistical detection and it is also not secure. Therefore, as a conclusion, more investigation should be done on PVD-based steganographic methods, to enhance their statistical undetectability.},
author = {Ji, Liping and Li, Xiaolong and Yang, Bin and Liu, Zhihong},
booktitle = {2010 International Conference on Multimedia Information Networking and Security},
doi = {10.1109/MINES.2010.149},
isbn = {978-1-4244-8626-7},
keywords = {data embedding,data extraction,human visual system},
pages = {686--690},
publisher = {Ieee},
title = {{A Further Study on a PVD-Based Steganography}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5671141},
year = {2010}
}
@inproceedings{Layman2006c,
abstract = {Collaborative work has been in use as an instructional tool to increase student understanding through collaborative learning and to improve student performance in computer science courses. However, little work has been done to understand how the act of collaboration, through pair programming or group work, impacts a student's knowledge of the benefits and difficulties of collaborative work experience in collaborative work is essential preparation for professional software development. A study was conducted at North Carolina State University to assess changes in advanced undergraduate students' perceptions of pair programming and collaboration. Student personality types, learning styles, and other characteristics were gathered during two semesters of an undergraduate software engineering course. The study found that, after experiencing pair programming, most students indicated a stronger preference to work with another student, believed that pairing made them more organized, and believed that pairing saved time on homework assignments. Students who disliked their collaborative experiences were predominantly reflective learners, introverts, and strong coders. Those students also cited that a non-participatory partner and difficulties scheduling meeting times outside of the classroom were primary reasons for disliking pair programming. Personality type and learning style had little effect on the changes in perceptions of collaboration},
author = {Layman, L.},
booktitle = {19th Conference on Software Engineering Education and Training (CSEET'06)},
doi = {10.1109/CSEET.2006.10},
isbn = {0-7695-2557-1},
issn = {1093-0175},
keywords = {Collaboration,Collaborative software,Collaborative work,Computer science,Educational programs,Engineering management,North Carolina State University,Processor scheduling,Programming profession,Software engineering,Teamwork,advanced undergraduate students,collaborative learning,collaborative software development,collaborative work experience,computer science courses,computer science education,continuing education,educational courses,group work,homework assignments,mypubs,pair programming,professional software development,student learning styles,student perceptions,student personality types,student understanding,teaching},
mendeley-tags = {mypubs},
pages = {159--166},
publisher = {IEEE},
shorttitle = {Software Engineering Education and Training, 2006.},
title = {{Changing Students' Perceptions: An Analysis of the Supplementary Benefits of Collaborative Software Development}},
year = {2006}
}
@article{Parsons2004,
author = {Parsons, Jeffrey and Saunders, Chad},
journal = {IEEE Transactions on Software Engineering},
number = {12},
pages = {873--888},
title = {{Cognitive Heuristics in Software Engineering: Applying and Extending Anchoring and Adjustment to Artifact Reuse}},
volume = {30},
year = {2004}
}
@incollection{bursik1995,
address = {Boulder, CO},
author = {Jr., Robert J Bursik and Grasmick, Harold G},
booktitle = {Crime and Public Policy: Putting Theory to Work},
editor = {Barlow, Hugh D},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {107--130},
publisher = {Westview Press},
title = {{Neighborhood-based networks and the control of crime and delinquency}},
year = {1995}
}
@inproceedings{Carver2005,
address = {Mississippi, USA},
author = {Carver, J and Lemon, Krystle},
keywords = {Empirical Software Engineering,Software Engineering},
pages = {1--18},
series = {2005 International Symposium on Empirical Software Engineering (Late Breaking Research Track).},
title = {{Architecture Reading Techniques: A Feasibility Study}},
url = {http://cs.ua.edu/{~}carver/Papers/Conference/2005/2005{\_}ISESE{\_}ARTs.pdf},
year = {2005}
}
@inproceedings{Guillemot2006,
abstract = {In this paper we describe WebTest, an Open Source tool for automated testing of web applications. In particular we will show how to quickly create tests that shine with excellent maintainability and runtime performance as well as perfect integration in the application development cycle.},
annote = {Web testing made easy},
author = {Guillemot, M and K{\"{o}}nig, D},
booktitle = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {692--693},
title = {{Web testing made easy}},
url = {citeulike-article-id:3934630 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248333023{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@inproceedings{4534511,
abstract = {High-quality, microscopic transportation simulation output visualization systems are extremely difficult and time consuming to generate. Existing systems do not allow for real-time, interactive simulation visualization output. This project developed the means to dynamically display and interact with, in four-dimensions, the real-time simulation output from a stochastic microscopic traffic simulation model. The 4D visualization system described in this paper provides true spatial awareness and can convey information that cannot be easily explained by traditional two dimensional visualization methods. The system provides the means to take multiple large scale simulation outputs (like TSIS-CORSIM) at various time frames and allow them to be visualized concurrently (or sequentially) and seamlessly in an existing 4D environment. The system affords the viewing of the simulation output in a virtual world complete with aerial photography, terrain mapping, weather animations, buildings, and other features.},
author = {Bista, S and Pack, M L},
booktitle = {Technologies for Homeland Security, 2008 IEEE Conference on},
doi = {10.1109/THS.2008.4534511},
keywords = {interactive simulation visualization output,real-t},
month = {may},
pages = {543--548},
title = {{Real-Time Massive Data Simulation Visualization}},
year = {2008}
}
@article{Muraven2000,
abstract = {Reviews evidence that self-control may consume a limited resource. Exerting self-control may consume self-control strength, reducing the amount of strength available for subsequent self-control efforts. Coping with stress, regulating negative affect, and resisting temptations require self-control, and after such self-control efforts, subsequent attempts at self-control are more likely to fail. Continuous self-control efforts, such as vigilance, also degrade over time. These decrements in self-control are probably not due to negative moods or learned helplessness produced by the initial self-control attempt. These decrements appear to be specific to behaviors that involve self-control; behaviors that do not require self-control neither consume nor require self-control strength. The authors conclude that the executive component of the self—in particular, inhibition—relies on a limited, consumable resource.},
author = {Muraven, Mark and Baumeister, Roy F.},
journal = {Psychological Bulletin},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {247--259},
title = {{Self-regulation and depletion of limited resources: Does self-control resemble a muscle?}},
volume = {126},
year = {2000}
}
@inproceedings{Biggio2013,
address = {New York, New York, USA},
author = {Biggio, Battista and Pillai, Ignazio and {Rota Bul{\`{o}}}, Samuel and Ariu, Davide and Pelillo, Marcello and Roli, Fabio},
booktitle = {Proceedings of the 2013 ACM workshop on Artificial intelligence and security - AISec '13},
doi = {10.1145/2517312.2517321},
isbn = {9781450324885},
keywords = {adversarial learning,clustering,computer security,malware detection,security evaluation,unsupervised learning},
month = {nov},
pages = {87--98},
publisher = {ACM Press},
title = {{Is data clustering in adversarial settings secure?}},
url = {http://dl.acm.org/citation.cfm?id=2517312.2517321},
year = {2013}
}
@inproceedings{Holmes2006,
abstract = {Ever in search of a silver bullet for automated functional testing for Web Applications, many folks have turned to Selenium. Selenium is an open-source project for in-browser testing, originally developed by ThoughtWorks and now boasting an active community of developers and users. One of Selenium's stated goals is to become the de facto open-source replacement for proprietary tools such as WinRunner. Of particular interest to the agile community is that it offers the possibility of test-first design of web applications, red-green signals for customer acceptance tests, and an automated regression test bed for the web tier. This experience report describes the standard environment for testing with Selenium, as well as modifications we performed to incorporate our script pages into a wiki. It includes lessons we learned about continuous integration, script writing, and using the Selenium Recorder (renamed IDE). We also discuss how long it took to write and maintain the scripts in the iterative development environment, how close we came to covering all of the functional requirements with tests, how often the tests should be (and were) run, and whether additional automated functional testing below the GUI layer was still necessary and/or appropriate. While no silver bullet, Selenium has become a valuable addition to our agile testing toolkit, and is used on the majority of our web application projects. It promises to become even more valuable as it gains widespread adoption and continues to be actively developed. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Automating functional tests using selenium},
author = {Holmes, A and Kellogg, M},
booktitle = {Proceedings - AGILE Conference, 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {270--275},
title = {{Automating functional tests using selenium}},
url = {citeulike-article-id:3934643 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34247588324{\&}{\#}38 partnerID=40},
volume = {2006},
year = {2006}
}
@techreport{NationalAeronauticsandSpaceAdministration2004b,
address = {NASA-STD-8739.B},
author = {{National Aeronautics and Space Administration}},
institution = {National Aeronautics and Space Administration},
title = {{Software Assurance Standard}},
year = {2004}
}
@inproceedings{Lee2006b,
address = {New York, New York, USA},
author = {Lee, Christopher P. and Copeland, John A.},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179597},
isbn = {1595935495},
keywords = {folksonomy,honeynet,information visualization,network attack analysis,parallel coordinates,tagging,team collaboration,user interfaces},
month = {nov},
pages = {103},
publisher = {ACM Press},
title = {{Flowtag}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179597},
year = {2006}
}
@article{albrecht83,
author = {Albrecht, A.J. and Gaffney, J.E.},
doi = {10.1109/TSE.1983.235271},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {nov},
number = {6},
pages = {639--648},
title = {{Software Function, Source Lines of Code, and Development Effort Prediction: A Software Science Validation}},
url = {http://ieeexplore.ieee.org/document/1703110/},
volume = {9},
year = {1983}
}
@article{Graziotin2017,
abstract = {The growing literature on affect among software developers mostly reports on the linkage between happiness, software quality, and developer productivity. Understanding the positive side of happiness -- positive emotions and moods -- is an attractive and important endeavor. Scholars in industrial and organizational psychology have suggested that also studying the negative side -- unhappiness -- could lead to cost-effective ways of enhancing working conditions, job performance, and to limiting the occurrence of psychological disorders. Our comprehension of the consequences of (un)happiness among developers is still too shallow, and is mainly expressed in terms of development productivity and software quality. In this paper, we attempt to uncover the experienced consequences of unhappiness among software developers. Using qualitative data analysis of the responses given by 181 questionnaire participants, we identified 49 consequences of unhappiness while doing software development. We found detrimental consequences on developers' mental well-being, the software development process, and the produced artifacts. Our classification scheme, available as open data, will spawn new happiness research opportunities of cause-effect type, and it can act as a guideline for practitioners for identifying damaging effects of unhappiness and for fostering happiness on the job.},
archivePrefix = {arXiv},
arxivId = {1701.05789},
author = {Graziotin, Daniel and Fagerholm, Fabian and Wang, Xiaofeng and Abrahamsson, Pekka},
eprint = {1701.05789},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Graziotin et al. - 2017 - Consequences of Unhappiness While Developing Software.pdf:pdf},
keywords = {sentiment},
mendeley-tags = {sentiment},
month = {jan},
title = {{Consequences of Unhappiness While Developing Software}},
url = {http://arxiv.org/abs/1701.05789},
year = {2017}
}
@article{Geras2004a,
abstract = {Software organizations have typically de-emphasized the importance of software testing. In this paper, the results of a regional survey of software testing and software quality assurance techniques are described. Researchers conducted the study during the summer and fall of 2002 by surveying software organizations in the Province of Alberta. Results indicate that Alberta-based organizations tend to test less than their counterparts in the United States. The results also indicate that Alberta software organizations tend to train fewer personnel on testing-related topics. This practice has the potential for a two-fold impact: first, the ability to detect trends that lead to reduced quality and to identify the root causes of reductions in product quality may suffer from the lack of testing. This consequence is serious enough to warrant consideration, since overall quality may suffer from the reduced ability to detect and eliminate process or product defects. Second, the organization may have a more difficult time adopting methodologies such as extreme programming. This is significant because other industry studies have concluded that many software organizations have tried or will in the next few years try some form of agile method. Newer approaches to software development like extreme programming increase the extent to which teams rely on testing skills. Organizations should consider their testing skill level as a key indication of their readiness for adopting software development techniques such as test-driven development, extreme programming, agile modelling, or other agile methods.},
annote = {A survey of software testing practices in Alberta},
author = {Geras, A M and Smith, M R and Miller, J},
journal = {Canadian Journal of Electrical and Computer Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {183--191},
title = {{A survey of software testing practices in Alberta}},
url = {citeulike-article-id:3934623 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-4043152932{\&}{\#}38 partnerID=40},
volume = {29},
year = {2004}
}
@inproceedings{Guenther2010a,
address = {New York, New York, USA},
author = {Guenther, Jeffrey and Volk, Fred and Shaneck, Mark},
booktitle = {Proceedings of the Seventh International Symposium on Visualization for Cyber Security - VizSec '10},
doi = {10.1145/1850795.1850797},
isbn = {9781450300131},
keywords = {activity theory,cognitive task analysis,data exploration,natural user interfaces,network intrusion detection visualization},
month = {sep},
pages = {13--21},
publisher = {ACM Press},
title = {{Proposing a multi-touch interface for intrusion detection environments}},
url = {http://dl.acm.org/citation.cfm?id=1850795.1850797},
year = {2010}
}
@inproceedings{Jung2012,
author = {Jung, Min Yang and Kazanzides, P},
booktitle = {IEEE Intl. Conf. on Technologies for Practical Robot Applications (TePRA)},
title = {{Fault Detection and Diagnosis for Component-based Robotic Systems}},
year = {2012}
}
@misc{Hayes1997,
address = {Pittsburgh, PA},
author = {Hayes, Will and Over, James W},
isbn = {CMU/SEI-97-TR-001},
publisher = {Software Engineering Institute},
title = {{The Personal Software Process:  An Empirical Study of the Impact of PSP on Individual Engineers}},
year = {1997}
}
@techreport{PewResearchCenter2011a,
address = {Washington, D. C.},
author = {{Pew Research Center}},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pew Research Center - 2011 - 35 {\%} of American adults own a smartphone.pdf:pdf},
institution = {Pew Research Center},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{35 {\%} of American adults own a smartphone}},
url = {http://pewinternet.org/{~}/media//Files/Reports/2011/PIP{\_}Smartphones.pdf},
year = {2011}
}
@inproceedings{Walker2002,
address = {Perth, Australia.},
author = {Walker, Elizabeth and O'Neill, Loraine and Goody, A and Herrington, J and Northcote, M},
pages = {695--703},
title = {{IT courses and the IT industry:  Does the future rely on gender or generation?}},
year = {2002}
}
@inproceedings{5375531,
abstract = {Welcome to VizSec 2009! The 6th International Workshop on Visualization for Cyber Security continues to provide a forum bringing researchers and practitioners in information visualization and security together to address the specific needs of the cyber security community through new and insightful visualization techniques. VizSec 2009 continues the established practice of alternating our meeting between research conferences focused on cybersecurity, and researchers focused on analytics. This provides a balance between {\#}x201C;Viz {\#}x201D; (visualization and analytics) and {\#}x201C;Sec {\#}x201D; (cybersecurity). This balance is important {\#}x2014; as is the balance between practitioner goals and the interests of the long term researcher. While the immediate needs within the cybersecurity community are great, and visualization can provide much needed support, a focus only on the immediate analytical crisis will at best provide short bursts of improvement. Longer term research is also necessary, especially long term research that is undertaken with an eye towards improving the lot of the intended user. It is here that VizSec fills an important and unique niche.},
author = {Frincke, Deborah A and Gates, Carrie E and Goodall, John R},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375531},
pages = {iv --v},
title = {{Message from the workshop chairs}},
year = {2009}
}
@article{Thomas2000,
author = {Thomas, A and Benne, M R and Marr, M J and Thomas, E W and Hume, R M},
keywords = {briggs,mbti,myers,myers-briggs},
number = {1},
pages = {35--42},
title = {{The Evidence Remains Stable: The MBTI Predicts Attraction and Attrition in an Engineering Program}},
volume = {55},
year = {2000}
}
@article{Hindle2014,
author = {Hindle, Abram and Bird, Christian and Zimmermann, Thomas and Nagappan, Nachiappan},
doi = {10.1007/s10664-014-9312-1},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {jun},
number = {2},
pages = {479--515},
title = {{Do topics make sense to managers and developers?}},
url = {http://link.springer.com/10.1007/s10664-014-9312-1},
volume = {20},
year = {2014}
}
@inproceedings{1532059,
booktitle = {IEEE Workshop on Visualization for Computer Security, 2005. (VizSEC 05).},
doi = {10.1109/VIZSEC.2005.1532059},
isbn = {0-7803-9477-1},
pages = {i--viii},
publisher = {Ieee},
title = {{IEEE Workshop on Visualization for Computer Security 2005 (VizSEC 05)}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1532059},
year = {2005}
}
@inproceedings{Turner1999,
address = {New Orleans, LA, USA},
author = {Turner, Joseph A. and Zachary, Joseph L.},
booktitle = {SIGCSE 1999 - Proceedings of the 13th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/299649.299674},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turner, Zachary - 1999 - Using course-long programming projects in CS2.pdf:pdf},
isbn = {9781581130850},
issn = {0097-8418},
keywords = {CS2,Code maintenance,Education,Programming assignments,Software engineering},
mendeley-tags = {CS2},
pages = {43--47},
publisher = {ACM Press},
title = {{Using course-long programming projects in CS2}},
url = {http://portal.acm.org/citation.cfm?doid=299649.299674},
year = {1999}
}
@article{Kazanzides95,
author = {Kazanzides, P and Mittelstadt, B D and Musits, B L and Bargar, W L and Zuhars, J F and Williamson, B and Cain, P W and Carbone, E J},
journal = {IEEE Engin in Medicine and Biology Magazine},
number = {3},
title = {{An Integrated System for Cementless Hip Replacement}},
volume = {14},
year = {1995}
}
@article{Donzelli2006a,
author = {Donzelli, Paolo and Basili, Victor R},
journal = {Journal of Systems and Software},
keywords = {Journal,dependability},
mendeley-tags = {dependability},
number = {1},
pages = {107--119},
title = {{A Practical Framework for Eliciting and Modeling System Dependability Requirements: Experience from the NASA High Dependability Computing Project}},
volume = {79},
year = {2006}
}
@article{Turhan2009,
author = {Turhan, Burak and Menzies, Tim and Bener, Ayşe B. and {Di Stefano}, Justin},
doi = {10.1007/s10664-008-9103-7},
issn = {1382-3256},
journal = {Empirical Software Engineering},
month = {jan},
number = {5},
pages = {540--578},
title = {{On the relative value of cross-company and within-company data for defect prediction}},
url = {http://link.springer.com/10.1007/s10664-008-9103-7},
volume = {14},
year = {2009}
}
@article{Hall2012,
author = {Hall, T. and Beecham, S. and Bowes, D. and Gray, D. and Counsell, S.},
doi = {10.1109/TSE.2011.103},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hall et al. - 2012 - A Systematic Literature Review on Fault Prediction Performance in Software Engineering.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Analytical models,Bayes methods,Context modeling,Data models,Fault diagnosis,Predictive models,Software testing,Systematic literature review,Systematics,churn,contextual information,cost reduction,fault prediction models,fault prediction performance,fault prediction study,feature selection,independent variables,logistic regression,methodological information,naive Bayes,predictive performance,regression analysis,reliable methodology,simple modeling techniques,software engineering,software fault prediction,software fault tolerance,software quality,systematic literature review},
language = {English},
mendeley-tags = {churn},
month = {nov},
number = {6},
pages = {1276--1304},
publisher = {IEEE},
title = {{A Systematic Literature Review on Fault Prediction Performance in Software Engineering}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6035727},
volume = {38},
year = {2012}
}
@misc{Chen2006,
abstract = {A prototype test driven development tool for embedded systems has been developed with hardware-oriented extensions to CPPUnitLite. However xUnit tests are written in the language of the solution; problematic in the development of biomedical instruments as the customer, the "doctor", does not have "extensive knowledge of the domain". The biomedical application is often prototyped within MATLAB before movement down to the "plumbing level" on a high-speed, highly parallel, processor to meet the requirement for real-time application in a safe and secure manner "in the surgical theatre" or "on the ward". A long term research goal is an investigation of how to gain, as with standard business desktop system, the full advantage of using Fit and FitNesse as communication tools under these circumstances. We demonstrate the practical application of using indirection to permit a single set of Fit tests for both MATLAB and embedded system verification for a biomedical instrument. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {Making Fit / FitNesse appropriate for biomedical engineering research},
author = {Chen, J and Smith, M and Geras, A and Miller, J},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {186--190},
title = {{Making Fit / FitNesse appropriate for biomedical engineering research}},
url = {citeulike-article-id:3934569 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746200044{\&}{\#}38 partnerID=40},
volume = {4044 LNCS},
year = {2006}
}
@inproceedings{1297576,
abstract = { In the summer of 2000, the National Safe Skies Alliance awarded a project to the Applied Visualization Center at the University of Tennessee to develop a 3D computer tool to assist the US Federal Aviation Administration security group, now the Transportation Security Administration, in evaluating new equipment and procedures to improve airport checkpoint security. At the time of this writing, numerous detection equipment models, three specific airports, and a placement and passenger flow simulation tool for airport security planners have been developed. Similar tools have been constructed for simulating cargo/baggage inspection and other airport security operations. An extension of the original effort to consolidate the tools is currently underway.},
author = {Koch, D B},
booktitle = {Security Technology, 2003. Proceedings. IEEE 37th Annual 2003 International Carnahan Conference on},
doi = {10.1109/CCST.2003.1297576},
keywords = {3D computer tool,Applied Visualization Center,Cl},
pages = {298--304},
title = {{3D visualization to support airport security operations}},
year = {2003}
}
@inproceedings{4925082,
abstract = {The expansion of computer networks has lead to the increase of online network attacks. Therefore, an efficient method to detect and analyze network attacks is inevitable. As a result much research has been done on network visualization. This paper proposes a method which efficiently visualizes and analyzes network attacks using parallel coordinates. A brief review on the limitations on previous visualization methods and a structure which can analyze network attacks through visualization will be presented. Moreover, experimental results on visualization of scanning attacks, denial of service attacks and spoofing attacks using multi parallel coordinates will be shown.},
author = {Kim, Hoin and Lee, Inyong and Cho, Jaeik and Moon, Jongsub},
booktitle = {Computational Intelligence in Cyber Security, 2009. CICS '09. IEEE Symposium on},
doi = {10.1109/CICYBS.2009.4925082},
keywords = {attack analysis,computer networks,denial-of-servic},
pages = {1--8},
title = {{Visualization of network components for attack analysis}},
year = {2009}
}
@inproceedings{Henze2012b,
address = {New York, New York, USA},
author = {Henze, Niels and Rukzio, Enrico and Boll, Susanne},
booktitle = {Proceedings of the 30th Annual Conference on Human Factors in Computing Systems (CHI '12)},
doi = {10.1145/2207676.2208658},
isbn = {9781450310154},
keywords = {mobile phone,public study,touchscreen,virtual keyboard},
month = {may},
pages = {2659--2668},
publisher = {ACM Press},
title = {{Observational and experimental investigation of typing behaviour using virtual keyboards for mobile devices}},
url = {http://dl.acm.org/citation.cfm?id=2207676.2208658},
year = {2012}
}
@inproceedings{Tinkham2005,
abstract = {We teach a class on programmer-testing with a primary focus on test-driven development (TDD) as part of the software engineering curriculum at the Florida Institute of Technology. As of this writing, the course has been offered 3 times. Each session contained a mixture of undergraduate and graduate students. This paper discusses the evolution of the course, our lessons learned and plans for enhancing the course in the future. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {Experiences teaching a course in programmer testing},
author = {Tinkham, A and Kaner, C},
booktitle = {Proceedings - AGILE Confernce 2005},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {298--305},
title = {{Experiences teaching a course in programmer testing}},
url = {citeulike-article-id:3934818 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33847700945{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@misc{Vygotsky1978,
address = {Cambridge, MA},
author = {Vygotsky, L S and Cole, M and John-Steiner, V and S.Scribner and E.Souberman},
publisher = {Harvard University Press},
title = {{Mind in society: The development of higher psychological processes}},
year = {1978}
}
@article{AdityaAgrawal,
author = {{Aditya Agrawal}, Gyula Simon},
title = {{Semantic translation of Simulink/Stateflow models to hybrid automata using graph transformations}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.5069}
}
@article{Ynchausti2001,
abstract = {Unit testing was integrated into the software development process of a five-member programming team using a testduring-coding training module. The training approach and module are briefly described. Individual and pair developer performance was measured before and after the training module was presented. The improvements in quality achieved by the team ranged from 38 {\%} to 267{\%} fewer defects. Keywords extreme programming, software development process, test-during-coding, unit testing 1 SE  - 2nd International Conference on Extreme Programming and Flexible Processes in Software Engineering (XP 2001)},
author = {Ynchausti, Randy},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Integrating Unit Testing Into A Software Development Team`s Process}},
url = {citeulike-article-id:3934847 http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.5798},
year = {2001}
}
@inproceedings{1532067,
abstract = { We have designed an interactive visualization framework for the automated trust negotiation (ATN) protocol and we have implemented a prototype of the visualizer in Java. This framework provides capabilities to perform the interactive visualization of an ATN session, display credentials and policies, analyze the relations of negotiated components, and refine access control policies and negotiation strategies. We give examples of the visualization of ATN sessions and demonstrate the interactive features of the visualizer for the incremental construction of a trust target graph (TTG). Our prototype, which implements most components of the visualization framework, has played a key role in a research project that has developed working trust negotiation systems in an industrial environment.},
author = {Yao, Danfeng and Shin, M and Tamassia, R and Winsborough, W H},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532067},
keywords = {access control policy,automated trust negotiation},
pages = {65--74},
title = {{Visualization of automated trust negotiation}},
year = {2005}
}
@article{Goodall2008c,
abstract = {Networked computers are ubiquitous, and are subject to attack, misuse, and abuse. Automated systems to combat this threat are one potential solution, but most automated systems require vigilant human oversight. This automated approach under-values the strong analytic capabilities of humans. While automation affords opportunities for increased scalability, humans provide the ability to handle exceptions and novel patterns. One method to counteracting the ever increasing cyber threat is to provide the human security analysts with better tools to discover pattems, detect anomalies, identify correlations, and communicate their findings. This is what visualization for computer security (VizSec) researchers and developers are doing. VizSec is about putting robust information visualization tools into the hands of humans to take advantage of the power of the human perceptual and cognitive processes in solving computer security problems. This chapter is an introduction to the VizSec research community and the papers in this volume.},
author = {Goodall, John R},
editor = {Goodall, J R},
issn = {16123786},
journal = {Security},
pages = {1--17},
publisher = {Springer},
title = {{Introduction to Visualization for Computer Security}},
url = {http://www.springerlink.com/index/r361j11781l58411.pdf},
year = {2008}
}
@article{Fenton2001,
author = {Fenton, N E},
number = {3},
pages = {195--200},
title = {{Conducting and Presenting Empirical Software Engineering}},
volume = {6},
year = {2001}
}
@book{cook1979,
address = {Chicago},
author = {Cook, Thomas D. and Campbell, Donald Thomas},
publisher = {Rand McNally},
title = {{Quasi-experimentation: Design {\&} analysis issues for field settings}},
year = {1979}
}
@article{Garousi2006a,
address = {New York, New York, USA},
author = {Garousi, Vahid and Briand, Lionel C. and Labiche, Yvan},
doi = {10.1145/1134285.1134340},
isbn = {1595933751},
journal = {Proceeding of the 28th international conference on Software engineering - ICSE '06},
keywords = {distributed systems,model-based testing,stress testing,uml},
pages = {391},
publisher = {ACM Press},
title = {{Traffic-aware stress testing of distributed systems based on UML models}},
url = {http://portal.acm.org/citation.cfm?doid=1134285.1134340},
year = {2006}
}
@article{Boehm2001b,
author = {Boehm, B and Grunbacher, P and Briggs, B},
keywords = {win-win},
pages = {46--55},
title = {{Developing Groupware for Requirements Negotiation: Lessons Learned}},
year = {2001}
}
@book{Shaw2012,
address = {Berlin, Heidelberg},
author = {Shaw, Michael J},
doi = {10.1007/978-3-642-28115-0},
editor = {Daniel, Florian and Barkaoui, Kamel and Dustdar, Schahram},
isbn = {978-3-642-28114-3},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Business Information Processing},
title = {{Business Process Management Workshops}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-28115-0},
volume = {100},
year = {2012}
}
@inproceedings{Li2006a,
address = {New York, New York, USA},
author = {Li, L. and Liu, P. and Kesidis, G.},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179579},
isbn = {1595935495},
keywords = {animated topology,experiment specification,testbed,visual toolkit},
month = {nov},
pages = {7},
publisher = {ACM Press},
title = {{Visual toolkit for network security experiment specification and data analysis}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179579},
year = {2006}
}
@inproceedings{Orca2006,
author = {Makarenko, A and Brooks, A and Kaupp, T},
booktitle = {IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems (IROS), Workshop on Robotic Standardization},
month = {dec},
title = {{Orca: Components for Robotics}},
year = {2006}
}
@inproceedings{Falessi2013d,
abstract = {NASA anomaly databases are rich resources of software failure data in the field. These data are often captured in natural language that is not appropriate for trending or statistical analyses. This fast abstract describes a feasibility study of applying 60 natural language processing techniques for automatically classifying anomaly data to enable trend analyses.},
address = {Pasadena, CA},
author = {Falessi, Davide and Layman, Lucas},
booktitle = {2013 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
doi = {10.1109/ISSREW.2013.6688849},
isbn = {978-1-4799-2552-0},
keywords = {NLP,mypubs,natural language processing,software failure},
mendeley-tags = {mypubs},
month = {nov},
pages = {5--6},
publisher = {IEEE},
shorttitle = {Software Reliability Engineering Workshops (ISSREW},
title = {{Automated classification of NASA anomalies using natural language processing techniques}},
year = {2013}
}
@inproceedings{Smith2005,
abstract = {Test driven development (TDD) is one of the key Agile practices. A version of CppUnitLite was modified to meet the memory and speed constraints present on self-contained, high performance, digital signal processing (DSP) systems. The specific characteristics of DSP systems required that the concept of refactoring be extended to include concepts such as "refactoring for speed". We provide an experience report describing the instructor-related advantages of introducing an embedded test driven development tool E-TDD into third and fourth year undergraduate Computer Engineering Hardware-Software Co-design Laboratories. The TDD format permitted customer (instructor) hardware and software tests to be specified as "targets" so that the requirements for the components and full project were known "up-front". Details of CppUnitLit extensions necessary to permit tests specific for a small hardware-software co-design project, and lessons learnt when using the current E-TDD tool, are given. The next stage is to explore the use of the tool in an industrial context of a video project using the hybrid communication-media (HCM) dual core Analog Devices ADSP-BF561 Blackfin processor. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {E-TDD - Embedded test driven development a tool for hardware-software co-design projects},
author = {Smith, M and Kwan, A and Martin, A and Miller, J},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {145--153},
title = {{E-TDD - Embedded test driven development a tool for hardware-software co-design projects}},
url = {citeulike-article-id:3934801 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444461294{\&}{\#}38 partnerID=40},
volume = {3556},
year = {2005}
}
@inproceedings{Pautasso2005,
abstract = {Agile methodologies employ light-weight development practices emphasizing a test-driven approach to the development of software systems. Modern agile development environments support this approach by providing tools that automate most of the work required to effectively deal with change, including unit testing and different forms of refactoring. In this paper we discuss how to apply such techniques within the JOpera Visual Composition Language. More precisely, we show how we used the visual language to implement a regression testing framework for compositions written in the language itself, and how we introduced support in the visual environment for refactorings such as renaming, synchronization of service interface changes, and extraction/inlining across different levels of nesting. This is done in the context of the Web service composition tools provided with the JOpera for Eclipse research platform. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {JOpera: An agile environment for Web service composition with visual unit testing and refactoring},
author = {Pautasso, C},
booktitle = {Proceedings - 2005 IEEE Symposium on Visual Languages and Human-Centric Computing},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {311--313},
title = {{JOpera: An agile environment for Web service composition with visual unit testing and refactoring}},
url = {citeulike-article-id:3934752 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746523672{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@misc{Krebs2005,
address = {North Carolina State University, Department of Computer Science TR-2005-46 (http://goo.gl/PqS8WZ)},
author = {Krebs, Williams and Ho, Chih-Wei and Williams, Laurie and Layman, Lucas},
keywords = {mypubs},
mendeley-tags = {mypubs},
title = {{Rational Unified Process Evaluation Framework Version 1.0}},
year = {2005}
}
@article{Kraut1995,
author = {Kraut, Robert E. and Stredeter, Lynn A.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kraut, Stredeter - 1995 - Coordination in Software Development via genetic learning.pdf:pdf},
journal = {Communications of the ACM},
keywords = {collaboration},
mendeley-tags = {collaboration},
month = {aug},
number = {3},
pages = {69--81},
title = {{Coordination in Software Development via genetic learning}},
volume = {38},
year = {1995}
}
@article{Vieira2006,
address = {New York, New York, USA},
author = {Vieira, Marlon and Leduc, Johanne and Hasling, Bill and Subramanyan, Rajesh and Kazmeier, Juergen},
doi = {10.1145/1138929.1138932},
isbn = {1595934081},
journal = {Proceedings of the 2006 international workshop on Automation of software test - AST '06},
keywords = {gui verification,model based testing,uml},
pages = {9},
publisher = {ACM Press},
title = {{Automation of GUI testing using a model-driven approach}},
url = {http://portal.acm.org/citation.cfm?doid=1138929.1138932},
year = {2006}
}
@inproceedings{Bergin2005b,
abstract = {The purpose of this study was to investigate the relationship between self-regulated learning (SRL) and introductory programming performance. Participants were undergraduate students enrolled in an introductory computer programming module at a third-level (post-high school) institution. The instrument used in this study was designed to assess the motivations and learning strategies (cognitive, metacognitive and resource management strategies) of college students. The data gathered was analyzed to determine if a relationship existed between self-regulation and programming performance and investigate if SRL could be used to predict performance on the module. The study found that students who perform well in programming use more metacognitive and resource management strategies than lower performing students. In addition, students who have high levels of intrinsic motivation and task value perform better in programming and use more metacognitive and resource management strategies than students with low levels of intrinsic motivation and task value. Finally, a regression model based on cognitive, metacognitive and resource management strategies was able to account for 45{\%} of the variance in programming performance results. Copyright 2005 ACM.},
address = {Seattle, WA, USA},
author = {Bergin, Susan and Reilly, Ronan and Traynor, Desmond},
booktitle = {Proceedings of the 1st International Computing Education Research Workshop, ICER 2005},
doi = {10.1145/1089786.1089794},
isbn = {1595930434},
keywords = {CS1,Predictors,Programming,Self-regulated learning},
pages = {81--86},
publisher = {ACM Press},
title = {{Examining the role of self-regulated learning on introductory programming performance}},
url = {http://portal.acm.org/citation.cfm?doid=1089786.1089794},
year = {2005}
}
@article{Grottke2007,
author = {Grottke, Michael and Trivedi, Kishor S},
journal = {IEEE Computer},
number = {2},
pages = {107--109},
title = {{Fighting Bugs: Remove, Retry, Replicate, and Rejuvenate}},
volume = {40},
year = {2007}
}
@incollection{Akaike-InformationTheory,
annote = {(Reprint of the original 1973 paper)},
author = {Akaike, H},
booktitle = {Breakthroughs in Statistics - Volume I},
editor = {Kotz, S and Johnson, N L},
pages = {611--624},
title = {{Information Theory and an Extension of the Maximum Likelihood Principle}},
year = {1992}
}
@inproceedings{ray2014lang,
author = {Ray, Baishakhi and Posnett, Daryl and Filkov, Vladimir and Devanbu, Premkumar},
booktitle = {Proceedings of the ACM SIGSOFT 22nd International Symposium on the Foundations of Software Engineering},
organization = {ACM},
series = {FSE '14},
title = {{A Large Scale Study of Programming Languages and Code Quality in Github}},
year = {2014}
}
@article{Maimon2014,
author = {Maimon, David and Alper, Mariel and Sobesto, Bertrand and Cukier, Michel},
doi = {10.1111/1745-9125.12028},
issn = {00111384},
journal = {Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {nov},
number = {1},
pages = {33--59},
title = {{Restrictive Deterrent Effects of a Warning Banner in an Attacked Computer System}},
url = {http://doi.wiley.com/10.1111/1745-9125.12028},
volume = {52},
year = {2014}
}
@phdthesis{Kou2007,
abstract = {A recent focus of interest in software engineering research is on low-level software processes, which define how software developers or development teams should carry on development activities in short phases that last from several minutes to a few hours. Anecdotal evidence exists for the positive impact on quality and productivity of certain low-level software processes such as test-driven development and continuous integration. However, empirical research on low-level software processes often yields conflicting results. A significant threat to the validity of the empirical studies on low-level software processes is that they lack the ability to rigorously assess process conformance. That is to say, the degree to which developers follow the low-level software processes can not be evaluated. In order to improve the quality of empirical research on low-level software processes, I developed a technique called Software Development Stream Analysis (SDSA) that can infer development behaviors using automatically collected in-process software metrics. The collection of development activities is supported by Hackystat, a framework for automated software process and product metrics collection and analysis. SDSA abstracts the collected software metrics into a software development stream, a time-series data structure containing time-stamped development events. It then partitions the development stream into episodes, and then uses a rule-based system to infer low-level development behaviors exhibited in episodes. With the capabilities provided by Hackystat and SDSA, I developed the Zorro software system to study a specific low-level software process called Test-Driven Development (TDD). Experience reports have shown that TDD can greatly improve software quality with increased developer productivity, but empirical research findings on TDD are often mixed. An inability to rigorously assess process conformance is a possible explanation. Zorro can rigorously assess process conformance to a specific operational definition for TDD, and thus enable more controlled, comparable empirical studies. My research has demonstrated that Zorro can recognize the low-level software development behaviors that characterize TDD. Both the pilot and classroom case studies support this conclusion. The industrial case study shows that the automated data collection and development behavior inference has the potential to be useful for researchers.},
annote = {Automated Inference of Software Development Behaviors: Design, Implementation and Validation of Zorro for Test-Driven Development},
author = {Kou, Hongbing},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
publisher = {University of Hawaii, Department of Information and Computer Sciences,},
title = {{Automated Inference of Software Development Behaviors: Design, Implementation and Validation of Zorro for Test-Driven Development}},
url = {citeulike-article-id:3934683 http://csdl.ics.hawaii.edu/techreports/07-04/07-04.pdf},
year = {2007}
}
@article{VonMayrhauser1994,
author = {von Mayrhauser, A. and Vans, A. M.},
isbn = {0-8186-5855-X},
keywords = {maintenance,program comprehension},
mendeley-tags = {maintenance,program comprehension},
month = {may},
pages = {39--48},
title = {{Comprehension processes during large scale maintenance}},
url = {http://dl.acm.org/citation.cfm?id=257734.257741},
year = {1994}
}
@article{Cohoon2001,
author = {Cohoon, J McGrath},
journal = {Communications of the ACM},
number = {5},
pages = {108--114},
title = {{Toward improving female retention in the computer science major}},
volume = {44},
year = {2001}
}
@inproceedings{Saff2007,
abstract = {Writing developer tests as software is built can provide peace of mind. As the software grows, running the tests can prove that everything still works as the developer envisioned it. But what about the behavior the developer failed to envision? Although verifying a few well-picked scenarios is often enough, experienced developers know bugs can often lurk even in well-tested code, when correct but untested inputs provoke obviously wrong responses. This leads to worry. We suggest writing Theories alongside developer tests, to specify desired universal behaviors. We will demonstrate how writing theories affects test-driven development, how new features in JUnit can verify theories against hand-picked inputs, and how a new tool, Theory Explorer, can search for new inputs, leading to a new, less worrysome approach to development.},
annote = {Theory-infected or how i learned to stop worrying and love universal quantification},
author = {Saff, D},
booktitle = {Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {846--847},
title = {{Theory-infected or how i learned to stop worrying and love universal quantification}},
url = {citeulike-article-id:3934774 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42149166170{\&}{\#}38 partnerID=40},
year = {2007}
}
@techreport{DepartmentofDefense2012,
address = {MIL-STD-882E},
author = {{Department of Defense}},
booktitle = {systemsafetyskeptic.com},
institution = {Department of Defense},
number = {February 2000},
title = {{Standard Practice: System Safety}},
url = {http://systemsafetyskeptic.com/yahoo{\_}site{\_}admin/assets/docs/MIL-STD-882E{\_}final.135152939.pdf},
year = {2012}
}
@book{Yin2003,
address = {Thousand Oaks, CA},
author = {Yin, R K},
edition = {Third},
publisher = {Sage Publications},
title = {{Case Study Research: Design and Method}},
volume = {5},
year = {2003}
}
@inproceedings{Noel2004a,
address = {New York, New York, USA},
author = {Noel, Steven and Jajodia, Sushil},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029225},
isbn = {1581139748},
keywords = {clustered graphs,network attack graphs,network attack modeling,vulnerability analysis},
month = {oct},
pages = {109},
publisher = {ACM Press},
title = {{Managing attack graph complexity through visual hierarchical aggregation}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029225},
year = {2004}
}
@inproceedings{Williams2007,
abstract = {With today?s ever increasing demands on software, developers must produce software that can be changed without the risk of degrading the software architecture. Degraded software architecture is problematic because it makes the system more prone to defects and increases the cost of making future changes. The effects of making changes to software can be difficult to measure. One way to address software changes is to characterize their causes and effects. This paper introduces an initial architecture change characterization scheme created to assist developers in measuring the impact of a change on the architecture of the system. It also presents an initial study conducted to gain insight into the validity of the scheme. The results of this study indicated a favorable view of the viability of the scheme by the subjects, and the scheme increased the ability of novice developers to assess and adequately estimate change effort.},
address = {Madrid, Spain},
author = {Williams, Byron J. and Carver, Jeffrey C.},
booktitle = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
issn = {1938-6451},
keywords = {Computer architecture,Computer science,Costs,Degradation,Empirical Software Engineering,Measurement,Retirement,Software Engineering,Software architecture,Software engineering,Software maintenance,Software measurement,Software systems,TSAFE study,architecture change characterization scheme,software changes,software intensive system,software metrics},
language = {English},
mendeley-tags = {TSAFE study},
month = {sep},
pages = {410--419},
publisher = {IEEE},
series = {the First International Symposium on Empirical Software Engineering and Measurement.},
title = {{Characterizing Software Architecture Changes: An Initial Study}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=4343769' escapeXml='false'/{\%}3E},
year = {2007}
}
@inproceedings{Czerwonka2011,
address = {Berlin, Germany},
author = {Czerwonka, Jacek and Das, Rajiv and Nagappan, Nachiappan and Tarvo, Alex and Teterev, Alex},
booktitle = {2011 Fourth IEEE International Conference on Software Testing, Verification and Validation},
doi = {10.1109/ICST.2011.24},
isbn = {978-1-61284-174-8},
month = {mar},
pages = {357--366},
title = {{CRANE: Failure Prediction, Change Analysis and Test Prioritization in Practice -- Experiences from Windows}},
url = {http://ieeexplore.ieee.org/document/5770625/},
year = {2011}
}
@techreport{ISO/IEC/IEEE2010a,
address = {Geneva, Switzerland},
author = {ISO/IEC/IEEE},
keywords = {standard},
mendeley-tags = {standard},
publisher = {ISO},
title = {{24765:2010(E) Systems and software engineering - Vocabulary}},
volume = {2010},
year = {2010}
}
@inproceedings{Nawrocki2002,
address = {Essen, Germany},
annote = {Pretty crummy paper, but accepted to ICRE. Documents a few common problems with XP, though the solutions are poorly documented and untested.},
author = {Nawrocki, J and Jasinski, M and Walter, B and Wojciechowski, A},
keywords = {requirements,xp},
pages = {303--310},
title = {{Extreme Programming Modified: Embrace Requirements Engineering Practices}},
year = {2002}
}
@inproceedings{Anderson2014,
address = {Hyderabad, India},
author = {Anderson, Jeff and Salem, Saeed and Do, Hyunsook},
booktitle = {Proceedings of the 11th Working Conference on Mining Software Repositories - MSR 2014},
doi = {10.1145/2597073.2597084},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson, Salem, Do - 2014 - Improving the effectiveness of test suite through mining historical data.pdf:pdf},
isbn = {9781450328630},
pages = {142--151},
title = {{Improving the effectiveness of test suite through mining historical data}},
url = {http://dl.acm.org/citation.cfm?doid=2597073.2597084},
year = {2014}
}
@inproceedings{5375544,
abstract = {Passwords are a fundamental security vulnerability in many systems. Several researchers have investigated the tradeoff between password memorability versus resiliency to cracking and have looked at alternative systems such as graphical passwords and biometrics. To create stronger passwords, many systems enforce rules regarding the required length and types of characters passwords must contain. Another suggested approach is to use passphrases to combat dictionary attacks. One common ¿trick¿ used to remember passwords that conform to complex rules is to select a pattern of keys on the keyboard. While appearing random, the pattern is easy to remember. The purpose of this research was to investigate how often patterns are used, whether patterns could be classified into common categories, and whether those categories could be used to attack and defeat pattern-based passwords. Visualization techniques were used to collect data and assist in pattern categorization. The approach successfully identified two out of eleven passwords in a real-world password file that were not discovered with a traditional dictionary attack. This paper will present the approach used to collect and categorize patterns, and describe the resulting attack method that successfully identified passwords in a live system.},
author = {Schweitzer, D and Boleng, J and Hughes, C and Murphy, L},
booktitle = {Visualization for Cyber Security, 2009. VizSec 2009. 6th International Workshop on},
doi = {10.1109/VIZSEC.2009.5375544},
keywords = {dictionary attacks,keyboard pattern password visua},
pages = {69--73},
title = {{Visualizing keyboard pattern passwords}},
year = {2009}
}
@article{Mortensen2008,
abstract = {Aspect-based refactoring, called aspectualization, involves moving program code that implements cross-cutting concerns into aspects. Such refactoring can improve the maintainability of legacy systems. Long compilation and weave times, and the lack of an appropriate testing methodology are two challenges to the aspectualization of large legacy systems. We propose an iterative test driven approach for creating and introducing aspects. The approach uses mock systems that enable aspect developers to quickly experiment with different pointcuts and advice, and reduce the compile and weave times. The approach also uses weave analysis, regression testing, and code coverage analysis to test the aspects. We developed several tools for unit and integration testing. We demonstrate the test driven approach in the context of large industrial C++ systems, and we provide guidelines for mock system creation. {\^{A}}{\textcopyright} 2007 Elsevier B.V. All rights reserved.},
annote = {A test driven approach for aspectualizing legacy software using mock systems},
author = {Mortensen, M and Ghosh, S and Bieman, J M},
journal = {Information and Software Technology},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {7-8},
pages = {621--640},
title = {{A test driven approach for aspectualizing legacy software using mock systems}},
url = {citeulike-article-id:3934724 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-42649115446{\&}{\#}38 partnerID=40},
volume = {50},
year = {2008}
}
@misc{Kim2008,
abstract = {Due to the complexity and size of embedded software together with strong demands on time-to-market and quality, testing is a crucial point that should be addressed during software development. Traditionally, testing is carried out during the last phases of the software development life cycle. As a consequence, testing activities are often subject to high time pressure, which either results in delayed market introduction or low product quality. The validation of functional and real-time requirements of embedded systems is a difficult task. It usually needs the electronic control unit and the controlled hardware components. But very often the hardware components are not available for testing the control software at the beginning of the development. In this paper, we paper presents how test cases can be designed from use cases and how embedded control software can be validated without hardware components by simulating the test cases in early development phases using the AOP (Aspect Oriented Programming). For achieving an aspect oriented testable format, extended UML sequence diagrams are applied to formalize sequences of events, which have been specified in the use case scenarios. Provided that black box aspect oriented is used for developing embedded component applications, the monitoring of the dynamic behavior inside the components is not possible during simulation. But the simulated dynamic behavior is observable on the connections between the software components. In such a way monitored and recorded time stamp events are finally compared offline against the expected sequences of events specified in the test cases. The offline comparison validates the simulated behavior by demonstrating the fulfillment of user requirements and by detecting errors in case of contradictions during modeling. {\^{A}}{\textcopyright} 2008 Springer-Verlag Berlin Heidelberg.},
annote = {Aspect oriented testing frameworks for embedded software},
author = {Kim, H K},
booktitle = {Studies in Computational Intelligence},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {75--88},
title = {{Aspect oriented testing frameworks for embedded software}},
url = {citeulike-article-id:3934676 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-52049099621{\&}{\#}38 partnerID=40},
volume = {149},
year = {2008}
}
@inproceedings{Wright2006,
abstract = {One of the practices of Extreme Programming is Test-Driven Development (TDD), also known as Test-First Design. This style of development emphasizes an approach that is backwards to many programmers: writing tests before working code. Interestingly, this approach has found application in both agile methodologies and validation-intensive programming environments. TDD involves creating automated and repeatable unit tests, typically using a testing framework such as JUnit for Java and NUnit for Microsoft .NET languages. This paper will describe FUTS, a Framework for Unit Testing SAS{\^{A}}{\textregistered} programs in the spirit of JUnit and similar tools. Unit testing with FUTS is described, and an example of TDD in SAS is presented. In addition, pragmatic advice is given for dealing with challenges that commonly occur with automated testing.},
annote = {Drawkcab Gnimmargorp: Test-Driven Development with FUTS},
author = {Wright, J and Thotwave and Cary, N C},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Drawkcab Gnimmargorp: Test-Driven Development with FUTS}},
url = {citeulike-article-id:3934842 {\#}},
year = {2006}
}
@inproceedings{4622589,
abstract = {Denial of Service (DoS) attacks pose a serious threat to wireless networks. In WiMAX a large number of authentication requests sent to the Base Station (BS) by rouge Subscriber Stations (SSs) might lead to a DoS attack. Since authentication involves exchanging and checking certificates of both parties and is a computationally heavy task, the BS would allocate most of its resources for the process of evaluating certificates. An alternative for PKM-RSA is proposed in this paper which uses visual secret sharing in a pre-authentication scenario to avoid DoS attacks caused by the large number of rouge requests. A simple XOR operation is used to check the validity of requesting SS and BS both, thus providing mutual authentication scheme. It not only provides an additional layer of security but also successfully counters the above mentioned problem in the WiMAX authentication.},
author = {Altaf, Ayesha and Sirhindi, Rabia and Ahmed, Attiq},
booktitle = {2008 Second International Conference on Emerging Security Information, Systems and Technologies},
doi = {10.1109/SECURWARE.2008.52},
keywords = {DoS attacks,WiMAX authentication,base station,deni},
month = {aug},
pages = {238--242},
publisher = {Ieee},
title = {{A Novel Approach against DoS Attacks in WiMAX Authentication Using Visual Cryptography}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4622589},
year = {2008}
}
@article{Ali2007,
author = {Ali, Shaukat and Briand, Lionel C. and Rehman, Muhammad Jaffar-ur and Asghar, Hajra and Iqbal, Muhammad Zohaib Z. and Nadeem, Aamer},
doi = {10.1016/j.infsof.2006.11.002},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {automated testing,object-oriented systems,uml based testing},
month = {nov},
number = {11-12},
pages = {1087--1106},
title = {{A state-based approach to integration testing based on UML models}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0950584906001893},
volume = {49},
year = {2007}
}
@inproceedings{Falkner2012,
abstract = {How do we identify students who are at risk of failing our courses? Waiting to accumulate sufficient assessed work incurs a substantial lag in identifying students who need assistance. We want to provide students with support and guidance as soon as possible to reduce the risk of failure or disengagement. In small classes we can monitor students more directly and mark graded assessments to provide feedback in a relatively short time but large class sizes, where it is most easy for students to disappear and ultimately drop out, pose a much greater challenge. We need reliable and scalable mechanisms for identifying at-risk students as quickly as possible, before they disengage, drop out or fail. The volumes of student information retained in data warehouse and business intelligence systems are often not available to lecturing staff, who can only observe the course-level marks for previous study and participation behaviour in the current course, based on attendance and assignment submission. We have identified a measure of ``at-risk'' behaviour that depends upon the timeliness of initial submissions of any marked activity. By analysing four years of electronic submissions over our school's student body we have extracted over 220,000 individual records, spanning over 1900 students, to establish that early electronic submission behaviour provides can provide a reliable indicator of future behaviour. By measuring the impact on a student's Grade Point Average (GPA) we can show that knowledge of assignment submission and current course level provides a reliable guide to student performance.},
address = {Auckland, New Zealand},
annote = {Students who submit their first assignment on time are in good shape, those who do not are at risk as the behavior persists. GPA correlates with this behavior longitudinally.},
author = {Falkner, Nickolas J.G. and Falkner, Katrina},
booktitle = {ICER'12 - Proceedings of the 9th Annual International Conference on International Computing Education Research},
doi = {10.1145/2361276.2361288},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Falkner, Falkner - 2012 - A fast measure for identifying at-risk students in computer science.pdf:pdf},
isbn = {9781450316040},
keywords = {Assessment,CS2,Engagement},
mendeley-tags = {CS2},
pages = {55--62},
publisher = {ACM Press},
title = {{A fast measure for identifying at-risk students in computer science}},
url = {http://dl.acm.org/citation.cfm?doid=2361276.2361288},
year = {2012}
}
@inproceedings{ostrand10,
author = {Ostrand, Thomas J and Weyuker, Elaine J and Bell, Robert M},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
pages = {19:1----19:10},
series = {PROMISE '10},
title = {{Programmer-based fault prediction}},
year = {2010}
}
@techreport{Czerny2005,
address = {Warrendale, PA},
author = {Czerny, Barbara J. and D'Ambrosio, Joseph G. and Murray, Brian T. and Sundaram, Padma},
booktitle = {Engineering},
institution = {SAE International},
number = {724},
publisher = {Citeseer},
title = {{Effective Application of Software Safety Techniques for Automotive Embedded Control Systems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.5257{\&}rep=rep1{\&}type=pdf},
year = {2005}
}
@inproceedings{Maxwell2017,
abstract = {{\textcopyright} 2017 ACM. Context-based CS1 courses focusing on Media Computation, Robotics, Games, or Art have been shown to improve out- comes such as retention and gender balance, both important factors in CS education. Colby College has offered a Visual Media focused CS1 course since 2008, and in response to faculty and student feedback, we expanded our curriculum to include a second context-based CS1 course focused on Science applications. Our goal was to have completely different projects but teach the same fundamental concepts. In order to measure whether students in each version were learning the same concepts, and to reduce confounding factors, the same professors co-taught both versions of CS1 and students completed the same homework, quizzes, and final exam. Our analysis of the quiz, final exam, and final overall performance showed no statistically significant difference by context or by gender. There was also no difference by con- text or gender in whether students took additional CS courses in the following two semesters. Furthermore, as a percentage of the students eligible to take the next offering of CS2, Data Structures and Algorithms, 48{\%} of the students in these two offerings of CS1 registered for CS2, with no significant difference between contexts. Our conclusion is that we were successful in achieving similar outcomes, and the benefits of context-based CS1 courses, in both the Visual Media and Science versions of the course.},
address = {Seattle, WA, USA},
author = {Maxwell, Bruce A. and Taylor, Stephanie R.},
booktitle = {Proceedings of the Conference on Integrating Technology into Computer Science Education, ITiCSE},
doi = {10.1145/3017680.3017757},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maxwell, Taylor - 2017 - Comparing outcomes across different contexts in CS1.pdf:pdf},
isbn = {9781450346986},
keywords = {CS 1,CS2,Computational thinking,Context-based learning},
mendeley-tags = {CS2},
pages = {399--403},
publisher = {ACM Press},
title = {{Comparing outcomes across different contexts in CS1}},
url = {http://dl.acm.org/citation.cfm?doid=3017680.3017757},
year = {2017}
}
@inproceedings{DeSouza2003,
author = {de Souza, Cleidson .R.B. and Redmiles, David and Mark, Gloria and Penix, John and Sierhuis, Maarten},
booktitle = {Proceedings of the 2003 International Symposium on Empirical Software Engineering},
doi = {10.1109/ISESE.2003.1237990},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/de Souza et al. - 2003 - Management of interdependencies in collaborative software development.pdf:pdf},
isbn = {0-7695-2002-2},
keywords = {collaboration},
mendeley-tags = {collaboration},
pages = {294--303},
publisher = {IEEE Comput. Soc},
title = {{Management of interdependencies in collaborative software development}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1237990},
year = {2003}
}
@inproceedings{Gray1986,
author = {Gray, J},
booktitle = {Proc.$\backslash$ 5th Symposium on Reliability in Distributed Software and Database Systems},
pages = {3--12},
title = {{Why do computers stop and what can be done about it?}},
year = {1986}
}
@article{TrivediGrottkeAndrade2010,
author = {Trivedi, Kishor S and Grottke, Michael and Andrade, Ermeson C},
journal = {International Journal of Systems Assurance Engineering and Management},
number = {4},
pages = {340--350},
title = {{Software fault mitigation and availability assurance techniques}},
volume = {1},
year = {2010}
}
@inproceedings{me05a,
annote = {Available from
$\backslash$url{\{}http://menzies.us/pdf/05safewhen.pdf{\}}},
author = {Menzies, Tim and Chen, Zhihao and Port, Dan and Hihn, Jairus},
booktitle = {Proceedings, PROMISE workshop, ICSE 2005},
title = {{Simple Software Cost Estimation: Safe or Unsafe?}},
year = {2005}
}
@inproceedings{Edwards2003,
abstract = {A new approach to teaching software testing is proposed: students use test-driven development on programming assignments, and an automated grading tool assesses their testing performance and provides feedback. The basics of the approach, screenshots of the sytem, and a discussion of industrial tool use for grading Java programs are discussed.},
annote = {Teaching software testing: automatic grading meets test-first coding},
author = {Edwards, S H},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {318--319},
publisher = {ACM New York, NY, USA},
title = {{Teaching software testing: automatic grading meets test-first coding}},
url = {citeulike-article-id:3934595 {\#}},
year = {2003}
}
@article{Ambler2006,
abstract = {A survey conducted on the agile software development methods and techniques, which are gaining increasing attention within the IT industry is discussed. The survey reports show that organizations such as Shine Technologies have adopted the agile method such as Extreme Programming (EP), Scrum, Agile MSF, AUP, and in particular FDD. The organization has also adopted agile development techniques such as Test Driven Development (TDD) or pair programming. Agile database development techniques including database refactoring and database regression testing are also beginning to attract attention. The survey shows that the adoption on agile approaches to software development has successfully affected the overall productivity and the quality of the systems that they delivered. Agile software development's focus on collaborative techniques, such as active stakeholder participation and increased feedback, have also helped to improve stakeholder satisfaction.},
annote = {Survey says: Agile works in practice},
author = {Ambler, S W},
journal = {Dr. Dobb's Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {9},
pages = {62--64},
title = {{Survey says: Agile works in practice}},
url = {citeulike-article-id:3934537 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33748319380{\&}{\#}38 partnerID=40},
volume = {31},
year = {2006}
}
@article{Tadikamalla,
author = {Tadikamalla, Pandu R},
journal = {Communications in Statistics -- Simulation and Computation},
number = {1},
pages = {305--314},
title = {{Kolmogorov-{\{}S{\}}mirnov Type Test-Statistics For The Gamma, {\{}E{\}}rlang-2 And The Inverse {\{}G{\}}aussian Distributions When The Parameters Are Unknown}},
volume = {19},
year = {1990}
}
@book{Prasad1996,
address = {Upper Saddle River, NJ},
author = {Prasad, Biren},
publisher = {Prentice Hall},
title = {{Concurrent Engineering Fundamentals - Integrated Product and Process Organization}},
year = {1996}
}
@book{Miles1994a,
address = {Thousand Oaks, CA},
author = {Miles, M B and Huberman, A M},
keywords = {case studies,constant comparison,qualitative},
publisher = {Sage},
title = {{Qualitative Data Analysis: An Expanded Sourcebook}},
year = {1994}
}
@misc{Wang2004,
abstract = {Test-Driven Development (TDD) is a coding technique in which programmers write unit tests before writing or revising production code. We present a process measurement approach for TDD that relies on the analysis of fine-grained data collected during coding activities. This data is mined to produce abstractions regarding programmers' work patterns. Programmers, instructors, and coaches receive concrete feedback by visualizing these abstractions. Process measurement has the potential to accelerate the learning of TDD, enhance its effectiveness, aid in its empirical evaluation, and support project tracking. {\^{A}}{\textcopyright} Springer-Verlag 2004.},
annote = {The role of process measurement in test-driven development},
author = {Wang, Y and Erdogmus, H},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {32--42},
title = {{The role of process measurement in test-driven development}},
url = {citeulike-article-id:3934830 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35048865413{\&}{\#}38 partnerID=40},
volume = {3134},
year = {2004}
}
@inproceedings{5655042,
abstract = {Baggage scanning systems are widely used at security checkpoint in airports for homeland security applications. However, the CT baggage images suffer from background noise and the presence of low contrast. To address this problem, this paper introduces a new 3D CT baggage image enhancement algorithm using order statistic decomposition. Computer simulation and comparisons are given to demonstrate the excellent enhancement performance of the presented new algorithm and its capability of significantly improving visual quality of CT images while removing the background noise.},
author = {Zhou, Yicong and Panetta, Karen and Agaian, Sos},
booktitle = {2010 IEEE International Conference on Technologies for Homeland Security (HST)},
doi = {10.1109/THS.2010.5655042},
isbn = {978-1-4244-6047-2},
keywords = {3D CT baggage image enhancement,CT images quality},
month = {nov},
pages = {287--291},
publisher = {Ieee},
title = {{3D CT baggage image enhancement based on order statistic decomposition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5655042},
year = {2010}
}
@article{Furnas1987,
author = {Furnas, G W and Landauer, T K and Gomez, L M and Dumais, S T},
journal = {Communications of the ACM},
number = {11},
pages = {964--971},
title = {{The Vocabulary Problem in Human-System Communication}},
volume = {30},
year = {1987}
}
@techreport{Norton2013a,
author = {Norton},
institution = {Norton by Symantec},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{Norton Online Family Report 2012}},
year = {2013}
}
@incollection{Allport1989,
address = {Cambridge, MA},
author = {Allport, Alan and Posner, Michael I},
keywords = {attention,psychology},
pages = {631--682},
publisher = {The MIT Press},
title = {{Visual Attention}},
year = {1989}
}
@article{Daeninck2006,
abstract = {Despite the existence of 75 “different” xUNIT frameworks, their domain of application differs only in the programming language, compiler or operating system supported. If one is working in the embedded world, unit testing is still needed, but now our “testing requirements” differ significantly from the testing framework needed for the desktop world. Embedded systems often have significant non-functional requirements, which demand validation at the unit level. In addition, they interact intimately with hardware resources and often have only very limited input/output capabilities – imagine a xUNIT framework where printing to the screen is a technical challenge! SE  - Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
annote = {Source: Scopus Export Date: 8 January 2009},
author = {Daeninck, S and Smith, M and Miller, J and Ko, L},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Extending the embedded system E-TDD unit test driven development tool for development of a real time video security system prototype}},
url = {citeulike-article-id:3934579 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33746200036{\&}{\#}38 partnerID=40},
year = {2006}
}
@article{Edwards2003c,
abstract = {Students need to learn more software testing skills. This paper presents an approach to teaching software testing in a way that will encourage students to practice testing skills in many classes and give them concrete feedback on their testing performance, without requiring a new course, any new faculty resources, or a significant number of lecture hours in each course where testing will be practiced. The strategy is to give students basic exposure to test-driven development, and then provide an automated tool that will assess student submissions on-demand and provide feedback for improvement. This approach has been demonstrated in an undergraduate programming languages course using a prototype tool. The results have been positive, with students expressing appreciation for the practical benefits of test-driven development on programming assignments. Experimental analysis of student programs shows a 28{\%} reduction in defects per thousand lines of code. Copyright {\^{A}}{\textcopyright} 2005 ACM, Inc.},
annote = {Improving student performance by evaluating how well students test their own programs},
author = {Edwards, S H},
journal = {ACM Journal on Educational Resources in Computing},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
title = {{Improving student performance by evaluating how well students test their own programs}},
url = {citeulike-article-id:3934596 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-21244466765{\&}{\#}38 partnerID=40},
volume = {3},
year = {2003}
}
@inproceedings{5066477,
abstract = {Security and requirements engineering are one of the most important factor of success in the development of a software product line due to the complexity and extensive nature of them, given that a weakness in security can cause problems throughout all the products of a product line. However, without a CARE (computer-aided requirements engineering) tool, the application of any security requirements engineering process or methodology is much more difficult because it has to be manually performed. Therefore, in this paper, we will present a prototype of SREPPLineTool, which provides automated support to facilitate the application of the security quality requirements engineering process for software product lines, SREPPLine. SREPPLineTool simplifies the management of security requirements in product lines by providing us with a guided, systematic and intuitive way to deal with them from the early phases of product lines development, simplifying the management and the visualization of the artefacts variability and traceability links and the integration of the security standards, as well as the management of the security reference model proposed by SREPPLine. Finally we shall illustrate the application of SREPPLineTool by describing a simple example as a preliminary validation of it.},
author = {Mellado, D and Rodriguez, J and Fernandez-Medina, E and Piattini, M},
booktitle = {Availability, Reliability and Security, 2009. ARES '09. International Conference on},
doi = {10.1109/ARES.2009.23},
keywords = {CARE,SREPPLineTool,automated support,computer-aide},
month = {mar},
pages = {224--231},
title = {{Automated Support for Security Requirements Engineering in Software Product Line Domain Engineering}},
year = {2009}
}
@misc{TheInstituteofElectricalandElectronicsEngineers1990,
address = {New York, New York},
author = {{The Institute of Electrical and Electronics Engineers}},
publisher = {IEEE},
title = {{IEEE Std 610.12-1990}},
year = {1990}
}
@techreport{Higuera1996,
address = {CMU/SEI-96-TR-012},
author = {Higuera, Ronald P and Haimes, Yacov Y},
institution = {Carnegie-Mellon University Software Engineering Institute},
title = {{Software Risk Management}},
url = {http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2{\&}doc=GetTRDoc.pdf{\&}AD=ADA310913},
year = {1996}
}
@inproceedings{Crocker2001,
address = {Villasimius, Italy},
author = {Crocker, Ron},
booktitle = {Proceedings of the 2nd Int'l. Conference on Extreme Programming and Agile Processes in Software Engineering (XP2001)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Crocker - 2001 - The 5 reasons XP can't scale and what to do about them.pdf:pdf},
keywords = {agile},
mendeley-tags = {agile},
pages = {62--65},
title = {{The 5 reasons XP can't scale and what to do about them}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.18.9099{\&}rep=rep1{\&}type=pdf},
year = {2001}
}
@inproceedings{Bohnet2005,
abstract = {Traditional Test-Driven Development focuses on development of new units (classes) driven by programmer-facing unit tests. This paper describes our experiences when using business-facing tests (also known as "story tests") to guide the porting of a legacy application. Domain experts specified tests in a tabular format using Excel spreadsheets. Developers automated these spreadsheets in various ways over time: scripts, generation of JUnit source code, and Fit. These tests were run against the legacy system and guided the development of the newly ported system. We found test-driven porting to be an effective way to port a complex application. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {Test-driven porting},
author = {Bohnet, R and Meszaros, G},
booktitle = {Proceedings - AGILE Confernce 2005},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {259--266},
title = {{Test-driven porting}},
url = {citeulike-article-id:3934557 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33847721097{\&}{\#}38 partnerID=40},
volume = {2005},
year = {2005}
}
@inproceedings{5168090,
abstract = {Being aware of maritime activities is important for homeland security. We have worked with the US Coast Guard to develop a waterway visual surveillance system on Elba Island, in Savannah, GA for maritime awareness. With the system, we developed an enhanced temporal difference algorithm based on active zone (ETD-AZ) to detect waterway moving objects, including container ships, commercial vessels, small vessels, sailboats, tugboats, and dolphins, etc. In the algorithm, active zone is first defined to model the imaging locations of moving objects that appear in the waterway; the model reduces noises caused by winds, clouds, and waves in the near-view fields. A thresholded difference map is computed by using temporal difference and thresholding methods. The thresholded difference map is further processed by applying morphological operations to remove noises. A final decision rule is derived to decide whether an image contains moving objects or not based on motion pixels in thresholded difference map and in the difference map after morphological operation. The proposed ETD-AZ algorithm has been tested using 21, 844 night and day images taken from Aug. 1-2, 2008, in different weather conditions: sunny, cloudy, windy, stormy with lightening, etc. The algorithm detected 89 of 94 waterway moving objects with 5.3{\%} false negative rate while maintaining a low false positive rate of 4.0{\%} for a total of 21, 478 non-object images. When focusing only on vessels, it detected 78 of 79 moving vessels with a 1.3{\%} false negative rate.},
author = {Tsai, Yichang and Hu, Zhaozheng and Siplon, P},
booktitle = {Technologies for Homeland Security, 2009. HST '09. IEEE Conference on},
doi = {10.1109/THS.2009.5168090},
keywords = {ETD-AZ algorithm,Elba Island,GIS-enabled maritime},
month = {may},
pages = {584--590},
title = {{GIS-enabled maritime awareness system (GMAS)}},
year = {2009}
}
@inproceedings{Saff2005,
address = {Long Beach, CA},
author = {Saff, David and Artzi, Shay and Perkins, Jeff H. and Ernst, Michael D.},
booktitle = {Proceedings of the 20th IEEE/ACM international Conference on Automated software engineering - ASE '05},
doi = {10.1145/1101908.1101927},
isbn = {1595939934},
keywords = {a software,available,each individual changed component,end-to-end,focused tests are not,instead,long-running,mock objects,often,system may have system,test factoring,tests,unit testing},
pages = {114--123},
publisher = {ACM Press},
title = {{Automatic test factoring for java}},
url = {http://portal.acm.org/citation.cfm?doid=1101908.1101927},
year = {2005}
}
@article{me08a,
annote = {Available from
$\backslash$url{\{}http://menzies.us/pdf/07ivv.pdf{\}}},
author = {Menzies, Tim and Benson, Markland and Costello, Ken and Moats, Christina and Northey, Michelle and Richarson, Julian},
journal = {Innovations in Systems and Software Engineering},
month = {mar},
title = {{Learning Better {\{}IV{\&}V{\}} Practices}},
year = {2008}
}
@inproceedings{azhar13,
author = {Azhar, D and Riddle, P and Mendes, E and Mittas, N and Angelis, L},
booktitle = {Empirical Software Engineering and Measurement, 2013 ACM / IEEE International Symposium on},
doi = {10.1109/ESEM.2013.25},
issn = {1938-6451},
month = {oct},
pages = {173--182},
title = {{Using Ensembles for Web Effort Estimation}},
year = {2013}
}
@inproceedings{Edwards2003a,
abstract = {There is a need for better ways to teach software testing skills to computer science undergraduates, who are routinely underprepared in this area. This paper proposes the use of test-driven development in the classroom, requiring students to test their own code in programming assignments. In addition, an automated grading approach is used to assess student-written code and student-written tests together. Students receive clear, immediate feedback on the effectiveness and validity of their testing. This approach has been piloted in an undergraduate computer science class. Results indicate that students scored higher on their program assignments while producing code with 45{\%} fewer defects per thousand lines of code.},
annote = {Using test-driven development in the classroom: providing students with automatic, concrete feedback on performance},
author = {Edwards, S},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Using test-driven development in the classroom: providing students with automatic, concrete feedback on performance}},
url = {citeulike-article-id:3934593 http://www.onlined.org/papers/000145.pdf},
volume = {3},
year = {2003}
}
@inproceedings{Steel2004,
abstract = {Tefkat is an implementation of a rule- and pattern-based engine for the transformation of models defined using the Object Management Group's (OMG) Model-Driven Architecture (MDA). The process for the development of the engine included the concurrent development of a unit test suite for the engine. The test suite is constructed as a number of models, whose elements comprise the test cases, and which are passed to a test harness for processing. The paper discusses the difficulties and opportunities encountered in the process, and draws implications for the broader problem of testing in a model-driven environment, and of using models for testing. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {Model-based test driven development of the tefkat model-transformation engine},
author = {Steel, J and Lawley, M},
booktitle = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {151--160},
title = {{Model-based test driven development of the tefkat model-transformation engine}},
url = {citeulike-article-id:3934806 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-16244362079{\&}{\#}38 partnerID=40},
year = {2004}
}
@inproceedings{4804444,
abstract = {Large corporations and government agencies are continually bombarded by malicious network attacks through the cyber infrastructure. One common method to identify and assess the impacts of these malicious activities is through the monitoring and analysis of network flow data. While already somewhat aggregated, the data can quickly become overwhelming - a billion flow records a day for large organizations is not abnormal. We have integrated our visual analytics toolkit with network flow data to provide a seamless workflow for computer network defense analysts. This integration can facilitate the learning process of novice analysts and make expert analysts more productive.},
author = {Goodall, J R and Tesone, D R},
booktitle = {Conference For Homeland Security, 2009. CATCH '09. Cybersecurity Applications Technology},
doi = {10.1109/CATCH.2009.47},
keywords = {computer network defense analysts,cyber infrastruc},
month = {mar},
pages = {199--204},
title = {{Visual Analytics for Network Flow Analysis}},
year = {2009}
}
@inproceedings{Saha2008,
address = {New York, New York, USA},
author = {Saha, Diptikalyan},
booktitle = {Proceedings of the 15th ACM conference on Computer and communications security - CCS '08},
doi = {10.1145/1455770.1455780},
isbn = {9781595938107},
keywords = {attack graphs,incremental analysis,logic programming},
month = {oct},
pages = {63},
publisher = {ACM Press},
title = {{Extending logical attack graphs for efficient vulnerability analysis}},
url = {http://dl.acm.org/citation.cfm?id=1455770.1455780},
year = {2008}
}
@inproceedings{VanKasteren2008,
address = {New York, NY},
author = {van Kasteren, Tim and Noulas, Athanasios and Englebienne, Gwenn and Kr{\"{o}}se, Ben},
booktitle = {Proceedings of the 10th International Conference on Ubiquitous Computing (UbiComp '08)},
doi = {10.1145/1409635.1409637},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/van Kasteren et al. - 2008 - Accurate activity recognition in a home setting.pdf:pdf},
isbn = {9781605581361},
keywords = {activity recognition,agile,annotation,dataset,nsf,probabilistic models,sensor networks},
mendeley-tags = {agile,nsf},
month = {sep},
pages = {1--9},
publisher = {ACM Press},
title = {{Accurate activity recognition in a home setting}},
url = {http://dl.acm.org/citation.cfm?id=1409635.1409637},
year = {2008}
}
@article{Sengupta2004,
abstract = {In a global software development project, distributed teams need to have a consistent view of the system even in the face of frequently changing requirements. Thus how precisely requirements and changes therein are communicated to remote developers becomes a critical issue. In this position paper, we hypothesize that a test-driven methodology may help keep development across multiple sites consistent with changing requirements and with each other},
annote = {Test-driven global software development},
author = {Sengupta, B and Sinha, V and Chandra, S and Sampath, S and Prasad, K G},
isbn = {0863414273},
journal = {"Third International Workshop on Global Software Development GSD 2004" W12S Workshop 26th International Conference on Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {39--41},
title = {{Test-driven global software development}},
url = {citeulike-article-id:3934787 {\#}},
year = {2004}
}
@inproceedings{Shabtai2006b,
address = {New York, New York, USA},
author = {Shabtai, Asaf and Klimov, Denis and Shahar, Yuval and Elovici, Yuval},
booktitle = {Proceedings of the 3rd international workshop on Visualization for computer security - VizSEC '06},
doi = {10.1145/1179576.1179580},
isbn = {1595935495},
keywords = {human-computer interaction,intelligent visualization,knowledge-based systems,security,temporal-abstraction},
month = {nov},
pages = {15},
publisher = {ACM Press},
title = {{An intelligent, interactive tool for exploration and visualization of time-oriented security data}},
url = {http://dl.acm.org/citation.cfm?id=1179576.1179580},
year = {2006}
}
@book{kalton1983introduction,
author = {Kalton, Graham},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {35},
publisher = {SAGE Publications, Incorporated},
title = {{Introduction to survey sampling}},
volume = {7},
year = {1983}
}
@inproceedings{1425062,
abstract = { Current mechanisms for authenticating communication between devices that share no prior context are inconvenient for ordinary users, without the assistance of a trusted authority. We present and analyze seeing-is-believing, a system that utilizes 2D barcodes and camera-telephones to implement a visual channel for authentication and demonstrative identification of devices. We apply this visual channel to several problems in computer security, including authenticated key exchange between devices that share no prior context, establishment of a trusted path for configuration of a TCG-compliant computing platform, and secure device configuration in the context of a smart home.},
author = {McCune, J.M. and Perrig, a. and Reiter, M.K.},
booktitle = {2005 IEEE Symposium on Security and Privacy (S{\&}P'05)},
doi = {10.1109/SP.2005.19},
isbn = {0-7695-2339-0},
issn = {1081-6011},
keywords = {2D barcodes,TCG-compliant computing platform,aut},
month = {may},
pages = {110--124},
publisher = {Ieee},
title = {{Seeing-Is-Believing: Using Camera Phones for Human-Verifiable Authentication}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1425062},
year = {2005}
}
@inproceedings{Katira2004,
address = {Norfolk, VA},
author = {Katira, Neha and Williams, Laurie and Wiebe, Eric and Miller, C and Balik, Suzanne and Gehringer, Ed},
pages = {7--11},
publisher = {ACM Press},
title = {{On Understanding Compatibility of Student Pair Programmers}},
year = {2004}
}
@article{Jones2004,
abstract = {In industry experiments using test-driven development (TDD), some researchers report significantly increased code quality over traditional test-last approaches. Not surprisingly computing and information technology educators have begun to call for the introduction of TDD into the curriculum. This paper explores the pedagogical experience to date in using a test-first approach in the classroom. Selected studies include four experience reports, one conceptual paper, and three experiments comparing TDD against control groups. Issues in operationalizing TDD across the curriculum are examined, including programming language assertion mechanisms, the feasibility of employing test frameworks, and the automated verification of student test plans. Recommendations derived from the literature are presented.},
annote = {Test-driven development goes to school},
author = {Jones, C G},
journal = {Journal of Computing Sciences in Colleges},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {1},
pages = {220--231},
title = {{Test-driven development goes to school}},
url = {citeulike-article-id:3934664 http://portal.acm.org/citation.cfm?id=1040231.1040261},
volume = {20},
year = {2004}
}
@article{Carroll1979,
abstract = {Two stud/es of design problem solving are reported. Experiment 1 presents an observational study of an actual client designer worksession. Analysis of the session transcript reveals a systematically structured interaction. The client and the designer decompose the overall design problem into subproblems, each of which is smaller and somewhat more well structured than the overall problem. Experiment 2 is a laboratory study. The 'client' role is simulated by an instruction booklet; subjects play the 'designer' role. Again, it is found that subjects spontaneously structure the elements of a design problem into subproblems the nature of which is systematically related to aspects of problem structure. There is high intersubject agreement as to how the decomposition into subproblems should proceed.},
annote = {{\textless}m:note{\textgreater}In one experiment between client and designer, the client described general goals or high level problems, then "cycled" through its subproblems and sub-subproblems.  Then the next goal was presented.  {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}The order in which the goals are addressed can affect the solutions of later goals.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}"Several questions are suggested by our analysis of the library design session. First, why is the session organized into cycles at all? Why doesn't the client simply list all of{\textless}m:linebreak/{\textgreater}the requirements at the very outset of the session. It seems strange that a client, such as our librarian, who is well enough organized to structure the first five cycles of the session hierarchically, would not present the substance of all seven cycles immediately. It is stranger yet, since, as noted above, the solutions of the final two cycles alter the solutions for the first five. One hypothesis is based functionally: the limited memory and attention span of client and designer encourages them to decompose a 'complex' design problem into several simpler design cycles, each concerned specifically with a part of the overall design goal."{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}"A second question raised by our analysis is why the cycles should be structured in the particular way that they are. Some of the cycles are extremely brief (e.g. the example cited above), others are rather  complicated and require numerous exchanges between designer and client. Also, as noted above, the set of design requirements addressed in a given cycle seem always to be highly interrelated, each an aspect of a common subgoal in the overall problem. Prima facie, this seems to require an elaboration of the purely functional hypothesis; for if the cyclic structure of the session was entirely due to functional limitations of memory and attention, we would expect all of the cycles to be roughly of the same size, and we would expect the cycles to be structured indifferently with regard to the logical structure of the design problem. But we do not find this. Design cycles tend to address a single requirement or a set of highly related requirements, s or they consist of hierarchies of related cycles. °"{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}design cycle -  (suggestion of a subproblem; consideration of various subsolutions; acceptance/rejection; and reinitialization).{\textless}/m:note{\textgreater}},
author = {Carroll, J M and Thomas, J C and Malhotra, A},
keywords = {design psychology,experiment,qualitative},
number = {2},
pages = {143--155},
title = {{Clinical-experimental Analysis of Design Problem Solving}},
volume = {1},
year = {1979}
}
@inproceedings{5168022,
abstract = {Efforts are underway in the Surface and Microanalysis Science Division at the National Institute of Standards and Technology to study the vapor transport mechanisms inside explosive trace detection instruments (ETD's) and produce standard test materials to verify their performance. In most swipe-based ETD's, a woven cloth is swiped across a surface to collect micrometer-sized particles from explosive contamination. The swipe is then introduced into a thermal desorption unit where it is rapidly heated to produce an explosive aerosol or vapor. This vapor is transported to a chemical detector, typically an ion mobility spectrometer, for analysis. Understanding the underlying physics of the flow fields within these instruments allows researchers to design better test materials for calibration and verification. In this work, several ETD thermal desorption units are modeled using computational fluid dynamics (CFD). With CFD, the governing equations of fluid motion are solved numerically for a given model geometry and boundary conditions. CFD allows one to visualize and animate flow patterns, streamlines, and recirculation zones, and reveals how vapor is transported from the surface of a swipe to the chemical analyzer. The flow-fields inside these complex geometries would otherwise be difficult, if not impossible, to observe with traditional experimental flow visualization techniques. The thermal desorption units presented here have geometries representative of what is used in ETD's today. Results suggest that the transport efficiency of desorbed explosives can be optimized if appropriate screening procedures are followed. Issues such as velocity magnitude, pressure differential, transient effects, and buoyancy effects will be discussed.},
author = {Staymates, M E and Smith, W J and Gillen, G},
booktitle = {Technologies for Homeland Security, 2009. HST '09. IEEE Conference on},
doi = {10.1109/THS.2009.5168022},
keywords = {ETD thermal desorption units,buoyancy effects,chem},
month = {may},
pages = {107--113},
title = {{The internal fluid mechanics of explosive trace detectors using computational fluid dynamics}},
year = {2009}
}
@inproceedings{Sanchez2007,
abstract = {Test-Driven Development (TDD) is an agile practice that is widely accepted and advocated by most agile methods and methodologists. In this paper, we report on a post hoc analysis of the results of an IBM team who has sustained use of TDD for five years and over ten releases of a Java-implemented product. The team worked from a design and wrote tests incrementally before or while they wrote code and, in the process, developed a significant asset of automated tests. The IBM team realized sustained quality improvement relative to a pre-TDD project and consistently had defect density below industry standards. As a result, our data indicate that the TDD practice can aid in the production of high quality products. This quality improvement would compensate for the moderate perceived productivity losses. Additionally, our data indicates that the use of TDD may decrease the degree to which code complexity increases as software ages, as measured by cyclomatic complexity metric. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {On the sustained use of a test-driven development practice at IBM},
author = {Sanchez, J C and Williams, L and Maximilien, E M},
booktitle = {Proceedings - AGILE 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {5--14},
title = {{On the sustained use of a test-driven development practice at IBM}},
url = {citeulike-article-id:3934776 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-46449114737{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Dietrich2005,
abstract = {Abstract: In recent years we have seen the rise of a new type of software called business rule management systems (BRMS). These are systems to externalize business rules and to provide a facility for centralized business rule management. This addresses an urgent need businesses do have nowadays: to change their business rules in order to adapt to a rapidly business environment, and to overcome the restricting nature of slow IT change cycles. Early manifestations of business rule engines which have their roots in the realm of artificial intelligence and inference systems were complex, expensive to run and maintain and not very business-user friendly. Improved technology providing enhanced usability, scalability and performance, as well as less costly maintenance and better understanding of the underlying inference systems makes the current generation of business rule engines (BRE) and rules technology more usable. However, there are a number of risks and difficulties that have to be taken into account when employing a BRMS. Another recent trend that tries to address the same problem of slow IT change cycles is agile software engineering, in particular test driven development. In this paper, we investigate how BRMSs can be used in conjunction with test driven development. The result is an approach that facilitates the authoring of business rules significantly and safeguards it by providing means for automated validation and verification.},
annote = {On the test-driven development and validation of business rules},
author = {Dietrich, J and Paschke, A},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {23--25},
title = {{On the test-driven development and validation of business rules}},
url = {citeulike-article-id:3934587 http://wwwbichler.in.tum.de/staff/paschke/docs/ista2005-final.pdf},
year = {2005}
}
@article{Pratt2000,
author = {Pratt, Travis C. and Cullen, Francis T.},
doi = {10.1111/j.1745-9125.2000.tb00911.x},
issn = {0011-1384},
journal = {Criminology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {aug},
number = {3},
pages = {931--964},
title = {{The Empirical Status of Gottfredson and Hirschi's General Theory of Crime: A Meta-analysis}},
url = {http://doi.wiley.com/10.1111/j.1745-9125.2000.tb00911.x},
volume = {38},
year = {2000}
}
@article{Lund2006a,
address = {New York, New York, USA},
author = {Lund, Mass Soldal and St{\o}len, Ketil},
doi = {10.1145/1138929.1138934},
isbn = {1595934081},
journal = {Proceedings of the 2006 international workshop on Automation of software test - AST '06},
keywords = {sequence diagrams,test derivation,uml},
pages = {22},
publisher = {ACM Press},
title = {{Deriving tests from UML 2.0 sequence diagrams with neg and assert}},
url = {http://portal.acm.org/citation.cfm?doid=1138929.1138934},
year = {2006}
}
@article{Meyers1978,
author = {Meyers, G},
keywords = {inspection},
number = {9},
pages = {760--768},
title = {{A Controlled Experiment in Program Testing and Code Walkthrough/Inspection}},
volume = {21},
year = {1978}
}
@article{sampson2002,
author = {Sampson, Robert J. and Morenoff, Jeffrey D. and Gannon-Rowley, Thomas},
journal = {Annual review of sociology},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {443--478},
title = {{Assessing" neighborhood effects": Social processes and new directions in research}},
volume = {28},
year = {2002}
}
@inproceedings{Khoshgoftaar1996,
address = {White Plains, NY},
author = {Khoshgoftaar, T.M. and Allen, E.B. and Goel, N. and Nandi, A. and McMullan, J.},
booktitle = {Proceedings of ISSRE '96: 7th International Symposium on Software Reliability Engineering},
doi = {10.1109/ISSRE.1996.558896},
isbn = {0-8186-7707-4},
keywords = {churn},
mendeley-tags = {churn},
pages = {364--371},
title = {{Detection of software modules with high debug code churn in a very large legacy system}},
url = {http://ieeexplore.ieee.org/document/558896/},
year = {1996}
}
@article{Kolling2004,
abstract = {Different kinds of unit testing activities are used in practice. Organised unit testing (regression testing or test-first activities) are very popular in commercial practice, while ad-hoc (interactive) testing is popular in small scale development and teaching situations. These testing styles are usually kept separate. We introduce a design and implementation of a tool that combines these testing styles},
annote = {Going interactive: combining ad-hoc and regression testing},
author = {Kolling, M and Patterson, A},
isbn = {3540221379},
journal = {Extreme Programming and Agile Processes in Software Engineering 5th International Conference, XP 2004 Proceedings Lecture Notes in Comput Sci Vol 3092},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {270--273},
title = {{Going interactive: combining ad-hoc and regression testing}},
url = {citeulike-article-id:3934679 {\#}},
volume = {3092},
year = {2004}
}
@phdthesis{Janzen2006a,
abstract = {Test-driven development (TDD) has gained recent attention with the popularity of the Extreme Programming agile software development methodology. Advocates of TDD rely primarily on anecdotal evidence with relatively little empirical evidence of the benefits of the practice. This research is the first comprehensive evaluation of how TDD affects software architecture and internal design quality. Formal controlled experiments were conducted in undergraduate and graduate academic courses, in a professional training course, and with in-house professional development projects in a Fortune 500 company. The experiments involved over 230 student and professional programmers working on almost five hundred software projects ranging in size from one hundred to over 30,000 lines of code. The research also included a case study of fifteen software projects developed over five years in a Fortune 500 corporation. This research demonstrates that software developers applying a test-first (TDD) approach are likely to improve some software quality aspects at minimal cost over a comparable test-last approach. In particular this research has shown statistically significant differences in the areas of code complexity, size, and testing. These internal quality differences can substantially improve external software quality (defects), software maintainability, software understandability, and software reusability. Further this research has shown that mature programmers who have used both the test-first and test-last development approaches prefer the test-first approach. In addition, this research contributes a pedagogical approach called test-driven learning (TDL) that integrates TDD instruction at all levels. TDL was partially applied at all academic levels from early programming instruction through professional continuing education. Results indicate some differences between beginning and mature developers including reluctance by early programmers to adopt the TDD approach. By providing the first substantial empirical evidence on TDD and internal software quality, this research establishes a benchmark and framework for future empirical studies. By focusing on both software design and software testing, this research is already raising awareness of TDD as both a design and testing approach through publications and international awards.},
address = {Lawrence, KS},
annote = {An Empirical Evaluation of the Impact of Test-Driven Development on Software Quality},
author = {Janzen, David},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
publisher = {University of Kansas, Computer Science,},
title = {{An Empirical Evaluation of the Impact of Test-Driven Development on Software Quality}},
url = {citeulike-article-id:3934652 {\#}},
year = {2006}
}
@inproceedings{386801,
abstract = {The authors present the integrity and security requirements of AM/FM-GIS (geographic information system) systems, involving the security of the operating system, the DBMS (database management systems), and graphic information. Well-established security services and mechanisms developed for EDP on-line systems can be applied to AM/FM-GIS. In AM/FM-GIS, the security issue can be approached with formal methods in the pre-evaluation and evaluation phases. The security requirements should produce a well-defined input to the system design and could influence the selection of the data base with regard to the implementation technique. In AM/FM-GIS systems, different access modes should be possible on several data structures: layers, element groups, objects, and drawings},
author = {Orlandi, E},
booktitle = {Security Technology, 1993. Security Technology, Proceedings. Institute of Electrical and Electronics Engineers 1993 International Carnahan Conference on},
doi = {10.1109/CCST.1993.386801},
month = {oct},
pages = {203--207},
title = {{Integrity and security in AM/FM-GIS}},
year = {1993}
}
@inproceedings{6113166,
abstract = {We present Net EvViz, a visualization tool for analysis and exploration of a dynamic social network. There are plenty of visual social network analysis tools but few provide features for visualization of dynamically changing networks featuring the addition or deletion of nodes or edges. Our tool extends the code base of the Node XL template for Microsoft Excel, a popular network visualization tool. The key features of this work are (1) The ability of the user to specify and edit temporal annotations to the network components in an Excel sheet, (2) See the dynamics of the network with multiple graph metrics plotted over the time span of the graph, called the Timeline, and (3) Temporal exploration of the network layout using an edge coloring scheme and a dynamic Time slider. The objectives of the new features presented in this paper are to let the data analysts, computer scientists and others to observe the dynamics or evolution in a network interactively. We presented Net EvViz to five users of Node XL and received positive responses.},
author = {Khurana, U and Nguyen, Viet-An and Cheng, Hsueh-Chien and Ahn, Jae-wook and Chen, Xi and Shneiderman, B},
booktitle = {Privacy, security, risk and trust (passat), 2011 ieee third international conference on and 2011 ieee third international conference on social computing (socialcom)},
doi = {10.1109/PASSAT/SocialCom.2011.212},
keywords = {Microsoft excel,Net EvViz,NodeXL template,computer},
pages = {549--554},
title = {{Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines}},
year = {2011}
}
@techreport{Langr2001,
abstract = {Test-first design is one of the mandatory practices of Extreme Programming (XP). It requires that programmers do not write any production code until they have first written a unit test. By definition, this technique results in code that is testable, in contrast to the large volume of existing code that cannot be easily tested. This paper demonstrates by example how test coverage and code quality is improved through the use of test-first design. Approach: An example of code written without the use of automated tests is presented. Next, the suite of tests written for this legacy body of code is shown. Finally, the author iterates through the exercise of completely rebuilding the code, test by test. The contrast between both versions of the production code and the tests is used to demonstrate the improvements generated by virtue of employing test-first design. Specifics: The code body represents a CSV (comma-separated values) file reader, a common utility useful for reading files in the standard CSV format. The initial code was built in Java over two years ago. Unit tests for this code were written recently, using JUnit (http://www.junit.org) as the testing framework. The CSV reader was subsequently built from scratch, using JUnit as the driver for writing the tests first. The paper presents the initial code and subsequent tests wholesale. The test-first code is presented in an iterative approach, test by test.},
annote = {Evolution of Test and Code via Test-First Design},
author = {Langr, Jeff},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
publisher = {ObjectMentor},
title = {{Evolution of Test and Code via Test-First Design}},
url = {citeulike-article-id:3934686 http://www.objectmentor.com/resources/articles/tfd.pdf},
year = {2001}
}
@misc{Nour2003,
address = {Calgary, Alberta, Canada},
author = {Nour, P},
publisher = {University of Calgary},
title = {{Ontology-based Retrieval of Software Engineering Experiences}},
year = {2003}
}
@inproceedings{Ko2008,
address = {Leipzig, Germany},
author = {Ko, Andrew J. and Myers, Brad A.},
booktitle = {Proceedings of the 13th international conference on Software engineering (ICSE '08)},
doi = {10.1145/1368088.1368130},
isbn = {9781605580791},
keywords = {debugging,program comprehension,whyline},
month = {may},
pages = {301--310},
publisher = {ACM Press},
title = {{Debugging reinvented: Asking and answering why and why not questions about program behavior}},
url = {http://dl.acm.org/citation.cfm?id=1368088.1368130},
year = {2008}
}
@inproceedings{Xu1999,
author = {Xu, Jun and Kalbarczyk, Zbigniew and Iyer, Ravishankar K},
booktitle = {Proc.$\backslash$ 1999 Pacific Rim International Symposium on Dependable Computing},
isbn = {0-7695-0371-3},
pages = {178--185},
title = {{Networked {\{}Windows{\}} {\{}NT{\}} System Field Failure Data Analysis}},
year = {1999}
}
@inproceedings{Williams2007g,
abstract = {Millennial students (those born after 1982), particularly African Americans and women, have demonstrated a propensity toward collaborative activities. We conducted a collective case study at North Carolina State University and North Carolina A{\&}T to ascertain the role of collaboration and social interaction in attracting and retaining students in information technology. Responses from semi-structured interviews with 11 representative African American students in these classes were coded and analyzed. The responses from these minority students were used to evolve a social interaction model. The conjectures generated from the model suggest that pair programming and agile software methodologies effectively create a collaborative environment that is desirable to Millennial students, male and female, and, with the new evidence, minority and majority. Additionally, the African American Millennial students enjoy learning from their peers and believe that a collaborative environment better prepares them for the "real world".},
author = {Williams, Laurie and Layman, Lucas and Slaten, Kelli M. and Berenson, Sarah B. and Seaman, Carolyn},
booktitle = {29th International Conference on Software Engineering (ICSE'07)},
doi = {10.1109/ICSE.2007.58},
isbn = {0-7695-2828-7},
issn = {0270-5257},
keywords = {African American millennial students,Collaboration,Collaborative software,Collaborative work,Computer science,Educational institutions,Engineering profession,Information technology,North Carolina State University,Programming profession,Software engineering,Switches,agile software methodologies,collaborative environment,collaborative pedagogy,computer aided instruction,computer science education,groupware,mypubs,social interaction model},
mendeley-tags = {mypubs},
month = {may},
pages = {677--687},
publisher = {IEEE},
shorttitle = {Software Engineering, 2007. ICSE 2007. 29th Intern},
title = {{On the Impact of a Collaborative Pedagogy on African American Millennial Students in Software Engineering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4222629},
year = {2007}
}
@article{Philipose2004,
abstract = {A key aspect of pervasive computing is using computers and sensor networks to effectively and unobtrusively infer users' behavior in their environment. This includes inferring which activity users are performing, how they're performing it, and its current stage. Recognizing and recording activities of daily living is a significant problem in elder care. A new paradigm for ADL inferencing leverages radio-frequency-identification technology, data mining, and a probabilistic inference engine to recognize ADLs, based on the objects people use. We propose an approach that addresses these challenges and shows promise in automating some types of ADL monitoring. Our key observation is that the sequence of objects a person uses while performing an ADL robustly characterizes both the ADL's identity and the quality of its execution. So, we have developed Proactive Activity Toolkit (PROACT).},
author = {Philipose, Mathhai and Fishkin, Kenneth P. and Perkowitz, Mike and Patterson, Donald J. and Fox, Dieter and Kautz, Henry and Hahnel, Dirk},
doi = {10.1109/MPRV.2004.7},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Philipose et al. - 2004 - Inferring Activities from Interactions with Objects.pdf:pdf},
issn = {1536-1268},
journal = {IEEE Pervasive Computing},
keywords = {ADL inferencing,ADL monitoring,Context modeling,Engines,Monitoring,Pervasive computing,Proact,Proactive Activity Toolkit,RFID tags,Radio frequency,Radiofrequency identification,Robustness,Sensor phenomena and characterization,Sensor systems,agile,computerised monitoring,context-aware computing,daily living activity recognition,daily living activity recording,data mining,elder care,home automation,home computing,nsf,probabilistic inference engine,radio-frequency-identification technology,sensor networks,ubiquitous computing},
mendeley-tags = {agile,nsf},
month = {oct},
number = {4},
pages = {50--57},
shorttitle = {Pervasive Computing, IEEE},
title = {{Inferring Activities from Interactions with Objects}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1369161},
volume = {3},
year = {2004}
}
@article{Teasley1994,
author = {Teasley, Barbee E},
number = {5},
pages = {757--770},
title = {{The Effects of Naming Style and Expertise on Program Comprehension}},
volume = {40},
year = {1994}
}
@inproceedings{Chakraborty2013,
address = {New York, New York, USA},
author = {Chakraborty, Supriyo and Raghavan, Kasturi Rangan and Johnson, Matthew P. and Srivastava, Mani B.},
booktitle = {Proceedings of the 14th Workshop on Mobile Computing Systems and Applications - HotMobile '13},
doi = {10.1145/2444776.2444791},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chakraborty et al. - 2013 - A framework for context-aware privacy of sensor data on mobile systems.pdf:pdf},
isbn = {9781450314213},
keywords = {Android,agile,behavioral privacy,context-awareness,inferences,ipShield,model-based privacy,nsf},
mendeley-tags = {agile,nsf},
month = {feb},
pages = {1},
publisher = {ACM Press},
title = {{A framework for context-aware privacy of sensor data on mobile systems}},
url = {http://dl.acm.org/citation.cfm?id=2444776.2444791},
year = {2013}
}
@inproceedings{Zimmermann2006,
address = {Shanghai, China},
author = {Zimmermann, Thomas and Kim, Sunghun and Zeller, Andreas and Whitehead, E. James},
booktitle = {Proceedings of the 2006 international workshop on Mining software repositories  - MSR '06},
doi = {10.1145/1137983.1138001},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zimmermann et al. - 2006 - Mining version archives for co-changed lines.pdf:pdf},
isbn = {1595933972},
pages = {72--75},
publisher = {ACM Press},
title = {{Mining version archives for co-changed lines}},
url = {http://portal.acm.org/citation.cfm?doid=1137983.1138001},
year = {2006}
}
@inproceedings{Godleski1984,
author = {Godleski, E S},
keywords = {MBTI,myers-briggs},
pages = {362--364},
title = {{Learning Style Compatibility of Engineering Students and Faculty}},
year = {1984}
}
@inproceedings{Mimno2011,
author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP '11)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mimno et al. - 2011 - Optimizing semantic coherence in topic models.pdf:pdf},
isbn = {978-1-937284-11-4},
month = {jul},
pages = {262--272},
publisher = {Association for Computational Linguistics},
title = {{Optimizing semantic coherence in topic models}},
url = {http://dl.acm.org/citation.cfm?id=2145432.2145462},
year = {2011}
}
@article{5411760,
abstract = {Various approaches have been developed for quantifying and displaying network traffic information for determining network status and in detecting anomalies. Although many of these methods are effective, they rely on the collection of long-term network statistics. Here, we present an approach that uses short-term observations of network features and their respective time averaged entropies. Acute changes are localized in network feature space using adaptive Wiener filtering and auto-regressive moving average modeling. The color-enhanced datagram is designed to allow a network engineer to quickly capture and visually comprehend at a glance the statistical characteristics of a network anomaly. First, average entropy for each feature is calculated for every second of observation. Then, the resultant short-term measurement is subjected to first- and second-order time averaging statistics. These measurements are the basis of a novel approach to anomaly estimation based on the well-known Fisher linear discriminant (FLD). Average port, high port, server ports, and peered ports are some of the network features used for stochastic clustering and filtering. We empirically determine that these network features obey Gaussian-like distributions. The proposed algorithm is tested on real-time network traffic data from Ohio University's main Internet connection. Experimentation has shown that the presented FLD-based scheme is accurate in identifying anomalies in network feature space, in localizing anomalies in network traffic flow, and in helping network engineers to prevent potential hazards. Furthermore, its performance is highly effective in providing a colorized visualization chart to network analysts in the presence of bursty network traffic.},
author = {Celenk, M and Conley, T and Willis, J and Graham, J},
doi = {10.1109/TIFS.2010.2041808},
issn = {1556-6013},
journal = {Information Forensics and Security, IEEE Transactions on},
keywords = {Fisher linear discriminant,Gaussian-like distribut},
month = {jun},
number = {2},
pages = {288--299},
title = {{Predictive Network Anomaly Detection and Visualization}},
volume = {5},
year = {2010}
}
@article{Felder2005,
annote = {Also contains a reference for Cronbach{\&}{\#}039;s alpha cutoffs.},
author = {Felder, R M and Spurlin, J},
keywords = {education,learning styles},
number = {1},
pages = {103--112},
title = {{Applications, Reliability and Validity of the Index of Learning Styles}},
volume = {21},
year = {2005}
}
@book{Atkinson2005a,
address = {Berlin, Heidelberg},
annote = {Safety },
editor = {Atkinson, Colin and Bunse, Christian and Gross, Hans-Gerhard and Peper, Christian},
keywords = {Safety},
mendeley-tags = {Safety},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Component-Based Software Development for Embedded Systems}},
url = {http://www.springerlink.com/index/10.1007/11591962},
volume = {3778},
year = {2005}
}
@inproceedings{Bisgin2018,
address = {San Jose, CA, USA},
author = {Bisgin, Halil and Mani, Murali and Uludag, Suleyman},
booktitle = {2018 IEEE Frontiers in Education Conference},
doi = {10.1109/FIE.2018.8659300},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bisgin, Mani, Uludag - 2018 - Delineating Factors that Influence Student Performance in a Data Structures Course.pdf:pdf},
isbn = {9781538611739},
issn = {15394565},
keywords = {CS1,CS2,CSO,Success factors for introductory computing courses},
mendeley-tags = {CS2},
month = {oct},
pages = {1--9},
publisher = {IEEE},
title = {{Delineating Factors that Influence Student Performance in a Data Structures Course}},
url = {https://ieeexplore.ieee.org/document/8659300/},
year = {2018}
}
@article{Bertolino2007,
author = {Bertolino, Antonia and Faedo, Informazione A},
isbn = {0769528295},
number = {September},
title = {{Software Testing Research : Achievements , Challenges , Dreams Software Testing Research : Achievements , Challenges , Dreams}},
year = {2007}
}
@article{Kunkle2016,
abstract = {Many students experience difficulties learning to program. They find learning to program in the object-oriented paradigm particularly challenging. As a result, computing educators have tried a variety of instructional methods to assist beginning programmers. These include developing approaches geared specifically toward novices and experimenting with different introductory programming languages. However, having tried these different methods, computing educators are faced with yet another dilemma: how to tell if any of these interventions actually worked ? The research presented here was motivated by an interest in improving practices in computer science education in general and improving my own practices as a computer science educator in particular. Its purpose was to develop an instrument to assess student learning of fundamental and object-oriented programming concepts, and to use that instrument to investigate the impact of different teaching approaches and languages on students' ability to learn those concepts. Students enrolled in programming courses at two different universities in the Mid-Atlantic region during the 2009-2010 academic year participated in the study. Extensive data analysis showed that the assessment instrument performed well overall. Reliability estimates ranged from 0.65 to 0.79. The instrument is intrinsically valid since the questions are based on the core concepts of the Programming Fundamentals knowledge area defined by the 2008 ACM/IEEE curricular guidelines. Support for content validity includes: 71{\%} of correct responses varied directly with the students' scores; all possible responses were selected at least once; and 21 out of 24 questions discriminated well between high and low scoring students. CS faculty reviewers indicated that 19 out of 24 questions reflected basic concepts and should be used again "as is" or with "minor changes." Factor analysis extracted three comprehensible components, "methods and functions," "mathematical and logical expressions," and "control structures," suggesting the instrument is on its way to effectively representing the construct "understanding of fundamental programming concepts." Statistical analysis revealed significant differences in student performance based on language of instruction. Analyses revealed differences with respect to overall score and questions involving assignment, mathematical and logical expressions, and code-completion. Language of instruction did not appear to affect student performance on questions addressing object-oriented concepts.},
author = {Kunkle, Wanda M. and Allen, Robert B.},
doi = {10.1145/2785807},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kunkle, Allen - 2016 - The impact of different teaching approaches and languages on student learning of introductory programming concept.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computinig Education},
keywords = {CS1,CS2,Education},
mendeley-tags = {CS1,CS2},
month = {jan},
number = {1},
pages = {Article 3},
publisher = {Association for Computing Machinery},
title = {{The impact of different teaching approaches and languages on student learning of introductory programming concepts}},
volume = {16},
year = {2016}
}
@inproceedings{5557404,
abstract = {A proxy cache has quite a few difficult jobs to do. It is hard for people to effectively and efficiently do the maintenance work needless of mentioning the performance tuning work. In this paper we study the widely used proxy caching server--Squid. It suffers with well-designed tool to the management or evaluation of its performance or capability, which motivates us to design and implement friendly interfaces to relief the management work. On the other hand, these also help to support the evaluation or study of the proxy caching work. We study the public domain Squid proxy server first to find out what we could take advantage of. Then, we design and implement three visual interfaces to the management work of the proxy server. To support the visualization interface, we integrate the MRTG with the interface. We design and implement three different management assistants. They are SNMP agent, the interception of the run-time internal object and the instantaneously analysis of the log file. Each approach has its own limitation and fitness for specific circumstance. They make it easier to the maintenance work of the proxy cache server as well as the study or evaluation of them. Section 2 describes the usage of SNMP agent to the monitoring of the proxy server. Section 3 explains how to intercept the run-time internal object of Squid. Section 4 describes the simultaneously parsing of the access log file. We continue to discuss the situation or circumstance to apply one of the alternative choices. Finally, we point out the future work to be continued.},
author = {Feng, Shaowei and Zhang, Jing and Zeng, Bin},
booktitle = {Electronic Commerce and Security (ISECS), 2010 Third International Symposium on},
doi = {10.1109/ISECS.2010.52},
keywords = {Internet traffic,MRTG,SNMP agent,proxy caching ser},
month = {jul},
pages = {204--208},
title = {{Design of the Visualized Assistant for the Management of Proxy Server}},
year = {2010}
}
@article{Zhou2015,
author = {Zhou, Minghui and Mockus, Audris},
doi = {10.1109/TSE.2014.2349496},
journal = {IEEE Transactions on Software Engineering},
month = {jan},
number = {1},
pages = {82--99},
publisher = {IEEE},
title = {{Who Will Stay in the FLOSS Community? Modeling Participant's Initial Behavior}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6880395},
volume = {41},
year = {2015}
}
@inproceedings{Rigby2007,
address = {Minneapolis, MN},
author = {Rigby, Peter C. and Hassan, Ahmed E.},
booktitle = {Fourth International Workshop on Mining Software Repositories (MSR'07)},
doi = {10.1109/MSR.2007.35},
isbn = {0-7695-2950-X},
keywords = {sentiment},
mendeley-tags = {sentiment},
month = {may},
pages = {23--23},
publisher = {IEEE},
title = {{What Can OSS Mailing Lists Tell Us? A Preliminary Psychometric Text Analysis of the Apache Developer Mailing List}},
url = {http://ieeexplore.ieee.org/document/4228660/},
year = {2007}
}
@inproceedings{Lutz1999,
address = {Chantilly, VA},
author = {Lutz, R. and Woodhouse, R.},
booktitle = {1st International Software Assurance Certification Conference (ISACC'99)},
title = {{Bi-directional analysis for certification of safety-critical software}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.17.4350{\&}rep=rep1{\&}type=pdf},
year = {1999}
}
@article{Kicillof2007a,
address = {New York, New York, USA},
author = {Kicillof, Nicolas and Grieskamp, Wolfgang and Tillmann, Nikolai and Braberman, Victor},
doi = {10.1145/1291535.1291536},
isbn = {9781595938503},
journal = {Proceedings of the 3rd international workshop on Advances in model-based testing - A-MOST '07},
keywords = {concolic execu-,model-based testing,parameterized unit testing,symbolic execution,test-case generation,tion},
pages = {1--11},
publisher = {ACM Press},
title = {{Achieving both model and code coverage with automated gray-box testing}},
url = {http://portal.acm.org/citation.cfm?doid=1291535.1291536},
year = {2007}
}
@inproceedings{228224,
abstract = {The results of an experiment that shows that it is very simple to contaminate digital images with information that can later be extracted are presented. This contamination cannot be detected when the image is displayed on a good quality graphics workstation. Based on these results, it is recommended that image downgrading based on visual display of the image to be downgraded not be performed if there is any threat of image contamination by Trojan horse programs. Potential Trojan horse programs may include untrusted image processing software},
author = {Kurak, C and McHugh, J},
booktitle = {Computer Security Applications Conference, 1992. Proceedings., Eighth Annual},
doi = {10.1109/CSAC.1992.228224},
keywords = {Trojan horse programs,digital images,graphics work},
pages = {153--159},
title = {{A cautionary note on image downgrading}},
year = {1992}
}
@article{5759739,
abstract = {This paper investigates the effect of eavesdroppers on network connectivity, using a wiretap model and percolation theory. The wiretap model captures the effect of eavesdroppers on link security. A link exists between two nodes only if the secrecy capacity of that link is positive. Network connectivity is defined in a percolation sense, i.e., connectivity exists if an infinite connected component exists in the corresponding secrecy graph. We consider uncertainty in location of eavesdroppers, which is modeled directly at the network level as correlated failures in the secrecy graph. Our approach attempts to bridge the gap between physical layer security under uncertain channel state information and network level connectivity under secrecy constraints. For square and triangular lattice secrecy graphs, we obtain bounds on the percolation threshold, which is the critical value of the probability of occurrence of an eavesdropper, above which network connectivity does not exist. For Poisson secrecy graphs, degree distribution and mean value of upper and lower bounds on node degree are obtained. Further, inner and outer bounds on the achievable region for network connectivity are obtained. Both analytic and simulation results show that uncertainty in location of eavesdroppers has a dramatic effect on network connectivity in a secrecy graph.},
author = {Goel, S and Aggarwal, V and Yener, A and Calderbank, A R},
doi = {10.1109/TIFS.2011.2148714},
issn = {1556-6013},
journal = {Information Forensics and Security, IEEE Transactions on},
keywords = {Poisson secrecy graph;degree distribution;lattice},
number = {3},
pages = {712--724},
title = {{The Effect of Eavesdroppers on Network Connectivity: A Secrecy Graph Approach}},
volume = {6},
year = {2011}
}
@inproceedings{Lamkanfi2011,
abstract = {A critical item of a bug report is the so-called "severity", i.e. the impact the bug has on the successful execution of the software system. Consequently, tool support for the person reporting the bug in the form of a recommender or verification system is desirable. In previous work we made a first step towards such a tool: we demonstrated that text mining can predict the severity of a given bug report with a reasonable accuracy given a training set of sufficient size. In this paper we report on a follow-up study where we compare four well-known text mining algorithms (namely, Naive Bayes, Naive Bayes Multinomial, K-Nearest Neighbor and Support Vector Machines) with respect to accuracy and training set size. We discovered that for the cases under investigation (two open source systems: Eclipse and GNOME) Naive Bayes Multinomial performs superior compared to the other proposed algorithms.},
address = {Oldeburg, Germany},
author = {Lamkanfi, Ahmed and Demeyer, Serge and Soetens, Quinten David and Verdonck, Tim},
booktitle = {15th European Conference on Software Maintenance and Reengineering (CSMR '11)},
doi = {10.1109/CSMR.2011.31},
isbn = {978-1-61284-259-2},
issn = {1534-5351},
keywords = {Accuracy,Bug Reports,Bug Severity,Bugzilla,Computer bugs,Eclipse,GNOME,K-nearest neighbor,Naive Bayes,Prediction algorithms,Software,Text Mining,Text mining,Training,bug report,data mining,mining algorithm,naive Bayes multinomial,program debugging,program verification,software system,support vector machines,text analysis,text mining,verification system},
month = {mar},
pages = {249--258},
shorttitle = {Software Maintenance and Reengineering (CSMR), 201},
title = {{Comparing Mining Algorithms for Predicting the Severity of a Reported Bug}},
year = {2011}
}
@article{Jørgensen2005,
author = {J{\o}rgensen, Magne},
number = {3},
pages = {57--63},
title = {{Practical Guidelines for Expert-Judgment-Based Software Effort Estimation}},
volume = {22},
year = {2005}
}
@inproceedings{6102458,
abstract = {Diagnosing a large-scale sensor network is a crucial but challenging task. Particular challenges include the resource and bandwidth constraints on sensor nodes, the spatiotemporally dynamic network behaviors, and the lack of accurate models to understand such behaviors in a hostile environment. In this paper, we present the Sensor Anomaly Visualization Engine (SAVE), a system that fully leverages the power of both visualization and anomaly detection analytics to guide the user to quickly and accurately diagnose sensor network failures and faults. SAVE combines customized visualizations over separate sensor data facets as multiple coordinated views. Temporal expansion model, correlation graph and dynamic projection views are proposed to effectively interpret the topological, correlational and dimensional sensor data dynamics and their anomalies. Through a case study with real-world sensor network system and administrators, we demonstrate that SAVE is able to help better locate the system problem and further identify the root cause of major sensor network failure scenarios.},
author = {Shi, Lei and Liao, Qi and He, Yuan and Li, Rui and Striegel, A and Su, Zhong},
booktitle = {Visual Analytics Science and Technology (VAST), 2011 IEEE Conference on},
doi = {10.1109/VAST.2011.6102458},
keywords = {anomaly detection analytics,correlation graph,dyna},
pages = {201--210},
title = {{SAVE: Sensor anomaly visualization engine}},
year = {2011}
}
@article{levie1982effects,
author = {Levie, W Howard and Lentz, Richard},
journal = {Educational Technology Research and Development},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {4},
pages = {195--232},
publisher = {Springer},
title = {{Effects of text illustrations: A review of research}},
volume = {30},
year = {1982}
}
@inproceedings{Muller2001,
address = {Orlando, FL},
author = {Muller, M.M. and Tichy, W.F.},
booktitle = {Proceedings of the 23rd International Conference on Software Engineering},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Muller, Tichy - 2001 - Case study extreme programming in a university environment.pdf:pdf},
keywords = {TDD,agile},
mendeley-tags = {TDD,agile},
pages = {537--544},
publisher = {IEEE Computer Society},
title = {{Case study: extreme programming in a university environment}},
url = {http://portal.acm.org/citation.cfm?id=381473.381536},
year = {2001}
}
@inproceedings{386812,
abstract = {The problem of optimal allocation of security sensors and/or guards is formulated as a graph/network problem, where the importance of the concept of degree of security or multiple minimal cut set is stressed. An algorithm for solving the formulated graph/network problem is developed and it is shown that the algorithm is very effective from the viewpoint of computational complexity. Examples are given to illustrate the concept of degree of security, optimal allocation of security sensors/guards, multiple minimal cut set, and computational efficiency. A typical example among them is: assume a terrorist or a thief is going to attack a target in a town. The problem is to find the minimum number of sensors/guards and where and how to allocate them in the town, detecting him at least twice (or more generally k times) on his way to the target, even if he may take any route among numerous possible routes},
author = {Inoue, K and Kohda, T and Shirahama, M},
booktitle = {Security Technology, 1993. Security Technology, Proceedings. Institute of Electrical and Electronics Engineers 1993 International Carnahan Conference on},
doi = {10.1109/CCST.1993.386812},
keywords = {computational complexity; degree of security; grap},
month = {oct},
pages = {134--138},
title = {{Optimal allocation of security sensors/guards-graph theoretic approach}},
year = {1993}
}
@inproceedings{1532068,
abstract = { The field of security visualization is in need of a paradigm shift in order to allow visualization tools to be practically used by security engineers. Security engineers must complete two different tasks, that of discovery of a pattern, and that of searching for a pattern in a data set. Current security visualizations do not aid the user in creating symbolic rules that represent visual patterns. Transforming visual patterns to symbolic rules requires effort by the security engineer and detracts from their main task of discovering interesting patterns. In this paper we describe the idea of closing-the-loop, a system where symbolic rules are created from visual patterns.},
author = {Lakkaraju, Kiran and Bearavolu, Ratna and Slagell, A and Yurcik, W and North, S},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532068},
keywords = {NVisionIP,closing-the-loop method,intrusion dete},
pages = {75--82},
title = {{Closing-the-loop in NVisionIP: integrating discovery and search in security visualizations}},
year = {2005}
}
@inproceedings{4223215,
abstract = {To achieve end-to-end security, traditional machine-to-machine security measures are insufficient if the integrity of the human-computer interface is compromised. GUI logic flaws are a category of software vulnerabilities that result from logic bugs in GUI design/implementation. Visual spoofing attacks that exploit these flaws can lure even security- conscious users to perform unintended actions. The focus of this paper is to formulate the problem of GUI logic flaws and to develop a methodology for uncovering them in software implementations. Specifically, based on an in-depth study of key subsets of Internet Explorer (IE) browser source code, we have developed a formal model for the browser GUI logic and have applied formal reasoning to uncover new spoofing scenarios, including nine for status bar spoofing and four for address bar spoofing. The IE development team has confirmed all these scenarios and has fixed most of them in their latest build. Through this work, we demonstrate that a crucial subset of visual spoofing vulnerabilities originate from GUI logic flaws, which have a well-defined mathematical meaning allowing a systematic analysis.},
author = {Chen, Shuo and Meseguer, J and Sasse, R and Wang, H J and Wang, Yi-Min},
booktitle = {Security and Privacy, 2007. SP '07. IEEE Symposium on},
doi = {10.1109/SP.2007.6},
issn = {1081-6011},
keywords = {GUI logic flaw,Internet explorer browser source co},
month = {may},
pages = {71--85},
title = {{A Systematic Approach to Uncover Security Flaws in GUI Logic}},
year = {2007}
}
@inproceedings{Layman2014,
address = {Raleigh, NC},
author = {Layman, Lucas and Zazworka, Nico},
booktitle = {Proc. of the 2014 Symposium and Bootcamp on the Science of Security (HotSoS '14)},
file = {:C$\backslash$:/Users/laymanl/Desktop/layman pubs/Layman2014.pdf:pdf},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {article 9},
title = {{InViz: Instant Visualization of Cyber Attacks}},
year = {2014}
}
@inproceedings{Paetsch2003,
address = {Linz, Austria},
annote = {Overview of how agile methods handle some aspects of requirements engineering. Outlines the major differences between traditional RE practices and agile RE practices. Points out some flaws and potential areas for improvement.},
author = {Paetsch, F and Eberlein, A and Maurer, F},
keywords = {agile,requirements},
pages = {308--313},
title = {{Requirements Engineering and Agile Software Development}},
year = {2003}
}
@techreport{Turner2009,
abstract = {Final Technical Report},
author = {Turner, R. and Shull, F. and Boehm, B. and Carrigy, A. and Clarke, L. and Componation, P. and Dagli, C. and Lane, J. and Layman, L. and Miller, A. and O'Brien, S. and Osterweil, L. and Sabados, D. and Wise, S.},
institution = {Systems Engineering Research Center SERC-2009-TR004},
keywords = {mypubs},
mendeley-tags = {mypubs},
title = {{Evaluation of Systems Engineering Methods, Processes and Tools on Department of Defense and Intelligence Community Programs}},
year = {2009}
}
@inproceedings{Lever2005,
abstract = {Extreme programming (XP) emphasises the test-first strategy of developing software where if code passes unit tests developers gain more confidence in their software. Jester is a test tester for JUnit tests and thus allows developers to confirm their confidence in their tests and consequently in their code. Jester finds code that is not covered by JUnit tests and thus indicates either missing test cases or the redundancy of code that currently exists. The Eclipse IDE enables developers in any language to independently build tools that when combined together work as if they are part of a single integrated tool set. The implications of this open source IDE as an aid to software engineering are infinite and thus provide an ideal platform to nurture XP practices on. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
annote = {Eclipse platform integration of jester - The JUnit test tester},
author = {Lever, S},
booktitle = {Lecture Notes in Computer Science},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {325--326},
title = {{Eclipse platform integration of jester - The JUnit test tester}},
url = {citeulike-article-id:3934693 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-26444496610{\&}{\#}38 partnerID=40},
volume = {3556},
year = {2005}
}
@inproceedings{5333451,
abstract = {Intelligence analysts in the areas of defense and homeland security are now faced with the difficult problem of discerning the relevant details amidst massive data stores. We propose a component-based visualization architecture that is built specifically to encourage the flexible exploration of geospatial event databases. The proposed system is designed to deploy on a variety of display layouts, from a single laptop screen to a multi-monitor tiled-display. By utilizing a combination of parallel coordinates, principal components plots, and other data views, analysts may reduce the dimensionality of a data set to its most salient features. Of particular value to our target applications are understanding correlations between data layers, both within a single view and across multiple views. Our proposed system aims to address the limited scalability associated with coordinated multiple views (CMVs) through the implementation of an efficient core application which is extensible by the end-user.},
author = {Decker, J and Godwin, A and Livingston, M A and Royle, D},
booktitle = {Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Symposium on},
doi = {10.1109/VAST.2009.5333451},
keywords = {component-based visualization architecture,coordin},
pages = {221--222},
title = {{A scalable architecture for visual data exploration}},
year = {2009}
}
@inproceedings{4373490,
abstract = {X-ray screening of passenger bags is an essential component of airport security. However, the most expensive equipment is of limited value, if the humans who operate it are not selected and trained appropriately. Scientific studies have shown that human performance in X-ray image interpretation depends critically on individual abilities and visual knowledge acquired through experience on the job and training. The aim of this study was to investigate the effect of adaptive computer-based training for increasing the detection of guns, knives, improvised explosive devices (lEDs), and other prohibited items. 97 airport security screeners of a European airport participated in this study. At the beginning of the project all airport security screeners conducted the X-ray competency assessment test (X-ray CAT). Thereupon they received adaptive computer-based training (CBT) for about 4 months. Then they conducted the X-ray CAT the second time in the middle of the project This was followed by about 4 months of CBT and a third test with X-ray CAT at the end of the project. The goal was that each screener conducts at least one 20 minute training session per week. Substantial increases of detection performance were found as a result of training, which depended on the threat category (guns, lEDs, knives and other prohibited items). The largest training effects were found for lEDs. Additional analyses showed that training not only leads to an increase of detection performance but also results in faster response times when an X-ray image contains a threat object. Thus, recurrent CBT can be a powerful tool to increase efficiency in X-ray image interpretation by airport security screeners.},
author = {Michel, S and Koller, S M and de Ruiter, J C and Moerland, R and Hogervorst, M and Schwaninger, A},
booktitle = {Security Technology, 2007 41st Annual IEEE International Carnahan Conference on},
doi = {10.1109/CCST.2007.4373490},
keywords = {European airport,X-ray competency assessment test},
pages = {201--206},
title = {{Computer-Based Training Increases Efficiency in X-Ray Image Interpretation by Aviation Security Screeners}},
year = {2007}
}
@article{Hartman2004a,
author = {Hartman, a. and Nagin, K.},
doi = {10.1145/1013886.1007529},
isbn = {1581138202},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
keywords = {automated test generation,coverage analysis,defect analysis,execution framework,test,uml modeling},
month = {jul},
number = {4},
pages = {129},
title = {{The AGEDIS tools for model based testing}},
url = {http://portal.acm.org/citation.cfm?doid=1013886.1007529},
volume = {29},
year = {2004}
}
@inproceedings{Grinter1995,
address = {Milpitas, CA},
author = {Grinter, Rebecca E.},
booktitle = {Conference on Organizational Computing Systems},
doi = {10.1145/224019.224036},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grinter - 1995 - Using a configuration management tool to coordinate software development.pdf:pdf},
isbn = {0897917065},
keywords = {articulation work,cm,collaboration,computer-,configuration management,cscw,mechanisms of interaction,organizational memory,supported cooperative work},
mendeley-tags = {collaboration},
pages = {168--177},
publisher = {ACM Press},
title = {{Using a configuration management tool to coordinate software development}},
url = {http://portal.acm.org/citation.cfm?doid=224019.224036},
year = {1995}
}
@inproceedings{5633744,
abstract = {Graphics Processing Units (GPU) have been the extensive research topic in recent years and have been successfully applied to general purpose applications other than computer graphical area. The nVidia CUDA programming model provides a straightforward means of describing inherently parallel computations. In this paper, we present a study of the efficiency of emerging technology in applying General Purpose Graphics Processing Units (GPGPU) in high performance symmetric key cryptographic solutions. We implemented symmetric key cryptography algorithm using the novel CUDA platform on nVidia Geforce 280 GTX and compared its performance with an optimized CPU implementation on a high-end AMD Opteron Dual Core CPU. Our experimental results show that GPGPU can perform as an efficient cryptographic accelerator and the developed GPU based implementation achieve a significant performance improvement over CPU based implementation and the maximum observed speedups are about 100 times.},
author = {Wu, Fan and Chen, Chung-han and Narang, H},
booktitle = {Emerging Security Information Systems and Technologies (SECURWARE), 2010 Fourth International Conference on},
doi = {10.1109/SECURWARE.2010.44},
keywords = {GPGPU,computer graphical area,efficient accelerati},
month = {jul},
pages = {228--233},
title = {{An Efficient Acceleration of Symmetric Key Cryptography Using General Purpose Graphics Processing Unit}},
year = {2010}
}
@inproceedings{Radenski2006,
abstract = {The emphasis on Java and other commercial languages in CS1 has established the perception of computer science as a dry and technically difficult discipline among undecided students who are still seeking careers. This may not be a big problem during an enrolment boom, but in times of decreased enrolment such negative perception may have a devastating effect on computer science programs and therefore should not be ignored. We have made our CS1 course offerings more attractive to students (1) by introducing an easy to learn yet effective scripting language -Python, (2) by making all course resources available in a comprehensive online study pack, and (3) by offering an extensive set of detailed and easy to follow self-guided labs. Our custom-designed online study pack comprises a wealth of new, original learning modules: extensive e-texts, detailed self-guided labs, numerous sample programs, quizzes, and slides. Our recent student survey demonstrates that students like and prefer Python as a first language and that they also percept the online study pack as very beneficial. Our "Python First" course, originally required for computer science majors, has been so well received that it has been recently approved as a general education science elective, thus opening new recruitment opportunities for the computer science major. Our "Python First" digital pack is published online at http://studypack.com.},
address = {Bologna, Italy},
author = {Radenski, Atanas},
booktitle = {Working Group Reports on ITiCSE on Innovation and Technology in Computer Science Education 2006},
doi = {10.1145/1140124.1140177},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radenski - 2006 - Python first A lab-based digital introduction to computer science.pdf:pdf},
isbn = {1595936033},
keywords = {CS1,CS2,Java,OOP,Online study pack,Python,Self-guided lab},
mendeley-tags = {CS2},
pages = {197--201},
publisher = {ACM Press},
title = {{"Python first": A lab-based digital introduction to computer science}},
url = {http://portal.acm.org/citation.cfm?doid=1140124.1140177},
year = {2006}
}
@misc{SMERFScubed,
author = {Farr, William H},
month = {may},
title = {{{\{}SMERFS{\}} Downloads}},
url = {http://www.slingcode.com/smerfs/downloads/{\#}SMERFS3},
year = {2012}
}
@inproceedings{Jedlitschka2005,
abstract = {One major problem for integrating study results into a common body of knowledge is the heterogeneity of reporting styles: (1) it is difficult to locate relevant information and (2) important information is often missing. Reporting guidelines are expected to support a systematic, standardized presentation of empirical research, thus improving reporting in order to support readers in (1) finding the information they are looking for, (2) understanding how an experiment is conducted, and (3) assessing the validity of its results. The objective of this paper is to survey the most prominent published proposals for reporting guidelines, and to derive a unified standard that which can serve as a starting point for further discussion. We provide detailed guidance on the expected content of the sections and subsections for reporting a specific type of empirical studies, i.e., controlled experiments. Before the guidelines can be evaluated, feedback from the research community is required. For this purpose, we propose to adapt guideline development processes from other disciplines.},
author = {Jedlitschka, A. and Pfahl, D.},
booktitle = {2005 International Symposium on Empirical Software Engineering, 2005.},
doi = {10.1109/ISESE.2005.1541818},
isbn = {0-7803-9507-7},
keywords = {Costs,Data mining,Feedback,Guidelines,Programming,Proposals,Psychology,Software engineering,Standards publication,Taxonomy,empirical software engineering,guideline development process,software engineering},
pages = {92--101},
publisher = {IEEE},
shorttitle = {Empirical Software Engineering, 2005. 2005 Interna},
title = {{Reporting guidelines for controlled experiments in software engineering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1541818},
year = {2005}
}
@inproceedings{Kohn2019,
abstract = {The interaction between a novice programmer, and the compiler plays a crucial role in the learning process of the novice programmer. Of particular importance is the compiler's feedback on errors in the program code. Accordingly, compiler error messages are an important and active field of research. Yet, a language that has largely been left out of this discussion so far is Python. We have collected Python programs from high school students taking introductory courses. For each collected erroneous program, we sought to classify the effective error, and assess if the student was able to fix the error. Our study is a precursor to providing improved error messages in Python, and assess their effectiveness. As such, we are eventually interested in finding ways to automatically determine the effective error, so as to base the displayed message on. From our data, we found that a considerable part of students' errors can be attributed to minor mistakes, which can easily be identified and corrected. However, beyond such minor mistakes, a proper error diagnosis might have to be based on a goal/plan analysis of the entire program. Likewise, proper assessment of whether an error has been fixed frequently requires more context than is provided by the program alone.},
address = {Minneapolis, MN, USA},
author = {Kohn, Tobias},
booktitle = {SIGCSE 2019 - Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3287324.3287381},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohn - 2019 - The error behind the message Finding the cause of error messages in python.pdf:pdf},
isbn = {9781450358903},
month = {feb},
pages = {524--530},
publisher = {Association for Computing Machinery, Inc},
title = {{The error behind the message: Finding the cause of error messages in python}},
url = {https://dl.acm.org/doi/10.1145/3287324.3287381},
year = {2019}
}
@misc{Freeman2004,
abstract = {Mock Objects is an extension to Test-Driven Development that supports good Object-Oriented design by guiding the discovery of a coherent system of types within a code base. It turns out to be less interesting as a technique for isolating tests from third-party libraries than is widely thought. This paper describes the process of using Mock Objects with an extended example and reports best and worst practices gained from experience of applying the process. It also introduces jMock, a Java framework that embodies our collective experience.},
annote = {Mock Roles, Not Objects},
author = {Freeman, S and Mackinnon, T and Pryce, N and Walnes, J},
booktitle = {In OOPSLA 04: Companion to the 19th annual ACM SIGPLAN conference on Object-oriented programming systems, languages, and applications},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {236--246},
publisher = {ACM Press},
title = {{Mock Roles, Not Objects}},
url = {citeulike-article-id:3934609 {\#}},
year = {2004}
}
@book{Jacky2008,
abstract = {This book teaches new methods for specifying, analyzing, and testing software; essentials for creating high-quality software. These methods increase the automation in each of these steps, making them more timely, more thorough, and more effective. The authors work through several realistic case studies in-depth and detail, using a toolkit built on the C{\#} language and the .NET framework. Readers can also apply the methods in analyzing and testing systems in many other languages and frameworks. Intended for professional software developers including testers, and for university students, this book is suitable for courses on software engineering, testing, specification, or applications of formal methods.},
author = {Jacky, Jonathan},
pages = {349},
publisher = {Cambridge University Press},
title = {{Model-Based Software Testing and Analysis with C{\#} (Google eBook)}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=3LW7Y8y2F4oC{\&}pgis=1},
year = {2008}
}
@inproceedings{Braynov2003a,
address = {New York, New York, USA},
author = {Braynov, Sviatoslav and Jadiwala, Murtuza},
booktitle = {Proceedings of the 2003 ACM workshop on Formal methods in security engineering - FMSE '03},
doi = {10.1145/1035429.1035434},
isbn = {1581137818},
keywords = {adversary modelling,attack graph,attack plan,coordinated attack,model checking},
month = {oct},
pages = {43--51},
publisher = {ACM Press},
title = {{Representation and analysis of coordinated attacks}},
url = {http://dl.acm.org/citation.cfm?id=1035429.1035434},
year = {2003}
}
@inproceedings{Pappas2016,
abstract = {Research in the area of Computer Science (CS) education, has focused on identifying the reasons that students do not finish their studies in CS. Although there is increasing demand for CS professionals, there is not enough knowledge to explain the high dropout rates in CS education. This study aims to empirically examine how students' intention to complete their studies (retention) in CS is affected by variables playing a key role in higher education. By identifying which variables contribute to dropout in CS studies, we will be able to focus on how to improve aspects related with them in order to reduce dropout rates. To do so we identified the following variables: Year of studies, Gender, Age, Students' Effort, Absence from Classes, Expected Grade point average (GPA), and Current GPA, and tested their effect on retention, based on the responses collected from 241 CS student. Year of studies and Effort have positive effects on students' intention to finish their studies in CS. Interestingly, the expected GPA has a negative effect on students' intentions to finish their studies. The findings contribute to theory and practice, as they offer CS educators and policy makers insights that may aid towards increased student retention and reduced dropout rates.},
address = {Arequipa, Peru},
annote = {suspicious analysis. decent lit. 

revious studies in CS, regarding student dropout and retention, suggest that the first two years are critical for the students as they present the higher dropout point [14]. The findings indicate an almost 40{\%} dropout rate for these years, which may vary from 30{\%}-40{\%} based on the institution [15].

[14] Huang, P.M. and Brainard, S.G., 2001. Identifying determinants of academic selfconfidence among science, math, engineering, and technology students. Journal of women and minorities in science and engineering 7, 4. 

[15] Ohland, M.W., Sheppard, S.D., Lichtenstein, G., Eris, O., Chachra, D., and Layton, R.A., 2008. Persistence, engagement, and migration in engineering programs. Journal of Engineering Education 97, 3, 259-278. 

self reported questionnaire of 250 students},
author = {Pappas, Ilias O. and Giannakos, Michail N. and Jaccheri, Letizia},
booktitle = {Annual Conference on Innovation and Technology in Computer Science Education, ITiCSE},
doi = {10.1145/2899415.2899455},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pappas, Giannakos, Jaccheri - 2016 - Investigating factors influencing students intention to dropout computer science studies.pdf:pdf},
isbn = {9781450342315},
issn = {1942647X},
keywords = {CS2,Computer science education,Dropout,Higher education,Retention},
mendeley-tags = {CS2},
pages = {198--203},
publisher = {ACM Press},
title = {{Investigating factors influencing students intention to dropout computer science studies}},
url = {http://dl.acm.org/citation.cfm?doid=2899415.2899455},
year = {2016}
}
@article{Welsh2015,
abstract = {This article serves as a substantive introduction and guiding post for the journal's special issue on "Reimagining Broken Windows: From Theory to Policy." It describes the core concepts of the broken windows perspective, examines its theoretical underpinnings, and sets out priorities for future research and policy development. Important advancements have been made in the intellectual development and programmatic application of the broken windows perspective over the last 30 years. Some of these advancements include the measurement of disorder and experimentation of community and problem-solving strategies for policing disorder. There are also many challenges, including the need for a more consistent operationalization of disorder, a better understanding of potential mechanisms, and concerns about policy overreach in the name of broken windows. We predict that the broken windows perspective will be around for many more decades to come--its enduring qualities far exceed a smartly coined phrase.},
author = {Welsh, Brandon C. and Braga, Anthony A. and Bruinsma, Gerben J. N.},
doi = {10.1177/0022427815581399},
issn = {0022-4278},
journal = {Journal of Research in Crime and Delinquency},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jun},
number = {4},
pages = {447--463},
title = {{Reimagining Broken Windows: From Theory to Policy}},
url = {http://jrc.sagepub.com/content/52/4/447.short},
volume = {52},
year = {2015}
}
@inproceedings{Zazworka2009,
address = {Lake Buena Vista, FL},
author = {Zazworka, Nico and Basili, Victor R. and Shull, Forrest},
booktitle = {International Symposium on Empirical Software Engineering},
pages = {312--323},
title = {{Tool supported detection and judgment of nonconformance in process execution}},
year = {2009}
}
@article{Basili2004d,
author = {Basili, Victor and Donzelli, Paolo and Asgari, Sima},
journal = {Software, IEEE},
keywords = {dependability},
mendeley-tags = {dependability},
number = {6},
pages = {19--25},
title = {{A unified model of dependability: Capturing dependability in context}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1353219},
volume = {21},
year = {2004}
}
@inproceedings{4804434,
abstract = {Currently, network administrators must rely on labor-intensive processes for tracking network configurations and vulnerabilities, which requires a great deal of expertise and is error prone. The organization of networks and the inter dependencies of vulnerabilities are so complex as to make traditional vulnerability analysis inadequate. We describe a Topological Vulnerability Analysis (TVA) approach that analyzes vulnerability dependencies and shows all possible attack paths into a network. From models of the network vulnerabilities and potential attacker exploits, we discover attack paths (organized as graphs) that convey the impact of individual and combined vulnerabilities on overall security. We provide sophisticated attack graph visualizations, with high-level overviews and detail drill down. Decision support capabilities let analysts make optimal tradeoffs between safety and availability, and show how to best apply limited security resources. We employ efficient algorithms that scale well to larger networks.},
author = {Noel, Steven and Elder, Matthew and Jajodia, Sushil and Kalapa, Pramod and O'Hare, Scott and Prole, Kenneth},
booktitle = {2009 Cybersecurity Applications {\&} Technology Conference for Homeland Security},
doi = {10.1109/CATCH.2009.19},
isbn = {978-0-7695-3568-5},
keywords = {labor-intensive processes,network vulnerabilities},
month = {mar},
pages = {124--129},
publisher = {Ieee},
title = {{Advances in Topological Vulnerability Analysis}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4804434},
year = {2009}
}
@inproceedings{Krothapalli2009,
address = {New York, New York, USA},
author = {Krothapalli, Sunil D. and Sun, Xin and Sung, Yu-Wei E. and Yeo, Suan Aik and Rao, Sanjay G.},
booktitle = {Proceedings of the 2nd ACM workshop on Assurable and usable security configuration - SafeConfig '09},
doi = {10.1145/1655062.1655075},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krothapalli et al. - 2009 - A toolkit for automating and visualizing VLAN configuration.pdf:pdf},
isbn = {9781605587783},
keywords = {automation,toolkit,visualization,vlan},
month = {nov},
pages = {63},
publisher = {ACM Press},
title = {{A toolkit for automating and visualizing VLAN configuration}},
url = {http://dl.acm.org/citation.cfm?id=1655062.1655075},
year = {2009}
}
@inproceedings{Chou2001,
author = {Chou, Andy and Yang, Junfeng and Chelf, Benjamin and Hallem, Seth and Engler, Dawson},
booktitle = {Proc.$\backslash$ Eighteenth ACM Symposium on Operating Systems Principles},
isbn = {1-58113-389-8},
pages = {73--88},
title = {{An empirical study of operating systems errors}},
year = {2001}
}
@article{Cohen1981,
abstract = {A macrodynamic social indicator framework is used to demonstrate how accurate crime rate forecasts can be produced. Trends in reported robbery, burglary, larceny, and auto theft rates for the United States are modeled, using annual data for the years 1947-72. The poverty ratio and unemployment rate, two variables considered important predictors of crime in the traditional criminological literature, fail to account for the index crime trends in this analysis. A "criminal opportunity perspective" is used to formulate several substantively meaningful "social produc tion functions" for the above crime rate trends, showing how relatively moderate social changes can generate rather dramatic increments in the crime rate. For example, I consider how the participation of women in the labor force, the incidence of persons living alone, and the presence of lightweight durable goods provide offenders with opportunities favorable for carrying out the above illegal acts. Stochastic equations estimating these production functions indicate that the null hypothesis of no autocorrelation of disturbances is consistently accepted. Ex post forecasts of 1973-75 reported crime rates used to gauge the accuracy of the models usually err within a few percentage points. The data presented here indicate the efficacy of including criminal opportunity factors in crime rate forecasting designed to supply policy makers with technical information relevant to organizational goal criteria.},
author = {Cohen, Lawrence E.},
doi = {10.1177/002242788101800109},
issn = {0022-4278},
journal = {Journal of Research in Crime and Delinquency},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jan},
number = {1},
pages = {138--164},
title = {{Modeling Crime Trends: a Criminal Opportunity Perspective}},
url = {http://jrc.sagepub.com/content/18/1/138.short},
volume = {18},
year = {1981}
}
@inproceedings{1532066,
abstract = { This paper introduces an adaptation of polar coordinates called root polar plotting that we have developed for our network pixel map, a computer security visualization capable of representing tens of thousands of hosts at a time. Root polar coordinates overcome two important problems of normal polar coordinates: plot density distortion and severe occlusion near the origin. We discuss several approaches we took while investigating this problem and provide empirical data from experiments we conducted comparing root polar coordinates against both normal polar and Cartesian coordinates. In any application where a polar plot would be useful but distortion of the data must be avoided, or where it is important to avoid some markers from being occluded by others, root polar coordinates may be useful. Our approach provides: (1) a novel adaptation of polar coordinates that overcomes plotting distortion; (2) a means of plotting network data in near real-time without complex layout optimization; (3) an algorithm that reduces occlusion of plotted points while maintaining consistent placement; and (4) an empirical comparison of Cartesian vs. polar plots.},
author = {Fink, G A and North, C},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532066},
keywords = {Cartesian coordinates,Inte,Internet address data},
pages = {55--64},
title = {{Root polar layout of Internet address data for security administration}},
year = {2005}
}
@inproceedings{wohlin2014guidelines,
address = {London, UK},
author = {Wohlin, Claes},
booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering - EASE '14},
doi = {10.1145/2601248.2601268},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wohlin - 2014 - Guidelines for snowballing in systematic literature studies and a replication in software engineering.pdf:pdf},
isbn = {9781450324762},
keywords = {replication,snowball search,snowballing,systematic literature review,systematic mapping studies},
pages = {ARticle No. 38},
publisher = {ACM Press},
title = {{Guidelines for snowballing in systematic literature studies and a replication in software engineering}},
url = {http://dl.acm.org/citation.cfm?doid=2601248.2601268},
year = {2014}
}
@article{Muller2007,
abstract = {We conducted a quasi-experiment to compare the characteristics of experts' and novices' test-driven development processes. Our novices were 11 computers science students who participated in an Extreme Programming lab course, the expert group consisted of seven professionals who had industrial experience in test-driven development. The novices as well as two of the experts worked in a laboratory environment whereas the remaining five experts worked in their office. The experts complied more to the rules of test-driven development and had shorter test-cycles than the novices. The tests written by the experts were of higher quality in terms of statement and block coverage as well. All reported results are statistically significant on the 5{\%} level. We conclude that the results of studies which evaluate performance of test-driven development using subjects inexperienced in TDD are not easily generalisable. {\^{A}}{\textcopyright} 2007 Springer Science+Business Media, LLC.},
annote = {The effect of experience on the test-driven development process},
author = {M{\"{u}}ller, M M and H{\"{o}}fer, A},
journal = {Empirical Software Engineering},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {593--615},
title = {{The effect of experience on the test-driven development process}},
url = {citeulike-article-id:3934727 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-35748937602{\&}{\#}38 partnerID=40},
volume = {12},
year = {2007}
}
@article{Stobie2005,
author = {Stobie, Keith},
doi = {10.1016/j.entcs.2004.12.004},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {automated test case generation,formal testing methods,test selection},
month = {jan},
pages = {5--12},
title = {{Model Based Testing in Practice at Microsoft}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1571066104052296},
volume = {111},
year = {2005}
}
@inproceedings{Cysneiros2002,
address = {Essen, Germany},
author = {Cysneiros, L M},
pages = {350--356},
title = {{Requirements Engineering in the Health Care Domain}},
year = {2002}
}
@article{speier1999influence,
author = {Speier, Cheri and Valacich, Joseph S and Vessey, Iris},
journal = {Decision Sciences},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {2},
pages = {337--360},
publisher = {Wiley Online Library},
title = {{The influence of task interruption on individual decision making: An information overload perspective}},
volume = {30},
year = {1999}
}
@inproceedings{Xu2007a,
abstract = {This study investigates the expertise differences between intermediate and expert programmers during test-driven software development. The intermediates and experts performed programming on the same problem and the process was videotaped and the data was analyzed. Differences are identified in terms of Bloom's taxonomy, design decision generation, test case generation, the hypotheses generation when debugging the program. Experts' skills are characterized as generating higher levels, more general and higher quality hypotheses. Experts seem to be more able to evaluate their hypotheses and design decisions. Experts are also better able to disregard discredited hypotheses while intermediates tend to maintain hypotheses despite contradictory evidence. Experts try to generate an overview before starting the programming task. Experts are better in using the domain knowledge as well. These differences may inform software development education. {\^{A}}{\textcopyright}2007 IEEE.},
annote = {Programmer{\&}{\#}039;s expertise during test-driven software development},
author = {Xu, S and Cui, Z and Liu, D and Chen, X},
booktitle = {Proceedings of the 6th IEEE International Conference on Cognitive Informatics, ICCI 2007},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {456--461},
title = {{Programmer's expertise during test-driven software development}},
url = {citeulike-article-id:3934844 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-48049108872{\&}{\#}38 partnerID=40},
year = {2007}
}
@inproceedings{Brown2014,
abstract = {Educators often form opinions on which programming mistakes novices make most often {\{} for example, in Java: $\backslash$they always confuse equality with assignment", or $\backslash$they always call methods with the wrong types". These opinions are generally based solely on personal experience. We report a study to determine if programming educators form a consensus about which Java programming mistakes are the most common. We used the Blackbox data set to check whether the educators' opinions matched data from over 100,000 students {\{} and checked whether this agreement was mediated by educators' experience. We found that educators formed only a weak consensus about which mistakes are most frequent, that their rankings bore only a moderate correspondence to the students in the Blackbox data, and that educators' experience had no effect on this level of agreement. These results raise questions about claims educators make regarding which errors students are most likely to commit. Copyright {\textcopyright} 2014 ACM.}}},
address = {Glasgow, Scotland},
author = {Brown, Neil C.C. and Altadmri, Amjad},
booktitle = {ICER 2014 - Proceedings of the 10th Annual International Conference on International Computing Education Research},
doi = {10.1145/2632320.2632343},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Altadmri - 2014 - Investigating novice programming mistakes Educator beliefs vs student data.pdf:pdf},
isbn = {9781450327558},
keywords = {CS2,Educators,Programming mistakes},
mendeley-tags = {CS2},
pages = {43--50},
publisher = {ACM Press},
title = {{Investigating novice programming mistakes: Educator beliefs vs student data}},
url = {http://dl.acm.org/citation.cfm?doid=2632320.2632343},
year = {2014}
}
@inproceedings{Sarcia2008,
abstract = {Cost estimation is a critical issue for software organizations. Good estimates can help us make more informed decisions (controlling and planning software risks), if they are reliable (correct) and valid (stable). In this study, we apply a variable reduction technique (based on auto-associative feed--forward neural networks – called Curvilinear component analysis) to log-linear regression functions calibrated with ordinary least squares. Based on a COCOMO 81 data set, we show that Curvilinear component analysis can improve the estimation model accuracy by turning the initial input variables into an equivalent and more compact representation. We show that, the models obtained by applying Curvilinear component analysis are more parsimonious, correct, and reliable.},
author = {Sarcia, S A and Cantone, G and Basili, Victor R},
booktitle = {12 th International Conference on Evaluation and Assessment in Software Engineering},
keywords = {Software Engineering},
series = {Evaluation and Assessment in Software Engineering (EASE 2008)},
title = {{Adopting Curvilinear Component Analysis to Improve Software Cost Estimation Accuracy: Model, Application Strategy, and an Experimental Verification}},
url = {http://www.cs.umd.edu/{~}basili/publications/proceedings/P126.pdf},
year = {2008}
}
@article{Schultz2008,
abstract = {Application of appropriate design patterns can extend test driven development within an Access Visual Basic for Applications environment to allow unit testing of object class properties and methods. While this approach is not a simple as automating testing of functions in modules, the approach is tractable for intermediate-level developers and leads to significant benefits derived from a test driven approach.},
annote = {Design patterns for test driven development in Access VBA},
author = {Schultz, T},
journal = {J. of Computer Information Systems},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Design patterns for test driven development in Access VBA}},
url = {citeulike-article-id:3934784 http://www.iacis.org/iis/2008{\_}iis/pdf/S2008{\_}982.pdf},
year = {2008}
}
@article{Dohmke2007,
abstract = {With test-driven development, developers don't write new code until an automated test has failed, and duplicate functions, tests, or code fragments are always removed. TDD can lead to better-designed, higher-quality systems. slUnit combines the features of the xUnit testing frameworks and the Simulink graphical programming language to apply TDD to control-system design. Development of a controller for a simplified vehicle system illustrates this approach. This article is part of a special issue on test-driven development. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Test-driven development of a PID controller},
author = {Dohmke, T and Gollee, H},
journal = {IEEE Software},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {3},
pages = {44--50},
title = {{Test-driven development of a PID controller}},
url = {citeulike-article-id:3934589 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34248365279{\&}{\#}38 partnerID=40},
volume = {24},
year = {2007}
}
@inproceedings{Botta2007,
address = {New York, New York, USA},
author = {Botta, David and Werlinger, Rodrigo and Gagn{\'{e}}, Andr{\'{e}} and Beznosov, Konstantin and Iverson, Lee and Fels, Sidney and Fisher, Brian},
booktitle = {Proceedings of the 3rd symposium on Usable privacy and security - SOUPS '07},
doi = {10.1145/1280680.1280693},
isbn = {9781595938015},
keywords = {collaboration,ethnography,security management,security tasks,security tools,usable security},
month = {jul},
pages = {100},
publisher = {ACM Press},
title = {{Towards understanding IT security professionals and their tools}},
url = {http://dl.acm.org/citation.cfm?id=1280680.1280693},
year = {2007}
}
@article{Edwards2009,
abstract = {Personal construct theory (applied via the repertory grid technique) supports interpretivist research in a structured manner and, as such, has relevance for researchers conducting studies focused on the human and organisational aspects of software engineering. Personal construct theory (which underpins the repertory grid technique) is introduced, and the technique and its administration is discussed. Research studies from the literature are reviewed to provide illustrative examples of its application within a software engineering context. Since any research approach needs to answer questions about its reliability and validity within a particular study, these issues are considered for repertory grid investigations and criteria are offered that can be used to judge these issues within a planned, and/or reported, study.},
author = {Edwards, Helen M. and McDonald, Sharon and {Michelle Young}, S.},
doi = {10.1016/j.infsof.2008.08.008},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Critical review,Interpretive research,Personal construct theory,Repertory grids,Research process},
month = {apr},
number = {4},
pages = {785--798},
title = {{The repertory grid technique: Its place in empirical software engineering research}},
url = {http://www.sciencedirect.com/science/article/pii/S0950584908001298},
volume = {51},
year = {2009}
}
@inproceedings{Turner2007,
abstract = {Retention is an important issue for Computer Science Departments. In many cases students leave the major due to frustrations with programming in the complex languages often used in CS1 and CS2 or because they do not understand that computer science is ...},
address = {Covington, KY, USA},
author = {Turner, Elise H. and Albert, Erik and Turner, Roy M. and Latour, Laurence},
booktitle = {SIGCSE 2007: 38th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/1227310.1227321},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Turner et al. - 2007 - Retaining majors through the introductory sequence.pdf:pdf},
isbn = {1595933611},
issn = {0097-8418},
keywords = {CS1/2,CS2,Non-programming introductory computer science,Retention},
mendeley-tags = {CS2},
pages = {24--28},
publisher = {ACM Press},
title = {{Retaining majors through the introductory sequence}},
url = {http://portal.acm.org/citation.cfm?doid=1227310.1227321},
year = {2007}
}
@article{Ostrand2005,
author = {Ostrand, T.J. and Weyuker, E.J. and Bell, R.M.},
doi = {10.1109/TSE.2005.49},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ostrand, Weyuker, Bell - 2005 - Predicting the location and number of faults in large software systems.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Fault detection,Fault diagnosis,History,Index Terms- Software faults,Personnel,Predictive models,Resource management,Software systems,Software testing,Sorting,System testing,binomial distribution,binomial regression model,empirical study,fault-prone,large software systems,prediction,program testing,regression analysis,regression model,software fault prediction,software fault tolerance,software testing,software testing.},
language = {English},
month = {apr},
number = {4},
pages = {340--355},
publisher = {IEEE},
title = {{Predicting the location and number of faults in large software systems}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=1435354},
volume = {31},
year = {2005}
}
@inproceedings{Porter2018,
abstract = {Establishing learning goals for a course allows instructors to design course content to address those goals, helps students to focus their learning appropriately, and enables researchers to assess learning of those goals. In this work, we propose six learning goals for a topic prevalent in CS2 courses: Basic Data Structures. These learning goals arise from reviewing several CS2 courses at a variety of institutions, surveying faculty experts who commonly teach CS2, and meeting and working closely with these experts. We outline our process for creating learning goals, identify important topics underlying these goals, and provide examples of how the goals developed on the path to consensus. We also document that the term "CS2" does not have a unified interpretation within the CS education community and describe how this hurdle influenced our decision to focus on Basic Data Structures.},
address = {Baltimore, MD, USA},
author = {Porter, Leo and Zingaro, Daniel and Lee, Cynthia and Taylor, Cynthia and Webb, Kevin C. and Clancy, Michael},
booktitle = {SIGCSE 2018 - Proceedings of the 49th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/3159450.3159457},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Porter et al. - 2018 - Developing course-Level learning goals for basic data structures in CS2.pdf:pdf},
isbn = {9781450351034},
keywords = {CS2,Data structures,Learning goals},
mendeley-tags = {CS2},
pages = {858--863},
publisher = {ACM Press},
title = {{Developing course-Level learning goals for basic data structures in CS2}},
url = {http://dl.acm.org/citation.cfm?doid=3159450.3159457},
year = {2018}
}
@inproceedings{Tari2006,
address = {New York, New York, USA},
author = {Tari, Furkan and Ozok, A. Ant and Holden, Stephen H.},
booktitle = {Proceedings of the 2nd Symposium on Usable Privacy and Security (SOUPS '06)},
doi = {10.1145/1143120.1143128},
isbn = {1595934480},
keywords = {agile,authentication,graphical passwords,human factors,nsf,password security,shoulder surfing,social engineering,usable security},
mendeley-tags = {agile,nsf},
month = {jul},
pages = {56--66},
publisher = {ACM Press},
title = {{A comparison of perceived and real shoulder-surfing risks between alphanumeric and graphical passwords}},
url = {http://dl.acm.org/citation.cfm?id=1143120.1143128},
year = {2006}
}
@article{Deguet2008c,
abstract = {Computer assisted intervention (CAI) systems require the integration of an increasing number of devices, including medical monitors, sensors, tracking devices and robots. This complexity makes applications harder to develop, more difficult to debug and the accumulation of ad hoc interfaces reduces the overall portability. We describe a set of libraries, the cisst libraries, developed at the Johns Hopkins University to address some of the problems encountered when integrating devices for CAI. We focus on three main characteristics of the cisst libraries: software architecture, multi-threading and CAI specific interfaces.},
author = {Deguet, Anton and Kumar, Rajesh and Taylor, Russell and Kazanzides, Peter},
journal = {Insight},
pages = {1--8},
title = {{The cisst libraries for computer assisted intervention systems}},
url = {http://www.midasjournal.org/browse/publication/294},
year = {2008}
}
@article{Seaman1999b,
annote = {{\textless}m:note{\textgreater}A good reference for Qualitative research in SE.  Many references to other papers.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}Discusses at length the "participant-observer" model. {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}"coding" in this context is translating qualitative data into quantitative.  should be "encoding" in my opinion.  {\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}a single "participant-observer" is error-prone, often without knowing it.  correlating results with more than one observer is advisable, though not foolproof.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}recording an interview can be helpful and informative to the interviewer by keying them in to their own habits.{\textless}m:linebreak/{\textgreater}        {\textless}m:linebreak/{\textgreater}"In other words, the distinction between qualitative and quantitative data has to do with how the information is represented, not whether it is subjective or objective."{\textless}/m:note{\textgreater}},
author = {Seaman, Carolyn B},
journal = {IEEE Transactions on Software Engineering},
keywords = {interview,interviews,participant-observer,qualitative},
number = {24},
pages = {557--572},
title = {{Qualitative Methods in Empirical Studies of Software Engineering}},
volume = {25},
year = {1999}
}
@inproceedings{Chen2005,
abstract = {A common method for GUI testing is the CR (capture and replay) script technique. However, a deficiency of CR based technologies is that test scripts can not be generated before an application under test is ready for testing. Thus, test specification based approaches have been studied as alternatives. In this paper, we propose the concept of integrating the design of GUI specification languages and CR tools. Our approach is to endow a CR tool with the support of writing and replaying GUI specifications. We implemented a visual GUI testing tool (GTT) for Java applications to demonstrate our results. We discuss the techniques used in GTT, including event model, event abstraction, and test points. We also show how to apply GTT in a test-first programming style for GUI testing. {\^{A}}{\textcopyright} 2005 IEEE.},
annote = {Integration of specification-based and CR-based approaches for GUI testing},
author = {Chen, W K and Tsai, T H and Chao, H H},
booktitle = {Proceedings - International Conference on Advanced Information Networking and Applications, AINA},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {967--972},
title = {{Integration of specification-based and CR-based approaches for GUI testing}},
url = {citeulike-article-id:3934571 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33744474870{\&}{\#}38 partnerID=40},
volume = {1},
year = {2005}
}
@techreport{NationalAeronauticsandSpaceAdministration2004a,
address = {NASA-GB-8719.13},
author = {{National Aeronautics and Space Administration}},
institution = {National Aeronautics and Space Administration},
title = {{NASA Software Safety Guidebook}},
year = {2004}
}
@inproceedings{Wu2009b,
abstract = {The timely and reliable data transfer required by many networked applications necessitates the development of comprehensive security solutions to monitor and protect against an increasing number of malicious attacks. However, providing complete cyber space situation awareness is extremely challenging because of the lack of effective translation mechanisms from low-level situation information to high-level human cognition for decision making and action support. We propose an adaptive cyber security monitoring system that integrates a number of component techniques to collect time-series situation information, perform intrusion detection, keep track of event evolution, characterize and identify security events, and present a visual representation in order to provide comprehensive situational view so that corresponding defense actions can be taken in a timely and effective manner. We explore the principles of designing and applying appropriate visualization techniques for situation monitoring by defining graphical representations of security events. This differs from the traditional rule-based pattern matching techniques in that security events in the proposed system are represented as forms of correlation networks using random matrix theory and identified through the computation of network similarity measurement. The events and corresponding event types are visualized using a stemplot to show location and quantity. Extensive simulation results on event identification illustrate the efficacy of the proposed system.},
author = {Wu, Qishi and Ferebee, Denise and Lin, Yunyue and Dasgupta, Dipankar},
booktitle = {2009 IEEE Symposium on Computational Intelligence in Cyber Security},
doi = {10.1109/CICYBS.2009.4925091},
isbn = {978-1-4244-2769-7},
month = {mar},
pages = {61--68},
publisher = {IEEE},
title = {{Visualization of security events using an efficient correlation technique}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4925091},
year = {2009}
}
@inproceedings{Rastogi2016,
address = {Ottawa, Canada},
author = {Rastogi, Ayushi and Nagappan, Nachiappan},
booktitle = {2016 IEEE 27th International Symposium on Software Reliability Engineering (ISSRE)},
doi = {10.1109/ISSRE.2016.43},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rastogi, Nagappan - 2016 - On the Personality Traits of GitHub Contributors.pdf:pdf},
isbn = {978-1-4673-9002-6},
keywords = {sentiment},
mendeley-tags = {sentiment},
month = {oct},
pages = {77--86},
publisher = {IEEE},
title = {{On the Personality Traits of GitHub Contributors}},
url = {http://ieeexplore.ieee.org/document/7774509/},
year = {2016}
}
@article{Vessey1995,
author = {Vessey, Iris and Sravanapudi, Ajay P.},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vessey, Sravanapudi - 1995 - CASE tools as collaborative support technologies.pdf:pdf},
journal = {Communications of the ACM},
keywords = {collaboration},
mendeley-tags = {collaboration},
number = {1},
pages = {83--95},
publisher = {ACM},
title = {{CASE tools as collaborative support technologies}},
url = {http://portal.acm.org/citation.cfm?id=204865.204882{\&}coll=GUIDE{\&}dl=ACM{\&}idx=J79{\&}part=periodical{\&}WantType=periodical{\&}title=Communications of the ACM},
volume = {38},
year = {1995}
}
@inproceedings{5375540,
abstract = {We study techniques to visualize the behavior of malicious software (malware). Our aim is to help human analysts to quickly assess and classify the nature of a new malware sample. Our techniques are based on a parametrized abstraction of detailed behavioral reports automatically generated by sandbox environments. We then explore two visualization techniques: treemaps and thread graphs. We argue that both techniques can effectively support a human analyst (a) in detecting maliciousness of software, and (b) in classifying malicious behavior.},
author = {Trinius, Philipp and Holz, Thorsten and Gobel, Jan and Freiling, Felix C.},
booktitle = {2009 6th International Workshop on Visualization for Cyber Security},
doi = {10.1109/VIZSEC.2009.5375540},
isbn = {978-1-4244-5413-6},
keywords = {malicious software,malware,sandbox environments,th},
pages = {33--38},
publisher = {Ieee},
title = {{Visual analysis of malware behavior using treemaps and thread graphs}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5375540},
year = {2009}
}
@article{Paige2004a,
abstract = {We report on our experiences in teaching lightweight formal methods with Eiffel. In particular, we discuss how we introduce formal methods via Eiffels design-by-contract and agent technologies, and how we integrate these techniques with test-driven development, in an approach called specification-driven design. This approach demonstrates how formal methods techniques fit with industrial software engineering practice. SE  - Teaching Formal Methods CoLogNET/FME Symposium, TFM 2004 Proceedings Lecture Notes in Computer Science Vol 3294},
annote = {English Specification-driven design with Eiffel and agents for teaching lightweight formal methods Conference Paper},
author = {Paige, R F and Ostroff, J S},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Specification-driven design with Eiffel and agents for teaching lightweight formal methods}},
url = {citeulike-article-id:3934746 {\#}},
year = {2004}
}
@article{974517,
abstract = {Attacks and misuses of computer systems are major concerns in today's network-based world. With the growing concern with regard to cyberterrorism there is a need for new tools and techniques to monitor networks and systems for intrusions and misuse. The goal must be to identify an attack before an organization incurs damage, loses information (theft or otherwise), or has its integrity impugned. With today's network-based economic resources, a successful attack will negatively impact consumer confidence and decrease consumers' willingness to make electronic purchases. The authors present information visualization techniques based on a glyph metaphor for visually representing textual log information},
author = {Erbacher, R F and Walker, K L and Frincke, D A},
doi = {10.1109/38.974517},
issn = {0272-1716},
journal = {Computer Graphics and Applications, IEEE},
keywords = {computer systems,consumer confidence,cyberterroris},
number = {1},
pages = {38--47},
title = {{Intrusion and misuse detection in large-scale systems}},
volume = {22},
year = {2002}
}
@inproceedings{Ahadi2015,
abstract = {Copyright 2015 ACM. Methods for automatically identifying students in need of assistance have been studied for decades. Initially, the work was based on somewhat static factors such as students' educational background and results from various questionnaires, while more recently, constantly accumulating data such as progress with course assignments and behavior in lectures has gained attention. We contribute to this work with results on early detection of students in need of assistance, and provide a starting point for using machine learning techniques on naturally accumulating programming process data. When combining source code snapshot data that is recorded from students' programming process with machine learning methods, we are able to detect high- and low-performing students with high accuracy already after the very first week of an introductory programming course. Comparison of our results to the prominent methods for predicting students' performance using source code snapshot data is also provided. This early information on students' performance is beneficial from multiple viewpoints. Instructors can target their guidance to struggling students early on, and provide more challenging assignments for high-performing students. Moreover, students that perform poorly in the introductory programming course, but who nevertheless pass, can be monitored more closely in their future studies.},
address = {New York, New York, USA},
annote = {Evaluated multiple ML classifiers. Trained on one semester, tested on another. Found thar grade average (GPA) and steps (keypresses) in completing assignments were best predictors. Could be used relatively early on.},
author = {Ahadi, Alireza and Lister, Raymond and Haapala, Heikki and Vihavainen, Arto},
booktitle = {ICER 2015 - Proceedings of the 2015 ACM Conference on International Computing Education Research},
doi = {10.1145/2787622.2787717},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ahadi et al. - 2015 - Exploring machine learning methods to automatically identify students in need of assistance.pdf:pdf},
isbn = {9781450336284},
keywords = {CS1,CS2,Detecting students in need of assistance,Educational data mining,Introductory programming,Learning analytics,Novice programmers,Programming behavior,Source code snapshot analysis},
mendeley-tags = {CS1,CS2},
pages = {121--130},
publisher = {ACM Press},
title = {{Exploring machine learning methods to automatically identify students in need of assistance}},
url = {http://dl.acm.org/citation.cfm?doid=2787622.2787717},
year = {2015}
}
@misc{Fogarty2005,
address = {Portland, OR},
author = {Fogarty, James and Ko, Andrew J and Aung, Htet Htet and Golden, Elspeth and Tang, Karen P and Hudson, Scott E},
pages = {331--340},
title = {{Examining Task Engagement in Sensor-Based Statistical Models of Human Interruptibility}},
year = {2005}
}
@article{Reifer2003c,
author = {Reifer, Donald J.},
doi = {10.1109/MS.2003.1196314},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Reifer - 2003 - XP and the CMM.pdf:pdf},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {agile},
mendeley-tags = {agile},
month = {may},
number = {3},
pages = {14--15},
title = {{XP and the CMM}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1196314},
volume = {20},
year = {2003}
}
@article{Peluso1999,
author = {Peluso, Teresa and Ricciardelli, Lina A and Williams, Robert J},
journal = {Addictive Behaviors},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
number = {5},
pages = {715--718},
title = {{Self-control in relation to problem drinking and symptoms of disordered eating}},
volume = {24},
year = {1999}
}
@inproceedings{Chetty2011,
address = {New York, New York, USA},
author = {Chetty, Marshini and Haslem, David and Baird, Andrew and Ofoha, Ugochi and Sumner, Bethany and Grinter, Rebecca},
booktitle = {Proceedings of the 2011 annual conference on Human factors in computing systems - CHI '11},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chetty et al. - 2011 - Why is my internet slow.pdf:pdf},
keywords = {broadband speed,broadband tools,home networks},
month = {may},
pages = {1889},
publisher = {ACM Press},
title = {{Why is my internet slow?}},
url = {http://dl.acm.org/citation.cfm?id=1978942.1979217},
year = {2011}
}
@inproceedings{Maier1995,
address = {Bruges, Belgium},
author = {Maier, Thomas},
booktitle = {CSR 12th Annual Workshop on Safety and Reliability of Software Based Systems},
title = {{FMEA and FTA to Support Safe Design of Embedded Software in Safety-Critical Systems}},
year = {1995}
}
@inproceedings{Begel2009,
address = {Vancouver, BC},
author = {Begel, Andrew and Nagappan, Nachiappan and Poile, Christopher and Layman, Lucas},
booktitle = {2009 ICSE Workshop on Cooperative and Human Aspects on Software Engineering},
doi = {10.1109/CHASE.2009.5071401},
isbn = {978-1-4244-3712-2},
keywords = {mypubs},
mendeley-tags = {mypubs},
month = {may},
pages = {1--7},
publisher = {IEEE},
title = {{Coordination in large-scale software teams}},
year = {2009}
}
@article{Kerievsky2002,
abstract = {Patterns are a cornerstone of object-oriented design, while test-first programming and merciless refactoring are cornerstones of evolutionary design. To stop over- or under-engineering, balance these practices and evolve only what you need},
annote = {Stop over-engineering! [Software patterns]},
author = {Kerievsky, J},
isbn = {1070-8588},
journal = {Software Development},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {4},
pages = {44--46},
title = {{Stop over-engineering! [Software patterns]}},
url = {citeulike-article-id:3934672 {\#}},
volume = {10},
year = {2002}
}
@article{Holzmann2013,
author = {Holzmann, Gerard J.},
doi = {10.1109/MS.2013.32},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {Earth,Encoding,European Space Agency Mars Express,Hardware,Mars,Mars Reconnaissance Orbiter,NASA Curiosity rover communication,NASA Mars orbiters,NASA Mars rover drivers,NASA jet propulsion laboratory,Odyssey,Performance evaluation,Rocknest,Software devlopment,Space missions,Space vehicles,aerospace computing,artificial satellites,code reliability,descent and landing (spacecraft),entry,impact,planetary rovers,software,software development process,software reliability,spacecraft,spacecraft landing},
language = {English},
month = {mar},
number = {2},
pages = {83--86},
publisher = {IEEE},
title = {{Landing a Spacecraft on Mars}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6470593},
volume = {30},
year = {2013}
}
@inproceedings{Tilley2004,
abstract = {Test-driven development is a relatively new approach to software engineering, involving the iterative construction of test cases first, and then the application code that passes the test cases second. This panel session will discuss the impacts of test-driven development on long-term software maintenance costs. The panelists represent different research disciplines related to this topic, including software maintenance, software testing, program redocumentation, program understanding, and empirical studies. {\^{A}}{\textcopyright} 2004 IEEE.},
annote = {Test-driven development and software maintenance},
author = {Tilley, S and {Van Deursen}, A and Kaner, C and Linos, P},
booktitle = {IEEE International Conference on Software Maintenance, ICSM},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {488--489},
title = {{Test-driven development and software maintenance}},
url = {citeulike-article-id:3934817 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-18044390110{\&}{\#}38 partnerID=40},
year = {2004}
}
@misc{Mishra2007,
abstract = {In Test-Driven development, first test is made according to the customer requirement and then code is prepared to execute this test successfully. In this approach, design is not done exclusively before preparing test cases and coding. Design emerges as software evolves but this may result in lack of design quality. We adapted TDD by incorporating exclusive architectural design phase in the successful implementation of an innovative, large scale, complex project. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
annote = {Adapting test-driven development for innovative software development project},
author = {Mishra, D and Mishra, A},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {171--172},
title = {{Adapting test-driven development for innovative software development project}},
url = {citeulike-article-id:3934722 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38149015556{\&}{\#}38 partnerID=40},
volume = {4536 LNCS},
year = {2007}
}
@inproceedings{McDowell2003,
address = {Portland, OR},
author = {McDowell, C and Werner, L and Bullock, H and Fernald, J},
pages = {602--607},
publisher = {ACM},
title = {{The Impact of Pair Programming on Student Performance, Perception, and Persistence}},
year = {2003}
}
@inproceedings{4565029,
abstract = {When multiple users with diverse backgrounds and beliefs edit Wikipedia together, disputes often arise due to disagreements among the users. In this paper, we introduce a novel visualization tool known as WikiNetViz to visualize and analyze disputes among users in a dispute-induced social network. WikiNetViz is designed to quantify the degree of dispute between a pair of users using the article history. Each user (and article) is also assigned a controversy score by our proposed controversy rank model so as to measure the degree of controversy of a user (and an article) by the amount of disputes between the user (article) and other users in articles of varying degrees of controversy. On the constructed social network, WikiNetViz can perform clustering so as to visualize the dynamics of disputes at the user group level. It also provides an article viewer for examining an article revision so as to determine the article content modified by different users.},
author = {Datta, Anwitaman},
booktitle = {2008 IEEE International Conference on Intelligence and Security Informatics},
doi = {10.1109/ISI.2008.4565029},
isbn = {978-1-4244-2414-6},
keywords = {WikiNetViz,Wikipedia,controversy rank model,implic},
month = {jun},
pages = {52--57},
publisher = {Ieee},
title = {{WikiNetViz: Visualizing friends and adversaries in implicit social networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4565029},
year = {2008}
}
@inproceedings{Thomas2002,
address = {Covington, KY},
author = {Thomas, L and Ratcliffe, M and Woodbury, J and Jarman, E},
keywords = {learning styles},
pages = {33--37},
title = {{Learning Styles and Performance in the Introductory Programming Sequence}},
year = {2002}
}
@article{Campbell2008,
author = {Campbell, Colin and Tillmann, Nikolai and Grieskamp, Wolfgang and Schulte, Wolfram and Nachmanson, Lev},
journal = {Lecture Notes in Computer Science},
pages = {39--76},
title = {{Model-Based Testing of Object-Oriented Reactive Systems with Spec Explorer}},
volume = {4949},
year = {2008}
}
@inproceedings{Chinn2007a,
abstract = {Active learning techniques, including collaborative programming and problem solving environments, have been widely adopted by many computer science educators. A related approach is the Treisman model, which was originally designed for the first-year calculus course and involves intensive workshops where students collaborate in small groups to solve problems. We have adapted the model for both the data structures and algorithms courses at our institution. Regression analysis indicates that students who participate in the workshops for the algorithms course perform better (0.561 grade points on a 4-point scale) than those who do not, even after accounting for prior academic performance. However, the workshops appear to have less of an effect on student grades in the data structures course. This study provides evidence that the workshop model can be an effective learning environment for students in courses primarily involving analysis, but that for courses that involve large amounts of programming, further adaptations to the model might be needed.},
address = {Covington, KY, USA},
author = {Chinn, Donald and Martin, Kristofer and Spencer, Catherine},
booktitle = {SIGCSE 2007: 38th SIGCSE Technical Symposium on Computer Science Education},
doi = {10.1145/1227310.1227383},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chinn, Martin, Spencer - 2007 - Treisman workshops and student performance in CS.pdf:pdf},
isbn = {1595933611},
issn = {0097-8418},
keywords = {CS2,Collaborative learning environments,Problem-based learning,Treisman workshops},
mendeley-tags = {CS2},
number = {1},
pages = {203--207},
publisher = {ACM Press},
title = {{Treisman workshops and student performance in CS}},
url = {http://portal.acm.org/citation.cfm?doid=1227310.1227383},
volume = {39},
year = {2007}
}
@inproceedings{Chandola2010b,
address = {New York, New York, USA},
author = {Chandola, Varun and Boriah, Shyam and Kumar, Vipin},
booktitle = {Proceedings of the Sixth Annual Workshop on Cyber Security and Information Intelligence Research - CSIIRW '10},
doi = {10.1145/1852666.1852703},
isbn = {9781450300179},
keywords = {anomaly detection,intrusion detection,reference based analysis},
month = {apr},
pages = {1},
publisher = {ACM Press},
title = {{A reference based analysis framework for analyzing system call traces}},
url = {http://dl.acm.org/citation.cfm?id=1852666.1852703},
year = {2010}
}
@book{Jajodia2011,
address = {Berlin, Heidelberg},
doi = {10.1007/978-3-642-25560-1},
editor = {Jajodia, Sushil and Mazumdar, Chandan},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2011 - Information Systems Security.pdf:pdf},
isbn = {978-3-642-25559-5},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Information Systems Security}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-25560-1},
volume = {7093},
year = {2011}
}
@inproceedings{Adelson1984,
author = {Adelson, B and Littman, D and Ehrleich, K and Black, J and Soloway, E},
keywords = {design,psychology},
pages = {187--192},
title = {{Novice-Expert Differences in Software Design}},
year = {1984}
}
@inproceedings{Alonso2013,
address = {Budapest, Hungary},
author = {Alonso, Javier and Grottke, Michael and Nikora, Allen P and Trivedi, Kishor S},
booktitle = {Proc. 43rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks},
isbn = {978-1-4673-6471-3},
pages = {1--8},
title = {{An empirical investigation of fault repairs and mitigations in space mission system software}},
year = {2013}
}
@article{Fenton2000,
author = {Fenton, N.E. and Ohlsson, N.},
doi = {10.1109/32.879815},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fenton, Ohlsson - 2000 - Quantitative analysis of faults and failures in a complex software system.pdf:pdf},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Benchmark testing,Computer industry,Density measurement,Failure analysis,Pareto principle,Phase measurement,Programming,Software engineering,Software metrics,Software systems,Software testing,basic software engineering hypotheses,benchmarking,commercial software system,complex software system faults,complexity metrics,counter-intuitive relationship,data-points,early fault data,failure data,failure-prone modules,fault density,fault density measure,fault prediction,fault-prone postrelease,fault-prone prerelease,module size,operational use,postrelease faults,prerelease testing,quantitative analysis,quantitative study,software development process,software metrics,software performance evaluation,software reliability},
language = {English},
number = {8},
pages = {797--814},
publisher = {IEEE},
title = {{Quantitative analysis of faults and failures in a complex software system}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=879815},
volume = {26},
year = {2000}
}
@misc{Williams2004a,
address = {North Carolina State University, Department of Computer Science TR-2003-20},
author = {Williams, L and Krebs, W and Layman, L and Krebs, W and Layman, L},
isbn = {TR-2004-18},
keywords = {mypubs},
mendeley-tags = {mypubs},
publisher = {North Carolina State University Department of Computer Science},
title = {{Extreme Programming Evaluation Framework for Object-Oriented Languages -- Version 1.1}},
year = {2004}
}
@article{Shull2010a,
abstract = {What if someone argued that one of your basic conceptions about how to develop software was misguided? What would it take to change your mind? That's essentially the dilemma faced by advocates of test-driven development (TDD). The TDD paradigm argues that the basic cycle of developing code and then testing it to make sure it does what it's supposed to do-something drilled into most of us from the time we began learning software development- isn't the most effective approach. TDD replaces the traditional "code then test" cycle. First, you develop test cases for a small increment of functionality; then you write code that makes those tests run correctly. After each increment, you refactor the code to maintain code quality.},
author = {Shull, Forrest and Melnik, Grigori and Turhan, Burak and Layman, Lucas and Diep, Madeline and Erdogmus, Hakan},
doi = {http://dx.doi.org/10.1109/MS.2010.152},
journal = {IEEE Software},
keywords = {mypubs},
mendeley-tags = {mypubs},
number = {6},
pages = {16--19},
title = {{What Do We Know about Test-Driven Development}},
volume = {27},
year = {2010}
}
@article{Pratt2010,
abstract = {Routine activity theory predicts that changes in legitimate opportunity structures (e.g., technology) can increase the convergence of motivated offenders and suitable targets in the absence of capable guardianship. The Internet has fundamentally changed consumer practices and has simultaneously expanded opportunities for cyber-fraudsters to target online consumers. The authors draw on routine activity theory and consumer behavior research to understand how personal characteristics and online routines increase people's exposure to motivated offenders. Using a representative sample of 922 adults from a statewide survey in Florida, the results of the regression models are consistent with prior research in that sociodemographic characteristics shape routine online activity (e.g., spending time online and making online purchases). Furthermore, indicators of routine online activity fully mediate the effect of sociodemographic characteristics on the likelihood of being targeted for fraud online. These findings support the routine activity perspective and provide a theoretically informed direction for situational crime prevention in a largely unexplored consumer context.},
author = {Pratt, Travis C. and Holtfreter, Kristy and Reisig, Michael D.},
doi = {10.1177/0022427810365903},
issn = {0022-4278},
journal = {Journal of Research in Crime and Delinquency},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {aug},
number = {3},
pages = {267--296},
title = {{Routine Online Activity and Internet Fraud Targeting: Extending the Generality of Routine Activity Theory}},
url = {http://jrc.sagepub.com/content/47/3/267.short},
volume = {47},
year = {2010}
}
@article{Weiser1985,
author = {Weiser, M.D. and Gannon, J.D. and McMullin, P.R.},
doi = {10.1109/MS.1985.230356},
issn = {0740-7459},
journal = {IEEE Software},
keywords = {test coverage},
mendeley-tags = {test coverage},
month = {mar},
number = {2},
pages = {80--85},
title = {{Comparison of Structural Test Coverage Metrics}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1695302},
volume = {2},
year = {1985}
}
@inproceedings{Ball2004b,
address = {New York, New York, USA},
author = {Ball, Robert and Fink, Glenn A. and North, Chris},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029217},
isbn = {1581139748},
keywords = {information visualization,networks,security},
month = {oct},
pages = {55},
publisher = {ACM Press},
title = {{Home-centric visualization of network traffic for security administration}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029217},
year = {2004}
}
@book{Highsmith2002,
address = {Boston, MA},
author = {Highsmith, J and Cockburn, Alistair and Highsmith, Jim},
publisher = {Addison-Wesley},
title = {{Agile Software Development Ecosystems}},
year = {2002}
}
@inproceedings{6062341,
abstract = {Recently security problems in the Graphic User Interface (GUI) of applications have become a serious threat for system security. Because much of security experts don't design the GUI from end user's point of view, users have problems to practice security. The aim of Human amp; Computer Interaction (HCI) and Security (HCI-Sec) is to improve the usability of security features in end user applications. In this paper we apply the resources model (a model in HCI) to analyzing and designing system GUI with a security perspective to achieve a more secure and usable system. We studied Tests part of E-learning system in Ferdowsi University of Mashhad (FUM) as our case study. And we exploited faults that slow down user co-ordination with the system and used this model to explore design alternative. We generally analyzed GUI and proposed an alternative GUI in order to solve interaction problems. Finally we analyzed the GUI with a security perspective to improve the usability of security issues in this system. The results show this model works very well in the field of security.},
author = {Mehrnejad, M and Toreini, E and Bafghi, A G},
booktitle = {Information Security and Cryptology (ISCISC), 2011 8th International ISC Conference on},
doi = {10.1109/ISCISC.2011.6062341},
keywords = {FUM,Ferdowsi University of Mashhad,GUI,GUI design},
pages = {29--36},
title = {{Security analyzing and designing GUI with the resources model}},
year = {2011}
}
@article{Guthart00,
author = {Guthart, G S and Salisbury, J K},
journal = {Proc. IEEE Intl. Conf. on Roboticss and Automation (ICRA)},
pages = {618--621},
title = {{The Intuitive telesurgery system: Overview and application}},
volume = {1},
year = {2000}
}
@article{Basili1987a,
abstract = {This study applies an experimentation methodology to compare three state-of-the-practice software testing techniques: a) code reading by stepwise abstraction, b) functional testing using equivalence partitioning and boundary value analysis, and c) structural testing using 100 percent statement coverage criteria. The study compares the strategies in three aspects of software testing: fault detection effectiveness, fault detection cost, and classes of faults detected. Thirty-two professional programmers and 42 advanced students applied the three techniques to four unit-sized programs in a fractional factorial experimental design. The major results of this study are the following. 1) With the professional programmers, code reading detected more software faults and had a higher fault detection rate than did functional or structural testing, while functional testing detected more faults than did structural testing, but functional and structural testing were not different in fault detection rate. 2) In one advanced student subject group, code reading and functional testing were not different in faults found, but were both superior to structural testing, while in the other advanced student subject group there was no difference among the techniques. 3) With the advanced student subjects, the three techniques were not different in fault detection rate. 4) Number of faults observed, fault detection rate, and total effort in detection depended on the type of software tested. 5) Code reading detected more interface faults than did the other methods. 6) Functional testing detected more control faults than did the other methods.},
author = {Basili, V.R. and Selby, R.W.},
doi = {10.1109/TSE.1987.232881},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
month = {dec},
number = {12},
pages = {1278--1296},
title = {{Comparing the Effectiveness of Software Testing Strategies}},
url = {http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=1702179},
volume = {SE-13},
year = {1987}
}
@inproceedings{Murphy2008a,
abstract = {A qualitative analysis of debugging strategies of novice Java programmers is presented. The study involved 21 CS2 students from seven universities in the U.S. and U.K. Subjects "warmed up" by coding a solution to a typical introductory problem. This was followed by an exercise debugging a syntactically correct version with logic errors. Many novices found and fixed bugs using strategies such as tracing, commenting out code, diagnostic print statements and methodical testing. Some competently used online resources and debuggers. Students also used pattern matching to detect errors in code that "just didn't look right". However, some used few strategies, applied them ineffectively, or engaged in other unproductive behaviors. This led to poor performance, frustration for some, and occasionally the introduction of new bugs. Pedagogical implications and suggestions for future research are discussed.},
address = {Portland, OR, USA},
author = {Murphy, Laurie and Lewandowski, Gary and McCauley, Ren{\'{e}}e and Simon, Beth and Thomas, Lynda and Zander, Carol},
booktitle = {SIGCSE'08 - Proceedings of the 39th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/1352135.1352191},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murphy et al. - 2008 - Debugging The good, the bad, and the quirky -a qualitative analysis of novices' strategies.pdf:pdf},
isbn = {9781595937995},
issn = {0097-8418},
keywords = {CS2,Debugging,Novice programming,Pedagogy,Strategies},
mendeley-tags = {CS2},
number = {1},
pages = {163--167},
publisher = {ACM Press},
title = {{Debugging: The good, the bad, and the quirky -a qualitative analysis of novices' strategies}},
url = {http://portal.acm.org/citation.cfm?doid=1352135.1352191},
volume = {40},
year = {2008}
}
@inproceedings{Jongpil2007,
abstract = {This study proposed the solutions of three problems. The first problem is that there is no unified platform to integrate various methodologies. The second problem is that UML tools need improvements for practical use. The third problem is that even if the developed code matches with requirement specs, there are bugs. This study provides an unified and integrated test environment (UTE) that can be used to combine various methodologies. UTE can remove the dummy test cases and make the test cases practically usable. TDD or other methods only focus on meeting the requirement specs. This study provides test cases for the missing specs. {\^{A}}{\textcopyright} 2007 IEEE.},
annote = {Unified test environment-integrated platform for bridging the modeling, testing and code development flow},
author = {Jongpil, C and Islam, S and Shankar, R},
booktitle = {Proceedings of the 1st Annual 2007 IEEE Systems Conference},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {37--43},
title = {{Unified test environment-integrated platform for bridging the modeling, testing and code development flow}},
url = {citeulike-article-id:3934667 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34748861528{\&}{\#}38 partnerID=40},
year = {2007}
}
@article{Hummel2007,
abstract = {Component-based software reuse has been widely accepted as a way of making software development faster, better, and cheaper. However, component markets of the kind envisaged for many decades have not yet become a useful tool in mainstream development. In this article, the authors discuss the underlying problems and present a new opproach called "Extreme Harvesting" for test-driven component retrieval. They present examples that demonstrate how this concept works "in vitro", demonstrate its precision with the help of an experiment and discuss forther challenges to be solved to make this approach of practical utility.},
annote = {Improving the retrieval efficiency of software component markets},
author = {Hummel, O and Atkinson, C},
journal = {Wirtschaftsinformatik},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {6},
pages = {430--438},
title = {{Improving the retrieval efficiency of software component markets}},
url = {citeulike-article-id:3934649 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-37749022113{\&}{\#}38 partnerID=40},
volume = {49},
year = {2007}
}
@techreport{Norton2012a,
author = {Norton},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Norton by Symantec - 2012 - 2012 Norton Cybercrime Report.pdf:pdf},
institution = {Norton by Symantec},
keywords = {cybercrime,security},
mendeley-tags = {cybercrime,security},
title = {{2012 Norton Cybercrime Report}},
url = {http://now-static.norton.com/now/en/pu/images/Promotions/2012/cybercrimeReport/2012{\_}Norton{\_}Cybercrime{\_}Report{\_}Master{\_}FINAL{\_}050912.pdf},
year = {2013}
}
@inproceedings{Layman2008d,
address = {New York, New York, USA},
author = {Layman, Lucas and Nagappan, Nachiappan and Guckenheimer, Sam and Beehler, Jeff and Begel, Andrew},
booktitle = {Proceedings of the 2008 International Working Conference on Mining software repositories - MSR '08},
doi = {10.1145/1370750.1370762},
isbn = {9781605580241},
keywords = {effort estimation,mypubs,prediction},
mendeley-tags = {mypubs},
month = {may},
pages = {43--46},
publisher = {ACM Press},
title = {{Mining software effort data: A preliminary analysis of Visual Studio Team System Data}},
year = {2008}
}
@article{Aharoni2000,
abstract = {actions process object model},
address = {Austin, TX, USA},
author = {Aharoni, Dan},
doi = {10.1145/331795.331804},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aharoni - 2000 - Cogito, ergo sum! Cognitive processes of students dealing with data structures.pdf:pdf},
isbn = {1581132131},
issn = {00978418},
journal = {SIGCSE Bulletin (Association for Computing Machinery, Special Interest Group on Computer Science Education)},
keywords = {CS2},
mendeley-tags = {CS2},
number = {1},
pages = {26--30},
publisher = {ACM Press},
title = {{Cogito, ergo sum! Cognitive processes of students dealing with data structures}},
url = {http://portal.acm.org/citation.cfm?doid=330908.331804},
volume = {32},
year = {2000}
}
@inproceedings{Altadmri2015,
abstract = {Copyright {\textcopyright} 2015 ACM. Previous investigations of student errors have typically focused on samples of hundreds of students at individual institutions. This work uses a year's worth of compilation events from over 250,000 students all over the world, taken from the large Blackbox data set. We analyze the frequency, time-to-fix, and spread of errors among users, showing how these factors inter-relate, in addition to their development over the course of the year. These results can inform the design of courses, textbooks and also tools to target the most frequent (or hardest to fix) errors.},
address = {Kansas City, MO, USA},
author = {Altadmri, Amjad and Brown, Neil C.C.},
booktitle = {SIGCSE 2015 - Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
doi = {10.1145/2676723.2677258},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Altadmri, Brown - 2015 - 37 million compilations Investigating novice programming mistakes in large-scale student data.pdf:pdf},
isbn = {9781450329668},
keywords = {Blackbox,CS2,Programming mistakes},
mendeley-tags = {CS2},
pages = {522--527},
publisher = {ACM Press},
title = {{37 million compilations: Investigating novice programming mistakes in large-scale student data}},
url = {http://dl.acm.org/citation.cfm?doid=2676723.2677258},
year = {2015}
}
@inproceedings{Xie2012,
address = {Lund, Sweden},
author = {Xie, Xihao and Zhang, Wen and Yang, Ye and Wang, Qing},
booktitle = {Proceedings of the 8th International Conference on Predictive Models in Software Engineering - PROMISE '12},
doi = {10.1145/2365324.2365329},
isbn = {9781450312417},
keywords = {developer recommendation,latent dirichlet allocation,open bug repository,topic models},
month = {sep},
pages = {19--28},
publisher = {ACM Press},
title = {{DRETOM: Developer recommendation based on topic models for bug resolution}},
url = {http://dl.acm.org/citation.cfm?id=2365324.2365329},
year = {2012}
}
@misc{Kou2006,
abstract = {Zorro is a system designed to automatically determine whether a developer is complying with the Test-Driven Development (TDD) process. Automated recognition of TDD could benefit the software engineering community in a variety of ways, from pedagogical aids to support the learning of test-driven design, to support for more rigorous empirical studies on the effectiveness of TDD in practice. This paper presents the Zorro system and the results of a pilot validation study, which shows that Zorro was able to recognize test-driven design episodes correctly 89{\%} of the time. The results also indicate ways to improve Zorro's classification accuracy further, and provide evidence for the effectiveness of this approach to low-level software process recognition. {\^{A}}{\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
annote = {Automated recognition of low-level process: A pilot validation study of zorro for test-driven development},
author = {Kou, H and Johnson, P M},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {322--333},
title = {{Automated recognition of low-level process: A pilot validation study of zorro for test-driven development}},
url = {citeulike-article-id:3934684 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-33745925578{\&}{\#}38 partnerID=40},
volume = {3966 LNCS},
year = {2006}
}
@inproceedings{Dong2011,
address = {Beijing, China},
author = {Dong, Wen and Lepri, Bruno and Pentland, Alex (Sandy)},
booktitle = {Proceedings of the 10th International Conference on Mobile and Ubiquitous Multimedia (MUM '11)},
doi = {10.1145/2107596.2107613},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dong, Lepri, Pentland - 2011 - Modeling the co-evolution of behaviors and social relationships using mobile phone data.pdf:pdf},
isbn = {9781450310963},
keywords = {agile,human dynamics,living lab,multi-agent model,nsf,social computing,stochastic process},
mendeley-tags = {agile,nsf},
month = {dec},
pages = {134--143},
publisher = {ACM Press},
title = {{Modeling the co-evolution of behaviors and social relationships using mobile phone data}},
url = {http://dl.acm.org/citation.cfm?id=2107596.2107613},
year = {2011}
}
@phdthesis{Shah2003,
abstract = {The fundamentals of software testing and related activities are often elusive in undergraduate curricula. A direct consequence of the lack of software testing efforts during education is the huge losses suffered by the software industry when applications are not sufficiently tested. Software practitioners have exhorted faculty members and institutions to teach more software testing in universities. The purpose of this research is to provide answers to the needs of such practitioners and introduce software-testing activities throughout the curriculum. The most important goal is to introduce software-testing education without requiring a significant amount of extra effort on behalf of faculty members or teaching assistants. The approach taken comprises the development of the Web-based Center for Automated Testing (Web-CAT) and the introduction of test-driven development (TDD) in courses. Web-CAT serves as a learning environment for software testing tasks and helps automatically assess student assignments. A comparison of student programs developed using Web-CAT with historical records indicated a significant decrease in the number of bugs in submitted programs. Undergraduate students also received exposure to the principles of software testing and were able to write test cases that were on an average better than those generated by an automated test case generator designed specifically for the assignment.},
annote = {Web-CAT: A Web-based Center for Automated Testing},
author = {Shah, Anuj},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Web-CAT: A Web-based Center for Automated Testing}},
url = {citeulike-article-id:3934790 {\#}},
year = {2003}
}
@article{Olan2003,
abstract = {Testing is a critical part of good software development, but often gets only minimal coverage in introductory programming courses. Unit testing and selected aspects of test-driven development can be used to improve learning and encourage emphasis on quality and correctness. Tools like JUnit significantly simplify the generation of test cases. An additional benefit for instructors is that these tools can also be used to automate project grading. AD  -, USA},
annote = {Unit testing: test early, test often},
author = {Olan, Michael},
isbn = {1937-4771},
journal = {J. Comput. Small Coll.},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {319--328},
title = {{Unit testing: test early, test often}},
url = {citeulike-article-id:3934741 http://portal.acm.org/citation.cfm?id=948785.948830},
volume = {19},
year = {2003}
}
@misc{VerizonRiskTeam2013,
author = {{Verizon Risk Team}},
howpublished = {$\backslash$url{\{}http://www.verizonenterprise.com/DBIR/2013/{\}}},
institution = {Verizon},
title = {{2013 Data Breach Investigations Report}},
url = {http://www.verizonenterprise.com/DBIR/2013/},
urldate = {2019-01-09},
year = {2013}
}
@article{Shepard2011,
author = {Shepard, Clayton and Rahmati, Ahmad and Tossell, Chad and Zhong, Lin and Kortum, Phillip},
doi = {10.1145/1925019.1925023},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shepard et al. - 2011 - LiveLab.pdf:pdf},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
month = {jan},
number = {3},
pages = {15--20},
publisher = {ACM},
title = {{LiveLab}},
url = {http://dl.acm.org/citation.cfm?id=1925019.1925023},
volume = {38},
year = {2011}
}
@article{Gilovich1985,
abstract = {We investigate the origin and the validity of common beliefs regarding “the hot hand” and “streak shooting” in the game of basketball. Basketball players and fans alike tend to believe that a player's chance of hitting a shot are greater following a hit than following a miss on the previous shot. However, detailed analyses of the shooting records of the Philadelphia 76ers provided no evidence for a positive correlation between the outcomes of successive shots. The same conclusions emerged from free-throw records of the Boston Celtics, and from a controlled shooting experiment with the men and women of Cornell's varsity teams. The outcomes of previous shots influenced Cornell players' predictions but not their performance. The belief in the hot hand and the “detection” of streaks in random sequences is attributed to a general misconception of chance according to which even short random sequences are thought to be highly representative of their generating process.},
author = {Gilovich, Thomas and Vallone, Robert and Tversky, Amos},
doi = {10.1016/0010-0285(85)90010-6},
issn = {00100285},
journal = {Cognitive Psychology},
month = {jul},
number = {3},
pages = {295--314},
title = {{The hot hand in basketball: On the misperception of random sequences}},
url = {http://www.sciencedirect.com/science/article/pii/0010028585900106},
volume = {17},
year = {1985}
}
@incollection{Bossler2011,
author = {Bossler, Adam M. and Holt, Thomas J.},
booktitle = {Cyber Criminology: Exploring Internet Crimes and Criminal Behavior},
editor = {Jaishankar, K},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
pages = {317--346},
publisher = {CRC Press},
title = {{Malware Victimization: A Routine Activities Framework}},
year = {2011}
}
@article{Carroll1980,
abstract = {Two experimental studies of design problem-solving area presented. Eighty-one subjects worked on one of two design problems that were ismorphic in structure: a schedule for stages in a manufacturing process or a layout for a business office.  In Expt. 1, a difference between problem isomorphs is obtained: the 'spation' office layout problem obtains better perfromance and shorter solution times than the 'temporal' scheduling problem.  In Expt. 2, this difference attenuates when subjects are provided with a graphic representation in both isomorp conditions.  the availability of a graphic representation is discussed as an aid for procedural design.},
annote = {Design is an "ill-structured problem"

Designers often don't know what the desired goal state is, or have a aclear definition of the initial problem state.

Spatial isomorphs lead to better performance, faster solution times, and occasioned fewer comprehension failures that temporal isomorphs.},
author = {Carroll, J M and Thomas, J C and Malhotra, A},
keywords = {design,experiment,psychology,spatial,temporal},
number = {1},
pages = {143--153},
title = {{Presentation and Representation in Design Problem-solving}},
volume = {71},
year = {1980}
}
@article{Ying2004,
author = {Ying, Annie T T and Murphy, Gail C and Ng, Raymond and Chu-Carroll, Mark C},
number = {9},
pages = {574--586},
title = {{Predicting Source Code Changes by Mining Change History}},
volume = {30},
year = {2004}
}
@incollection{Wu2007,
address = {Boston, MA},
author = {Wu, Bing and Chen, Jianmin and Wu, Jie and Cardei, Mihaela},
booktitle = {Wireless Network Security},
doi = {10.1007/978-0-387-33112-6},
editor = {Xiao, Yang and Shen, Xuemin Sherman and Du, Ding-Zhu},
isbn = {978-0-387-28040-0},
keywords = {agile,eavesdropping,nsf},
mendeley-tags = {agile,eavesdropping,nsf},
pages = {103--135},
publisher = {Springer US},
series = {Signals and Communication Technology},
title = {{A Survey of Attacks and Countermeasures in Mobile Ad Hoc Networks}},
url = {http://www.springerlink.com/index/10.1007/978-0-387-33112-6},
year = {2007}
}
@inproceedings{Hennessy2006,
abstract = {In this paper we describe the porting of the C++ analysis tool, keystone [1], from C++ to Java using a testing strategy that ensured the correctness of the ported code. Our work extends the eXtreme Porting approach outlined by Varma et al. for porting C/C++ applications [2]. This process operates by porting one class at a time and then unit testing to ensure correctness. We extend this by defining an order for the porting of classes by using an Object Relation Diagram in order to minismise the need for class-stubs during unit-testing.The overall structure of the porting process was as follows:Step 1 The keystone front- and back-end were decoupled. Fortunately the keystone backend classes are easily identified.Step 2 The back-end keystone classes were then ported to Java. This consisted of three steps:Step 2a Dependencies on external libraries were identified. This involved mapping classes in the C++ Standard Template Library, to the Java class library.Step 2b The jKeystone class hierarchy was generated. The class hierarchy of keystone was reverse engineered using an in-house tool, and the corresponding class hierarchy of jKeystone was generated. No method-body code was ported automatically in this step.Step 2c Each class in turn was then manually ported and unit-tested, in the vein of the eXtreme Porting approach.Step 3 keystone's front-end was modified so that when run over a test case, it generated a Java test harness for that test case. These were then used to perform black-box system tests on jKeystone.Step 4 Aspect Oriented Programming was used to weave similar tracing aspects across both keystone and jKeystone. For each test case, the aspected version of keystone and jKeystone produced a sequence diagram, containing the objects and interactions involved in processing that test case. These sequence diagrams were then compared to verify that the behaviour of jKeystone was identical on a per test-case basis.The full version of this paper describes this process in detail, and analyses its impact in terms of its performance and its effectiveness in exposing bugs.},
address = {New York, NY, USA},
annote = {Ensuring behavioural equivalence in test-driven porting},
author = {Hennessy, Mark and Power, James},
booktitle = {CASCON '06: Proceedings of the 2006 conference of the Center for Advanced Studies on Collaborative research},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
publisher = {ACM Press},
title = {{Ensuring behavioural equivalence in test-driven porting}},
url = {citeulike-article-id:3934638 http://dx.doi.org/10.1145/1188966.1189017},
year = {2006}
}
@phdthesis{Hennessy2006a,
abstract = {Grammar-based software is increasingly becoming a prominent and well defined subset of software engineering through the popularity of analysis tools, metrics evaluators and the increasing prevalence of software tools that rely upon extensible data such as XML. Software-testing plays a crucial role in the lifecycle of any modern software system, hence the correct and adequate testing of grammar-based software is essential. This thesis provides a review of the current research and practice in the field of grammar-based software. The motivation behind software-testing is examined and a summary of existing testing techniques is presented. This leads to a detailed empirical comparison of test-suites for grammar-based software. The effectiveness of test-suite generation via methods such as Purdom{\^{a}}€™s algorithm is contrasted with test-suite reduction via a novel elaboration of the notion of coverage for a grammar. The effectiveness of this strategy is discussed with regard to two testing analysis criteria: code coverage and fault detection. The second contribution of this thesis is the definition of a framework for developing grammar-based software within a strict test-driven environment. We describe the techniques used to develop a static analysis tool for the ISO C++ grammar and the verification of its correct operation via standardised test-suites. This work was completed in two distinct phases. The first phase involves porting an existing system from C++ to Java by combining eXtreme Porting with an order for porting derived from a novel use of Object Relation Diagrams. The second phase involves a large automated testing process including black-box testing, coverage comparisons and comparisons of reverse-engineered UML sequence diagrams from program traces. The third contribution of this thesis involves the utilisation of the generalised parsing algorithm, GLR, in the implementation of a parser generator. We describe the design and test-driven development of this parser generator, and its practical application as part of an analysis tool for ISO C++. Finally, we provide a study of the performance of a generated GLR parser when parsing ISO C++ programs, and a comparison with an existing system based on a generated backtracking parser.},
annote = {A test-driven development strategy for the construction of grammar-based software},
author = {Hennessy, M},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
publisher = {Department of Computer Science, National University of Ireland,},
title = {{A test-driven development strategy for the construction of grammar-based software}},
url = {citeulike-article-id:3934637 http://www.cs.nuim.ie/{~}markh/pubs/Thesis.pdf},
year = {2006}
}
@inproceedings{Layman2004b,
address = {Newport Beach, CA},
author = {Layman, L and Williams, L and Cunningham, L},
keywords = {mypubs},
mendeley-tags = {mypubs},
pages = {to appear},
title = {{Motivations and Measurements in an Agile Case Study}},
year = {2004}
}
@article{Czerwinski1991,
author = {Czerwinski, Mary and Chrisman, Steve and Schumacher, Bob},
keywords = {hci,interruption,multitask,task similarity},
number = {4},
pages = {38--39},
title = {{The Effects of Warnings and Display Similarity on Interruption in Multitasking Environments}},
volume = {23},
year = {1991}
}
@inproceedings{Cheng2004,
abstract = {Most researches on the testing of Web Service are based on the methodology of black box view. We propose a testing framework about unit testing of Web Service. By implementing our framework of testing on Web Service, the developers of softwares can run their works of testing inside the codes to assure the reliability of Web Service. Furthmore, it is also expected to point the future development of test-driving of Web Service.},
annote = {On the development of software tools for testing Web service},
author = {Cheng, T T and Fu, C H},
booktitle = {Proceedings of the International Conference on Internet Computing, IC'04},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {761--765},
title = {{On the development of software tools for testing Web service}},
url = {citeulike-article-id:3934572 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-12744268217{\&}{\#}38 partnerID=40},
volume = {2},
year = {2004}
}
@article{Ambler2008,
abstract = {Test-driven development (TDD) is a common agile software development technique that helps to specify a software in detail on a just-in-time (JIT) basis via executable tests that run in a regression manner. TDD is the combination of test-first development (TFD) and refactoring. TFD can be done at the requirements level by writing a single customer test, the equivalent of a function test or acceptance test in the traditional world, and at the design level with developer tests. Refactoring is a technique where a small change is made to the existing code to improve its design without changing its semantics. TDD is an important agile development technique that enables to specify and validate functionality at a finely detailed level.},
annote = {Scaling test-driven development},
author = {Ambler, S W},
journal = {Dr. Dobb's Journal},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
number = {2},
pages = {71--73},
title = {{Scaling test-driven development}},
url = {citeulike-article-id:3934539 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-38749087267{\&}{\#}38 partnerID=40},
volume = {33},
year = {2008}
}
@book{Utting2007,
abstract = {This book gives a practical introduction to model-based testing, showing how to write models for testing purposes and how to use model-based testing tools to generate test suites. It is aimed at testers and software developers who wish to use model-based testing, rather than at tool-developers or academics.The book focuses on the mainstream practice of functional black-box testing and covers different styles of models, especially transition-based models (UML state machines) and pre/post models (UML/OCL specifications and B notation). The steps of applying model-based testing are demonstrated on examples and case studies from a variety of software domains, including embedded software and information systems. From this book you will learn:* The basic principles and terminology of model-based testing* How model-based testing differs from other testing processes* How model-based testing fits into typical software lifecycles such as agile methods and the Unified Process* The benefits and limitations of model-based testing, its cost effectiveness and how it can reduce time-to-market* A step-by-step process for applying model-based testing* How to write good models for model-based testing* How to use a variety of test selection criteria to control the tests that are generated from your models* How model-based testing can connect to existing automated test execution platforms such as Mercury Test Director, Java JUnit, and proprietary test execution environments* Presents the basic principles and terminology of model-based testing* Shows how model-based testing fits into the software lifecycle, its cost-effectiveness, and how it can reduce time to market* Offers guidance on how to use different kinds of modeling techniques, useful test generation strategies, how to apply model-based testing techniques to real applications using case studies},
author = {Utting, Mark and Legeard, Bruno},
isbn = {0123725011},
title = {{Practical Model-Based Testing: A Tools Approach (Google eBook)}},
url = {http://books.google.com/books?hl=de{\&}lr={\&}id=8hAGtY4-oOoC{\&}pgis=1},
year = {2007}
}
@article{Pothier2007,
author = {Pothier, Guillaume and Tanter, {\'{E}}ric and Piquer, Jos{\'{e}}},
doi = {10.1145/1297105.1297067},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pothier, Tanter, Piquer - 2007 - Scalable omniscient debugging.pdf:pdf},
isbn = {978-1-59593-786-5},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
keywords = {execution traces,interface components,omniscient debugging,partial traces,scalability,specialized distributed database},
month = {oct},
number = {10},
pages = {535},
title = {{Scalable omniscient debugging}},
url = {http://dl.acm.org/citation.cfm?id=1297105.1297067},
volume = {42},
year = {2007}
}
@article{Neto2007,
author = {Neto, AC Dias and Subramanyan, R},
isbn = {9781595938800},
journal = {Proceedings of the 1st {\ldots}},
keywords = {2,model-based testing,survey,systematic review,test case generation,testing approaches},
title = {{A survey on model-based testing approaches: a systematic review}},
url = {http://dl.acm.org/citation.cfm?id=1353681},
year = {2007}
}
@inproceedings{1594876,
abstract = { Aviation security screening has become very important in recent years. It was shown by Schwaninger et al. (2004) that certain image-based factors influence detection when visually inspecting X-ray images of passenger bags. Threat items are more difficult to recognize when placed in close-packed bags (effect of bag complexity), when superimposed by other objects (effect of superposition), and when rotated (effect of viewpoint). The X-ray object recognition rest (X-ray ORT) was developed to measure the abilities needed to cope with these factors. In this study, we examined the reliability and validity of the X-ray ORT based on a sample of 453 aviation security screeners and 453 novices. Cronbach Alpha and split-half analysis revealed high reliability. Validity was examined using internal, convergent, discriminant and criterion-related validity estimates. The results show that the X-ray ORT is a reliable and valid instrument for measuring visual abilities needed in X-ray screening. This makes the X-ray ORT an interesting tool for competency and pre-employment assessment purposes.},
author = {Hardmeier, D. and Hofer, F. and Schwaninger, a.},
booktitle = {Proceedings 39th Annual 2005 International Carnahan Conference on Security Technology},
doi = {10.1109/CCST.2005.1594876},
isbn = {0-7803-9245-0},
keywords = {X-ray detection,X-ray image,X-ray object recogni},
pages = {189--192},
publisher = {Ieee},
title = {{The X-ray object recognition test (X-ray ORT) - a reliable and valid instrument for measuring visual abilities needed in X-ray screening}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1594876},
year = {2005}
}
@inproceedings{Grottke2008,
author = {Grottke, M and Matias, R and Trivedi, K S},
booktitle = {Proc.$\backslash$ 1st International Workshop on Software Aging and Rejuvantion},
keywords = {base taxonomy;software aging phenomenon;software r},
title = {{The fundamentals of software aging}},
year = {2008}
}
@inproceedings{Xia2013,
address = {Kingston, ON},
author = {Xia, Xin and Lo, David and Wang, Xinyu and Zhou, Bo},
booktitle = {2013 20th Working Conference on Reverse Engineering (WCRE)},
doi = {10.1109/WCRE.2013.6671282},
isbn = {978-1-4799-2931-3},
keywords = {BR-based analysis,Bugzie,Communities,Composite Method,Computer bugs,D-based analysis,DREX,DevRec,Developer Recommendation,Eclipse,Educational institutions,Euclidean distance,Feature extraction,GCC,Mozilla,Multi-label Learning,Netbeans,OpenOffice,Software,Topic Model,Vectors,bug reports based analysis,bug resolution,developer based analysis,developer recommendation,program debugging,software development,software maintenance},
language = {English},
month = {oct},
pages = {72--81},
publisher = {IEEE},
title = {{Accurate developer recommendation for bug resolution}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=6671282},
year = {2013}
}
@inproceedings{Yin2004b,
address = {New York, New York, USA},
author = {Yin, Xiaoxin and Yurcik, William and Treaster, Michael and Li, Yifan and Lakkaraju, Kiran},
booktitle = {Proceedings of the 2004 ACM workshop on Visualization and data mining for computer security - VizSEC/DMSEC '04},
doi = {10.1145/1029208.1029214},
isbn = {1581139748},
keywords = {link analysis,link relationships,netflows,parallel axes,parallel coordinates,security,security visualization,situational awareness},
month = {oct},
pages = {26},
publisher = {ACM Press},
title = {{VisFlowConnect}},
url = {http://dl.acm.org/citation.cfm?id=1029208.1029214},
year = {2004}
}
@article{Lagus2018,
abstract = {The computing education research literature contains a wide variety of methods that can be used to identify students who are either at risk of failing their studies or who could benefit from additional challenges. Many of these are based on machine-learning models that learn to make predictions based on previously observed data. However, in educational contexts, differences between courses set huge challenges for the generalizability of these methods. For example, traditional machine-learning methods assume identical distribution in all data-in our terms, traditional machine-learning methods assume that all teaching contexts are alike. In practice, data collected from different courses can be very different as a variety of factors may change, including grading, materials, teaching approach, and the students. Transfer-learning methodologies have been created to address this challenge. They relax the strict assumption of identical distribution for training and test data. Some similarity between the contexts is still needed for efficient learning. In this work, we review the concept of transfer learning especially for the purpose of predicting the outcome of an introductory programming course and contrast the results with those from traditional machine-learning methods. The methods are evaluated using data collected in situ from two separate introductory programming courses. We empirically show that transfer-learning methods are able to improve the predictions, especially in cases with limited amount of training data, for example, when making early predictions for a new context. The difference in predictive power is, however, rather subtle, and traditional machine-learning models can be sufficiently accurate assuming the contexts are closely related and the features describing the student activity are carefully chosen to be insensitive to the fine differences. 1 INTRODUCTION Educational sciences have started to adopt machine-learning methods, computer algorithms that learn predictive models based on observed data. These methods have been applied in research to, for example, automate tutoring [17], predict course outcomes [23], identify at-risk students [1], detect student characteristics [7], predict learning disabilities [25, 40], model students' learning process [30], and much more. Many of these tasks can be performed transparently, as they only use data that can be naturally gathered from existing learning environments. One stream of such research is related to the analysis of source code snapshots, i.e., process data, that is recorded as students work on programming assignments [18]. Typically, the models are applied in post hoc manner for analyzing data from a single course and semester, with only a few studies seeking to perform cross-course or cross-context analysis [18]. Analyzing multiple contexts, such as the same course during different semesters or similar courses in different universities , in combination has two obvious advantages: It increases the amount of available data for learning, and it allows us to study relationships between different contexts. Cross-context datasets can, however, be problematic from the point of view of traditional machine-learning methods. The core principle of machine learning is that the predictive model is learned from some observations collected for training the model, and, consequently, the algorithms learn to make predictions for future data instances that are similar to those used for training the model-in technical terms, they are assumed to be drawn from the same probability distribution. The more this assumption is violated the less useful the models become. If the contexts are sufficiently different, then it can be better to ignore the different contexts altogether. Indeed, using machine-learning methods to construct a predictive model on data from one semester and using it to predict the outcomes of another semester has been observed to produce worse predictions than using data from the same semester for both learning the model and making the predictions [1, 6]. In this work, which is motivated for the need of context-independent methodologies [18, 29], we study whether transfer-learning methodologies could be used for the task of predicting students' outcomes in a cross-context setting. Transfer learning is an area of machine learning that deals with data coming from multiple sources that may have different distributions in data, e.g., different programming assignments, teaching approaches, and so on, and combines that information to improve the prediction model of the task at hand. Naturally, as is often the case with machine-learning approaches, transfer learning relies on having at least some data with values-i.e., course outcomes-from all contexts. Our analysis is based on two introductory programming courses. One of the courses had lectures and individual assignments, while the other had no lectures, and the students had both pairwise and individual assignments. The grading scheme of both courses also differs from each other. This article is organized as follows. First, in Section 2, we provide a brief overview of machine learning and transfer learning and visit the related work on predicting course outcomes. Then, in Section 3 we discuss our research methodology and data in more detail, followed by the results of our study in Section 4. The results are discussed in Section 5, where we also address the main limitations of our work. Finally, in Section 6, we briefly review the main contributions of this article and outline possible future directions for study.},
annote = {For a more extensive review, see, e.g., References [1, 18].},
author = {Lagus, Jarkko and Longi, Krista and Klami, Arto and Hellas, Arto},
doi = {10.1145/3152714},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lagus et al. - 2018 - Transfer-learning methods in programming course outcome prediction.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {CS2,Course outcome prediction,Educational data mining,Introductory programming,Learning analytics,Machine learning,Novice programmers,Source code snapshots,Transfer learning},
mendeley-tags = {CS2},
month = {nov},
number = {4},
pages = {Article 19},
publisher = {Association for Computing Machinery},
title = {{Transfer-learning methods in programming course outcome prediction}},
volume = {18},
year = {2018}
}
@misc{FederalAviationAdministration2008,
author = {{Federal Aviation Administration}},
booktitle = {http://www.faa.gov/library/manuals/aviation/risk{\_}management/ss{\_}handbook/},
title = {{System Safety Handbook}},
url = {http://www.faa.gov/library/manuals/aviation/risk{\_}management/ss{\_}handbook/},
year = {2008}
}
@inproceedings{4721549,
abstract = {We present and evaluate various methods for purely automated attacks against click-based graphical passwords. Our purely automated methods combine click-order heuristics with focus-of-attention scan-paths generated from a computational model of visual attention. Our method results in a significantly better automated attack than previous work, guessing 8-15{\%} of passwords for two representative images using dictionaries of less than 224.6 entries, and about 16{\%} of passwords on each of these images using dictionaries of less than 231.4 entries (where the full password space is 243). Relaxing our click-order pattern substantially increased the efficacy of our attack albeit with larger dictionaries of 234.7 entries, allowing attacks that guessed 48-54{\%} of passwords (compared to previous results of 0.9{\%} and 9.1{\%} on the same two images with 235 guesses). These latter automated attacks are independent of focus-of-attention models, and are based on image-independent guessing patterns. Our results show that automated attacks, which are easier to arrange than human-seeded attacks and are more scalable to systems that use multiple images, pose a significant threat.},
author = {Salehi-Abari, Amirali and Thorpe, Julie and Oorschot, P.C. Van},
booktitle = {2008 Annual Computer Security Applications Conference (ACSAC)},
doi = {10.1109/ACSAC.2008.18},
isbn = {978-0-7695-3447-3},
issn = {1063-9527},
keywords = {automated attacks,c,click-based graphical password},
month = {dec},
pages = {111--120},
publisher = {Ieee},
title = {{On Purely Automated Attacks and Click-Based Graphical Passwords}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4721549},
year = {2008}
}
@article{Odgers2012,
abstract = {BACKGROUND: Children growing up in poor versus affluent neighborhoods are more likely to spend time in prison, develop health problems and die at an early age. The question of how neighborhood conditions influence our behavior and health has attracted the attention of public health officials and scholars for generations. Online tools are now providing new opportunities to measure neighborhood features and may provide a cost effective way to advance our understanding of neighborhood effects on child health. METHOD: A virtual systematic social observation (SSO) study was conducted to test whether Google Street View could be used to reliably capture the neighborhood conditions of families participating in the Environmental-Risk (E-Risk) Longitudinal Twin Study. Multiple raters coded a subsample of 120 neighborhoods and convergent and discriminant validity was evaluated on the full sample of over 1,000 neighborhoods by linking virtual SSO measures to: (a) consumer based geo-demographic classifications of deprivation and health, (b) local resident surveys of disorder and safety, and (c) parent and teacher assessments of children's antisocial behavior, prosocial behavior, and body mass index. RESULTS: High levels of observed agreement were documented for signs of physical disorder, physical decay, dangerousness and street safety. Inter-rater agreement estimates fell within the moderate to substantial range for all of the scales (ICCs ranged from .48 to .91). Negative neighborhood features, including SSO-rated disorder and decay and dangerousness corresponded with local resident reports, demonstrated a graded relationship with census-defined indices of socioeconomic status, and predicted higher levels of antisocial behavior among local children. In addition, positive neighborhood features, including SSO-rated street safety and the percentage of green space, were associated with higher prosocial behavior and healthy weight status among children. CONCLUSIONS: Our results support the use of Google Street View as a reliable and cost effective tool for measuring both negative and positive features of local neighborhoods.},
author = {Odgers, Candice L and Caspi, Avshalom and Bates, Christopher J and Sampson, Robert J and Moffitt, Terrie E},
doi = {10.1111/j.1469-7610.2012.02565.x},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Odgers et al. - 2012 - Systematic social observation of children's neighborhoods using Google Street View a reliable and cost-effective.pdf:pdf},
issn = {1469-7610},
journal = {Journal of Child Psychology and Psychiatry, and Allied Disciplines},
keywords = {Antisocial Personality Disorder,Antisocial Personality Disorder: psychology,Body Mass Index,Child,Child Behavior,Child Behavior: psychology,Child Welfare,Child Welfare: psychology,Child, Preschool,Environment Design,Female,Follow-Up Studies,Geographic Information Systems,Great Britain,Humans,Internet,Longitudinal Studies,Male,Observer Variation,Preschool,Reproducibility of Results,Residence Characteristics,Residence Characteristics: statistics {\&} numerical,Risk Assessment,Risk Assessment: economics,Risk Assessment: methods,Safety,Safety: statistics {\&} numerical data,Social Environment,Socioeconomic Factors,agile,nsf},
mendeley-tags = {agile,nsf},
month = {oct},
number = {10},
pages = {1009--17},
pmid = {22676812},
title = {{Systematic social observation of children's neighborhoods using Google Street View: a reliable and cost-effective method.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3537178{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {53},
year = {2012}
}
@article{ElEmam2003,
author = {{El Emam}, K},
number = {11},
title = {{Finding Success in Small Software Projects}},
volume = {4},
year = {2003}
}
@techreport{InternetCrimeComplaintCenter2013,
address = {Washington, DC},
author = {{Internet Crime Complaint Center}},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Internet Crime Complaint Center - 2013 - 2012 Internet Crime Report.pdf:pdf},
institution = {Internet Crime Complaint Center},
keywords = {agile,nsf},
mendeley-tags = {agile,nsf},
title = {{2012 Internet Crime Report}},
year = {2013}
}
@article{Choudhury2008,
abstract = {Activity-aware systems have inspired novel user interfaces and new applications in smart environments, surveillance, emergency response, and military missions. Systems that recognize human activities from body-worn sensors can further open the door to a world of healthcare applications, such as fitness monitoring, eldercare support, long-term preventive and chronic care, and cognitive assistance. Wearable systems have the advantage of being with the user continuously. So, for example, a fitness application could use real-time activity information to encourage users to perform opportunistic activities. Furthermore, the general public is more likely to accept such activity recognition systems because they are usually easy to turn off or remove.},
author = {Choudhury, Tanzeem and Borriello, Gaetano and Consolvo, Sunny and Haehnel, Dirk and Harrison, Beverly and Hemingway, Bruce and Hightower, Jeffrey and Klasnja, Predrag "Pedja" and Koscher, Karl and LaMarca, Anthony and Landay, James A. and LeGrand, Louis and Lester, Jonathan and Rahimi, Ali and Rea, Adam and Wyatt, Danny},
doi = {10.1109/MPRV.2008.39},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choudhury et al. - 2008 - The Mobile Sensing Platform An Embedded Activity Recognition System.pdf:pdf},
issn = {1536-1268},
journal = {IEEE Pervasive Computing},
keywords = {Accelerometers,Algorithm design and analysis,Medical services,Pervasive computing,Privacy,Protection,Robustness,Software algorithms,Software design,Ubiquitous computing,activity recognition,activity recognition systems,activity-aware systems,agile,body-worn sensors,embedded activity recognition system,embedded systems,emergency response,healthcare applications,machine learning,military missions,mobile sensing,nsf,real-time activity information,sensors,smart environments,surveillance,ubiquitous computing,user interfaces,wearable computers,wearable systems},
language = {English},
mendeley-tags = {agile,nsf},
month = {apr},
number = {2},
pages = {32--41},
publisher = {IEEE},
shorttitle = {Pervasive Computing, IEEE},
title = {{The Mobile Sensing Platform: An Embedded Activity Recognition System}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4487086 http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=4487086},
volume = {7},
year = {2008}
}
@inproceedings{Feathers2002,
abstract = {Programmers are often scared that they won't be able to optimize later. For that reason, they tend to optimize early leading to brittle design. Test driven design can lead to emergent optimization and code that is readily optimizable. If programmers develop test first, many of their upfront concerns about performance can be deferred.},
annote = {Emergent Optimization in Test Driven Design},
author = {Feathers, M},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Emergent Optimization in Test Driven Design}},
url = {citeulike-article-id:3934603 http://www.objectmentor.com/resources/articles/MichaelFeathers--EmergentOptimizationInTestDrivenDesign.pdf},
year = {2002}
}
@inproceedings{Bevan2002,
address = {Kentucky},
author = {Bevan, J and Werner, L and McDowell, C},
pages = {100--107},
title = {{Guidelines for the User of Pair Programming in a Freshman Programming Class}},
year = {2002}
}
@inproceedings{Chaudhary1980,
address = {New York, New York, USA},
author = {Chaudhary, B. D. and Sahasrabuddhe, H. V.},
booktitle = {Proceedings of the ACM 1980 annual conference on - ACM 80},
doi = {10.1145/800176.810001},
isbn = {0897910281},
month = {jan},
pages = {457--466},
publisher = {ACM Press},
title = {{Meaningfulness as a factor of program complexity}},
url = {http://dl.acm.org/citation.cfm?id=800176.810001},
year = {1980}
}
@article{Heiskanen2010a,
author = {Heiskanen, Henri and Maunumaa, Mika and Katara, Mika and Systems, Software},
title = {{Test Process Improvement for Automated Test Generation}},
volume = {51},
year = {2010}
}
@inproceedings{Gousios2013,
address = {San Francisco, CA},
author = {Gousios, Georgios},
booktitle = {2013 10th Working Conference on Mining Software Repositories (MSR)},
doi = {10.1109/MSR.2013.6624034},
isbn = {978-1-4673-2936-1},
month = {may},
pages = {233--236},
title = {{The GHTorent dataset and tool suite}},
url = {http://ieeexplore.ieee.org/document/6624034/},
year = {2013}
}
@article{Garcia2012,
abstract = {Upon an intrusion, security staff must analyze the IT system that has been compromised, in order to determine how the attacker gained access to it, and what he did afterward. Usually, this analysis reveals that the attacker has run an exploit that takes advantage of a system vulnerability. Pinpointing, in a given log file, the execution of one such an exploit, if any, is very valuable for computer security. This is both because it speeds up the process of gathering evidence of the intrusion, and because it helps taking measures to prevent a further intrusion, e.g., by building and applying an appropriate attack signature for intrusion detection system maintenance. This problem, which we call postmortem intrusion detection, is fairly complex, given both the overwhelming length of a standard log file, and the difficulty of identifying exactly where the intrusion has occurred. In this paper, we propose a novel approach for postmortem intrusion detection, which factors out repetitive behavior, thus, speeding up the process of locating the execution of an exploit, if any. Central to our intrusion detection mechanism is a classifier, which separates abnormal behavior from normal one. This classifier is built upon a method that combines a hidden Markov model with k -means. Our experimental results establish that our method is able to spot the execution of an exploit, with a cumulative detection rate of over 90{\%}. In addition, we propose an entropy-based approach that speeds up the construction of a profile for ordinary system behavior.},
author = {Garcia, Karen A. and Monroy, Ra{\'{u}}l and Trejo, Luis A. and Mex-Perera, Carlos and Aguirre, Eduardo},
doi = {10.1109/TSMCC.2012.2217325},
issn = {1094-6977},
journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
keywords = {Anomaly,Computational modeling,Computer crime,Hidden Markov models,IT system,Intrusion detection,Monitoring,Network security,attack signature,authorisation,computer security,cumulative detection rate,entropy,entropy-based approach,hidden Markov model,hidden Markov model (HMM),host-based intrusion detection,intrusion detection system maintenance,k-means classifier,log file analysis,pattern classification,postmortem intrusion detection,repetitive behavior,sequitur,system vulnerability},
month = {nov},
number = {6},
pages = {1690--1704},
shorttitle = {Systems, Man, and Cybernetics, Part C: Application},
title = {{Analyzing Log Files for Postmortem Intrusion Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6392466},
volume = {42},
year = {2012}
}
@inproceedings{Favre2003,
address = {Portland, OR},
author = {Favre, Jean-Marie and Estublier, Jacky and Sanlaville, Remy},
booktitle = {3rd Int'l Workshop on Adoption-Centric Software Engineering},
pages = {81--89},
title = {{Tool Adoption Issues in a Very Large Software Company}},
year = {2003}
}
@misc{Layman2004a,
address = {Raleigh, NC},
author = {Layman, L},
keywords = {mypubs},
mendeley-tags = {mypubs},
publisher = {North Carolina State University},
title = {{Identifying Potential Deficiencies in Agile Requirements Engineering Practices}},
year = {2004}
}
@inproceedings{Stikic2008,
abstract = {The manual assessment of activities of daily living (ADLs) is a fundamental problem in elderly care. The use of miniature sensors placed in the environment or worn by a person has great potential in effective and unobtrusive long term monitoring and recognition of ADLs. This paper presents an effective and unobtrusive activity recognition system based on the combination of the data from two different types of sensors: RFID tag readers and accelerometers. We evaluate our algorithms on non-scripted datasets of 10 housekeeping activities performed by 12 subjects. The experimental results show that recognition accuracy can be significantly improved by fusing the two different types of sensors. We analyze different acceleration features and algorithms, and based on tag detections we suggest the best tagspsila placements and the key objects to be tagged for each activity.},
author = {Stikic, Maja and Huynh, Tam and {Van Laerhoven}, Kristof and Schiele, Bernt},
booktitle = {Proceedings of the 2nd International Conference on Pervasive Computing Technologies for Healthcare},
doi = {10.1109/PCTHEALTH.2008.4571084},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stikic et al. - 2008 - ADL recognition based on the combination of RFID and accelerometer sensing.pdf:pdf},
isbn = {978-963-9799-15-8},
keywords = {Acceleration,Accelerometers,Algorithm design and analysis,Monitoring,Performance evaluation,RFID,RFID tags,Radiofrequency identification,Senior citizens,Sensor systems,Wearable sensors,accelerometer sensing,activities of daily living,agile,elderly care,geriatrics,health care,mobile computing,nsf,sensor fusion,unobtrusive activity recognition system},
mendeley-tags = {agile,nsf},
month = {jan},
pages = {258--263},
publisher = {IEEE},
shorttitle = {Pervasive Computing Technologies for Healthcare, 2},
title = {{ADL recognition based on the combination of RFID and accelerometer sensing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4571084},
year = {2008}
}
@phdthesis{Wege2004,
abstract = {Test-Driven Development (TDD) is a style of agile software development that has received much attention recently in the software development community. Agile software development methods stress the importance of software as the most significant output of a development team, leading to a continuous flow of source code changes. The view on past source code changes as input for a better understanding of how a team has produced the software is a topic that deserves much more attention than it has received thus far. In this dissertation, I claim that an analysis of past software changes can indicate TDD process violations. I propose a tool to prepare and analyze software changes from a source code repository. I propose process compliance indices (PCIs) to interpret the analysis results in order to focus a manual process assessment effort. This dissertation facilitates a better understanding of how TDD developers change software, where they are lazy in following the process discipline, and to help them improve their development practices.},
annote = {Automated Support for Process Assessment in Test-Driven Development},
author = {Wege, Christian},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
title = {{Automated Support for Process Assessment in Test-Driven Development}},
url = {citeulike-article-id:3934833 {\#}},
year = {2004}
}
@inproceedings{Naveed2011,
address = {Koblenz, Germany},
author = {Naveed, Nasir and Gottron, Thomas and Kunegis, J{\'{e}}r{\^{o}}me and Alhadi, Arifah Che},
booktitle = {Proceedings of the 3rd International Web Science Conference on - WebSci '11},
doi = {10.1145/2527031.2527052},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Naveed et al. - 2011 - Bad news travel fast.pdf:pdf},
isbn = {9781450308557},
keywords = {twitter},
mendeley-tags = {twitter},
pages = {Article No. 8},
publisher = {ACM Press},
title = {{Bad news travel fast}},
url = {http://dl.acm.org/citation.cfm?doid=2527031.2527052},
year = {2011}
}
@article{Basili1999a,
author = {Basili, V and Shull, F and Lanubile, F},
keywords = {case studies,case study,family of experiments},
number = {4},
pages = {456--473},
title = {{Building Knowledge Through Families of Experiments}},
volume = {25},
year = {1999}
}
@book{Fenton1998,
author = {Fenton, N E and Pfleeger, S L},
publisher = {Brooks/Cole Pub Co.},
title = {{Software Metrics: A Rigorous and Practical Approach}},
year = {1998}
}
@inproceedings{1532063,
abstract = { Many methods have been developed for monitoring network traffic, both using visualization and statistics. Most of these methods focus on the detection of suspicious or malicious activities. But what they often fail to do refine and exercise measures that contribute to the characterization of such activities and their sources, once they are detected. In particular, many tools exist that detect network scans or visualize them at a high level, but not very many tools exist that are capable of categorizing and analyzing network scans. This paper presents a means of facilitating the process of characterization by using visualization and statistics techniques to analyze the patterns found in the timing of network scans through a method of continuous improvement in measures that serve to separate the components of interest in the characterization so the user can control separately for the effects of attack tool employed, performance characteristics of the attack platform, and the effects of network routing in the arrival patterns of hostile probes. The end result is a system that allows large numbers of network scans to be rapidly compared and subsequently identified.},
author = {Muelder, C and Ma, Kwan-Liu and Bartoletti, T},
booktitle = {Visualization for Computer Security, 2005. (VizSEC 05). IEEE Workshop on},
doi = {10.1109/VIZSEC.2005.1532063},
keywords = {adversary characterization,cyber forensics,graph},
pages = {29--38},
title = {{A visualization methodology for characterization of network scans}},
year = {2005}
}
@inproceedings{Taeksu2006,
abstract = {Test driven development uses unit tests for driving the design of the code. Mock object is an object that imitates the behavior of an object with which class under test has an association to assist the unit testing. Although many tools for mock object are used in practice, there has been few research on defining the mock object model. For the comparisons of the testing capability of the tools, it is important to define the models. In this paper we represent the models of the existing mock object tools and indicate the limitations of them. In addition we propose a new model to overcome the limitations of the existing tools. With this model, more general behavior of the class under test related to the mock object can be expressed and tested. {\^{A}}{\textcopyright} 2006 IEEE.},
annote = {Mock object models for test driven development},
author = {Taeksu, K and Chanjin, P and Chisu, W},
booktitle = {Proceedings - Fourth International Conference on Software Engineering Research, Management and Applications, SERA 2006},
keywords = {TDD,file-import-09-01-23},
mendeley-tags = {TDD},
pages = {221--228},
title = {{Mock object models for test driven development}},
url = {citeulike-article-id:3934812 http://www.scopus.com/scopus/inward/record.url?eid=2-s2.0-34547272836{\&}{\#}38 partnerID=40},
year = {2006}
}
@inproceedings{Breckel2012,
address = {Zurich, Switzerland},
author = {Breckel, Alexander},
booktitle = {Proceedings of the 9th IEEE Working Conference on Mining Software Repositories},
isbn = {9781467317610},
pages = {175--178},
title = {{Error mining: Bug detection through comparison with large code databases}},
url = {http://dl.acm.org/citation.cfm?id=2664474{\&}CFID=910003172{\&}CFTOKEN=41235350},
year = {2012}
}
@book{Vapnik1995,
address = {New York, NY},
author = {Vapnik, Vladimir N},
keywords = {SVM,learning},
publisher = {Springer-Verlag},
title = {{The Nature of Statistical Learning Theory}},
year = {1995}
}
@article{Marin2019,
abstract = {An Empirical Investigation on the Benefits of Gamification in Programming Courses B. MAR{\'{I}}N and J. FREZ, Universidad Diego Portales J. CRUZ-LEMUS and M. GENERO, Universidad de Castilla-La Mancha Context: Programming courses are compulsory for most engineering degrees, but students' performance on these courses is often not as good as expected. Programming is difficult for students to learn, given that it includes a lot of new, complex, and abstract topics. All of this has led experts to the conclusion that new teaching techniques are required if students are to be motivated and engaged in learning on programming courses. Gamification has come to be an effective technique in education in general, and is especially useful in programming courses. This motivated us to develop an open source gamified platform, called UDPiler, for use in a programming course. Objective: The main goal of this article is to obtain empirical evidence on the improvement of students' learning performance when using UDPiler in comparison to a non-gamified compiler. Method: A quasi-experiment was performed with two groups of first-year engineering students at Diego Portales University in Chile, using a non-gamified compiler and a gamified platform, respectively. Results: The results reveal that the students obtained better marks when the gamified platform was used to learn C programming. In addition, there is statistical significance in favor of there being a positive effect on the learning performance of those students who used the gamified platform. Conclusions: The results allow us to conclude that gamification is an encouraging approach with which to teach C programming, a finding that is aligned with previous empirical studies concerning gamification on programming courses, carried out in academic contexts. Nonetheless, we are aware that further validation is also required to corroborate and strengthen the findings obtained and to investigate whether the kind of gamified elements (mechanics, dynamics, and aesthetics) used have any influence on students' performance, among other issues that deserve further investigation and that are explained throughout this article.},
author = {Mar{\'{i}}n, B. and Frez, J. and Cruz-Lemus, J. and Genero, M.},
doi = {10.1145/3231709},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mar{\'{i}}n et al. - 2019 - An empirical investigation on the benefits of gamification in programming courses.pdf:pdf},
issn = {19466226},
journal = {ACM Transactions on Computing Education},
keywords = {CS2,Gamification,Programming courses,Quasi-experiment,Undergraduate education},
mendeley-tags = {CS2},
month = {jan},
number = {1},
pages = {Article 4},
publisher = {Association for Computing Machinery},
title = {{An empirical investigation on the benefits of gamification in programming courses}},
volume = {19},
year = {2019}
}
@inproceedings{Nakamura2005,
address = {Como, Italy},
author = {Nakamura, T and Basili, Victor R.},
booktitle = {Proceedings of the 11th IEEE International Software Metrics Symposium},
pages = {8--17},
title = {{Metrics of software architecture changes based on structural distance}},
year = {2005}
}
@article{Ryu2008,
abstract = {An intrusion detection system plays an important role in a firm's overall security protection. Its main purpose is to identify potentially intrusive events and alert the security personnel to the danger. A typical intrusion detection system, however, is known to be imperfect in detection of intrusive events, resulting in high false-alarm rates. Nevertheless, current intrusion detection models unreasonably assume that upon alerts raised by a system, an information security officer responds to all alarms without any delay and avoids damages of hostile activities. This assumption of responding to all alarms with no time lag is often impracticable. As a result, the benefit of an intrusion detection system can be overestimated by current intrusion detection models. In this article, we extend previous models by including an information security officer's alarm inspection under a constraint as a part of the process in determining the optimal intrusion detection policy. Given a potentially hostile environment for a firm, in which the intrusion rates and costs associated with intrusion and security officers' inspection can be estimated, we outline a framework to establish the optimal operating points for intrusion detection systems under security officers' inspection constraint. The optimal solution to the model will provide not only a basis of better evaluation of intrusion detection systems but also useful insights into operations of intrusion detection systems. The firm can estimate expected benefits for running intrusion detection systems and establish a basis for increase in security personnel to relax security officers' inspection constraint. {\textcopyright} 2008 ACM.},
author = {Ryu, Young U. and Rhee, Hyeun Suk},
doi = {10.1145/1380564.1380566},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ryu, Rhee - 2008 - Evaluation of intrusion detection systems under a resource constraint(2).pdf:pdf},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {Computer security,Intrusion detection,Optimal inspection rates,Optimal operating points},
month = {jul},
number = {4},
pages = {Article 20},
title = {{Evaluation of intrusion detection systems under a resource constraint}},
url = {https://dl.acm.org/doi/10.1145/1380564.1380566},
volume = {11},
year = {2008}
}
@inproceedings{Bustamante2005,
abstract = {The purpose of this study was to examine the effects of task-critical and likelihood information on participants' sensitivity and bias to alarm signals under varying levels of workload. Participants performed a complex primary task at the same time they performed a secondary task. Likelihood information was manipulated through the use of either a Binary Alarm System (BAS) or a Likelihood Alarm System (LAS). As expected, task-critical and likelihood information significantly increased participants' sensitivity, and this varied across workload levels. Participants benefited from task-critical information only when they were interacting with the BAS. However, participants benefited from likelihood information regardless of task-critical information, particularly under high-workload conditions. Furthermore, task-critical information increased participants' response bias under low workload, making them less likely to respond to alarm signals. These results demonstrated the superior advantage of an LAS over a traditional BAS and showed support for the use of an LAS as a way to mitigate the cry-wolf effect above and beyond task-critical information.},
address = {Orlando, FL, USA},
author = {Bustamante, Ernesto A.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/154193120504901702},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bustamante - 2005 - A signal detection analysis of the effects of workload, task-critical and likelihood information on human alarm resp.pdf:pdf},
isbn = {094528926X},
issn = {10711813},
pages = {1513--1517},
title = {{A signal detection analysis of the effects of workload, task-critical and likelihood information on human alarm response}},
year = {2005}
}
@incollection{Deatherage1972,
address = {Washington, DC},
author = {Deatherage, Bruce H.},
booktitle = {Human Engineering Guide to Equipment Design},
editor = {{Van Cott}, Harold P and Kinkade, Robert G},
publisher = {American Institutes for Research},
title = {{Auditory and other sensory forms of information presentation}},
url = {https://www.semanticscholar.org/paper/Auditory-and-other-sensory-forms-of-information-Deatherage/95062220de9ea12c9bc25b494fe2ed8d956e79ed},
year = {1972}
}
@book{Zaccaro2016,
address = {New York},
editor = {Zaccaro, Stephen J. and Dalal, Reeshad S. and Tetrick, Lois E. and Steinke, Julie A.},
publisher = {Taylor {\&} Francis},
title = {{Psychosocial Dynamics of Cyber Security}},
year = {2016}
}
@inproceedings{DAmico2008,
abstract = {Abstract This paper reports on investigations of how computer network defense (CND) analysts conduct their analysis on a day-to-day basis and discusses the implications of these cognitive requirements for designing effective CND visualizations. The supporting data come from a cognitive task analysis (CTA) conducted to base- line the state of the practice in the U.S. Department of Defense CND community. The CTA collected data from CND analysts about their analytic goals, workflow, tasks, types of decisions made, data sources used to make those decisions, cognitive demands, tools used and the biggest challenges that they face. The effort focused on understanding how CND analysts inspect raw data and build their comprehension into a diagnosis or decision, especially in cases requiring data fusion and correla- tion across multiple data sources. This paper covers three of the findings from the CND CTA: (1) the hierarchy of data created as the analytical process transforms data into security situation awareness; (2) the definition and description of different CND analysis roles; and (3) the workflow that analysts and analytical organizations engage in to produce analytic conclusions.},
address = {Berlin, Heidelberg},
author = {D'Amico, A. and Whitley, K.},
booktitle = {Proceedings of the Workshop on Visualization for Computer Security},
doi = {10.1007/978-3-540-78243-8_2},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/DAmico2008.pdf:pdf},
isbn = {3540782427},
issn = {2197666X},
pages = {19--37},
publisher = {Springer Berlin Heidelberg},
title = {{The real work of computer network defense analysts: The analysis roles and processes that transform network data into security situation awareness}},
url = {http://link.springer.com/10.1007/978-3-540-78243-8{\_}2},
year = {2008}
}
@book{Endsley2016,
author = {Endsley, Mica R.},
doi = {10.1201/b11371},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Endsley - 2016 - Designing for Situation Awareness.pdf:pdf},
isbn = {9780429146732},
month = {apr},
publisher = {CRC Press},
title = {{Designing for Situation Awareness}},
url = {https://www.taylorfrancis.com/books/9781420063585},
year = {2016}
}
@article{Ning2004a,
abstract = {Several alert correlation methods have been proposed over the past several years to construct high-level attack scenarios from low-level intrusion alerts reported by intrusion detection systems (IDSs). However, all of these methods depend heavily on the underlying IDSs, and cannot deal with attacks missed by IDSs. In order to improve the performance of intrusion alert correlation and reduce the impact of missed attacks, this paper presents a series of techniques to hypothesize and reason about attacks possibly missed by the IDSs. In addition, this paper also discusses techniques to infer attribute values for hypothesized attacks, to validate hypothesized attacks through raw audit data, and to consolidate hypothesized attacks to generate concise attack scenarios. The experimental results in this paper demonstrate the potential of these techniques in building high-level attack scenarios.},
author = {Ning, Peng and Xu, Dingbang},
doi = {10.1145/1042031.1042036},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ning, Xu - 2004 - Hypothesizing and reasoning about attacks missed by intrusion detection systems.pdf:pdf},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {Intrusion alert correlation,Intrusion detection,Missed attacks},
month = {nov},
number = {4},
pages = {591--627},
title = {{Hypothesizing and reasoning about attacks missed by intrusion detection systems}},
url = {https://dl.acm.org/doi/10.1145/1042031.1042036},
volume = {7},
year = {2004}
}
@misc{CanadianInstituteforCybersecurity2019,
author = {{Canadian Institute for Cybersecurity}},
title = {{Datasets}},
url = {https://www.unb.ca/cic/datasets/index.html},
year = {2019}
}
@book{Ebel1979,
address = {Englewood Cliffs, NJ},
author = {Ebel, Robert L.},
edition = {3rd},
publisher = {Prentice-Hall, Inc.},
title = {{Essentials of Educational Measurement}},
year = {1979}
}
@inproceedings{Mancuso2020,
abstract = {Cyber SA is described as the current and predictive knowledge of cyberspace in relation to the Network, Missions and Threats across friendly, neutral and adversary forces. While this model provides a good high-level understanding of Cyber SA, it does not contain actionable information to help inform the development of capabilities to improve SA. In this paper, we present a systematic, human-centered process that uses a card sort methodology to understand and conceptualize Senior Leader Cyber SA requirements. From the data collected, we were able to build a hierarchy of high- and low- priority Cyber SA information, as well as uncover items that represent high levels of disagreement with and across organizations. The findings of this study serve as a first step in developing a better understanding of what Cyber SA means to Senior Leaders, and can inform the development of future capabilities to improve their SA and Mission Performance.},
address = {USA},
author = {Mancuso, Vincent and McGuire, Sarah and Staheli, Diane},
booktitle = {Advances in Human Factors in Cybersecurity},
editor = {Ahram, Tareq and Karwowski, Waldemar},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/Mancuso2020{\_}Chapter{\_}HumanCenteredCyberSituationAwa.pdf:pdf},
isbn = {978-3-030-20488-4},
pages = {69--78},
publisher = {Springer International Publishing},
title = {{Human Centered Cyber Situation Awareness}},
year = {2020}
}
@article{Bliss1995b,
abstract = {This research examined the feasibility of two methods to reverse the cry-wolf effect: a hearsay method and an urgency method. In session 1, all subjects were presented 10 moderate-urgency alarms to establish the cry-wolf effect. After Session 1, subjects in the Hearsay group were told that the next session would include alarms that were 75{\%} true (in reality there was no reliability change). They were then presented a second session of alarms on a 50{\%} reliability schedule. Subjects in the Urgency group were also presented alarms that were 50{\%} true in the second session, but the urgency level was greater. For Control subjects, there was no difference between Session 1 and 2. Response frequency data were analyzed by using a series of t-test. Results indicated that only the hearsay strategy increased subject responding. Hearsay subjects responded to significantly more alarms in Session 2 (P{\textless} 0.05); however, subjects in the Urgency and Control groups showed no change in alarm response frequency.},
author = {Bliss, James P. and Washington, Mariea D. and Fuller, Bryan S.},
doi = {10.2466/pms.1995.80.3c.1231},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bliss, Washington, Fuller - 1995 - Reversal of the cry-wolf effect an investigation of two methods to increase alarm response rates.doc:doc},
issn = {10711813},
journal = {Perceptual and Motor Skills},
month = {jun},
number = {3},
pages = {1231--1242},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Reversal of the cry-wolf effect: an investigation of two methods to increase alarm response rates}},
url = {http://journals.sagepub.com/doi/10.2466/pms.1995.80.3c.1231},
volume = {80},
year = {1995}
}
@article{Forlano1941,
abstract = {100 girls from grades 5 and 6 were given a study habits inventory, and 100 boys from grades 5 to 8 were given a home background and survey test. 5 methods of grouping for item validation were applied to the results, namely, upper and lower 50{\%}, 33 1/3{\%}, 27{\%}, 16{\%}, and 7{\%}. "No one method occupies first rank." "For a simple and rapid, rough and ready method of item validation of test items of the inventory type, the Upper and Lower twenty-seven per cent Method is to be preferred, even though the distributions are more or less non-normal." (PsycINFO Database Record (c) 2006 APA, all rights reserved). ?? 1941 American Psychological Association.},
author = {Forlano, G. and Pinter, R.},
doi = {10.1037/h0058501},
file = {::},
issn = {00220663},
journal = {Journal of Educational Psychology},
keywords = {EDUCATIONAL PSYCHOLOGY (INCL. VOCATIONAL GUIDANCE),ITEM,TEST,VALIDATION},
month = {oct},
number = {7},
pages = {544--549},
title = {{Selection of upper and lower groups for item validation}},
volume = {32},
year = {1941}
}
@techreport{Anderson1980,
author = {{James P. Anderson Co.}},
file = {::},
institution = {James P. Anderson Co.},
keywords = {audit,intrusion detection,log,monitoring,surveillance,variation},
title = {{Computer Security Threat Monitoring and Surveillance}},
url = {https://csrc.nist.gov/csrc/media/publications/conference-paper/1998/10/08/proceedings-of-the-21st-nissc-1998/documents/early-cs-papers/ande80.pdf},
year = {1980}
}
@article{Hoque2012,
abstract = {Nowadays it is very important to maintain a high level security to ensure safe and trusted communication of information between various organizations. But secured data communication over internet and any other network is always under threat of intrusions and misuses. So Intrusion Detection Systems have become a needful component in terms of computer and network security. There are various approaches being utilized in intrusion detections, but unfortunately any of the systems so far is not completely flawless. So, the quest of betterment continues. In this progression, here we present an Intrusion Detection System (IDS), by applying genetic algorithm (GA) to efficiently detect various types of network intrusions. Parameters and evolution processes for GA are discussed in details and implemented. This approach uses evolution theory to information evolution in order to filter the traffic data and thus reduce the complexity. To implement and measure the performance of our system we used the KDD99 benchmark dataset and obtained reasonable detection rate.},
author = {{Sazzadul Hoque}, Mohammad},
doi = {10.5121/ijnsa.2012.4208},
file = {::},
issn = {09752307},
journal = {International Journal of Network Security {\&} Its Applications},
keywords = {Computer {\&} Network Security,Genetic Algorithm,Intrusion Detection,Intrusion Detection System,KDD Cup 1999 Dataset},
number = {2},
pages = {109--120},
title = {{An Implementation of Intrusion Detection System Using Genetic Algorithm}},
url = {https://arxiv.org/ftp/arxiv/papers/1204/1204.1336.pdf},
volume = {4},
year = {2012}
}
@incollection{Dain2002,
abstract = {An algorithm for fusing the alerts produced by multiple heterogeneous intrusion detection systems is pre- sented. The algorithm runs in realtime, combining the alerts into scenarios; each is composed of a sequence of alerts produced by a single actor or organization. The software is capable of discovering scenarios even if stealthy attack methods, such as forged IP addresses or long attack latencies, are employed. The algorithm generates sce- narios by estimating the probability that a new alert belongs to a given scenario. Alerts are then added to the most likely candidate scenario. Two alternative probability estimation techniques are compared to an algorithm that builds scenarios using a set of rules. Both probability estimate approaches make use of training data to learn the appropriate probability measures. Our algorithm can determine the scenario membership of a new alert in time proportional to the number of candidate scenarios},
address = {Boston, MA},
author = {Dain, Oliver and Cunningham, Robert K.},
booktitle = {Applications of Data Mining in Comp[uter Security},
doi = {10.1007/978-1-4615-0953-0_5},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dain, Cunningham - 2002 - Fusing A Heterogeneous Alert Stream Into Scenarios.pdf:pdf},
pages = {103--122},
publisher = {Springe},
title = {{Fusing A Heterogeneous Alert Stream Into Scenarios}},
url = {https://link.springer.com/chapter/10.1007/978-1-4615-0953-0{\_}5},
year = {2002}
}
@article{Rajivan2018,
abstract = {Objective: Incident correlation is a vital step in the cybersecurity threat detection process. This article presents research on the effect of group-level information-pooling bias on collaborative incident correlation analysis in a synthetic task environment. Background: Past research has shown that uneven information distribution biases people to share information that is known to most team members and prevents them from sharing any unique information available with them. The effect of such biases on security team collaborations are largely unknown. Method: Thirty 3-person teams performed two threat detection missions involving information sharing and correlating security incidents. Incidents were predistributed to each person in the team based on the hidden profile paradigm. Participant teams, randomly assigned to three experimental groups, used different collaboration aids during Mission 2. Results: Communication analysis revealed that participant teams were 3 times more likely to discuss security incidents commonly known to the majority. Unaided team collaboration was inefficient in finding associations between security incidents uniquely available to each member of the team. Visualizations that augment perceptual processing and recognition memory were found to mitigate the bias. Conclusion: The data suggest that (a) security analyst teams, when conducting collaborative correlation analysis, could be inefficient in pooling unique information from their peers; (b) employing off-the-shelf collaboration tools in cybersecurity defense environments is inadequate; and (c) collaborative security visualization tools developed considering the human cognitive limitations of security analysts is necessary. Application: Potential applications of this research include development of team training procedures and collaboration tool development for security analysts.},
author = {Rajivan, Prashanth and Cooke, Nancy J.},
doi = {10.1177/0018720818769249},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rajivan, Cooke - 2018 - Information-Pooling Bias in Collaborative Security Incident Correlation Analysis.pdf:pdf},
issn = {15478181},
journal = {Human Factors},
keywords = {cybersecurity,hidden profile paradigm,security visualization,teamwork,threat detection},
month = {aug},
number = {5},
pages = {626--639},
publisher = {SAGE Publications Inc.},
title = {{Information-Pooling Bias in Collaborative Security Incident Correlation Analysis}},
url = {http://journals.sagepub.com/doi/10.1177/0018720818769249},
volume = {60},
year = {2018}
}
@article{Biros2001,
author = {Biros, David P. and Eppich, Todd},
journal = {SIGNAL},
title = {{Human Element Key to Intrusion Detection}},
url = {https://www.afcea.org/content/human-element-key-intrusion-detection},
year = {2001}
}
@inproceedings{Mancuso2014,
abstract = {Cyber security has been a growing focus within the human factors community. Over the last several years, human-centered cyber research has provided valuable insights into the cognitive and collaborative work within cyber operations, but has largely ignored how the genesis, intentions, methods and outcomes of cyber attacks impact human-related outcomes. Leveraging insights from other, more technologically focused communities, the goal of this paper is to synthesize previous work and to present a unified, descriptive framework of cyber attacks. Our framework, which consists of three dimensions, adversarial, methodological, and operational, aims to maintain the rich interactions between the components of a cyber attack while offering a further abstraction useful to future human factors research. We present each dimension in terms of the previous techno-centered research, demonstrate how the human factors community can contribute to our understanding, and ground each within the context of the StuxNet virus.},
author = {Mancuso, Vincent F. and Strang, Adam J. and Funke, Gregory J. and Finomore, Victor S.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931214581091},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mancuso et al. - 2014 - Human factors of cyber attacks A framework for human-centered research.pdf:pdf},
isbn = {9780945289456},
issn = {10711813},
month = {sep},
number = {1},
pages = {437--441},
publisher = {Human Factors an Ergonomics Society Inc.},
title = {{Human factors of cyber attacks: A framework for human-centered research}},
url = {http://journals.sagepub.com/doi/10.1177/1541931214581091},
volume = {2014-Janua},
year = {2014}
}
@inproceedings{Wall2018,
abstract = {Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.},
author = {Wall, Emily and Blaha, Leslie M. and Franklin, Lyndsey and Endert, Alex},
booktitle = {2017 IEEE Conference on Visual Analytics Science and Technology, VAST 2017 - Proceedings},
doi = {10.1109/VAST.2017.8585669},
isbn = {9781538631638},
keywords = {Cognitive bias,H.5.0 [Information Systems]: Human-Computer Intera,Human-in-the-loop,Mixed initiative,User interaction,Visual analytics},
month = {dec},
pages = {104--115},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics}},
year = {2018}
}
@article{Lord1952,
abstract = {Under certain assumptions an expression, in terms of item difficulties and intercorrelations, is derived for the curvilinear correlation of test score on the “ability underlying the test,” this ability being defined as the common factor of the item tetrachoric intercorrelations corrected for guessing. It is shown that this curvilinear correlation is equal to the square root of the test reliability. Numerical values for these curvilinear correlations are presented for a number of hypothetical tests, defined in terms of their item parameters. These numerical results indicate that the reliability and the curvilinear correlation will be maximized by (1) minimizing the variability of item difficulty and (2) making the level of item difficulty somewhat easier than the halfway point between a chance percentage of correct answers and 100 per cent correct answers. {\textcopyright} 1952, Psychometric Society. All rights reserved.},
author = {Lord, Frederic M.},
doi = {10.1007/BF02288781},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lord - 1952 - The relation of the reliability of multiple-choice tests to the distribution of item difficulties.pdf:pdf},
issn = {18600980},
journal = {Psychometrika},
number = {2},
pages = {181--194},
title = {{The relation of the reliability of multiple-choice tests to the distribution of item difficulties}},
volume = {17},
year = {1952}
}
@article{Julisch2003,
abstract = {It is a well-known problem that intrusion detection systems overload their human operators by triggering thousands of alarms per day. This paper presents a new approach for handling intrusion detection alarms more efficiently. Central to this approach is the notion that each alarm occurs for a reason, which is referred to as the alarm's root causes. This paper observes that a few dozens of rather persistent root causes generally account for over 90{\%} of the alarms that an intrusion detection system triggers. Therefore, we argue that alarms should be handled by identifying and removing the most predominant and persistent root causes. To make this paradigm practicable, we propose a novel alarm-clustering method that supports the human analyst in identifying root causes. We present experiments with real-world intrusion detection alarms to show how alarm clustering helped us identify root causes. Moreover, we show that the alarm load decreases quite substantially if the identified root causes are eliminated so that they can no longer trigger alarms in the future.},
author = {Julisch, Klaus},
doi = {10.1145/950191.950192},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Julisch - 2003 - Clustering intrusion detection alarms to support root cause analysis.pdf:pdf},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {Cluster analysis,Data mining,False positives,Intrusion detection,Root cause analysis},
number = {4},
pages = {443--471},
title = {{Clustering intrusion detection alarms to support root cause analysis}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.136.1949{\&}rep=rep1{\&}type=pdf},
volume = {6},
year = {2003}
}
@article{Sawyer2016,
abstract = {The public reporting burden for this collection of information is estimated to average 1 hour per response, including the time for reviewing instructions, searching existing data sources, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden, to Department of Defense, Washington Headquarters Services, Directorate for Information Operations and Reports (0704-0188), 1215 Jefferson Davis Highway, Suite 1204, Arlington, VA 22202-4302. Respondents should be aware that notwithstanding any other provision of law, no person shall be subject to any penalty for failing to comply with a collection of information if it does not display a currently valid OMB control number. PLEASE DO NOT RETURN YOUR FORM TO THE ABOVE ADDRESS.},
author = {Sawyer, Ben D and Finomore, Victor S and Funke, Gregory J and Matthews, Gerald and Mancuso, Vincent and Funke, Matthew and Warm, Joel S and Hancock, Peter A},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawyer et al. - 2016 - Cyber Vigilance The Human Factor.pdf:pdf},
issn = {0704-0188},
journal = {American Intelligence Journal},
number = {2},
pages = {157--165},
title = {{Cyber Vigilance: The Human Factor}},
volume = {32},
year = {2016}
}
@phdthesis{Zhong2016,
author = {Zhong, Chen},
file = {::},
title = {{A cognitive process tracing approach to cybersecurity data triage operations automation}},
url = {https://etda.libraries.psu.edu/catalog/zp38wc617},
year = {2016}
}
@article{Kelley1939,
abstract = {It is suggested that items may be studied best when whole experimental samples yield 50{\%} right responses, and that upper and lower groups consisting of 27{\%} from the extremes of the criterion score distribution are optimal for the study of test items, provided the differences in criterion scores among the members of each group are not utilized. (PsycINFO Database Record (c) 2006 APA, all rights reserved). ?? 1939 American Psychological Association.},
author = {Kelley, T. L.},
doi = {10.1037/h0057123},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kelley - 1939 - The selection of upper and lower groups for the validation of test items.pdf:pdf},
issn = {00220663},
journal = {Journal of Educational Psychology},
keywords = {MENTAL TESTS,TESTS,VALIDATION},
month = {jan},
number = {1},
pages = {17--24},
title = {{The selection of upper and lower groups for the validation of test items}},
volume = {30},
year = {1939}
}
@book{Schraagen2000,
abstract = {Introduction to cognitive task analysis / Susan F. Chipman, Jan Maarten Schraagen, and Valerie L. Shalin -- Theoretical and pragmatic influences on task analysis methods / John Annett -- Describing job expertise using cognitively oriented task analyses (COTA) / David DuBois and Valerie L. Shalin -- Training of troubleshooting : a structured, task analytical approach / Alma Schaafstal and Jan Maarten Schraagen -- DNA : providing the blueprint for instruction / Valerie J. Shute, Lisa A. Torreano, and Ross E. Willis -- Discovering situated meaning : an ecological approach to task analysis / John M. Flach -- Work domain analysis and task analysis : a difference that matters / Kim J. Vicente -- A functional task analysis of time-pressured decision making / Earl Hunt and Susan Joslyn -- A skill based cognitive task analysis framework / Thomas L. Seamster, Richard E. Redding, and George L. Kaempf -- Towards a theory-based form of cognitive task analysis of broad scope and applicability / Philip J. Barnard and Jon May -- An automated aid for modeling human-computer interaction / Kent E. Williams -- Using task analysis as a primary design method : the SGT approach / Thomas C. Ormerod -- Getting the knowledge into HCI : theoretical and practical aspects of task knowledge structures / Peter Johnson, Hilary Johnson, and Fraser Hamilton -- Cognitive task analysis using ATLAS / W. Ian Hamilton -- The role of cognitive task analysis in the application of predictive models of human performance / David E. Kieras and David E. Meyer -- Novel uses of task models : two case studies / Cécile Paris, Sandrine Balbo, and Nadine Ozkan -- Analyzing a novel expertise : an unmarked road / Wayne D. Gray and Susan S. Kirschenbaum -- Active design documents as software agents that mediate participatory design and traceability / Guy A. Boy -- Cognitive analyses within increasingly larger organizational contexts / Franz Schmalhofer and Ludger van Elst -- Bootstrapping multiple converging cognitive task analysis techniques for system design / Scott S. Potter [and others] -- Attuning computer-supported work to human knowledge and processing capabilities in ship control centers / Mark A. Neerincx, Henk van Doorne, and Mark Ruijsendaal -- Building cognitive task analyses and models of a decision-making team in complex real-time environment / Wayne W. Zachary, Joan M. Ryder, and James H. Hicinbothom -- Modeling a command center / Peter J.M.D. Essens, Wilfried M. Post, and Peter C. Rasker -- Analyzing command team skills / John Annett and David Cunningham -- Cognitive task analysis of teams / Gary Klein -- Analyzing knowledge requirements in team tasks / Elizabeth Blickensderfer [and others] -- On the future of cognitive task analysis / Alan Lesgold -- State-of-the-art review of cognitive task analysis techniques / Jan Maarten Schraagen, Susan F. Chipman, and Valerie J. Shute.},
address = {Mahwah, NJ},
author = {Schraagen, Jan Maarten. and Chipman, Susan F. and Shalin, Valerie L.},
isbn = {0805833838},
publisher = {L. Erlbaum Associates},
title = {{Cognitive task analysis}},
year = {2000}
}
@inproceedings{Julisch2001,
abstract = {It is a well-known problem that intrusion detection systems overload their human operators by triggering thousands of alarms per day. As a matter of fact, IBM Research's Zurich Research Laboratory has been asked by one of our service divisions to help them deal with this problem. This paper presents the results of our research, validated thanks to a large set of operational data. We show that alarms should be managed by identifying and resolving their root causes. Alarm clustering is introduced as a method that supports the discovery of root causes. The general alarm clustering problem is proved to be NP-complete, an approximation algorithm is proposed, and experiments are presented.},
address = {New Orleans, LA, USA},
author = {Julisch, Klaus},
booktitle = {Proceedings - Annual Computer Security Applications Conference, ACSAC},
doi = {10.1109/ACSAC.2001.991517},
isbn = {0769514057},
issn = {10639527},
keywords = {Approximation algorithms,Clustering algorithms,Humans,Intrusion detection,Laboratories,Monitoring,Network address translation,Pattern matching,TCPIP,Telecommunication traffic},
pages = {12--21},
publisher = {IEEE Computer Society},
title = {{Mining alarm clusters to improve alarm handling efficiency}},
year = {2001}
}
@inproceedings{Bos2016,
abstract = {Cyber defense requires decision making under uncertainty, yet this critical area has not been a focus of research in judgment and decision-making. Future defense systems, which will rely on software-defined networks and may employ "moving target" defenses, will increasingly automate lower level detection and analysis, but will still require humans in the loop for higher level judgment. We studied the decision making process and outcomes of 17 experienced network defense professionals who worked through a set of realistic network defense scenarios. We manipulated gain versus loss framing in a cyber defense scenario, and found significant effects in one of two focal problems. Defenders that began with a network already in quarantine (gain framing) used a quarantine system more, as measured by cost, than those that did not (loss framing). We also found some difference in perceived workload and efficacy. Alternate explanations of these findings and implications for network defense are discussed.},
address = {Washington DC},
author = {Bos, Nathan and Paul, Celeste Lyn and Gersh, John R. and Greenberg, Ariel and Piatko, Christine and Sperling, Scott and Spitaletta, Jason and Arendt, Dustin L. and Burtner, Russ},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931213601038},
file = {::},
issn = {10711813},
month = {sep},
pages = {168--172},
publisher = {Human Factors an Ergonomics Society Inc.},
title = {{Effects of gain/loss framing in Cyber defense decision-making}},
url = {http://journals.sagepub.com/doi/10.1177/1541931213601038},
volume = {60},
year = {2016}
}
@inproceedings{Mancuso2014a,
abstract = {Cyber operations offer a unique environment in which the lines between cognition and technology are constantly blurred. Within the greater research community, current work often focuses solely on the technology, often only acknowledging the human in passing, if at all. More recently, the Human Factors community has begun to address human-centered issues in cyber operations, but in comparison to technological communities, we have yet to scratch the surface. Even with publications on Cyber Human Factors gaining momentum, we still lack a complete and holistic understanding of the domain itself, creating a major gap in the field. The purpose of this panel is to continue to expand the role Human Factors in cyber research by introducing the community to current work being done, and to facilitate collaborations to drive future research. We have assembled a panel of scientists across multiple specializations in the Human Factors community to have an open discussion on how we can leverage previous work in human factors and current work in cyber operations to continue to push the bounds of the field.},
author = {Mancuso, Vincent F. and Christensen, James C. and Cowley, Jennifer and Finomore, Victor and Gonzalez, Cleotide and Knott, Benjamin},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931214581085},
file = {::},
isbn = {9780945289456},
issn = {10711813},
month = {sep},
number = {1},
pages = {415--418},
publisher = {Human Factors an Ergonomics Society Inc.},
title = {{Human factors in cyber warfare II: Emerging perspectives}},
url = {http://journals.sagepub.com/doi/10.1177/1541931214581085},
volume = {2014-Janua},
year = {2014}
}
@book{Suchman1987,
address = {Cambridge, MA},
author = {Suchman, Lucy},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Mills1940.pdf:pdf},
publisher = {Cambridge University Press},
title = {{Plans and situated actions: The problem of human-machine communication}},
year = {1987}
}
@misc{Departmen2019,
author = {{U.S. Department of Homeland Security}},
title = {{Impact Cyber Trust}},
url = {https://www.impactcybertrust.org/},
urldate = {2019-12-18},
year = {2019}
}
@inproceedings{Sawyer2014,
abstract = {Cyber security operators in the military and civilian sector face a lengthy repetitive work assignment with few critical signal occurrences under conditions in which they have little control over what transpires. In this sense, their task is similar to vigilance tasks that have received considerable attention from human factors specialists in regard to other operational assignments such as air traffic control, industrial process control, and medical monitoring. Accordingly, this study was designed to determine if cyber security tasks can be linked to more traditional vigilance tasks in regard to several factors known to influence vigilance performance and perceived mental workload including time on task, the probability of critical signal occurrence, and event rate (the number of stimulus events that must be monitored in order to detect critical signals). Consistent with the results obtained in traditional vigilance experiments, signal detection on a 40-minute simulated cyber security task declined significantly over time, was directly related to signal probability, and inversely related to event rate. In addition, as in traditional vigilance tasks, perceived mental workload in the cyber task, as reflected by the NASA Task Load Index, was high. The results of this study have potential meaning for designers of cyber security systems in regard to psychophysical factors that might influence task performance and the need to keep the workload of such systems from exceeding the information processing bounds of security operators.},
address = {Chicago, IL},
author = {Sawyer, Ben D. and Finomore, Victor S. and Funke, Gregory J. and Mancuso, Vincent F. and Funke, Matthew E. and Matthews, Gerald and Warm, Joel S.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931214581369},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawyer et al. - 2014 - Cyber vigilance Effects of signal probability and event rate.pdf:pdf},
isbn = {9780945289456},
issn = {10711813},
month = {sep},
pages = {1771--1775},
publisher = {Human Factors an Ergonomics Society Inc.},
title = {{Cyber vigilance: Effects of signal probability and event rate}},
url = {http://journals.sagepub.com/doi/10.1177/1541931214581369},
year = {2014}
}
@book{Alazab2019,
address = {Cham, Switzerland},
author = {Alazab, Mamoun and Tang, MingJian},
doi = {10.1007/978-3-030-13057-2},
editor = {Alazab, Mamoun and Tang, MingJian},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/2019{\_}Book{\_}DeepLearningApplicationsForCyb.pdf:pdf},
isbn = {978-3-030-13056-5},
publisher = {Springer International Publishing},
series = {Advanced Sciences and Technologies for Security Applications},
title = {{Deep Learning Applications for Cyber Security}},
url = {http://link.springer.com/10.1007/978-3-030-13057-2 https://link.springer.com/content/pdf/10.1007/978-3-030-13057-2.pdf},
year = {2019}
}
@article{Getty1995,
abstract = {An automated detector designed to warn a system operator of a dangerous condition often has a low positive predictive value (PPV); that is, a small proportion of its warnings truly indicate the condition to be avoided. This is the case even for very sensitive detectors operating at very strict thresholds for issuing a warning because the prior probability of a dangerous condition is usually very low. As a consequence, operators often respond to a warning slowly or not at all. Reported here is a preliminary laboratory experiment designed in the context of signal detection theory that was conducted to examine the effects of variation in PPV on the latency of participants' response to a warning. Bonuses and penalties placed premiums on accurate performance in a background tracking task and on rapid response to the warnings. Observed latencies were short for high values of PPV, bimodal for middle-to-low values, and predominantly long for low values. The participants' response strategies for different PPVs were essentially optimal for the cost-benefit structure of the experiment. Some implications for system design are discussed. {\textcopyright} 1995 American Psychological Association.},
author = {Getty, David J. and Swets, John A. and Pickett, Ronald M. and Gonthier, David},
doi = {10.1037/1076-898X.1.1.19},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Getty et al. - 1995 - System Operator Response to Warnings of Danger A Laboratory Investigation of the Effects of the Predictive Value o.pdf:pdf},
issn = {1076898X},
journal = {Journal of Experimental Psychology: Applied},
number = {1},
pages = {19--33},
title = {{System Operator Response to Warnings of Danger: A Laboratory Investigation of the Effects of the Predictive Value of a Warning on Human Response Time}},
volume = {1},
year = {1995}
}
@article{Ning2004,
abstract = {Traditional intrusion detection systems (IDSs) focus on low-level attacks or anomalies, and raise alerts independently, though there may be logical connections between them. In situations where there are intensive attacks, not only will actual alerts be mixed with false alerts, but the amount of alerts will also become unmanageable. As a result, it is difficult for human users or intrusion response systems to understand the alerts and take appropriate actions. This paper presents a sequence of techniques to address this issue. The first technique constructs attack scenarios by correlating alerts on the basis of prerequisites and consequences of attacks. Intuitively, the prerequisite of an attack is the necessary condition for the attack to be successful, while the consequence of an attack is the possible outcome of the attack. Based on the prerequisites and consequences of different types of attacks, the proposed method correlates alerts by (partially) matching the consequences of some prior alerts with the prerequisites of some later ones. Moreover, to handle large collections of alerts, this paper presents a set of interactive analysis utilities aimed at facilitating the investigation of large sets of intrusion alerts. This paper also presents the development of a toolkit named TIAA, which provides system support for interactive intrusion analysis. This paper finally reports the experiments conducted to validate the proposed techniques with the 2000 DARPA intrusion detection scenario-specific datasets, and the data collected at the DEFCON 8 Capture the Flag event.},
author = {Ning, Peng and Cui, Yun and Reeves, Douglas S. and Xu, Dingbang},
doi = {10.1145/996943.996947},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ning et al. - 2004 - Techniques and tools for analyzing intrusion alerts.pdf:pdf},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {Alert correlation,Intrusion detection,Security management},
month = {may},
number = {2},
pages = {274--318},
title = {{Techniques and tools for analyzing intrusion alerts}},
url = {https://dl.acm.org/doi/10.1145/996943.996947},
volume = {7},
year = {2004}
}
@article{Tversky1981,
abstract = {The psychological principles that govern the perception of decision problems and the evaluation of probabilities and outcomes produce predictable shifts of preference when the same problem is framed in different ways. Reversals of preference are demonstrated in choices regarding monetary outcomes, both hypothetical and real, and in questions pertaining to the loss of human lives. The effects of frames on preferences are compared to the effects of perspectives on perceptual appearance. The dependence of preferences on the formulation of decision problems is a significant concern for the theory of rational choice. Copyright {\textcopyright} 1981 AAAS.},
author = {Tversky, Amos and Kahneman, Daniel},
doi = {10.1126/science.7455683},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tversky, Kahneman - 1981 - The framing of decisions and the psychology of choice.pdf:pdf},
issn = {00368075},
journal = {Science},
number = {4481},
pages = {453--458},
pmid = {7455683},
title = {{The framing of decisions and the psychology of choice}},
volume = {211},
year = {1981}
}
@book{Breznitz1984,
address = {Hillsdale, NJ},
author = {Breznitz, Shelomo},
publisher = {Lawrence Erlbaum Associates},
title = {{Cry wolf: the psychology of false alarms}},
year = {1984}
}
@article{Gutzwiller2018,
abstract = {Inverting human factors can aid in cyber defense by flipping well-known guidelines and using them to degrade and disrupt the performance of a cyber attacker. There has been significant research on how we perform cyber defense tasks and how we should present information to operators, cyber defenders, and analysts to make them more efficient and more effective. We can actually create these situations just as easily as we can mitigate them. Oppositional human factors are a new way to apply well-known research on human attention allocation to disrupt potential cyber attackers and provide much-needed asymmetric benefits to the defender.},
author = {Gutzwiller, Robert S and Ferguson-Walter, Kimberly and Gutzwiller, Robert and Fugate, Sunny and Rogers, Andrew},
doi = {10.1177/1541931218621063},
file = {::},
issn = {1541-9312},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
month = {sep},
number = {1},
pages = {272--276},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{"Oh, look, a butterfly!" A framework for distracting attackers to improve cyber defense.}},
url = {http://journals.sagepub.com/doi/10.1177/1541931218621063 https://www.researchgate.net/publication/326561335},
volume = {62},
year = {2018}
}
@article{Rice2009,
abstract = {The author examined the effects of human responses to automation alerts and nonalerts. Previous research has shown that automation false alarms and misses have differential effects on human trust (i.e., automation false alarms tend to affect operator compliance, whereas automation misses tend to affect operator reliance). Participants performed a simulated combat task, whereby they examined aerial photographs for the presence of enemy targets. A diagnostic aid provided a recommendation during each trial. The author manipulated the reliability and response bias of the aid to provide appropriate data for state-trace analyses. The analyses provided strong evidence that only a multiple-process theory of operator trust can explain the effects of automation errors on human dependence behaviors. The author discusses the theoretical and practical implications of this finding. {\textcopyright} 2009 Heldref Publications.},
author = {Rice, Stephen},
doi = {10.3200/GENP.136.3.303-322},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rice - 2009 - Examining single- and multiple-process theories of trust in automation.pdf:pdf},
issn = {00221309},
journal = {Journal of General Psychology},
keywords = {Automation,Dependence,State,Trace,Trust},
month = {jul},
number = {3},
pages = {303--322},
publisher = {Routledge},
title = {{Examining single- and multiple-process theories of trust in automation}},
url = {https://www.tandfonline.com/doi/abs/10.3200/GENP.136.3.303-322},
volume = {136},
year = {2009}
}
@misc{Roden,
author = {Roden, Williams and Layman, Lucas},
booktitle = {GitHub},
title = {{The Cry Wolf Dataset - A repository of simulated IDS alerts for experimentation}},
url = {https://uncw-hfcs.github.io/ids-simulator-analysis/},
urldate = {2020-09-16},
year = {2019}
}
@misc{Vast2012,
author = {Cook, Kris and Grinstein, Georges and Whiting, Mark},
booktitle = {Visual Analytics Community},
howpublished = {http://www.vacommunity.org/VAST+Challenge+2012},
title = {{VAST Challenge 2012}},
url = {http://www.vacommunity.org/VAST+Challenge+2012},
urldate = {2019-04-28},
year = {2012}
}
@misc{Roden2019a,
author = {Roden, William and Layman, Lucas},
booktitle = {GitHub},
title = {{The Cry Wolf IDS Simulator - An environment for conducting controlled experiments of cyber security analysis tasks}},
url = {https://uncw-hfcs.github.io/ids-simulator/},
urldate = {2020-09-16},
year = {2019}
}
@inproceedings{Gutzwiller2015,
abstract = {Technology's role in the fight against malicious cyber-attacks is critical to the increasingly networked world of today. Yet, technology does not exist in isolation: the human factor is an aspect of cyber-defense operations with increasingly recognized importance. Thus, the human factors community has a unique responsibility to help create and validate cyber defense systems according to basic principles and design philosophy. Concurrently, the collective science must advance. These goals are not mutually exclusive pursuits: therefore, toward both these ends, this research provides cyber-cognitive links between cyber defense challenges and major human factors and ergonomics (HFE) research areas that offer solutions and instructive paths forward. In each area, there exist cyber research opportunities and realms of core HFE science for exploration. We raise the cyber defense domain up to the HFE community at-large as a sprawling area for scientific discovery and contribution.},
address = {Los Angeles, CA},
author = {Gutzwiller, Robert S. and Fugate, Sunny and Sawyer, Benjamin D. and Hancock, P. A.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931215591067},
file = {::},
isbn = {9780945289470},
issn = {10711813},
month = {sep},
pages = {322--326},
publisher = {Human Factors an Ergonomics Society Inc.},
title = {{The human factors of cyber network defense}},
url = {http://journals.sagepub.com/doi/10.1177/1541931215591067},
volume = {59},
year = {2015}
}
@article{Mitroff2014,
abstract = {Accuracy is paramount in radiology and security screening, yet many factors undermine success. Target prevalence is a particularly worrisome factor, as targets are rarely present (e.g., the cancer rate in mammography is {\~{}}0.5{\%}), and low target prevalence has been linked to increased search errors. More troubling is the fact that specific target types can have extraordinarily low frequency rates (e.g., architectural distortions in mammography—a specific marker of potential cancer—appear in fewer than 0.05{\%} of cases). By assessing search performance across millions of trials from the Airport Scanner smartphone application, we demonstrated that the detection of ultra-rare items was disturbingly poor. A logarithmic relationship between target detection and target frequency (adjusted R2 = .92) revealed that ultra-rare items had catastrophically low detection rates relative to targets with higher frequencies. Extraordinarily low search performance for these extraordinarily rare targets—what we term the ultra-rar...},
author = {Mitroff, Stephen R. and Biggs, Adam T.},
doi = {10.1177/0956797613504221},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Mitroff2014.pdf:pdf},
issn = {0956-7976},
journal = {Psychological Science},
keywords = {visual attention,visual memory,visual search},
month = {jan},
number = {1},
pages = {284--289},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{The Ultra-Rare-Item Effect}},
url = {http://journals.sagepub.com/doi/10.1177/0956797613504221},
volume = {25},
year = {2014}
}
@inproceedings{Bustamante2004,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 270723219 Effects Systems Performance . . . Article DOI : 10 . 1177 / 154193120404801633 CITATIONS 6 READS 56 3 , including : James Old 116 , 315 SEE All . The . Using a dual - task paradigm , we examined how alarm system detection threshold and task complexity affected human performance and perceived workload . We hypothesized that using an alarm system would improve task performance and lower perceived workload , particularly when task complexity was high and at the medium threshold level . Twenty - one students from Old Dominion University participated in this study . Results showed that alarm use improved performance during low task complexity . For high task complexity , improvement was accomplished only when alarm system threshold was low or intermediate . Results also indicatedthatchangingthealarmsystemthresholdaffectedperformanceonlyunderhightaskloadconditions.Optimalperformancewasachievedbysettingthethresholdofthealarmsystematitslowestlevel.Theuseofalarmsreducedworkloadunderbothlowandhightaskcomplexitylevels,butonlywhenthethresholdwashigh.Theincreaseduseofautomatedsystemshasnotreplacedtheneedforhumanoperators.Instead,automationhaschangedtheroleofhumansfromoperatorstosystemmonitors(Parasuraman{\&}Riley,1997).Aclearadvantageofautomatedsystemsisthattheyperformcertaintasksmoreaccuratelyandreliablythanhumanoperators.Anotheradvantageisthattheycanimprovehumanperformanceoncomplextasksbyreducingworkload.Alarmsystemsareanessentialfeatureofautomatedsystemsbecausetheymayreducetheworkloadassociatedwithmonitoringtasks.However,whetheranalarmsystemaccomplishesthisdependsonitsreliability,aswellasthedemandsofthetaskathand.Thepurposeofthisstudywastoexaminehowchangingthethresholdofanalarmsystemandthecomplexityofthetaskaffecthumanperformanceandperceivedworkload.XiaoandSeagull(1999)emphasizedthatalarmsystemsmayservemanyfunctionsbycompensatingforhumanlimitations.Otherresearchershaveshownthatalarmsystemsmayimprovehumanperformanceinmanycomplextasks},
address = {New Orleans, LA, USA},
author = {Bustamante, Ernesto A. and Anderson, Brittany L. and Bliss, James P.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
doi = {10.1177/154193120404801633},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bustamante, Anderson, Bliss - 2004 - Effects of Varying the Threshold of Alarm Systems and Task Complexity on Human Performance and Perc.pdf:pdf},
issn = {1541-9312},
month = {sep},
pages = {1948--1952},
publisher = {SAGE Publications},
title = {{Effects of Varying the Threshold of Alarm Systems and Task Complexity on Human Performance and Perceived Workload}},
url = {http://journals.sagepub.com/doi/10.1177/154193120404801633},
volume = {48},
year = {2004}
}
@article{Heckman2013,
abstract = {In January 2012, MITRE performed a real-time, red team/blue team cyber-wargame experiment. This presented the opportunity to blend cyber-warfare with traditional mission planning and execution, including denial and deception tradecraft. The cyber-wargame was designed to test a dynamic network defense cyber-security platform being researched in The MITRE Corporation's Innovation Program called Blackjack, and to investigate the utility of using denial and deception to enhance the defense of information in command and control systems. The Blackjack tool failed to deny the adversary access to real information on the command and control mission system. The adversary had compromised a number of credentials without the computer network defenders' knowledge, and thereby observed both the real command and control mission system and the fake command and control mission system. However, traditional denial and deception techniques were effective in denying the adversary access to real information on the real command and control mission system, and instead provided the adversary with access to false information on a fake command and control mission system. {\textcopyright} 2013 Elsevier Ltd. All rights reserved.},
author = {Heckman, Kristin E. and Walsh, Michael J. and Stech, Frank J. and O'Boyle, Todd A. and Dicato, Stephen R. and Herber, Audra F.},
doi = {10.1016/j.cose.2013.03.015},
issn = {01674048},
journal = {Computers and Security},
keywords = {Authentication,Computer security,Counterdeception,Deception,Denial,Wargame},
month = {sep},
pages = {72--77},
publisher = {Elsevier Advanced Technology},
title = {{Active cyber defense with denial and deception: A cyber-wargame experiment}},
volume = {37},
year = {2013}
}
@inproceedings{Mancuso2015,
abstract = {Modern cyber operations require operators to maintain supervisory control of remote computer agents. A current operational concern is the number of agents an operator can control at once. This type of task resonates with similar Human Supervisory Control (HSC) research that has been conducted in environments such as Unmanned Aerial Vehicle operations. Within the relevant literature, there has been limited discussion of cyber-HSC, and no available experimental research. In this paper, we present an initial exploration cyber-HSC. Using the BOARD 1.5 Simulation, we manipulated the number of autonomous assets accessible to a human operator. We expected that as the number of autonomous agents available increased, we would observe concomitant changes in human performance and cognition. However, our results indicated that participants' overall span-of-control did not vary with additional agents. Our findings highlight the need for continued research on issues of supervisory control within cyber operations.},
address = {Los Angeles, CA},
author = {Mancuso, Vincent F. and Funke, Gregory J. and Strang, Adam J. and Eckold, Monica B.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931215591066},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mancuso et al. - 2015 - Capturing performance in cyber human supervisory control.pdf:pdf},
isbn = {9780945289470},
issn = {10711813},
month = {sep},
pages = {317--321},
publisher = {Human Factors an Ergonomics Society Inc.},
title = {{Capturing performance in cyber human supervisory control}},
url = {http://journals.sagepub.com/doi/10.1177/1541931215591066},
volume = {2015-Janua},
year = {2015}
}
@article{Bliss2000,
abstract = {The research was conducted to investigate the effect of increasing primary task and alarm workload on alarm mistrust as reflected by alarm and primary task performances. A total of 126 undergraduate students performed a complex psychomotor task battery three times, with the number of concurrent tasks increasing each time. During their performance, the students were required to react to an alarm system (including visual and auditory components) of questionable reliability. Depending on the group to which participants were assigned, the alarm presentation rate constituted a low-, medium- or highworkload condition. Alarm response data (times, frequencies, accuracies) and primary task data (tracking error) were analyzed to assess performance differences as a function of primary and secondary task workload levels. Results generally supported the hypotheses: increasing primary task and alarm task workload degraded alarm response performance. Also, response frequencies supported earlier research suggesting that ...},
author = {Bliss, James P. and Dunn, Mariea C.},
doi = {10.1080/001401300421743},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Bliss01.pdf:pdf},
issn = {0014-0139},
journal = {Ergonomics},
keywords = {Attention Alarm Psychomotor Performance Workload M},
month = {sep},
number = {9},
pages = {1283--1300},
publisher = {Taylor {\&} Francis Group},
title = {{Behavioural implications of alarm mistrust as a function of task workload}},
url = {https://www.tandfonline.com/doi/full/10.1080/001401300421743},
volume = {43},
year = {2000}
}
@book{Green1966,
address = {New York},
author = {Green, David M. and Swets, John A.},
publisher = {Wiley},
title = {{Signal detection theory and psychophysics}},
year = {1966}
}
@book{Rosenthal1991a,
address = {New York},
author = {Rosenthal, Robert and Rosnow, Ralph L.},
publisher = {McGraw-Hill},
title = {{Essentials of behavioral research : methods and data analysis}},
year = {1991}
}
@book{Kahneman1973,
address = {Englewood Cliffs, NJ},
author = {Kahneman, Daniel},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kahneman - 1973 - Attention and Effort.pdf:pdf},
publisher = {Prentice Hall Inc.},
title = {{Attention and Effort}},
year = {1973}
}
@article{Hancock2011,
abstract = {Objective: We evaluate and quantify the effects of human, robot, and environmental factors on perceived trust in human-robot interaction (HRI).Background: To date, reviews of trust in HRI have been qualitative or descriptive. Our quantitative review provides a fundamental empirical foundation to advance both theory and practice.Method: Meta-analytic methods were applied to the available literature on trust and HRI. A total of 29 empirical studies were collected, of which 10 met the selection criteria for correlational analysis and 11 for experimental analysis. These studies provided 69 correlational and 47 experimental effect sizes.Results: The overall correlational effect size for trust was r- = +0.26, with an experimental effect size of d- = +0.71. The effects of human, robot, and environmental characteristics were examined with an especial evaluation of the robot dimensions of performance and attribute-based factors. The robot performance and attributes were the largest contributors to the development of trust in HRI. Environmental factors played only a moderate role.Conclusion: Factors related to the robot itself, specifically, its performance, had the greatest current association with trust, and environmental factors were moderately associated. There was little evidence for effects of human-related factors.Application: The findings provide quantitative estimates of human, robot, and environmental factors influencing HRI trust. Specifically, the current summary provides effect size estimates that are useful in establishing design and training guidelines with reference to robot-related factors of HRI trust. Furthermore, results indicate that improper trust calibration may be mitigated by the manipulation of robot design. However, many future research needs are identified. {\textcopyright} 2011, Human Factors and Ergonomics Society.},
author = {Hancock, Peter A. and Billings, Deborah R. and Schaefer, Kristin E. and Chen, Jessie Y.C. and {De Visser}, Ewart J. and Parasuraman, Raja},
doi = {10.1177/0018720811417254},
file = {::},
issn = {00187208},
journal = {Human Factors},
keywords = {human-robot team,robotics,trust,trust development},
month = {oct},
number = {5},
pages = {517--527},
pmid = {22046724},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{A meta-analysis of factors affecting trust in human-robot interaction}},
url = {http://journals.sagepub.com/doi/10.1177/0018720811417254},
volume = {53},
year = {2011}
}
@incollection{Albanese2016,
address = {New York},
author = {Albanese, Massimiliano and Jajodia, Sushil},
booktitle = {Psychosocial Dynamics of Cyber Security},
editor = {Zaccaro, Stephen J. and Dalal, Reeshad S. and Tetrick, Lois E. and Steinke, Julie A.},
pages = {291--304},
title = {{Technological Solutions for Improving Performance of Cyber Security Analysts}},
year = {2016}
}
@article{Durst1999,
abstract = {Accounting Office (GAO) disclosed that approximately 250,000 break-ins into Federal computer systems were attempted over the previous year. At least 10 major agencies, comprising 98{\%} of the total Federal budget, had been attacked. The GAO went on to say that an estimated 64{\%} of these attacks (about 160,000) were successful. It gets worse: the number of attacks is doubling every year. Based on previous studies, the GAO estimates that only 1{\%} and 4{\%} of these attacks will be detected and only about 1{\%} will be reported. 1 Your mission-critical software has vulnerabilities that intruders will try to exploit. The U.S. Air Force is developing new systems to catch hackers as they search for weak points.},
author = {Durst, Robert and Champion, Terrence and Witten, Brian and Spagnuolo, Luigi and Miller, Eric},
doi = {10.1145/306549.306571},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Durst et al. - 1999 - Testing and Evaluating Computer Intrusion Detection Systems.pdf:pdf},
issn = {15577317},
journal = {Communications of the ACM},
month = {jul},
number = {7},
pages = {53--61},
title = {{Testing and Evaluating Computer Intrusion Detection Systems}},
url = {https://dl.acm.org/doi/10.1145/306549.306571},
volume = {42},
year = {1999}
}
@inproceedings{10.1007/978-3-319-60585-2_15,
abstract = {In most cyber security contexts, users need to make trade-offs for information security. This research examined this issue by quantifying the relative value of information security within a value system that comprises of multiple conflicting objectives. Using this quantification as a platform, this research also examined the effect of different usage contexts on information security concern. Users were asked to indicate how much loss in productivity and time, and how much more money they were willing to incur to acquire an effective phishing filter. The results indicated that users prioritize productivity and time over information security while there was much more heterogeneity in the concern about cost. The value of information security was insignificantly different across different usage contexts. The relative value of information security was found to be predictive of self-reported online security behaviors. These results offer valuable implications for the design of a more usable information security system.},
address = {Orlando, FL, USA},
author = {Nguyen, Kenneth D and Rosoff, Heather and John, Richard S},
booktitle = {Advances in Human Factors in Cybersecurity},
editor = {Nicholson, Denise},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/Nguyen2018{\_}Chapter{\_}ValuingInformationSecurityFrom.pdf:pdf},
isbn = {978-3-319-60585-2},
pages = {146--157},
publisher = {Springer International Publishing},
title = {{Valuing Information Security from a Phishing Attack}},
year = {2018}
}
@article{Manganaris2000,
abstract = {IBM's emergency response service provides real-time intrusion detection (RTID) services through the Internet for a variety of clients. As the number of clients increases, the volume of alerts generated by the RTID sensors becomes intractable. This problem is aggravated by the fact that some sensors may generate hundreds or even thousands of innocent alerts per day. With an eye towards managing these alerts more effectively, IBM's data mining services group analyzed a database of RTID reports. The first objective was an approach for characterizing the `normal' stream of alerts from a sensor. Using such models tuned to individual sensors, we then developed a methodology for detecting anomalies. In contrast to many popular approaches, the decision to filter an alarm out or not takes into consideration the context in which it occurred and the historical behavior of the sensor it came from. Our second objective was to identify all the different profiles of our clients. Based on their history of alerts, we discovered several different types of clients, with different alert behaviors and thus different monitoring needs. We present the issues encountered, solutions, and findings, and discuss how our results may be used in large-scale RTID operations.},
author = {Manganaris, Stefanos and Christensen, Marvin and Zerkle, Dan and Hermiz, Keith},
doi = {10.1016/S1389-1286(00)00138-9},
issn = {13891286},
journal = {Computer Networks},
month = {oct},
number = {4},
pages = {571--577},
publisher = {Elsevier},
title = {{Data mining analysis of RTID alarms}},
volume = {34},
year = {2000}
}
@article{DAmico2005,
abstract = {A Cognitive Task Analysis (CTA) was performed to investigate the workflow, decision processes, and cognitive demands of information assurance (IA) analysts responsible for defending against attacks on critical computer networks. We interviewed and observed 41 IA analysts responsible for various aspects of cyber defense in seven organizations within the US Department of Defense (DOD) and industry. Results are presented as workflows of the analytical process and as attribute tables including analyst goals, decisions, required knowledge, and obstacles to successful performance. We discuss how IA analysts progress through three stages of situational awareness and how visual representations are likely to facilitate cyber defense situational awareness.},
author = {D'Amico, Anita and Whitley, Kirsten and Tesone, Daniel and O'Brien, Brianne and Roth, Emilie},
doi = {10.1177/154193120504900304},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/DAmico2005.pdf:pdf},
isbn = {094528926X},
issn = {1541-9312},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
month = {sep},
number = {3},
pages = {229--233},
pmid = {22444050},
title = {{Achieving Cyber Defense Situational Awareness: A Cognitive Task Analysis of Information Assurance Analysts}},
url = {http://journals.sagepub.com/doi/10.1177/154193120504900304},
volume = {49},
year = {2005}
}
@article{Gutzwiller2019,
abstract = {We report on whether cyber attacker behaviors contain decision making biases. Data from a prior experiment were analyzed in an exploratory fashion, making use of think-aloud responses from a small group of red teamers. The analysis provided new observational evidence of traditional decision-making biases in red team behaviors (confirmation bias, anchoring, and take-the-best heuristic use). These biases may disrupt red team decisions and goals, and simultaneously increase their risk of detection. Interestingly, at least part of the bias induction may be related to the use of cyber deception. Future directions include the development of behavioral measurement techniques for these and additional cognitive biases in cyber operators, examining the role of attacker traits, and identifying the conditions where biases can be induced successfully in experimental conditions.},
author = {Gutzwiller, Robert S. and Ferguson-Walter, Kimberly J. and Fugate, Sunny J.},
doi = {10.1177/1071181319631096},
file = {::},
issn = {2169-5067},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
month = {nov},
number = {1},
pages = {427--431},
publisher = {SAGE Publications},
title = {{Are Cyber Attackers Thinking Fast and Slow? Exploratory Analysis Reveals Evidence of Decision-Making Biases in Red Teamers}},
url = {http://journals.sagepub.com/doi/10.1177/1071181319631096},
volume = {63},
year = {2019}
}
@book{Wickens1992,
address = {New York},
author = {Wickens, Christopher D and Hollands, Justin G},
edition = {Second},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wickens, Hollands - 1992 - Engineering Psychology and Human Performance.pdf:pdf},
isbn = {0321047117},
publisher = {Harper Collins},
title = {{Engineering Psychology and Human Performance}},
url = {http://www-personal.umich.edu/{~}itm/688/wk2/WickensHollands-EngineeringPsych-Ch3.pdf},
year = {1992}
}
@inproceedings{Guenzler2013,
abstract = {Providing access towards raw data is often considered to be a good solution for improving human decision making in interaction with imperfect automated decision support such as alarm systems. However, there is some evidence that such cross-checking measures are used in an asymmetric manner with respect to the amount of uncertainty involved in the decision. Namely, people seem to accept low amounts of uncertainty when complying with an alarm cue, but not when contradicting it. The current study investigates the question whether this phenomenon is limited to alarm systems and a high risk environment. Within a multi-task PC simulation participants performed a low risk monitoring task which was supported by a system neutrally framed as "assistant system". In one group the cues emitted by the system were 90{\%} correct, in the other 10{\%} were correct, thus causing a 10{\%} uncertainty about the real state in both conditions. Results show a strong asymmetry as participants in the latter condition spent a high amount of effort in reducing their uncertainty, while participants in the former condition did not. Furthermore participants' behavior almost exactly replicates the asymmetric cross-checking pattern found in a former study which employed a comparatively high risk monitoring task supported by an "alarm system". This supports the hypothesis that the observed commission bias represents a general phenomenon in the context of automated decision support, irrespective of the risk attributed to the environment and irrespective of whether the system represents an alarm system or not. Copyright 2013 by Human Factors and Ergonomics Society, Inc.},
address = {San Diego, CA, USA},
annote = {Same experiments as Gerar * Manzey except that it was an "alert system" not an "alarm system", and it was a "beer labeling task" not a "chemical process control".

Participants had the possibility to inspect the raw data (i.e. the actual wort concentration) directly, thereby reducing uncertainty upon a given system cue to zero, BUT the double-checking procedure was tedious and time-consuming.

Results are similar to Gerard {\&} Manzey - they investigate 10{\%} PPV much more than 90{\%} PPV, despite the lower subjective risks.},
author = {Guenzler, Torsten and Manzey, Dietrich},
booktitle = {Proceedings of the Human Factors and Ergonomics Society 57th Annual Meeting},
doi = {10.1177/1541931213571301},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guenzler, Manzey - 2013 - Asymmetries in human tolerance of uncertainty in interaction with alarm systems Effects of risk perception or.pdf:pdf},
isbn = {9780945289432},
issn = {10711813},
month = {sep},
pages = {1362--1366},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Asymmetries in human tolerance of uncertainty in interaction with alarm systems: Effects of risk perception or evidence for a general commission bias?}},
url = {http://journals.sagepub.com/doi/10.1177/1541931213571301},
volume = {57},
year = {2013}
}
@inproceedings{Zhong2015,
address = {New York, New York, USA},
annote = {specification of analysis actions

possible reference to Sarah for analysis experiment},
author = {Zhong, Chen and Yen, John and Liu, Peng and Erbacher, Rob and Etoty, Renee and Garneau, Christopher},
booktitle = {Proceedings of the 2015 Symposium and Bootcamp on the Science of Security - HotSoS '15},
doi = {10.1145/2746194.2746203},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Zhong2015.pdf:pdf},
isbn = {9781450333764},
pages = {1--11},
publisher = {ACM Press},
title = {{An integrated computer-aided cognitive task analysis method for tracing cyber-attack analysis processes}},
url = {http://dl.acm.org/citation.cfm?doid=2746194.2746203},
year = {2015}
}
@article{McArthur1972,
abstract = {Investigated H. Kelley's (see record) attribution theory. 87 male undergraduates filled out a questionnaire that reported 16 different responses ostensibly made by other people. These responses represented 4 verb categories emotions, accomplishments, opinions, and actions and, for the 64 experimental Ss, each was accompanied by high or low consensus, distinctiveness, and consistency information. Control Ss were not given any information regarding the response. All Ss were asked to attribute each response to characteristics of the person (i.e., the actor), the stimulus, the circumstances, or to some combination of these 3 factors. In addition Ss' expectancies for future response and stimulus generalization on the part of the actor were measured. The 3 information variables and verb category each had a significant effect on causal attribution and on expectancy for behavioral generalization. (16 ref.) (PsycINFO Database Record (c) 2006 APA, all rights reserved). {\textcopyright} 1972 American Psychological Association.},
author = {McArthur, Leslie A.},
doi = {10.1037/h0032602},
file = {:C$\backslash$:/Users/laymanl/Desktop/10.1.1.464.8655.pdf:pdf},
issn = {00223514},
journal = {Journal of Personality and Social Psychology},
keywords = {accomplishments {\&},action responses {\&},causality attributions,consistency responses,contribution to H. Kelley's attribution theory,distinctiveness {\&},emotion {\&},high vs. low consensus {\&},opinions {\&}},
month = {may},
number = {2},
pages = {171--193},
title = {{The how and what of why: Some determinants and consequences of causal attribution}},
url = {/record/1972-27156-001},
volume = {22},
year = {1972}
}
@incollection{Coovert2016,
address = {New York},
author = {Coovert, Michael D. and Dreibelbis, Rachel and Borum, Randy},
booktitle = {Psychosocial Dynamics of Cyber Security},
editor = {Zaccaro, Stephen J. and Dalal, Rasheed S. and Tetrick, Lois E. and Steinke, Julie A.},
pages = {267--290},
publisher = {Taylor {\&} Francis},
title = {{Factors Influencing the Human-Technology Interface for Effective Cyber Security Performance}},
year = {2016}
}
@article{Parasuraman2010,
author = {Parasuraman, Raja and Manzey, Dietrich},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parasuraman, Manzey - 2010 - Complacency and Bias in Human Use of Automation An Attentional Integration.pdf:pdf},
journal = {Human Factors},
number = {3},
pages = {381--410},
title = {{Complacency and Bias in Human Use of Automation: An Attentional Integration}},
volume = {52},
year = {2010}
}
@article{Schmidt1993,
author = {Schmidt, Frank L. and Hunter, John E.},
doi = {10.1111/1467-8721.ep10770456},
issn = {14678721},
journal = {Current Directions in Psychological Science},
month = {feb},
number = {1},
pages = {8--9},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Tacit Knowledge, Practical Intelligence, General Mental Ability, and Job Knowledge}},
url = {http://journals.sagepub.com/doi/10.1111/1467-8721.ep10770456},
volume = {2},
year = {1993}
}
@techreport{Verizon2010a,
author = {Verizon},
institution = {Verizon},
title = {{2010 Data Breach Investigations Report, http://goo.gl/28pPGM}},
year = {2010}
}
@article{Hart1988,
abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload. {\textcopyright} 1988 Elsevier Science {\&} Technology.},
author = {Hart, Sandra G. and Staveland, Lowell E.},
doi = {10.1016/S0166-4115(08)62386-9},
issn = {01664115},
journal = {Advances in Psychology},
month = {jan},
number = {C},
pages = {139--183},
title = {{Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research}},
volume = {52},
year = {1988}
}
@techreport{Werlinger2008,
abstract = {An intrusion detection system (IDS) can be a key component of security incident response within organizations. Traditionally , intrusion detection research has focused on improving the accuracy of IDSs, but recent work has recognized the need to support the security practitioners who receive the IDS alarms and investigate suspected incidents. To examine the challenges associated with deploying and maintaining an IDS, we analyzed 9 interviews with IT security practitioners who have worked with IDSs and performed participatory observations in an organization deploying a network IDS. We had three main research questions: (1) What do security practitioners expect from an IDS?; (2) What difficulties do they encounter when installing and configuring an IDS?; and (3) How can the usability of an IDS be improved? Our analysis reveals both positive and negative perceptions that security practitioners have for IDSs, as well as several issues encountered during the initial stages of IDS deployment. In particular, practitioners found it difficult to decide where to place the IDS and how to best configure it for use within a distributed environment with multiple stakeholders. We provide recommendations for tool support to help mitigate these challenges and reduce the effort of introducing an IDS within an organization.},
author = {Werlinger, Rodrigo and Hawkey, Kirstie and Muldner, Kasia and Jaferian, Pooya and Beznosov, Konstantin},
file = {::},
keywords = {Collaboration,Design Keywords Intrusion Detection,K65 [Management of Computing and Information Syste,Organizational Factors,Quali-tative Research,Security Management,Security Tools,Usable Security},
title = {{The Challenges of Using an Intrusion Detection System: Is It Worth the Effort?}},
url = {https://cups.cs.cmu.edu/soups/2008/proceedings/p107Werlinger.pdf},
year = {2008}
}
@article{Erbacher2010,
abstract = {The goal of our project is to create a set of next-generation cyber situational-awareness capabilities with applications to other domains in the long term. The objective is to improve the decision-making process to enable decision makers to choose better actions. To this end, we put extensive effort into making certain that we had feedback from network analysts and managers and understand what their genuine needs are. This article discusses the cognitive task-analysis methodology that we followed to acquire feedback from the analysts. This article also provides the details we acquired from the analysts on their processes, goals, concerns, the data and metadata that they analyze. Finally, we describe the generation of a novel task-flow diagram representing the activities of the target user base.},
author = {Erbacher, Robert F. and Frincke, Deborah A. and Wong, Pak Chung and Moody, Sarah and Fink, Glenn},
doi = {10.1057/ivs.2010.5},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Erbacher2010.5.pdf:pdf},
issn = {1473-8716},
journal = {Information Visualization},
keywords = {cognitive task analysis,cyber security visualization,security analyst feedback,task-flow diagram},
month = {sep},
number = {3},
pages = {204--219},
publisher = {SAGE PublicationsSage UK: London, England},
title = {{A Multi-Phase Network Situational Awareness Cognitive Task Analysis}},
url = {http://journals.sagepub.com/doi/10.1057/ivs.2010.5},
volume = {9},
year = {2010}
}
@misc{Kdd1999,
author = {Newman, David},
booktitle = {UCI KDD Archive},
howpublished = {http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html},
title = {{KDD Cup 1999 Data}},
url = {http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html},
urldate = {2019-04-29},
year = {1999}
}
@article{Dutt2013a,
abstract = {Objective: To determine the effects of an adversary's behavior on the defender's accurate and timely detection of network threats. Background: Cyber attacks cause major work disruption. It is important to understand how a defender's behavior (experience and tolerance to threats), as well as adversarial behavior (attack strategy), might impact the detection of threats. In this article, we use cognitive modeling to make predictions regarding these factors. Method: Different model types representing a defender, based on Instance-Based Learning Theory (IBLT), faced different adversarial behaviors. A defender's model was defined by experience of threats: threat-prone (90{\%} threats and 10{\%} nonthreats) and nonthreat-prone (10{\%} threats and 90{\%} nonthreats); and different tolerance levels to threats: risk-averse (model declares a cyber attack after perceiving one threat out of eight total) and risk-seeking (model declares a cyber attack after perceiving seven threats out of eight total). Adversarial behavior is simulated by considering different attack strategies: patient (threats occur late) and impatient (threats occur early). Results: For an impatient strategy, risk-averse models with threat-prone experiences show improved detection compared with risk-seeking models with nonthreat-prone experiences; however, the same is not true for a patient strategy. Conclusions: Based upon model predictions, a defender's prior threat experiences and his or her tolerance to threats are likely to predict detection accuracy; but considering the nature of adversarial behavior is also important. Application: Decision-support tools that consider the role of a defender's experience and tolerance to threats along with the nature of adversarial behavior are likely to improve a defender's overall threat detection.Copyright {\textcopyright} 2012, Human Factors and Ergonomics Society.},
author = {Dutt, Varun and Ahn, Young Suk and Gonzalez, Cleotilde},
doi = {10.1177/0018720812464045},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dutt, Ahn, Gonzalez - 2013 - Cyber situation awareness Modeling detection of cyber attacks with instance-based learning theory.pdf:pdf},
issn = {00187208},
journal = {Human Factors},
keywords = {Instance-Based Learning Theory,adversarial behavior,cyber situation awareness,defender,experiences,tolerance},
month = {jun},
number = {3},
pages = {605--618},
pmid = {23829034},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Cyber situation awareness: Modeling detection of cyber attacks with instance-based learning theory}},
url = {http://journals.sagepub.com/doi/10.1177/0018720812464045},
volume = {55},
year = {2013}
}
@inproceedings{Wiczorek2010,
abstract = {Most theories about operators' responses to alarm systems suggest that the operators' behavior is guided by their trust towards the system which in turn results from the subjective perception of system properties, namely the perceived reliability of the alarm system. However, some doubts about that assumption have arisen as recent research has not proven the mediating effect of trust. The purpose of this research was to examine the relalionship between alarm system properties, trust, and behavior. The alarm reliability was varied while keeping the other system properties constant. It was found that participants' response-rates to alarms were predicted by their perceived alarm reliabilities. However, no mediation by trust could be established. These results suggest that operators' behavior is not always guided by their trust towards the system. Under specific circumstances their compliance rather depends on rational consideration regarding the most efficient strategy. Copyright 2010 by Human Factors and Ergonomics Society, Inc. All rights reserved.},
address = {San Francisco, CA},
author = {Wiczorek, Rebecca and Manzey, Dietrich},
booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
doi = {10.1177/154193121005401976},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiczorek, Manzey - 2010 - Is Operators' Compliance with Alarm Systems a Product of Rational Consideration.pdf:pdf},
issn = {1541-9312},
month = {sep},
pages = {1722--1726},
publisher = {SAGE Publications},
title = {{Is Operators' Compliance with Alarm Systems a Product of Rational Consideration?}},
volume = {54},
year = {2010}
}
@inproceedings{Boyce2011,
abstract = {This paper provides an overview of critical areas of human performance research required to support the development and deployment of effective cybersecurity systems. These areas include usability and security compliance, mitigation of human error and risk reduction, enhancement of situation awareness, and development of effective visualization tools and techniques. We describe the nature of the research and development efforts required to support effective human-centered design of cybersecurity systems and make specific recommendations for near-term work in this area.},
author = {Boyce, Michael W. and Duma, Katherine Muse and Hettinger, Lawrence J. and Malone, Thomas B. and Wilson, Darren P. and Lockett-Reynolds, Janae},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1071181311551233},
file = {::},
isbn = {9780945289395},
issn = {10711813},
month = {sep},
number = {1},
pages = {1115--1119},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Human performance in cybersecurity: A research agenda}},
url = {http://pro.sagepub.com/lookup/doi/10.1177/1071181311551233},
volume = {55},
year = {2011}
}
@misc{Franke2014,
abstract = {Cyber situational awareness is attracting much attention. It features prominently in the national cyber strategies of many countries, and there is a considerable body of research dealing with it. However, until now, there has been no systematic and up-to-date review of the scientific literature on cyber situational awareness. This article presents a review of cyber situational awareness, based on systematic queries in four leading scientific databases. 102 articles were read, clustered, and are succinctly described in the paper. The findings are discussed from the perspective of both national cyber strategies and science, and some directions for future research are examined. {\textcopyright} 2014 Elsevier Ltd. All rights reserved.},
author = {Franke, Ulrik and Brynielsson, Joel},
booktitle = {Computers and Security},
doi = {10.1016/j.cose.2014.06.008},
file = {::},
issn = {01674048},
keywords = {Cyber security,Literature review,National cyber strategies,Research strategy,Situational awareness},
month = {oct},
pages = {18--31},
publisher = {Elsevier Ltd},
title = {{Cyber situational awareness - A systematic review of the literature}},
volume = {46},
year = {2014}
}
@article{Cybersecurity2018,
abstract = {Two threat-induced emotions and their respective ability to sway cybersecurity preferences were investigated after a cyberattack on financial institutions. Our theoretical aim was to advance the functionalist claim and differentiate between fear and anxiety by their action tendencies. The emotions were expected to have unique motivation power and thus show mutually exclusive ties to the three types of safety behaviors emerged in our study. Avoidance would be uniquely embraced by fearful participants, whereas surveillance and vigilance would uniquely appeal to anxious participants. Study 1 (N = 199) used a cross-sectional design and found full support for the hypothesis regarding anxiety but only partial support for the hypothesis regarding fear. Study 2 (N = 304), an experiment of fearful, anxious, and relaxed groups, did not yield significant results but did offer methodological recommendations. The quasi-experiments in Study 3 (N = 120) and Study 4 (N = 156) supported the hypotheses on fear and anxiety. Our results in the novel domain of cyber threat brought new evidence to bear on the mixed literature on fear and anxiety. A discussion is offered on the methodological challenges of differentiating two closely related emotions as well as implications for the emerging debate of cybersecurity solutions. (PsycINFO Database Record (c) 2018 APA, all rights reserved).},
author = {Cybersecurity, Fear-based Versus Anxiety-based and Cheung-blunden, Violet and Cropper, Kiefer and Panis, Aleesa and Davis, Kamilah and Preferences, Fear-based Versus Anxiety-based Cybersecurity and Cheung-blunden, Violet and Cropper, Kiefer and Panis, Aleesa and Davis, Kamilah},
doi = {http://dx.doi.org/10.1037/emo0000508},
issn = {1528-3542},
journal = {Emotion (Washington, D.C.)},
keywords = {action tendencies,cybersecurity,general anxiety disorder,hypervigilance,security motivation},
month = {nov},
pmid = {30475029},
title = {{Emotion Functional Divergence of Two Threat-Induced Emotions :}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/30475029},
year = {2018}
}
@inproceedings{Layman2014b,
abstract = {While automated methods are the first line of defense for detecting attacks on webservers, a human agent is required to understand the attacker's intent and the attack process. The goal of this research is to understand the value of various log fields and the cognitive processes by which log information is grouped, searched, and correlated. Such knowledge will enable the development of human-focused log file investigation technologies. We performed controlled experiments with 65 subjects (IT professionals and novices) who investigated excerpts from six Webserver log files. Quantitative and qualitative data were gathered to: 1) analyze subject accuracy in identifying malicious activity; 2) identify the most useful pieces of log file information; and 3) understand the techniques and strategies used by subjects to process the information. Statistically significant effects were observed in the accuracy of identifying attacks and time taken depending on the type of attack. Systematic differences were also observed in the log fields used by high-performing and low-performing groups. The findings include: 1) new insights into how specific log data fields are used to effectively assess potentially malicious activity; 2) obfuscating factors in log data from a human cognitive perspective; and 3) practical implications for tools to support log file investigations. Copyright 2014 ACM.},
address = {Raleigh, NC},
author = {Layman, Lucas and Diffo, Sylvain David and Zazworka, Nico},
booktitle = {Proc. of the 2014 Symposium and Bootcamp on the Science of Security (HotSoS '14)},
doi = {10.1145/2600176.2600185},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Layman, Diffo, Zazworka - 2014 - Human Factors in Webserver Log File Analysis A Controlled Experiment on Investigating Malicious Activit.pdf:pdf},
isbn = {9781450329071},
keywords = {Human factors,Log files,Science of security,Security,mypubs,security},
mendeley-tags = {mypubs,security},
pages = {9:1--9:11},
title = {{Human Factors in Webserver Log File Analysis: A Controlled Experiment on Investigating Malicious Activity}},
year = {2014}
}
@article{Goodall2009,
abstract = {Purpose – The paper seeks to provide a foundational understanding of the socio‐technical system that is computer network intrusion detection, including the nature of the knowledge work, situated expertise, and processes of learning as supported by information technology.Design/methodology/approach – The authors conducted a field study to explore the work of computer network intrusion detection using multiple data collection methods, including semi‐structured interviews, examination of security tools and resources, analysis of information security mailing list posts, and attendance at several domain‐specific user group meetings.Findings – The work practice of intrusion detection analysts involves both domain expertise of networking and security and a high degree of situated expertise and problem‐solving activities that are not predefined and evolve with the dynamically changing context of the analyst's environment. This paper highlights the learning process needed to acquire these two types of knowledge, c...},
author = {Goodall, John R. and Lutters, Wayne G. and Komlodi, Anita},
doi = {10.1108/09593840910962186},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Goodall2009.pdf:pdf},
issn = {0959-3845},
journal = {Information Technology {\&} People},
keywords = {Computer networks,Data security,Working practices},
month = {jun},
number = {2},
pages = {92--108},
publisher = {Emerald Group Publishing Limited},
title = {{Developing expertise for network intrusion detection}},
url = {http://www.emeraldinsight.com/doi/10.1108/09593840910962186},
volume = {22},
year = {2009}
}
@inproceedings{Aggarwal2020,
abstract = {Deception, an art of making someone believe in something that is not true, may provide a promising real-time solution against cyber-attacks. In this paper, we propose a human-in-the-loop real-world simulation tool called HackIT, which could be configured to create different cyber-security scenarios involving deception. We discuss how researchers can use HackIT to create networks of different sizes; use deception and configure different webservers as honeypots; and, create any number of fictitious ports, services, fake operating systems, and fake files on honeypots. Next, we report a case-study involving HackIT where adversaries were tasked with stealing information from a simulated network over multiple rounds. In one condition in HackIT, deception occurred early; and, in the other condition, it occurred late. Results revealed that participants used different attack strategies across the two conditions. We discuss the potential of using HackIT in helping cyber-security teams understand adversarial cognition in the laboratory.},
address = {USA},
author = {Aggarwal, Palvi and Gautam, Aksh and Agarwal, Vaibhav and Gonzalez, Cleotilde and Dutt, Varun},
booktitle = {Advances in Human Factors in Cybersecurity},
editor = {Ahram, Tareq and Karwowski, Waldemar},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/Aggarwal2020{\_}Chapter{\_}HackITAHuman-in-the-LoopSimula.pdf:pdf},
isbn = {978-3-030-20488-4},
pages = {109--121},
publisher = {Springer International Publishing},
title = {{HackIT: A Human-in-the-Loop Simulation Tool for Realistic Cyber Deception Experiments}},
year = {2020}
}
@article{Mahoney2010,
abstract = {Cyber Network degradation and exploitation can covertly turn an organization's technological strength into an operational weakness. It has become increasingly imperative, therefore, for an organization's personnel to have an awareness of the state of the Cyber Network that they use to carry out their mission. Recent high-level government initiatives along with hacking and exploitation in the commercial realm highlight this need for general Cyber Situational Awareness (SA). While much of the attention in both the military and commercial cyber security communities is on abrupt and blunt attacks on the network, the most insidious cyber threat to organizations are subtle and persistent attacks leading to compromised databases, processing algorithms, and displays. We recently began an effort developing software tools to support the Cyber SA of users at varying levels of responsibility and expertise (i.e., not just the network administrators). This paper presents our approach and preliminary findings from a CTA...},
author = {Mahoney, Samuel and Roth, Emilie and Steinke, Kristin and Pfautz, Jonathan and Wu, Curt and Farry, Mike},
doi = {10.1177/154193121005400403},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Mahoney2010.pdf:pdf},
issn = {1541-9312},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
month = {sep},
number = {4},
pages = {279--283},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{A Cognitive Task Analysis for Cyber Situational Awareness}},
url = {http://journals.sagepub.com/doi/10.1177/154193121005400403},
volume = {54},
year = {2010}
}
@inproceedings{Patterson2020,
abstract = {A growing concern in the cybersecurity community evaluation of the strengths and defenses regarding cyberattacks. One approach that has not been often explored is to estimate the strength of an attack or defense in economic terms. For example, estimation of the memory required for code used for an attack, or what is equivalent, the computer time to execute an attack. We choose to express the costs in economic terms, and thus define the method of analyzing an important line of research known as ``behavioral economics'', pioneered by Kahneman and Tversky, and translated into cybersecurity terms. In this way we attempt to determine a cybersecurity analog for well-known results in economic prospect theory to be able to estimate the costs of cyberattacks and defenses.},
address = {USA},
annote = {Patterson and Gergely{\~{}}$\backslash$cite{\{}Patterson2020{\}} translated Kahneman and Tversky's{\~{}}$\backslash$cite{\{}Kahneman1979{\}} classical experiment on prospect theory to a cybersecurity context. Cyberattacker and cyberdefender perspective by student participants. They found little agreement between decisions made in cyber context and those from classical prospect theory, suggesting that perhaps the domain altered the valuation of probability or reward. Attacker vs. defender did not seem to matter as much. Since first 8 questions are pretty much the same, this more suggests that there is something fundamentally different about the subject populations in the study.},
author = {Patterson, Wayne and Gergely, Marton},
booktitle = {Advances in Human Factors in Cybersecurity},
editor = {Corradini, Isabella and Nardelli, Enrico and Ahram, Tareq},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/Patterson-Gergely2020{\_}Chapter{\_}EconomicProspectTheoryAppliedT.pdf:pdf},
isbn = {978-3-030-52581-1},
pages = {113--121},
publisher = {Springer International Publishing},
title = {{Economic Prospect Theory Applied to Cybersecurity}},
year = {2020}
}
@inproceedings{Om2012,
abstract = {In this paper, we propose a hybrid intrusion detection system that combines k-Means, and two classifiers: K-nearest neighbor and Naïve Bayes for anomaly detection. It consists of selecting features using an entropy based feature selection algorithm which selects the important attributes and removes the irredundant attributes. This algorithm operates on the KDD-99 Data set; this data set is used worldwide for evaluating the performance of different intrusion detection systems. The next step is clustering phase using k-Means. We have used the KDD99 (knowledge Discovery and Data Mining) intrusion detection contest. This system can detect the intrusions and further classify them into four categories: Denial of Service (DoS), U2R (User to Root), R2L (Remote to Local), and probe. The main goal is to reduce the false alarm rate of IDS1.},
address = {Dhanbad, India},
author = {Om, Hari and Kundu, Aritra},
booktitle = {2012 1st International Conference on Recent Advances in Information Technology, RAIT-2012},
doi = {10.1109/RAIT.2012.6194493},
isbn = {9781457706974},
keywords = {Classification,Clustering,KDD Cup 99 Data set,Na??ve Bayes,detection rate,false alarm rate,intrusion detection,k-Means},
month = {mar},
pages = {131--136},
publisher = {IEEE},
title = {{A hybrid system for reducing the false alarm rate of anomaly intrusion detection system}},
url = {http://ieeexplore.ieee.org/document/6194493/},
year = {2012}
}
@incollection{Wogalter2019,
address = {Boca Raton, FL},
author = {Wogalter, Michael S.},
booktitle = {Forensic Human Factors {\&} Ergonomics: Case Studies and Analyses},
chapter = {3},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wogalter - 2019 - Communication-Human Information Processing (C-HIP) Model.pdf:pdf},
pages = {33--49},
publisher = {CRC Press, Taylor and Francis Group},
title = {{Communication-Human Information Processing (C-HIP) Model}},
year = {2019}
}
@inproceedings{Ryan1997,
abstract = {With the rapid expansion of computer networks during the past few years, security has become a crucial issue for modern computer systems. A good way to detect illegitimate use is through monitoring unusual user activity. Methods of intrusion detection based on hand-coded rule sets or predicting commands on-line are laborous to build or not very reliable. This paper proposes a new way of applying neural networks to detect intrusions. We believe that a user leaves a 'print' when using the system; a neural network can be used to learn this print and identify each user much like detectives use thumbprints to place people at crime scenes. If a user's behavior does not match hislher print, the system administrator can be alerted of a possible security breech. A backpropagation neural network called NNID (Neural Network Intrusion Detector) was trained in the identification task and tested experimentally on a system of 10 users. The system was 96{\%} accurate in detecting unusual activity, with 7{\%} false alarm rate. These results suggest that learning user profiles is an effective way for detecting intrusions.},
address = {Providence, RI},
author = {Ryan, Jake and Lin, Meng-Jang and Miikkulainen, Risto},
booktitle = {AI Approaches to Fraud Detection and Risk Management: Papers from the 1997 AAAI Workshop},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ryan, Lin, Miikkulainen - 1997 - Intrusion Detection with Neural Networks.pdf:pdf},
pages = {72--79},
publisher = {AAAI Press},
title = {{Intrusion Detection with Neural Networks}},
url = {http://papers.nips.cc/paper/1459-intrusion-detection-with-neural-networks.pdf ryan:intrusion},
year = {1997}
}
@article{Ben-Asher2015,
abstract = {Ensuring cyber security is a complex task that relies on domain knowledge and requires cognitive abilities to determine possible threats from large amounts of network data. This study investigates how knowledge in network operations and information security influence the detection of intrusions in a simple network. We developed a simplified Intrusion Detection System (IDS), which allows us to examine how individuals with or without knowledge in cyber security detect malicious events and declare an attack based on a sequence of network events. Our results indicate that more knowledge in cyber security facilitated the correct detection of malicious events and decreased the false classification of benign events as malicious. However, knowledge had less contribution when judging whether a sequence of events representing a cyber-attack. While knowledge of cyber security helps in the detection of malicious events, situated knowledge regarding a specific network at hand is needed to make accurate detection decisions. Responses from participants that have knowledge in cyber security indicated that they were able to distinguish between different types of cyber-attacks, whereas novice participants were not sensitive to the attack types. We explain how these findings relate to cognitive processes and we discuss their implications for improving cyber security.},
author = {Ben-Asher, Noam and Gonzalez, Cleotilde},
doi = {10.1016/j.chb.2015.01.039},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ben-Asher, Gonzalez - 2015 - Effects of cyber security knowledge on attack detection.pdf:pdf},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Cyber security,Dynamic decision-making,Expertise,Intrusion-detection system,Knowledge},
month = {jul},
pages = {51--61},
pmid = {26467632},
publisher = {Pergamon},
title = {{Effects of cyber security knowledge on attack detection}},
url = {https://www.sciencedirect.com/science/article/pii/S0747563215000539},
volume = {48},
year = {2015}
}
@article{Molloy1996,
abstract = {The present study examined the effects of task complexity and time on task on the monitoring of a single automation failure during performance of a complex flight simulation task involving tracking, fuel management, and engine-status monitoring. Two groups of participants performed either all three flight simulation tasks simultaneously (multicomplex task) or the monitoring task alone (single-complex task); a third group performed a simple visual vigilance task (simple task). For the multicomplex task, monitoring for a single failure of automation control was poorer than when participants monitored engine malfunctions under manual control. Furthermore, more participants detected the automation failure in the first 10 min of a 30-min session than in the last 10 min of the session, for both the simple and the multicomplex task. Participants in the single-complex condition detected the automation failure equally well in both periods. The results support previous findings of inefficiency in monitoring automat...},
author = {Molloy, Robert and Parasuraman, Raja},
doi = {10.1177/001872089606380211},
issn = {0018-7208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
month = {jun},
number = {2},
pages = {311--322},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Monitoring an Automated System for a Single Failure: Vigilance and Task Complexity Effects}},
url = {http://journals.sagepub.com/doi/10.1177/001872089606380211},
volume = {38},
year = {1996}
}
@inproceedings{Wiczorek2014,
abstract = {Recent research has shown that the use of 3-stage likelihood alarm systems (LAS) has the potential to mitigate performance deficits associated with the use of binary alarm systems (BAS). The additional likelihood information can guide operators' behavior and improve their decision-making accuracy. Comparisons of LAS with different numbers of stages are missing so far. Therefore, the current study compared a BAS with a 3-stage LAS and a 4-stage LAS. Participants were found to make significantly fewer wrong decisions with the 4-stage LAS than with the other two systems, and still significantly fewer errors with the 3-stage LAS compared to the BAS. We found that this performance benefit resulted from a reduced number of false alarms, whereas no difference was found with regard to misses. Results are further discussed with regard to their theoretical implications for LAS and threshold setting in BAS.},
address = {Chicago, IL, USA},
author = {Wiczorek, Rebecca and Manzey, Dietrich and Zirk, Anna},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931214581078},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wiczorek, Manzey, Zirk - 2014 - Benefits of decision-support by likelihood versus binary alarm systems Does the number of stages make a.pdf:pdf},
isbn = {9780945289456},
issn = {10711813},
pages = {380--384},
publisher = {Human Factors an Ergonomics Society Inc.},
title = {{Benefits of decision-support by likelihood versus binary alarm systems: Does the number of stages make a difference?}},
volume = {2014-Janua},
year = {2014}
}
@article{Axelsson2000,
author = {Axelsson, Stefan},
doi = {10.1145/357830.357849},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Axelsson - 2000 - The Base-rate Fallacy and the Difficulty of Intrusion Detection(2).pdf:pdf},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {base-rate fallacy,detection rate,false alarm rate,intrusion detection},
month = {aug},
number = {3},
pages = {186--205},
publisher = {ACM},
title = {{The base-rate fallacy and the difficulty of intrusion detection}},
url = {http://portal.acm.org/citation.cfm?doid=357830.357849},
volume = {3},
year = {2000}
}
@incollection{Gerard2010,
address = {Maastricht, the Netherlands},
annote = {G{\'{e}}rard and Manzey (2010) followed the straight-forward hypothesis that double-checking behavior would directly depend on the level of uncertainty. They assumed that people would not check raw data if uncertainty is low, but make use of double-checking when uncertainty is high. In their study the level of uncertainty was manipulated by varying the PPV (positive predictive value) of an alarm within a range of .1 (10{\%} of alarms correct) to .9 (90{\%} of alarms correct) between groups. Note that the extreme PPVs – in this case .1 and .9 – cause the lowest levels of uncertainty while the medium PPV of .5 causes maximum uncertainty. Specifically, when a PPV=.1 alarm occurs it is 90{\%} certain, that there actually is no critical event. Hence there is a 10{\%} error risk if not double-checking such an alarm but ignoring it altogether. When a PPV=.9 alarm occurs, it is 90{\%} certain that there really is a critical event. Therefore there is a similar 10{\%} error risk if not double-checking such an alarm but directly following it. In case of a PPV=.5 alarm uncertainty is at a maximum of 50{\%} and so would be the error risk without double-checking. Accordingly, G{\'{e}}rard and Manzey assumed that the checking rate would relate to PPV in an inverted u-shape. Interestingly, participants' decision behavior at the same time supports and contradicts this uncertainty hypothesis. In accordance to the hypothesis participants double-checked most of the PPV=.5 alarms and rarely checked PPV=.9 alarms. In contrast to the assumptions however checking rates for the PPV=.1 alarm were as high as for the PPV=.5 alarm. So while participants most of the time accepted the 10{\%} uncertainty in the PPV=.9 case without looking at the raw data they did not accept the 10{\%} uncertainty in the PPV=.1 case. In the latter case they spend a high amount of effort in checking the raw data, thereby compromising their overall monetary outcome as supervising the alarm system was only part of their multi-task mission. It has to be noted that objective costs of false alarms and misses do not account for this asymmetry as the monetary payoff structure was symmetrical and both errors had the same costs.

One possibly explanation is that missing a true alarm has higher consequences.},
author = {G{\'{e}}rard, Nina and Manzey, Dietrich},
booktitle = {Human Factors: A system view of human, technology and organisation},
editor = {de Waard, Dick and Axelsson, Arne and Berglund, Martina and Peters, Bj{\"{o}}rn and Weikert, Clemens},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/G{\'{e}}rard, Manzey - 2010 - Are false alarms not as bad as supposed after all A study investigating operators' responses to imperfect alarms.pdf:pdf},
pages = {55--69},
publisher = {Shaker Publishing},
title = {{Are false alarms not as bad as supposed after all? A study investigating operators' responses to imperfect alarms}},
year = {2010}
}
@article{Hancock1989,
abstract = {This paper examines the effects of stress on sustained attention. With recognition of the task itself as the major source of cognitive stress, a dynamic model is presented that addresses the effects of stress on vigilance and, potentially, a wide variety of attention performance tasks.},
author = {Hancock, P. A. and Warm, J. S.},
doi = {10.1177/001872088903100503},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Hancock{\_}Warm{\_}1989.pdf:pdf},
issn = {00187208},
journal = {Human Factors},
number = {5},
pages = {519--537},
title = {{A dynamic model of stress and sustained attention}},
volume = {31},
year = {1989}
}
@inproceedings{Bliss2003,
abstract = {Response patterns to unreliable alarm systems often conform to probability matching theory. However, some participants have been observed to respond to all or no alarms in a given set. In this pape...},
address = {Maastricht, the Netherlands},
author = {Bliss, James P.},
booktitle = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
doi = {10.1177/154193120304701319},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bliss - 2003 - An Investigation of Extreme Alarm Response Patterns in Laboratory Experiments.pdf:pdf},
issn = {1541-9312},
month = {oct},
pages = {1683--1687},
publisher = {SAGE Publications},
title = {{An Investigation of Extreme Alarm Response Patterns in Laboratory Experiments}},
volume = {47},
year = {2003}
}
@article{Ceric2019,
abstract = {Purpose: The purpose of this paper is to explore the role of four cognitive biases, namely, selective perception, exposure to limited alternatives, adjustment and anchoring, and illusion of control in anticipating and responding to Distributed-Denial-of-Service (DDoS) attacks. Design/methodology/approach: The paper is based on exploratory case study research and secondary data on decision making in the Australian Bureau of Statistics (ABS) in regards to planning and managing DDoS attacks on Census day in 2016. Findings: Cognitive biases limited the ABS's awareness of the eCensus system's vulnerabilities, preparation for and management of DDoS attacks. Cyberattacks are on the increase, and managers should expect and be prepared to deal with them. Research limitations/implications: Due to the sensitivity of the topic, it was not possible to interview relevant stakeholders. Analysis is based on high-quality secondary data that includes comprehensive government reports investigating the events on Census day. Practical implications: Cyberattacks are inevitable and not an aberration. A checklist of actions is identified to help organisations avoid the failures revealed in the case study. Managers need to increase their awareness of cyberattacks, develop clear processes for dealing with them and increase the robustness of their decision-making processes relating to cybersecurity. Originality/value: This the authors believe that it is the first major study of the DDoS attacks on the Australian census. DDoS is a security reality of the twenty-first century and this case study illustrates the significance of cognitive biases and their impact on developing effective decisions and conducting regular risk assessments in managing cyberattacks.},
author = {Ceric, Arnela and Holland, Peter},
doi = {10.1108/ITP-11-2017-0390},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ceric, Holland - 2019 - The role of cognitive biases in anticipating and responding to cyberattacks.pdf:pdf},
issn = {09593845},
journal = {Information Technology and People},
keywords = {Absorptive capacity,Case study,Decision making,Judgement bias,Management practices,Security},
month = {feb},
number = {1},
pages = {171--188},
publisher = {Emerald Group Publishing Ltd.},
title = {{The role of cognitive biases in anticipating and responding to cyberattacks}},
volume = {32},
year = {2019}
}
@inproceedings{McNeese2012,
abstract = {The cyber security task is an intensely cognitive task that is embedded in a large multi-layered sociotechnical system of analysts, computers, and networks. Effective performance in this world is hampered by enormous size and complexity of the network data, the adaptive nature of intelligent adversaries, the lack of ground truth to assess performance, the high number of false alarms presented by automated alerting systems, by organizational stove pipes thwarting collaboration, and by technology that is thrown at the problem without an adequate understanding of the human needs. Further, the consequences of effective system performance in the cyber security domain are unparalleled because our world is so dependent on its cyber infrastructure. We have assembled a panel of six experts in cognitive engineering to provide perspectives on the cyber security problem and promising solutions. Copyright 2012 by Human Factors and Ergonomics Society, Inc. All rights reserved.},
author = {McNeese, Michael and Cooke, Nancy J. and D'Amico, Anita and Endsley, Mica R. and Gonzalez, Cleotilde and Roth, Emilie and Salas, Eduardo},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1071181312561063},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McNeese et al. - 2012 - Perspectives on the role of cognition in cyber security.pdf:pdf},
isbn = {9780945289418},
issn = {10711813},
month = {sep},
number = {1},
pages = {268--271},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Perspectives on the role of cognition in cyber security}},
url = {http://journals.sagepub.com/doi/10.1177/1071181312561063},
volume = {56},
year = {2012}
}
@inproceedings{Horvitz1999c,
address = {New York, New York, USA},
author = {Horvitz, Eric and Eric},
booktitle = {Proceedings of the SIGCHI conference on Human factors in computing systems (CHI '99)},
doi = {10.1145/302979.303030},
file = {::},
isbn = {0201485591},
keywords = {UI design,decision theory,direct manipulaton,intelligent agents,probability,user modeling},
pages = {159--166},
publisher = {ACM Press},
title = {{Principles of mixed-initiative user interfaces}},
url = {http://portal.acm.org/citation.cfm?doid=302979.303030},
year = {1999}
}
@inproceedings{Mullins2020,
abstract = {Cybersecurity operations are highly complex, requiring the coordination of specialized skills across multiple teams to successfully execute missions. Command and control within security operations centers is dominated by fragile mental models, demonstrating a need for systems that reinforce shared situational awareness across the organization. In this paper, we present the results of our research to: (1) define the needs associated with tactical cyber situational awareness; and (2) evaluate the usability and utility of a prototype tactical situational awareness dashboard. We found that incident tracking, tasking structure, execution timeline, and resource health constitute the essential aspects of tactical cyber situational awareness. Evaluations of prototypes suggest that three visualizations are well suited for conveying this information. We believe these results generalizable and will enable the development of tactical situational awareness capabilities in Security Operations Centers across public and private enterprises.},
address = {USA},
author = {Mullins, Ryan and Nargi, Ben and Fouse, Adam},
booktitle = {Advances in Human Factors in Cybersecurity},
editor = {Corradini, Isabella and Nardelli, Enrico and Ahram, Tareq},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/Mullins2020{\_}Chapter{\_}UnderstandingAndEnablingTactic.pdf:pdf},
isbn = {978-3-030-52581-1},
pages = {75--82},
publisher = {Springer International Publishing},
title = {{Understanding and Enabling Tactical Situational Awareness in a Security Operations Center}},
year = {2020}
}
@article{Kortschot2018,
abstract = {Objective:The authors seek to characterize the behavioral costs of attentional switches between points in a network map and assess the efficacy of interventions intended to reduce those costs.Background:Cybersecurity network operators are tasked with determining an appropriate attentional allocation scheme given the state of the network, which requires repeated attentional switches. These attentional switches may result in temporal performance decrements, during which operators disengage from one attentional fixation point and engage with another.Method:We ran two experiments where participants identified a chain of malicious emails within a network. All interactions with the system were logged and analyzed to determine if users experienced disengagement and engagement delays.Results:Both experiments revealed significant costs from attentional switches before (i.e., disengagement) and after (i.e., engagement) participants navigated to a new area in the network. In our second experiment, we found that inte...},
author = {Kortschot, Sean W. and Sovilj, Dusan and Jamieson, Greg A. and Sanner, Scott and Carrasco, Chelsea and Soh, Harold},
doi = {10.1177/0018720818784107},
file = {:C$\backslash$:/Users/laymanl/Downloads/kortschot{\_}HF.pdf:pdf},
issn = {0018-7208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
keywords = {adaptive automation,attentional processes,cybersecurity,interface evaluation,visual search},
month = {nov},
number = {7},
pages = {962--977},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Measuring and Mitigating the Costs of Attentional Switches in Active Network Monitoring for Cybersecurity}},
url = {http://journals.sagepub.com/doi/10.1177/0018720818784107},
volume = {60},
year = {2018}
}
@article{Kusumastuti2019,
abstract = {This study examines how exploiting biases in probability judgment can enhance deterrence using a fixed allocation of defensive resources. We investigate attacker anchoring heuristics for conjunctive events with missing information to distort attacker estimates of success for targets with equal defensive resources. We designed and conducted a behavioral experiment functioning as an analog cyber attack with multiple targets requiring three stages of attack to successfully acquire a target. Each stage is associated with a probability of successfully attacking a layer of defense, reflecting the allocation of resources for each layer. There are four types of targets that have nearly equal likelihood of being successfully attacked, including one type with equally distributed success probabilities over every layer and three types with success probabilities that are concentrated to be lowest in the first, second, or third layer. Players are incentivized by a payoff system that offers a reward for successfully attacked targets and a penalty for failed attacks. We collected data from a total of 1,600 separate target selections from 80 players and discovered that the target type with the lowest probability of success on the first layer was least preferred among attackers, providing the greatest deterrent. Targets with equally distributed success probabilities across layers were the next least preferred among attackers, indicating greater deterrence for uniform-layered defenses compared to defenses that are concentrated at the inner (second or third) levels. This finding is consistent with both attacker anchoring and ambiguity biases and an interpretation of failed attacks as near misses.},
author = {Kusumastuti, Sarah A. and Blythe, Jim and Rosoff, Heather and John, Richard S.},
doi = {10.1111/risa.13402},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kusumastuti et al. - 2019 - Behavioral Determinants of Target Shifting and Deterrence in an Analog Cyber-Attack Game.pdf:pdf},
issn = {15396924},
journal = {Risk Analysis},
keywords = {Ambiguity bias,anchoring,cybersecurity,deterrence,near miss},
month = {mar},
number = {3},
pages = {476--493},
publisher = {Blackwell Publishing Inc.},
title = {{Behavioral Determinants of Target Shifting and Deterrence in an Analog Cyber-Attack Game}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.13402},
volume = {40},
year = {2019}
}
@article{Ulvila2004,
abstract = {This paper presents a decision analysis method for evaluating computer intrusion detection systems. The method integrates and extends receiver operating characteristic (ROC) and cost analysis methods to provide an expected cost metric. We demonstrate that both the ROC analysis and cost analysis methods are incomplete. Furthermore, we demonstrate how a decision tree can combine and extend the ROC and cost analysis methods to provide an expected cost metric that reflects the intrusion detection system's ROC curve, costs, and assessments of the hostility of the environment as summarized by the prior probability of intrusion. We further demonstrate how this method can be used to decide the optimal operating point on an intrusion detector's ROC curve, choose the best intrusion detection system, compare the value of one intrusion detection system with another's, determine the value of an intrusion detector over no detector, and determine how to adjust the operation of an intrusion detector to respond to changes in its environment. General results are given and the method is illustrated in several numerical examples that involve both hypothetical and real intrusion detection systems. We demonstrate that, contrary to common advice, the value of an intrusion detection system depends not only on its ROC curve, but also on various costs (such as those associated with making incorrect decisions about detection) and the hostility of the operating environment. Conclusions are drawn about the design and evaluation of intrusion detection systems and the role for decision analysis in that design and evaluation.},
author = {Ulvila, Jacob W. and Gaffney, John E.},
doi = {10.1287/deca.1030.0001},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ulvila, Gaffney - 2004 - A Decision Analysis Method for Evaluating Computer Intrusion Detection Systems.pdf:pdf},
issn = {1545-8490},
journal = {Decision Analysis},
month = {mar},
number = {1},
pages = {35--50},
publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
title = {{A Decision Analysis Method for Evaluating Computer Intrusion Detection Systems}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/deca.1030.0001},
volume = {1},
year = {2004}
}
@incollection{Paul2013,
abstract = {This paper offers insights to how cyber security analysts establish and maintain situation awareness of a large computer network. Through a series of interviews, observations, and a card sorting activity, we examined the ques-tions analysts asked themselves during a network event. We present the results of our work as a taxonomy of cyber awareness questions that represents a mental model of situation awareness in cyber security analysts. 1 Introduction This paper presents a taxonomy of cyber awareness questions derived from a series of user-centered research activities that can be used to inform the design and development of cyber situation awareness technology. One of the most important responsibilities of a cyber security analyst is to watch over and protect his network from harm. Maintaining situation awareness of the wide variety of events that occur and massive amounts of data generated is one of many analytic challenges. Situation awareness technology aims to reduce the data overload burden placed on the analyst. Good situation awareness technology requires good design, and good design requires a good understanding of the user and a focus on the user during the design process. In the case of a cyber security analyst, the practice of good user-centered design is focused on his security-related work processes on a large computer network. One way of understanding how a cyber security analyst accomplishes situation awareness on a large computer network is to study the questions he may ask himself during the course of a network event. Studying the relationships between these questions will lead to a better understanding of the analysts' mental model of cyber situation awareness. A mental model of cyber situation awareness is a valuable tool in the user-centered design of cyber-related technology and to researchers who cannot always study cyber security analysts in the field.},
author = {Paul, Celeste Lyn and Whitley, Kirsten},
booktitle = {Human Aspects of Information Security, Privacy, and Trust},
doi = {10.1007/978-3-642-39345-7_16},
keywords = {Computer security,computer security,situation awareness,user-centered design},
pages = {145--154},
title = {{A Taxonomy of Cyber Awareness Questions for the User-Centered Design of Cyber Situation Awareness}},
url = {http://link.springer.com/10.1007/978-3-642-39345-7{\_}16 https://pdfs.semanticscholar.org/0ba6/8b5f0ef35ef94fa238275e2f571bce446538.pdf},
volume = {LNCS 8030},
year = {2013}
}
@article{Farris2018,
abstract = {Vulnerability remediation is a critical task in operational software and network security management. In this article, an effective vulnerability management strategy, called VULCON (VULnerability CONtrol), is developed and evaluated. The strategy is based on two fundamental performance metrics: (1) time-to-vulnerability remediation (TVR) and (2) total vulnerability exposure (TVE). VULCON takes as input real vulnerability scan reports, metadata about the discovered vulnerabilities, asset criticality, and personnel resources. VULCON uses a mixed-integer multiobjective optimization algorithm to prioritize vulnerabilities for patching, such that the above performance metrics are optimized subject to the given resource constraints. VULCON has been tested on multiple months of real scan data from a cyber-security operations center (CSOC). Results indicate an overall TVE reduction of 8.97{\%} when VULCON optimizes a realistic security analyst workforce's effort. Additionally, VULCON demonstrates that it can determine monthly resources required to maintain a target TVE score. As such, VULCON provides valuable operational guidance for improving vulnerability response processes in CSOCs.},
author = {Farris, Katheryn A. and Shah, Ankit and Cybenko, George and Ganesan, Rajesh and Jajodia, Sushil},
doi = {10.1145/3196884},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Farris et al. - 2018 - VULCON A system for vulnerability prioritization, mitigation, and management.pdf:pdf},
issn = {24712574},
journal = {ACM Transactions on Privacy and Security},
keywords = {Cyber-security analysts,Cyber-security operations center (CSOC),Management,Multiobjective optimization,Structured vulnerability response programs,Vulnerability triage},
month = {jul},
number = {4},
pages = {Article 16},
publisher = {Association for Computing Machinery},
title = {{VULCON: A system for vulnerability prioritization, mitigation, and management}},
url = {https://dl.acm.org/doi/10.1145/3196884},
volume = {21},
year = {2018}
}
@techreport{Bace2001,
author = {Bace, Rebecca and Mell, Peter},
file = {::},
institution = {National Institute of Standards and Technology},
title = {{NIST Special Publication on Intrusion Detection Systems}},
url = {http://mega-internet.mobigator.com:88/reports/sp800-31.pdf http://www.dtic.mil/cgi-bin/GetTRDoc?Location=U2{\&}doc=GetTRDoc.pdf{\&}AD=ADA393326},
year = {2001}
}
@phdthesis{Roden2019,
address = {Wilmington, NC},
author = {Roden, William T.},
school = {University of North Carolina, Wilmington},
title = {{An Empirical Study of Factors Impacting Cyber Security Analyst Performance in the Use of Intrusion Detection Systems}},
year = {2019}
}
@book{Jahankhani2020,
address = {Cham, Switzerland},
author = {Jahankhani, Hamid and Kendzierskyj, Stefan and Chelvachandran, Nishan and Ibarra, Jaime},
doi = {10.1007/978-3-030-35746-7},
editor = {Jahankhani, Hamid and Kendzierskyj, Stefan and Chelvachandran, Nishan and Ibarra, Jaime},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/2020{\_}Book{\_}CyberDefenceInTheAgeOfAISmartS.pdf:pdf},
isbn = {9783030357450},
publisher = {Springer International Publishing},
series = {Advanced Sciences and Technologies for Security Applications},
title = {{Cyber Defence in the Age of AI , Smart Societies and Augmented Humanity}},
url = {http://link.springer.com/10.1007/978-3-030-35746-7},
year = {2020}
}
@book{Crocker1986,
address = {Belmont, CA, USA},
author = {Crocker, Linda and Algina, James},
publisher = {Wadsworth/Thomson Learning},
title = {{Introduction to Classical and Modern Test Theory}},
year = {1986}
}
@article{Mills1940,
author = {Mills, C. Wright},
journal = {American Sociological Review1},
number = {6},
pages = {904--913},
title = {{Situated Actions and Vocabularies of Motive}},
volume = {5},
year = {1940}
}
@inproceedings{Dutt2011,
abstract = {In a corporate network, the situation awareness (SA) of a security analyst is of particular interest. A security analyst is in charge of observing the online operations of a corporate network (e.g., an online retail company with an external webserver and an internal fileserver) from threats of random or organized cyber-attacks. The current work describes a cognitive Instance-based Learning (IBL) model of the recognition and comprehension processes of a security analyst in a simple cyber-attack scenario. The IBL model first recognizes cyber-events (e.g., execution of a file on a server) in the network based upon events' situation attributes and the similarity of events' attributes to past experiences (instances) stored in analyst's memory. Then, the model reasons about a sequence of observed events being a cyber-attack or not, based upon instances retrieved from memory and the risk-tolerance of a simulated analyst. The execution of the IBL model generates predictions of the recognition and comprehension processes of security analyst in a cyber-attack. An analyst's decisions are evaluated in the model based upon two cyber SA metrics of accuracy and timeliness of analyst's decision actions. Future work in this area will focus on collecting human data to validate the predictions made by the model. {\textcopyright} 2011 IFIP International Federation for Information Processing.},
address = {Richmond, VA},
author = {Dutt, Varun and Ahn, Young Suk and Gonzalez, Cleotilde},
booktitle = {Data and Applications Security and Privacy XXV},
doi = {10.1007/978-3-642-22348-8_24},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dutt, Ahn, Gonzalez - 2011 - Cyber situation awareness Modeling the security analyst in a cyber-attack scenario through instance-based l.pdf:pdf},
isbn = {9783642223471},
issn = {03029743},
keywords = {cyber-attack,cyber-situation awareness,dynamic decision-making,instance-based learning theory,intrusion-detection system,security analyst,threat event},
pages = {280--292},
publisher = {Springer, Berlin, Heidelberg},
title = {{Cyber situation awareness: Modeling the security analyst in a cyber-attack scenario through instance-based learning}},
url = {https://link.springer.com/chapter/10.1007/978-3-642-22348-8{\_}24},
volume = {6818 LNCS},
year = {2011}
}
@article{Denning1987b,
abstract = {A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system's audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.},
author = {Denning, D.E. Dorothy E},
doi = {10.1109/TSE.1987.232894},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Denning - 1987 - An Intrusion-Detection Model.pdf:pdf},
issn = {0098-5589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Abnormal behavior,Computerized monitoring,Contracts,Environmental economics,Expert systems,Index Terms-Abnormal behavior,Invasive software,Joining processes,Object detection,Operating systems,Real time systems,Security,auditing,intrusions,monitoring,profiles,security,statistical measures},
month = {feb},
number = {2},
pages = {222--232},
shorttitle = {Software Engineering, IEEE Transactions on},
title = {{An Intrusion-Detection Model}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1702202 https://apps.dtic.mil/dtic/tr/fulltext/u2/a484998.pdf},
volume = {13},
year = {1987}
}
@article{Sawyer2018,
abstract = {Objective:This work assesses the efficacy of the “prevalence effect” as a form of cyberattack in human-automation teaming, using an email task.Background:Under the prevalence effect, rare signals are more difficult to detect, even when taking into account their proportionally low occurrence. This decline represents diminished human capability to both detect and respond. As signal probability (SP) approaches zero, accuracy exhibits logarithmic decay. Cybersecurity, a context in which the environment is entirely artificial, provides an opportunity to manufacture conditions enhancing or degrading human performance, such as prevalence effects. Email cybersecurity prevalence effects have not previously been demonstrated, nor intentionally manipulated.Method:The Email Testbed (ET) provides a simulation of a clerical email work involving messages containing sensitive personal information. Using the ET, participants were presented with 300 email interactions and received cyberattacks at rates of either 1{\%}, 5{\%}, or...},
author = {Sawyer, Ben D. and Hancock, Peter A.},
doi = {10.1177/0018720818780472},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawyer, Hancock - 2018 - Hacking the Human The Prevalence Paradox in Cybersecurity.pdf:pdf},
journal = {Human Factors},
keywords = {antimalware,antivirus,design,human-computer interaction,information security,internet,malware,messages,risk,signal detection,vigilance,virus},
month = {aug},
number = {5},
pages = {597--609},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Hacking the Human: The Prevalence Paradox in Cybersecurity}},
url = {http://journals.sagepub.com/doi/10.1177/0018720818780472},
volume = {60},
year = {2018}
}
@book{Jajodia2010,
address = {Boston, MA},
author = {Jajodia, Sushil and Liu, Peng and Swarup, Vipin and Wang, Cliff},
booktitle = {Advances in Information Security},
doi = {10.1007/978-1-4419-0140-8},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/2010{\_}Book{\_}CyberSituationalAwareness.pdf:pdf},
isbn = {9781441901392},
issn = {15682633},
publisher = {Springer US},
title = {{Cyber situational awareness: Issues and research}},
volume = {46},
year = {2010}
}
@article{McElroy2007,
abstract = {Previous research on anchoring has shown this heuristic to be a very robust psychological phenomenon ubiquitous across many domains of human judgment and decision-making. Despite the prevalence of anchoring effects, researchers have only recently begun to investigate the underlying factors responsible for how and in what ways a person is susceptible to them. This paper examines how one such factor, the Big-Five personality trait of openness-to-experience, influences the effect of previously presented anchors on participants' judgments. Our findings indicate that participants high in openness-to-experience were significantly more influenced by anchoring cues relative to participants low in this trait. These findings were consistent across two different types of anchoring tasks providing convergent evidence for our hypothesis. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
address = {McElroy, Todd: Department of Psychology, College of Arts and Sciences, P.O. Box 32109, 265 College St., Boone, NC, US, 28608, mcelroygt@appstate.edu},
author = {McElroy, Todd and Dowd, Keith},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McElroy, Dowd - 2007 - Susceptibility to anchoring effects How openness-to-experience influences responses to anchoring cues.pdf:pdf},
issn = {1930-2975(Print)},
journal = {Judgment and Decision Making},
keywords = {*Cues,*Decision Making,*Heuristics,*Judgment,*Openness to Experience,Individual Differences},
number = {1},
pages = {48--53},
publisher = {Society for Judgment and Decision Making},
title = {{Susceptibility to anchoring effects: How openness-to-experience influences responses to anchoring cues.}},
volume = {2},
year = {2007}
}
@article{Bliss1995a,
author = {Bliss, James P. and Gilson, Richard D. and Deaton, John E.},
doi = {10.1080/00140139508925269},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Bliss95.pdf:pdf},
issn = {0014-0139},
journal = {Ergonomics},
month = {nov},
number = {11},
pages = {2300--2312},
title = {{Human probability matching behaviour in response to alarms of varying reliability}},
url = {http://www.tandfonline.com/doi/abs/10.1080/00140139508925269},
volume = {38},
year = {1995}
}
@article{Lee2000,
abstract = {Intrusion detection (ID) is an important component of infrastructure protection mechanisms. Intrusion detection systems (IDSs) need to be accurate, adaptive, and extensible. Given these requirements and the complexities of today's network environments, we need a more systematic and automated IDS development process rather than the pure knowledge encoding and engineering approaches. This article describes a novel framework, MADAM ID, for Mining Audit Data for Automated Models for Intrusion Detection. This framework uses data mining algorithms to compute activity patterns from system audit data and extracts predictive features from the patterns. It then applies machine learning algorithms to the audit records that are processed according to the feature definitions to generate intrusion detection rules. Results from the 1998 DARPA Intrusion Detection Evaluation showed that our ID model was one of the best performing of all the participating systems. We also briefly discuss our experience in converting the detection models produced by off-line data mining programs to real-time modules of existing IDSs. {\textcopyright} 2000, ACM. All rights reserved.},
author = {Lee, Wenke and Stolfo, Salvatore J.},
doi = {10.1145/382912.382914},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Stolfo - 2000 - A Framework for Constructing Features and Models for Intrusion Detection Systems.pdf:pdf},
issn = {15577406},
journal = {ACM Transactions on Information and System Security},
keywords = {Data mining,Design,Experimentation,Security,feature construction,intrusion detection},
month = {nov},
number = {4},
pages = {227--261},
title = {{A Framework for Constructing Features and Models for Intrusion Detection Systems}},
url = {https://dl.acm.org/doi/10.1145/382912.382914},
volume = {3},
year = {2000}
}
@article{Wogalter1987,
abstract = {The purpose of the present work was to identify some of the factors that influence effectiveness of warnings. Two laboratory experiments designed to examine behavioral effectiveness indicated that ...},
author = {Wogalter, Michael S. and Godfrey, Sandra S. and Fontenelle, Gail A. and Desaulniers, David R. and Rothstein, Pamela R. and Laughery, Kenneth R.},
doi = {10.1177/001872088702900509},
issn = {00187208},
journal = {Human Factors},
month = {oct},
number = {5},
pages = {599--612},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Effectiveness of warnings}},
url = {http://journals.sagepub.com/doi/10.1177/001872088702900509},
volume = {29},
year = {1987}
}
@incollection{Clancey1997,
address = {Menlo Park},
author = {Clancey, William J.},
booktitle = {Human and Machine: Expertise in Context},
pages = {247--291},
publisher = {The AAAI Press},
title = {{The Conceptual Nature of Knowledge, Situations, and Activity}},
year = {1997}
}
@book{Davies1982,
address = {London},
author = {Davies, D.R. and Parasuraman, R.},
publisher = {Academic Press},
title = {{The psychology of vigilance}},
year = {1982}
}
@inproceedings{dutt2012modeling,
address = {Berlin, Germany},
author = {Dutt, Varun and Ahn, Young-Suk and Ben-Asher, Noam and Gonzalez, Cleotilde},
booktitle = {Proceedings of the 11th International Conference on Cognitive Modeling (ICCM 2012)},
file = {::},
pages = {88--93},
publisher = {Universit{\"{a}}tsverlag der TU, Berlin},
title = {{Modeling the effects of base-rates on cyber threat detection performance}},
year = {2012}
}
@article{Bustamante2007,
abstract = {The purpose of this research was to investigate the effects of varying the threshold of alarm systems and workload on human response to alarm signals and performance on a complex task. A receiver operating characteristic (ROC) curve was selected to reflect the sensitivity of the alarm system. The threshold of the alarm system was manipulated by changing the value of beta along the ROC curve. A total of 84 students participated in experiment 1 and 48 students participated in experiment 2. Participants performed a compensatory-tracking, a resource management and a monitoring task. As expected, results showed that participants responded significantly faster to true alarm signals when they were using the system with the highest threshold under low-workload conditions. Results also indicated that changing the threshold of the alarm system had a significant effect on overall performance and this effect was greater under high-workload conditions. However, contrary to expectations, the highest level of performance was achieved by setting the threshold at a low level. Results from both experiments revealed that the advantage of faster alarm reaction time as a result of increasing the system's threshold was lost because of its increased probability of missed events.},
author = {Bustamante, E. A. and Bliss, J. P. and Anderson, B. L.},
doi = {10.1080/00140130701237345},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bustamante, Bliss, Anderson - 2007 - Effects of varying the threshold of alarm systems and workload on human performance.pdf:pdf},
issn = {00140139},
journal = {Ergonomics},
keywords = {False-Alarm-Prone and Miss-Prone Automation},
number = {7},
pages = {1127--1147},
pmid = {17510826},
title = {{Effects of varying the threshold of alarm systems and workload on human performance}},
volume = {50},
year = {2007}
}
@article{Parasuraman1997,
abstract = {This paper addresses theoretical, empirical, and analytical studies pertaining to human use, misuse, disuse, and abuse of automation technology. Use refers to the voluntary activation or disengagement of automation by human operators. Trust, mental workload, and risk can influence automation use, but interactions between factors and large individual differences make prediction of automation use difficult. Misuse refers to over reliance on automation, which can result in failures of monitoring or decision biases. Factors affecting the monitoring of automation include workload, automation reliability and consistency, and the saliency of automation state indicators. Disuse, or the neglect or underutilization of automation, is commonly caused by alarms that activate falsely. This often occurs because the base rate of the condition to be detected is not considered in setting the trade-off between false alarms and omissions. Automation abuse, or the automation of functions by designers and implementation by man...},
author = {Parasuraman, Raja and Riley, Victor},
doi = {10.1518/001872097778543886},
issn = {0018-7208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
month = {jun},
number = {2},
pages = {230--253},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Humans and Automation: Use, Misuse, Disuse, Abuse}},
url = {http://journals.sagepub.com/doi/10.1518/001872097778543886},
volume = {39},
year = {1997}
}
@inproceedings{Lippmann2000,
address = {Hilton Head, SC},
author = {Lippmann, Richard P and Fried, David J and Graf, Isaac and Haines, Joshua W and Kendall, Kristopher R and Mcclung, David and Weber, Dan and Webster, Seth E and Wyschogrod, Dan and Cunningham, Robert K and Zissman, Marc A},
booktitle = {Proceedings DARPA Information Survivability Conference and Exposition. DISCEX'00},
doi = {10.1109/DISCEX.2000.821506},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Lippmann2000.pdf:pdf;:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Webster et al. - 2000 - Evaluating intrusion detection systems the 1998 DARPA off-line intrusion detection evaluation.pdf:pdf},
isbn = {0-7695-0490-6},
pages = {12--26},
publisher = {IEEE Comput. Soc},
title = {{Evaluating intrusion detection systems: the 1998 DARPA off-line intrusion detection evaluation}},
url = {https://www.researchgate.net/publication/3837677 http://ieeexplore.ieee.org/document/821506/},
year = {2000}
}
@article{Berghel2017,
author = {Berghel, Hal},
doi = {10.1109/MC.2017.4451227},
issn = {0018-9162},
journal = {Computer},
month = {dec},
number = {12},
pages = {72--76},
title = {{Equifax and the Latest Round of Identity Theft Roulette}},
url = {http://ieeexplore.ieee.org/document/8220474/},
volume = {50},
year = {2017}
}
@article{Garcia-Teodoro2009,
abstract = {The Internet and computer networks are exposed to an increasing number of security threats. With new types of attacks appearing continually, developing flexible and adaptive security oriented approaches is a severe challenge. In this context, anomaly-based network intrusion detection techniques are a valuable technology to protect target systems and networks against malicious activities. However, despite the variety of such methods described in the literature in recent years, security tools incorporating anomaly detection functionalities are just starting to appear, and several important problems remain to be solved. This paper begins with a review of the most well-known anomaly-based intrusion detection techniques. Then, available platforms, systems under development and research projects in the area are presented. Finally, we outline the main challenges to be dealt with for the wide scale deployment of anomaly-based intrusion detectors, with special emphasis on assessment issues.},
author = {Garc{\'{i}}a-Teodoro, P. and D{\'{i}}az-Verdejo, J. and Maci{\'{a}}-Fern{\'{a}}ndez, G. and V{\'{a}}zquez, E.},
doi = {10.1016/J.COSE.2008.08.003},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garc{\'{i}}a-Teodoro et al. - 2009 - Anomaly-based network intrusion detection Techniques, systems and challenges.pdf:pdf},
issn = {0167-4048},
journal = {Computers {\&} Security},
month = {feb},
number = {1-2},
pages = {18--28},
publisher = {Elsevier Advanced Technology},
title = {{Anomaly-based network intrusion detection: Techniques, systems and challenges}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404808000692},
volume = {28},
year = {2009}
}
@inproceedings{ChandranSundaramurthy2015,
abstract = {Security Operation Centers (SOCs) are being operated by universities, government agencies, and corporations to defend their enterprise networks in general and in particular to identify malicious behaviors in both networks and hosts. The success of a SOC depends on having the right tools, processes and, most importantly, efficient and effective analysts. One of the worrying issues in recent times has been the consistently high burnout rates of security analysts in SOCs. Burnout results in analysts making poor judgments when analyzing security events as well as frequent personnel turnovers. In spite of high awareness of this problem, little has been known so far about the factors leading to burnout. Various coping strategies employed by SOC management such as career progression do not seem to address the problem but rather deal only with the symptoms. In short, burnout is a manifestation of one or more underlying issues in SOCs that are as of yet unknown. In this work we performed an anthropological study of a corporate SOC over a period of six months and identified concrete factors contributing to the burnout phenomenon. We use Grounded Theory to analyze our fieldwork data and propose a model that explains the burnout phenomenon. Our model indicates that burnout is a human capital management problem resulting from the cyclic interaction of a number of human, technical, and managerial factors. Specifically, we identified multiple vicious cycles connecting the factors affecting the morale of the analysts. In this paper we provide detailed descriptions of the various vicious cycles and suggest ways to turn these cycles into virtuous ones. We further validated our results on the fieldnotes from a SOC at a higher education institution. The proposed model is able to successfully capture and explain the burnout symptoms in this other SOC as well.},
address = {Ottawa, Canada},
author = {{Chandran Sundaramurthy}, Sathya and Bardas, Alexandru G and Case, Jacob and Ou, Xinming and Wesch, Michael and McHugh, John and {Raj Rajagopalan}, S},
booktitle = {Eleventh Symposium On Usable Privacy and Security (SOUPS 2015)},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandran Sundaramurthy et al. - 2015 - A Human Capital Model for Mitigating Security Analyst Burnout.pdf:pdf},
pages = {347--359},
publisher = {The USENIX Association},
title = {{A Human Capital Model for Mitigating Security Analyst Burnout}},
url = {https://www.usenix.org/system/files/conference/soups2015/soups15-paper-sundaramurthy.pdf},
year = {2015}
}
@book{Rasmussen1986,
abstract = {Includes indexes.},
address = {New York},
author = {Rasmussen, Jens},
isbn = {0444009876},
pages = {215},
publisher = {Elsevier Science Ltd},
title = {{Information processing and human-machine interaction : an approach to cognitive engineering}},
url = {https://dl.acm.org/citation.cfm?id=536144},
year = {1986}
}
@article{Woods1995,
abstract = {This paper uses results of field studies from multiple domains to explore the cognitive activities involved in dynamic fault management. Fault diagnosis has a different character in dynamic fault management situations as compared to troubleshooting a broken device that has been removed from service. In fault management there is some underlying process (an engineered or physiological process that will be referred to as the monitored process) whose state changes over time. Faults disturb the monitored process and diagnosis goes on in parallel with responses to maintain process integrity and to correct the underlying problem. These situations frequently involve time pressure, multiple interacting goals, high consequences of failure, and multiple interleaved tasks. Typical examples of fields of practice where dynamic fault management occurs include flight deck operations in commercial aviation, control of space systems, anaesthetic management under surgery, and terrestrial process control. The point of departure is the ‘alarm problem', which is used to introduce an attentional view of alarm systems as tools for supporting dynamic fault management. The work is based on the concept of directed attention—a cognitive function that inherently involves the co-ordination of multiple agents through the use of external media. Directed attention suggests several techniques for developing more effective alarm systems. {\textcopyright} 1995 Taylor and Francis Group, LLC.},
author = {Woods, David D.},
doi = {10.1080/00140139508925274},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Woods - 1995 - The alarm problem and directed attention in dynamic fault management.pdf:pdf},
issn = {13665847},
journal = {Ergonomics},
keywords = {Alarms,Cognitive systems,Directed attention,Fault diagnosis,Joint reference},
number = {11},
pages = {2371--2393},
publisher = {Taylor {\&} Francis Group},
title = {{The alarm problem and directed attention in dynamic fault management}},
volume = {38},
year = {1995}
}
@article{LalondeLevesque2018,
abstract = {The success (or failure) of malware attacks depends upon both technological and human factors. The most security-conscious users are susceptible to unknown vulnerabilities, and even the best security mechanisms can be circumvented as a result of user actions. Although there has been significant research on the technical aspects of malware attacks and defence, there has been much less research on how users interact with both malware and current malware defences. This article describes a field study designed to examine the interactions between users, antivirus (AV) software, and malware as they occur on deployed systems. In a fashion similar to medical studies that evaluate the efficacy of a particular treatment, our experiment aimed to assess the performance of AV software and the human risk factors of malware attacks. The 4-month study involved 50 home users who agreed to use laptops that were instrumented to monitor for possible malware attacks and gather data on user behaviour. This study provided some very interesting, non-intuitive insights into the efficacy of AV software and human risk factors. AV performance was found to be lower under real-life conditions compared to tests conducted in controlled conditions. Moreover, computer expertise, volume of network usage, and peer-to-peer activity were found to be significant correlates of malware attacks. We assert that this work shows the viability and the merits of evaluating security products, techniques, and strategies to protect systems through long-term field studies with greater ecological validity than can be achieved through other means.},
author = {{Lalonde L{\'{e}}vesque}, Fanny and Chiasson, Sonia and Somayaji, Anil and Fernandez, Jos{\'{e}} M.},
doi = {10.1145/3210311},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lalonde L{\'{e}}vesque et al. - 2018 - Technological and human factors of malware attacks A computer security clinical trial approach.pdf:pdf},
issn = {24712574},
journal = {ACM Transactions on Privacy and Security},
keywords = {Antivirus,Clinical trial,Computer security,Malware,Risk factors},
month = {jul},
number = {4},
pages = {Article 18},
publisher = {Association for Computing Machinery},
title = {{Technological and human factors of malware attacks: A computer security clinical trial approach}},
url = {https://dl.acm.org/doi/10.1145/3210311},
volume = {21},
year = {2018}
}
@article{Wogalter2002,
abstract = {During the past two decades, the body of empirical research on warning design and evaluation has grown. Consequently, there are now basic principles and guidelines addressing warning design (e.g., signal words, color, symbols, and text/content), placement (e.g., location within product instructions), and how to enhance the usability of designs by considering factors internal to the user (e.g., beliefs, perceptions of risk, stress). Similarly, evaluation methods have been developed that can be used to measure the effectiveness of warnings such as the degree to which warnings are communicated to recipients and the degree to which they encourage or influence behavioral compliance. An overview of the empirical literature on warning guidelines and evaluation approaches is provided. Researchers, practitioners, and manufacturers can use these guidelines in various contexts to reduce the likelihood that injury and product damage from exposure to a hazard will occur. {\textcopyright} 2002 Published by Elsevier Science Ltd.},
author = {Wogalter, Michael S. and Conzola, Vincent C. and Smith-Jackson, Tonya L.},
doi = {10.1016/S0003-6870(02)00009-1},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wogalter, Conzola, Smith-Jackson - 2002 - Research-based guidelines for warning design and evaluation.pdf:pdf},
issn = {00036870},
journal = {Applied Ergonomics},
keywords = {Communication,Design,Label,Risk,Safety,Sign,Warning},
month = {may},
number = {3},
pages = {219--230},
pmid = {12164506},
publisher = {Elsevier Ltd},
title = {{Research-based guidelines for warning design and evaluation}},
volume = {33},
year = {2002}
}
@inproceedings{Gutzwiller2016a,
address = {San Diego, CA},
author = {Gutzwiller, Robert S. and Hunt, Sarah M. and Lange, Douglas S.},
booktitle = {2016 IEEE International Multi-Disciplinary Conference on Cognitive Methods in Situation Awareness and Decision Support (CogSIMA)},
doi = {10.1109/COGSIMA.2016.7497780},
isbn = {978-1-5090-0632-8},
month = {mar},
pages = {14--20},
publisher = {IEEE},
title = {{A task analysis toward characterizing cyber-cognitive situation awareness (CCSA) in cyber defense analysts}},
url = {http://ieeexplore.ieee.org/document/7497780/},
year = {2016}
}
@inproceedings{McClain2015,
abstract = {Human performance has become a pertinent issue within cyber security. However, this research has been stymied by the limited availability of expert cyber security professionals. This is partly attributable to the ongoing workload faced by cyber security professionals, which is compounded by the limited number of qualified personnel and turnover of personnel across organizations. Additionally, it is difficult to conduct research, and particularly, openly published research, due to the sensitivity inherent to cyber operations at most organizations. As an alternative, the current research has focused on data collection during cyber security training exercises. These events draw individuals with a range of knowledge and experience extending from seasoned professionals to recent college graduates to college students. The current paper describes research involving data collection at two separate cyber security exercises. This data collection involved multiple measures which included behavioral performance based on human-machine transactions and a questionnaire-based assessments of cyber security experience. It was found that participants reporting more experience with cyber security topics and cyber security software tools made greater use of general purpose software tools, combining the use of general purpose tools with specialized cyber security software applications. Given that organizations make substantial investments in cyber security software tools, it is important to recognize that while these tools enable specialized analyses that would not be possible otherwise, they are not sufficient. Instead, effective cyber security operations involve a range of activities that extends from the highly general (e.g., taking notes, Internet search) to domain specific (e.g., disk forensics) and the accompanying work environment should accommodates this range of activities.},
address = {Las Vegas, NV, USA},
author = {McClain, Jonathan and Silva, Austin and Emmanuel, Glory and Anderson, Benjamin and Nauer, Kevin and Abbott, Robert and Forsythe, Chris},
booktitle = {6th International Conference on Applied Human Factors and Ergonomics (AHFE2015)},
doi = {10.1016/j.promfg.2015.07.621},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McClain et al. - 2015 - Human Performance Factors in Cyber Security Forensic Analysis.pdf:pdf},
issn = {23519789},
keywords = {Type your keywords here,separated by semicolons},
pages = {5301--5307},
publisher = {Elsevier},
title = {{Human Performance Factors in Cyber Security Forensic Analysis}},
url = {www.sciencedirect.comwww.elsevier.com/locate/procedia2351-9789},
volume = {3},
year = {2015}
}
@article{Manworren2016,
abstract = {Data breaches are becoming more frequent and more damaging to the bottom line of many businesses. The Target data breach marked the beginning of increased scrutiny of cybersecurity practices. In the past, data breaches were seen as a cost of doing business, but Target's negligence and the scale of the data loss forced businesses and the courts to reevaluate current practices and regulatory frameworks. Businesses must make strategic use of their chief information officers, adopt cybersecurity best practices, and effectively train their employees to respond to growing security threats. They must also shape the cybersecurity narrative to influence regulatory responses to these threats.},
author = {Manworren, Nathan and Letwat, Joshua and Daily, Olivia},
doi = {10.1016/j.bushor.2016.01.002},
file = {::},
issn = {00076813},
journal = {Business Horizons},
keywords = {Best practices,Cyberattack,Cybersecurity,Data,Data breach,Internet,Lawsuit,Malware,Privacy,Security,Target},
number = {3},
pages = {257--266},
title = {{Why you should care about the Target data breach}},
url = {http://dx.doi.org/10.1016/j.bushor.2016.01.002},
volume = {59},
year = {2016}
}
@inproceedings{Qin2007,
abstract = {Social engineering (as used by the military or law-enforcement) is the emerging technique for obtaining classified information by interacting and deceiving people who can access that information. Rather than using traditional techniques of attacking the technical shields such as firewalls, many sophisticated computer hackers find that social engineering is more effective and difficult to detect by humans. Why can people not effectively detect social engineering or more specifically, the art of deception? What can be done to augment human abilities for the task? The current findings warrant several possibilities that influence human ability to detect deception. Factors include such things as truth-bias, stereotypical thinking and processing ability. Knowing that human detection ability is limited, we propose a method to automatically detect deception that potentially assists humans. Results show that a system, using discriminant analysis to classify deception performed significantly better than humans in detecting deception. The findings can also be applied to general situations to ensure information authentication-scenarios other than social engineering. {\textcopyright} 2007 IEEE.},
author = {Qin, Tiantian and Burgoon, Judee K.},
booktitle = {ISI 2007: 2007 IEEE Intelligence and Security Informatics},
doi = {10.1109/isi.2007.379548},
isbn = {1424413303},
keywords = {Automatic deception detection,Social engineering},
pages = {152--159},
title = {{An investigation of heuristics of human judgment in detecting deception and potential implications in countering social engineering}},
year = {2007}
}
@inproceedings{Knott2013,
abstract = {There has been a dramatic increase in the total number of reported cyber security breaches and attacks in recent years. In response, government, and corporate entities have invested billions of dollars in funding research and development efforts for cyber operations, including computer network defense (CND) and computer network attack (CNA). While cyber operations have become an important national priority over the last decade, the Human Factors community has yet to approach it with critical mass. In its purest form, cyber operations are a complex sociotechnical system that can have effects across all levels of an organization. Due to a consistent interplay between human cognition, technology, and organizational constraints within the environment, the Human Factors community is particularly well-suited to address the problem space. We have assembled a panel of six scientists, technologists, and subject matter experts across multiple specializations in the Human Factors community to help begin this increasingly important discussion. The goal of this panel is to have an open discussion on how we can leverage our specializations and expertise to address the cyber operations landscape as a community. Copyright 2013 by Human Factors and Ergonomics Society, Inc.},
author = {Knott, Benjamin A and Mancuso, Vincent F and Bennett, Kevin and Finomore, Victor and McNeese, Michael and McKneely, Jennifer A. and Beecher, Maj Maria},
booktitle = {Proceedings of the Human Factors and Ergonomics Society},
doi = {10.1177/1541931213571086},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Knott et al. - 2013 - Human factors in cyber warfare Alternative perspectives.pdf:pdf},
isbn = {9780945289432},
issn = {10711813},
month = {sep},
number = {1},
pages = {399--403},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Human factors in cyber warfare: Alternative perspectives}},
url = {http://journals.sagepub.com/doi/10.1177/1541931213571086},
volume = {57},
year = {2013}
}
@inproceedings{Roden2020,
address = {Tampa, FL, USA},
author = {Roden, William and Layman, Lucas},
booktitle = {Proceedings of the 2020 ACM Southeast Conference (ACMSE 2020)},
doi = {10.1145/3374135.3385301},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roden, Layman - 2020 - Cry Wolf Toward an Experimentation Platform and Dataset for Human Factors in Cyber Security Analysis.pdf:pdf},
pages = {264--267},
publisher = {ACM},
title = {{Cry Wolf : Toward an Experimentation Platform and Dataset for Human Factors in Cyber Security Analysis}},
url = {https://arxiv.org/abs/2002.10530},
year = {2020}
}
@article{Meyer2001,
abstract = {The effects of a warning's validity and display characteristics on the responses to binary warnings were studied in a categorization task that resembled the control of a simulated production environment. Students performed a visual signal detection task and were aided by a binary warning indicator. Experimental conditions differed in the validity of the warning and its proximity to the judged stimulus. Participants' performance improved over the course of the experiment, and they partly adjusted their responses to the validity of the warnings but continued to respond to nonvalid warnings throughout the experiment. It was particularly difficult to ignore the nonvalid information when it was integrated with the continuous information. There was evidence for nonoptimal use of the information from the warning system, whether it was valid or not valid. The results indicate a possible distinction between two dimensions of users' trust in warning systems: compliance and reliance. Actual or potential implications...},
author = {Meyer, Joachim},
doi = {10.1518/001872001775870395},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Meyer01.pdf:pdf},
issn = {0018-7208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
month = {dec},
number = {4},
pages = {563--572},
title = {{Effects of Warning Validity and Proximity on Responses to Warnings}},
url = {http://journals.sagepub.com/doi/10.1518/001872001775870395},
volume = {43},
year = {2001}
}
@book{Crandall2006,
author = {Crandall, Beth and Klein, Gary A. and Hoffman, Robert R.},
isbn = {9780262296946},
publisher = {MIT Press},
title = {{Working Minds: A Practitioner's Guide to Cognitive Task Analysis}},
url = {https://books.google.com/books?id=ahjvCwAAQBAJ{\&}source=gbs{\_}navlinks{\_}s},
year = {2006}
}
@techreport{IBMSecurityandthePonemonInstitute,
author = {{Ponemon Institute LLC}},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/2018-global-codb-report{\_}06271811{\_}55017055USEN.pdf:pdf},
institution = {Ponemon Institute LLC and IBM Security},
title = {{2018 Cost of Data Breach Study: Global Overview}},
url = {https://www.ibm.com/security/data-breach},
year = {2018}
}
@article{Bedard1992,
author = {B{\'{e}}dard, Jean and Chi, Michelene T.H.},
doi = {10.1111/1467-8721.ep10769799},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Bedard92.pdf:pdf},
issn = {0963-7214},
journal = {Current Directions in Psychological Science},
month = {aug},
number = {4},
pages = {135--139},
title = {{Expertise}},
url = {http://journals.sagepub.com/doi/10.1111/1467-8721.ep10769799},
volume = {1},
year = {1992}
}
@inproceedings{Sharma2020,
abstract = {Humans continue to be considered as the weakest link in securing systems. While there are a variety of sophisticated system attacks, phishing emails continues to be successful in gaining users attention and leading to disastrous security consequences. In designing strategies to protect users from fraudulent phishing emails, system designers need to know which attack approaches and type of content seems to exploit human limitations and vulnerabilities. In this study, we are focusing on the attackers' footprints (emails) and examining the phishing email content and characteristics utilizing publicly available phishing attack repository databases. We analyzed several variables to gain a better understanding of the techniques and language used in these emails to capture users' attention. Our findings reveal that the words primarily used in these emails are targeting users' emotional tendencies and triggers to apply their attacks. In addition, attackers employ user-targeted words and subjects that exploits certain emotional triggers such as fear and anticipation. We believe our human centered study and findings is a critical step forward towards improving detection and training programs to decrease phishing attacks and to promote the inclusion of human factors in securing systems.},
address = {USA},
author = {Sharma, Tanusree and Bashir, Masooda},
booktitle = {Advances in Human Factors in Cybersecurity},
editor = {Corradini, Isabella and Nardelli, Enrico and Ahram, Tareq},
file = {:C$\backslash$:/Users/laymanl/Desktop/cybersecurity/Sharma-Bashir2020{\_}Chapter{\_}AnAnalysisOfPhishingEmailsAndH.pdf:pdf},
isbn = {978-3-030-52581-1},
pages = {49--55},
publisher = {Springer International Publishing},
title = {{An Analysis of Phishing Emails and How the Human Vulnerabilities are Exploited}},
year = {2020}
}
@article{Zhong2018,
abstract = {Data triage is a fundamental stage of cyber defense analysis for achieving cyber situational awareness in a Security Operations Center (SOC). It has a high requirement for cyber security analysts' capabilities of information processing and expertise in cyber defense. However, the present situation is that most novice analysts who are responsible for performing data triage tasks suffer a great deal from the complexity and intensity of their tasks. To fill the gap, we propose to provide novice analysts with on-the-job suggestions by presenting the relevant data triage operations conducted by senior analysts in previous tasks. In a previous study, a tracing method has been developed to track an analyst's data triage operations. This paper mainly presents a data triage operation retrieval system that (1) models the context of a data triage analytic process, (2) uses a centroid similarity matching method to compare contexts, and (3) presents the matched traces to the novice analysts as suggestions. We have implemented and evaluated the performance of the system through both automated testing and human evaluation. The results show that the proposed retrieval system can effectively identify the relevant traces based on an analyst's current analytic process.},
author = {Zhong, Chen and Lin, Tao and Liu, Peng and Yen, John and Chen, Kai},
doi = {10.1016/J.COSE.2018.02.011},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhong et al. - 2018 - A cyber security data triage operation retrieval system.pdf:pdf},
issn = {0167-4048},
journal = {Computers {\&} Security},
month = {jul},
pages = {12--31},
publisher = {Elsevier Advanced Technology},
title = {{A cyber security data triage operation retrieval system}},
url = {https://www.sciencedirect.com/science/article/pii/S0167404818301408},
volume = {76},
year = {2018}
}
@article{Shah2019,
abstract = {Intrusion detection systems (IDSs) analyze data that are collected by sensors, which monitor the network traffic. Any alert generated by the IDS is transmitted to a cybersecurity operations center (CSOC), which performs the important task of analyzing the alerts. In order to deliver a strong security against threats, an efficient CSOC requires the following characteristics: 1) all alerts must be analyzed in a timely manner; 2) there must be an ideal mix of analyst expertise levels in the organization because the quality of analysis performed depends on the mix; and 3) there must be adequate operating budget to hire the required number of analyst personnel. However, it is non-trivial for a CSOC manager to establish the parameter settings for the above characteristics for a desired CSOC efficiency, and current literature lacks a thorough analysis of the tradeoffs between them. This void is filled by this paper whose research objective is to develop an optimized tradeoff study model of the CSOC that studies and quantifies the interactions between the above characteristics, and to use the knowledge gained from the above study to provide the foundation principles to establish and operate an efficient CSOC. A constraint-optimization tradeoff study model is built to drive the decisions that optimize the above characteristics of the CSOC, which is then tested via several simulation runs of the alert arrival and service processes at the CSOC. The paper serves as the first step toward a unified tradeoff study model that integrates the throughput performance, the quality of analysis, and the cost metrics to design and establish an efficient CSOC. Results from the above optimization-simulation tests capture several valuable insights along with parameter settings of the metrics that explain how to operate an efficient CSOC, and quantifies the economic impact of scaling-up the CSOC operation.},
author = {Shah, Ankit and Ganesan, Rajesh and Jajodia, Sushil and Cam, Hasan},
doi = {10.1109/TIFS.2018.2871744},
issn = {15566013},
journal = {IEEE Transactions on Information Forensics and Security},
keywords = {Cybersecurity analysts,analyst expertise levels,budget and cost,constraint-optimization,economics of a CSOC,level of operational effectiveness,simulation,trade-off analysis},
month = {may},
number = {5},
pages = {1155--1170},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Understanding tradeoffs between throughput, quality, and cost of alert analysis in a CSOC}},
volume = {14},
year = {2019}
}
@article{Warm2008,
abstract = {Objective: We describe major discoveries and developments in vigilance research. Background: Vigilance tasks have typically been viewed as undemanding assignments requiring little mental effort. The vigilance decrement function has also been considered to result from a decline in arousal brought about by understimulation. Methods: Recent research in vigilance is reviewed in four areas: studies of task type, perceived mental workload during vigilance, neural measures of resource demand in vigilance, and studies of task-induced stress. Results: Experiments comparing successive and simultaneous vigilance tasks support an attentional resource theory of vigilance. Subjective reports also show that the workload of vigilance is high and sensitive to factors that increase processing demands. Neuroimaging studies using transcranial Doppler sonography provide strong, independent evidence for resource changes linked to performance decrement in vigilance tasks. Finally, physiological and subjective reports confirm th...},
author = {Warm, Joel S. and Parasuraman, Raja and Matthews, Gerald},
doi = {10.1518/001872008X312152},
issn = {0018-7208},
journal = {Human Factors: The Journal of the Human Factors and Ergonomics Society},
month = {jun},
number = {3},
pages = {433--441},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Vigilance Requires Hard Mental Work and Is Stressful}},
url = {http://journals.sagepub.com/doi/10.1518/001872008X312152},
volume = {50},
year = {2008}
}
@article{Campbell2015a,
abstract = {Cybersecurity is a broad and growing job field, encompassing many different job categories with different cognitive demands. Traditional, knowledge-based assessments may exclude candidates who are cognitively suited to performing cybersecurity work but who have not had the opportunity to learn the subject matter. Using the job categories included in the National Initiative for Cybersecurity Education (NICE) framework for the cybersecurity workforce, we propose a model for predicting cybersecurity aptitude beyond a general- intelligence approach. In addition to including general intelligence, the model is based on a classification of jobs as requiring real-time or deliberate performance, and proactive or reactive actions. We suggest that tasks, work roles, and people can be represented along the same set of axes to match job requirements to person attributes. These constructs can then be used to create assessments of potential for cybersecurity applicants, including one we propose, called the Cyber Aptitud...},
author = {Campbell, Susan G. and O'Rourke, Polly and Bunting, Michael F.},
doi = {10.1177/1541931215591170},
file = {:C$\backslash$:/Users/laymanl/Desktop/papers/Campbell2015.pdf:pdf},
issn = {1541-9312},
journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
month = {sep},
number = {1},
pages = {721--725},
publisher = {SAGE PublicationsSage CA: Los Angeles, CA},
title = {{Identifying Dimensions of Cyber Aptitude}},
url = {http://journals.sagepub.com/doi/10.1177/1541931215591170},
volume = {59},
year = {2015}
}
@article{McHugh2000,
abstract = {In 1998 and again in 1999, the Lincoln Laboratory of MIT conducted a comparative evaluation of intrusion detection systems (IDSs) developed under DARPA funding. While this evaluation represents a significant and monumental undertaking, there are a number of issues associated with its design and execution that remain unsettled. Some methodologies used in the evaluation are questionable and may have biased its results. One problem is that the evaluators have published relatively little concerning some of the more critical aspects of their work, such as validation of their test data. The appropriateness of the evaluation techniques used needs further investigation. The purpose of this article is to attempt to identify the shortcomings of the Lincoln Lab effort in the hope that future efforts of this kind will be placed on a sounder footing. Some of the problems that the article points out might well be resolved if the evaluators were to publish a detailed description of their procedures and the rationale that led to their adoption, but other problems would clearly remain. {\textcopyright} 2000, ACM. All rights reserved.},
author = {McHugh, John},
doi = {10.1145/382912.382923},
file = {:C$\backslash$:/Users/laymanl/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/McHugh - 2000 - Testing Intrusion detection systems a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as perf.pdf:pdf},
issn = {10949224},
journal = {ACM Transactions on Information and System Security},
keywords = {Computer security,Security,computer security,intrusion detection,receiver operating curves (ROC),software evaluation},
month = {nov},
number = {4},
pages = {262--294},
publisher = {ACM},
title = {{Testing Intrusion detection systems: a critique of the 1998 and 1999 DARPA intrusion detection system evaluations as performed by Lincoln Laboratory}},
url = {http://portal.acm.org/citation.cfm?doid=382912.382923 https://dl.acm.org/doi/10.1145/382912.382923},
volume = {3},
year = {2000}
}
@article{George2000,
abstract = {Psychologists have identified several limitations to, and biases in, human decision-making processes. One such bias is the anchoring and adjustment effect, which has been demonstrated to be robust both inside and outside the experimental laboratory. Some decision support systems (DSS) have been designed to lessen the effects of decision-making limitations with promising results. This study tested a DSS designed to mitigate the effects of the anchoring and adjustment bias. The results show that anchoring and adjustment remains robust within the context of automated decision support. Implications that follow these results are offered.},
author = {George, Joey F. and Duffy, Kevin and Ahuja, Manju},
doi = {10.1016/S0167-9236(00)00074-9},
issn = {01679236},
journal = {Decision Support Systems},
month = {aug},
number = {2},
pages = {195--206},
publisher = {Elsevier Science Publishers B.V.},
title = {{Countering the anchoring and adjustment bias with decision support systems}},
volume = {29},
year = {2000}
}
